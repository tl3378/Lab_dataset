Title,Authors,Publish Date,Topic,Abstract,Citation
"Harnessing Retrieval-Augmented Generation (RAG) for Uncovering Knowledge
  Gaps",Joan Figuerola Hurtado,2023-12-12T23:22:57Z,RAG,"  The paper presents a methodology for uncovering knowledge gaps on the
internet using the Retrieval Augmented Generation (RAG) model. By simulating
user search behaviour, the RAG system identifies and addresses gaps in
information retrieval systems. The study demonstrates the effectiveness of the
RAG system in generating relevant suggestions with a consistent accuracy of
93%. The methodology can be applied in various fields such as scientific
discovery, educational enhancement, research development, market analysis,
search engine optimisation, and content development. The results highlight the
value of identifying and understanding knowledge gaps to guide future
endeavours.
",0
"RAGGED: Towards Informed Design of Retrieval Augmented Generation
  Systems","Jennifer Hsia, Afreen Shaikh, Zhiruo Wang, Graham Neubig",2024-03-14T02:26:31Z,"Retrieval Augmented Generation, RAG","  Retrieval-augmented generation (RAG) can significantly improve the
performance of language models (LMs) by providing additional context for tasks
such as document-based question answering (DBQA). However, the effectiveness of
RAG is highly dependent on its configuration. To systematically find the
optimal configuration, we introduce RAGGED, a framework for analyzing RAG
configurations across various DBQA tasks. Using the framework, we discover
distinct LM behaviors in response to varying context quantities, context
qualities, and retrievers. For instance, while some models are robust to noisy
contexts, monotonically performing better with more contexts, others are more
noise-sensitive and can effectively use only a few contexts before declining in
performance. This framework also provides a deeper analysis of these
differences by evaluating the LMs' sensitivity to signal and noise under
specific context quality conditions. Using RAGGED, researchers and
practitioners can derive actionable insights about how to optimally configure
their RAG systems for their specific question-answering tasks.
",0
"Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy
  with Semantic Search and Hybrid Query-Based Retrievers","Kunal Sawarkar, Abhilasha Mangal, Shivam Raj Solanki",2024-03-22T17:13:46Z,RAG,"  Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a
private knowledge base of documents with Large Language Models (LLM) to build
Generative Q\&A (Question-Answering) systems. However, RAG accuracy becomes
increasingly challenging as the corpus of documents scales up, with Retrievers
playing an outsized role in the overall RAG accuracy by extracting the most
relevant document from the corpus to provide context to the LLM. In this paper,
we propose the 'Blended RAG' method of leveraging semantic search techniques,
such as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid
query strategies. Our study achieves better retrieval results and sets new
benchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID
datasets. We further extend such a 'Blended Retriever' to the RAG system to
demonstrate far superior results on Generative Q\&A datasets like SQUAD, even
surpassing fine-tuning performance.
",7
"Improving the Domain Adaptation of Retrieval Augmented Generation (RAG)
  Models for Open Domain Question Answering","Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kaluarachchi, Rajib Rana, Suranga Nanayakkara",2022-10-06T01:21:25Z,"Retrieval Augmented Generation, RAG","  Retrieval Augment Generation (RAG) is a recent advancement in Open-Domain
Question Answering (ODQA). RAG has only been trained and explored with a
Wikipedia-based external knowledge base and is not optimized for use in other
specialized domains such as healthcare and news. In this paper, we evaluate the
impact of joint training of the retriever and generator components of RAG for
the task of domain adaptation in ODQA. We propose \textit{RAG-end2end}, an
extension to RAG, that can adapt to a domain-specific knowledge base by
updating all components of the external knowledge base during training. In
addition, we introduce an auxiliary training signal to inject more
domain-specific knowledge. This auxiliary signal forces \textit{RAG-end2end} to
reconstruct a given sentence by accessing the relevant information from the
external knowledge base. Our novel contribution is unlike RAG, RAG-end2end does
joint training of the retriever and generator for the end QA task and domain
adaptation. We evaluate our approach with datasets from three domains:
COVID-19, News, and Conversations, and achieve significant performance
improvements compared to the original RAG model. Our work has been open-sourced
through the Huggingface Transformers library, attesting to our work's
credibility and technical consistency.
",0
"The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented
  Generation (RAG)","Shenglai Zeng, Jiankun Zhang, Pengfei He, Yue Xing, Yiding Liu, Han Xu, Jie Ren, Shuaiqiang Wang, Dawei Yin, Yi Chang, Jiliang Tang",2024-02-23T18:35:15Z,RAG,"  Retrieval-augmented generation (RAG) is a powerful technique to facilitate
language model with proprietary and private data, where data privacy is a
pivotal concern. Whereas extensive research has demonstrated the privacy risks
of large language models (LLMs), the RAG technique could potentially reshape
the inherent behaviors of LLM generation, posing new privacy issues that are
currently under-explored. In this work, we conduct extensive empirical studies
with novel attack methods, which demonstrate the vulnerability of RAG systems
on leaking the private retrieval database. Despite the new risk brought by RAG
on the retrieval data, we further reveal that RAG can mitigate the leakage of
the LLMs' training data. Overall, we provide new insights in this paper for
privacy protection of retrieval-augmented LLMs, which benefit both LLMs and RAG
systems builders. Our code is available at
https://github.com/phycholosogy/RAG-privacy.
",0
"Evaluation of Semantic Search and its Role in
  Retrieved-Augmented-Generation (RAG) for Arabic Language","Ali Mahboub, Muhy Eddin Za'ter, Bashar Al-Rfooh, Yazan Estaitia, Adnan Jaljuli, Asma Hakouz",2024-03-27T08:42:31Z,RAG,"  The latest advancements in machine learning and deep learning have brought
forth the concept of semantic similarity, which has proven immensely beneficial
in multiple applications and has largely replaced keyword search. However,
evaluating semantic similarity and conducting searches for a specific query
across various documents continue to be a complicated task. This complexity is
due to the multifaceted nature of the task, the lack of standard benchmarks,
whereas these challenges are further amplified for Arabic language. This paper
endeavors to establish a straightforward yet potent benchmark for semantic
search in Arabic. Moreover, to precisely evaluate the effectiveness of these
metrics and the dataset, we conduct our assessment of semantic search within
the framework of retrieval augmented generation (RAG).
",0
"Dynamic Retrieval Augmented Generation of Ontologies using Artificial
  Intelligence (DRAGON-AI)","Sabrina Toro, Anna V Anagnostopoulos, Sue Bello, Kai Blumberg, Rhiannon Cameron, Leigh Carmody, Alexander D Diehl, Damion Dooley, William Duncan, Petra Fey, Pascale Gaudet, Nomi L Harris, Marcin Joachimiak, Leila Kiani, Tiago Lubiana, Monica C Munoz-Torres, Shawn O'Neil, David Osumi-Sutherland, Aleix Puig, Justin P Reese, Leonore Reiser, Sofia Robb, Troy Ruemping, James Seager, Eric Sid, Ray Stefancsik, Magalie Weber, Valerie Wood, Melissa A Haendel, Christopher J Mungall",2023-12-18T03:19:31Z,"Retrieval Augmented Generation, RAG","  Background: Ontologies are fundamental components of informatics
infrastructure in domains such as biomedical, environmental, and food sciences,
representing consensus knowledge in an accurate and computable form. However,
their construction and maintenance demand substantial resources and necessitate
substantial collaboration between domain experts, curators, and ontology
experts. We present Dynamic Retrieval Augmented Generation of Ontologies using
AI (DRAGON-AI), an ontology generation method employing Large Language Models
(LLMs) and Retrieval Augmented Generation (RAG). DRAGON-AI can generate textual
and logical ontology components, drawing from existing knowledge in multiple
ontologies and unstructured text sources.
  Results: We assessed performance of DRAGON-AI on de novo term construction
across ten diverse ontologies, making use of extensive manual evaluation of
results. Our method has high precision for relationship generation, but has
slightly lower precision than from logic-based reasoning. Our method is also
able to generate definitions deemed acceptable by expert evaluators, but these
scored worse than human-authored definitions. Notably, evaluators with the
highest level of confidence in a domain were better able to discern flaws in
AI-generated definitions. We also demonstrated the ability of DRAGON-AI to
incorporate natural language instructions in the form of GitHub issues.
  Conclusions: These findings suggest DRAGON-AI's potential to substantially
aid the manual ontology construction process. However, our results also
underscore the importance of having expert curators and ontology editors drive
the ontology generation process.
",0
"SoK: Adversarial Machine Learning Attacks and Defences in Multi-Agent
  Reinforcement Learning","Maxwell Standen, Junae Kim, Claudia Szabo",2023-01-11T04:25:00Z,"Reinforcement Learning, Adversarial Machine Learning","  Multi-Agent Reinforcement Learning (MARL) is vulnerable to Adversarial
Machine Learning (AML) attacks and needs adequate defences before it can be
used in real world applications. We have conducted a survey into the use of
execution-time AML attacks against MARL and the defences against those attacks.
We surveyed related work in the application of AML in Deep Reinforcement
Learning (DRL) and Multi-Agent Learning (MAL) to inform our analysis of AML for
MARL. We propose a novel perspective to understand the manner of perpetrating
an AML attack, by defining Attack Vectors. We develop two new frameworks to
address a gap in current modelling frameworks, focusing on the means and tempo
of an AML attack against MARL, and identify knowledge gaps and future avenues
of research.
",3
"Explainable AI is Responsible AI: How Explainability Creates Trustworthy
  and Socially Responsible Artificial Intelligence","Stephanie Baker, Wei Xiang",2023-12-04T00:54:04Z,"Responsible AI, Explainable AI","  Artificial intelligence (AI) has been clearly established as a technology
with the potential to revolutionize fields from healthcare to finance - if
developed and deployed responsibly. This is the topic of responsible AI, which
emphasizes the need to develop trustworthy AI systems that minimize bias,
protect privacy, support security, and enhance transparency and accountability.
Explainable AI (XAI) has been broadly considered as a building block for
responsible AI (RAI), with most of the literature considering it as a solution
for improved transparency. This work proposes that XAI and responsible AI are
significantly more deeply entwined. In this work, we explore state-of-the-art
literature on RAI and XAI technologies. Based on our findings, we demonstrate
that XAI can be utilized to ensure fairness, robustness, privacy, security, and
transparency in a wide range of contexts. Our findings lead us to conclude that
XAI is an essential foundation for every pillar of RAI.
",2
On the Multiple Roles of Ontologies in Explainable AI,"Roberto Confalonieri, Giancarlo Guizzardi",2023-11-08T15:57:26Z,Explainable AI,"  This paper discusses the different roles that explicit knowledge, in
particular ontologies, can play in Explainable AI and in the development of
human-centric explainable systems and intelligible explanations. We consider
three main perspectives in which ontologies can contribute significantly,
namely reference modelling, common-sense reasoning, and knowledge refinement
and complexity management. We overview some of the existing approaches in the
literature, and we position them according to these three proposed
perspectives. The paper concludes by discussing what challenges still need to
be addressed to enable ontology-based approaches to explanation and to evaluate
their human-understandability and effectiveness.
",0
Active Retrieval Augmented Generation,"Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig",2023-05-11T17:13:40Z,Retrieval Augmented Generation,"  Despite the remarkable ability of large language models (LMs) to comprehend
and generate language, they have a tendency to hallucinate and create factually
inaccurate output. Augmenting LMs by retrieving information from external
knowledge resources is one promising solution. Most existing retrieval
augmented LMs employ a retrieve-and-generate setup that only retrieves
information once based on the input. This is limiting, however, in more general
scenarios involving generation of long texts, where continually gathering
information throughout generation is essential. In this work, we provide a
generalized view of active retrieval augmented generation, methods that
actively decide when and what to retrieve across the course of the generation.
We propose Forward-Looking Active REtrieval augmented generation (FLARE), a
generic method which iteratively uses a prediction of the upcoming sentence to
anticipate future content, which is then utilized as a query to retrieve
relevant documents to regenerate the sentence if it contains low-confidence
tokens. We test FLARE along with baselines comprehensively over 4 long-form
knowledge-intensive generation tasks/datasets. FLARE achieves superior or
competitive performance on all tasks, demonstrating the effectiveness of our
method. Code and datasets are available at https://github.com/jzbjyb/FLARE.
",0
Dynamic Retrieval-Augmented Generation,"Anton Shapkin, Denis Litvinov, Yaroslav Zharov, Egor Bogomolov, Timur Galimzyanov, Timofey Bryksin",2023-12-14T14:26:57Z,Other,"  Current state-of-the-art large language models are effective in generating
high-quality text and encapsulating a broad spectrum of world knowledge. These
models, however, often hallucinate and lack locally relevant factual data.
Retrieval-augmented approaches were introduced to overcome these problems and
provide more accurate responses. Typically, the retrieved information is simply
appended to the main request, restricting the context window size of the model.
We propose a novel approach for the Dynamic Retrieval-Augmented Generation
(DRAG), based on the entity-augmented generation, which injects compressed
embeddings of the retrieved entities into the generative model. The proposed
pipeline was developed for code-generation tasks, yet can be transferred to
some domains of natural language processing. To train the model, we collect and
publish a new project-level code generation dataset. We use it for the
evaluation along with publicly available datasets. Our approach achieves
several targets: (1) lifting the length limitations of the context window,
saving on the prompt size; (2) allowing huge expansion of the number of
retrieval entities available for the context; (3) alleviating the problem of
misspelling or failing to find relevant entity names. This allows the model to
beat all baselines (except GPT-3.5) with a strong margin.
",0
Corrective Retrieval Augmented Generation,"Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, Zhen-Hua Ling",2024-01-29T04:36:39Z,Retrieval Augmented Generation,"  Large language models (LLMs) inevitably exhibit hallucinations since the
accuracy of generated texts cannot be secured solely by the parametric
knowledge they encapsulate. Although retrieval-augmented generation (RAG) is a
practicable complement to LLMs, it relies heavily on the relevance of retrieved
documents, raising concerns about how the model behaves if retrieval goes
wrong. To this end, we propose the Corrective Retrieval Augmented Generation
(CRAG) to improve the robustness of generation. Specifically, a lightweight
retrieval evaluator is designed to assess the overall quality of retrieved
documents for a query, returning a confidence degree based on which different
knowledge retrieval actions can be triggered. Since retrieval from static and
limited corpora can only return sub-optimal documents, large-scale web searches
are utilized as an extension for augmenting the retrieval results. Besides, a
decompose-then-recompose algorithm is designed for retrieved documents to
selectively focus on key information and filter out irrelevant information in
them. CRAG is plug-and-play and can be seamlessly coupled with various
RAG-based approaches. Experiments on four datasets covering short- and
long-form generation tasks show that CRAG can significantly improve the
performance of RAG-based approaches.
",29
"Collective eXplainable AI: Explaining Cooperative Strategies and Agent
  Contribution in Multiagent Reinforcement Learning with Shapley Values","Alexandre Heuillet, Fabien Couthouis, Natalia Díaz-Rodríguez",2021-10-04T10:28:57Z,"Reinforcement Learning, Explainable AI","  While Explainable Artificial Intelligence (XAI) is increasingly expanding
more areas of application, little has been applied to make deep Reinforcement
Learning (RL) more comprehensible. As RL becomes ubiquitous and used in
critical and general public applications, it is essential to develop methods
that make it better understood and more interpretable. This study proposes a
novel approach to explain cooperative strategies in multiagent RL using Shapley
values, a game theory concept used in XAI that successfully explains the
rationale behind decisions taken by Machine Learning algorithms. Through
testing common assumptions of this technique in two cooperation-centered
socially challenging multi-agent environments environments, this article argues
that Shapley values are a pertinent way to evaluate the contribution of players
in a cooperative multi-agent RL context. To palliate the high overhead of this
method, Shapley values are approximated using Monte Carlo sampling.
Experimental results on Multiagent Particle and Sequential Social Dilemmas show
that Shapley values succeed at estimating the contribution of each agent. These
results could have implications that go beyond games in economics, (e.g., for
non-discriminatory decision making, ethical and responsible AI-derived
decisions or policy making under fairness constraints). They also expose how
Shapley values only give general explanations about a model and cannot explain
a single run, episode nor justify precise actions taken by agents. Future work
should focus on addressing these critical aspects.
",44
Reinforcement Learning for Optimizing RAG for Domain Chatbots,"Mandar Kulkarni, Praveen Tangarajan, Kyung Kim, Anusua Trivedi",2024-01-10T02:57:20Z,"RAG, Reinforcement Learning","  With the advent of Large Language Models (LLM), conversational assistants
have become prevalent for domain use cases. LLMs acquire the ability to
contextual question answering through training, and Retrieval Augmented
Generation (RAG) further enables the bot to answer domain-specific questions.
This paper describes a RAG-based approach for building a chatbot that answers
user's queries using Frequently Asked Questions (FAQ) data. We train an
in-house retrieval embedding model using infoNCE loss, and experimental results
demonstrate that the in-house model works significantly better than the
well-known general-purpose public embedding model, both in terms of retrieval
accuracy and Out-of-Domain (OOD) query detection. As an LLM, we use an open
API-based paid ChatGPT model. We noticed that a previously retrieved-context
could be used to generate an answer for specific patterns/sequences of queries
(e.g., follow-up queries). Hence, there is a scope to optimize the number of
LLM tokens and cost. Assuming a fixed retrieval model and an LLM, we optimize
the number of LLM tokens using Reinforcement Learning (RL). Specifically, we
propose a policy-based model external to the RAG, which interacts with the RAG
pipeline through policy actions and updates the policy to optimize the cost.
The policy model can perform two actions: to fetch FAQ context or skip
retrieval. We use the open API-based GPT-4 as the reward model. We then train a
policy model using policy gradient on multiple training chat sessions. As a
policy model, we experimented with a public gpt-2 model and an in-house BERT
model. With the proposed RL-based optimization combined with similarity
threshold, we are able to achieve significant cost savings while getting a
slightly improved accuracy. Though we demonstrate results for the FAQ chatbot,
the proposed RL approach is generic and can be experimented with any existing
RAG pipeline.
",0
"What does it mean to be a responsible AI practitioner: An ontology of
  roles and skills","Shalaleh Rismani, AJung Moon",2022-05-08T20:25:13Z,"Responsible AI, Ontology","  With the growing need to regulate AI systems across a wide variety of
application domains, a new set of occupations has emerged in the industry. The
so-called responsible AI practitioners or AI ethicists are generally tasked
with interpreting and operationalizing best practices for ethical and safe
design of AI systems. Due to the nascent nature of these roles, however, it is
unclear to future employers and aspiring AI ethicists what specific function
these roles serve and what skills are necessary to serve the functions. Without
clarity on these, we cannot train future AI ethicists with meaningful learning
objectives.
  In this work, we examine what responsible AI practitioners do in the industry
and what skills they employ on the job. We propose an ontology of existing
roles alongside skills and competencies that serve each role. We created this
ontology by examining the job postings for such roles over a two-year period
(2020-2022) and conducting expert interviews with fourteen individuals who
currently hold such a role in the industry. Our ontology contributes to
business leaders looking to build responsible AI teams and provides educators
with a set of competencies that an AI ethics curriculum can prioritize.
",0
"Neurosymbolic AI -- Why, What, and How","Amit Sheth, Kaushik Roy, Manas Gaur",2023-05-01T13:27:22Z,Neurosymbolic AI,"  Humans interact with the environment using a combination of perception -
transforming sensory inputs from their environment into symbols, and cognition
- mapping symbols to knowledge about the environment for supporting
abstraction, reasoning by analogy, and long-term planning. Human
perception-inspired machine perception, in the context of AI, refers to
large-scale pattern recognition from raw data using neural networks trained
using self-supervised learning objectives such as next-word prediction or
object recognition. On the other hand, machine cognition encompasses more
complex computations, such as using knowledge of the environment to guide
reasoning, analogy, and long-term planning. Humans can also control and explain
their cognitive functions. This seems to require the retention of symbolic
mappings from perception outputs to knowledge about their environment. For
example, humans can follow and explain the guidelines and safety constraints
driving their decision-making in safety-critical applications such as
healthcare, criminal justice, and autonomous driving. This article introduces
the rapidly emerging paradigm of Neurosymbolic AI combines neural networks and
knowledge-guided symbolic approaches to create more capable and flexible AI
systems. These systems have immense potential to advance both algorithm-level
(e.g., abstraction, analogy, reasoning) and application-level (e.g.,
explainable and safety-constrained decision-making) capabilities of AI systems.
",0
Neurosymbolic AI and its Taxonomy: a survey,"Wandemberg Gibaut, Leonardo Pereira, Fabio Grassiotto, Alexandre Osorio, Eder Gadioli, Amparo Munoz, Sildolfo Gomes, Claudio dos Santos",2023-05-12T19:51:13Z,Neurosymbolic AI,"  Neurosymbolic AI deals with models that combine symbolic processing, like
classic AI, and neural networks, as it's a very established area. These models
are emerging as an effort toward Artificial General Intelligence (AGI) by both
exploring an alternative to just increasing datasets' and models' sizes and
combining Learning over the data distribution, Reasoning on prior and learned
knowledge, and by symbiotically using them. This survey investigates research
papers in this area during recent years and brings classification and
comparison between the presented models as well as applications.
",0
Retrieval Augmented Generation using Engineering Design Knowledge,"L. Siddharth, Jianxi Luo",2023-07-13T17:25:28Z,Retrieval Augmented Generation,"  Aiming to support Retrieval Augmented Generation (RAG) in the design process,
we present a method to identify explicit, engineering design facts - {head
entity :: relationship :: tail entity} from patented artefact descriptions.
Given a sentence with a pair of entities (based on noun phrases) marked in a
unique manner, our method extracts the relationship that is explicitly
communicated in the sentence. For this task, we create a dataset of 375,084
examples and fine-tune language models for relation identification (token
classification) and elicitation (sequence-to-sequence). The token
classification approach achieves up to 99.7 % accuracy. Upon applying the
method to a domain of 4,870 fan system patents, we populate a knowledge base of
over 2.93 million facts. Using this knowledge base, we demonstrate how Large
Language Models (LLMs) are guided by explicit facts to synthesise knowledge and
generate technical and cohesive responses when sought out for knowledge
retrieval tasks in the design process.
",0
RAGAS: Automated Evaluation of Retrieval Augmented Generation,"Shahul Es, Jithin James, Luis Espinosa-Anke, Steven Schockaert",2023-09-26T19:23:54Z,"Retrieval Augmented Generation, RAG","  We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework
for reference-free evaluation of Retrieval Augmented Generation (RAG)
pipelines. RAG systems are composed of a retrieval and an LLM based generation
module, and provide LLMs with knowledge from a reference textual database,
which enables them to act as a natural language layer between a user and
textual databases, reducing the risk of hallucinations. Evaluating RAG
architectures is, however, challenging because there are several dimensions to
consider: the ability of the retrieval system to identify relevant and focused
context passages, the ability of the LLM to exploit such passages in a faithful
way, or the quality of the generation itself. With RAGAs, we put forward a
suite of metrics which can be used to evaluate these different dimensions
\textit{without having to rely on ground truth human annotations}. We posit
that such a framework can crucially contribute to faster evaluation cycles of
RAG architectures, which is especially important given the fast adoption of
LLMs.
",0
Context Tuning for Retrieval Augmented Generation,"Raviteja Anantha, Tharun Bethi, Danil Vodianik, Srinivas Chappidi",2023-12-09T23:33:16Z,Retrieval Augmented Generation,"  Large language models (LLMs) have the remarkable ability to solve new tasks
with just a few examples, but they need access to the right tools. Retrieval
Augmented Generation (RAG) addresses this problem by retrieving a list of
relevant tools for a given task. However, RAG's tool retrieval step requires
all the required information to be explicitly present in the query. This is a
limitation, as semantic search, the widely adopted tool retrieval method, can
fail when the query is incomplete or lacks context. To address this limitation,
we propose Context Tuning for RAG, which employs a smart context retrieval
system to fetch relevant information that improves both tool retrieval and plan
generation. Our lightweight context retrieval model uses numerical,
categorical, and habitual usage signals to retrieve and rank context items. Our
empirical results demonstrate that context tuning significantly enhances
semantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for
context retrieval and tool retrieval tasks respectively, and resulting in an
11.6% increase in LLM-based planner accuracy. Additionally, we show that our
proposed lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART
outperforms GPT-4 based retrieval. Moreover, we observe context augmentation at
plan generation, even after tool retrieval, reduces hallucination.
",0
Benchmarking Retrieval-Augmented Generation for Medicine,"Guangzhi Xiong, Qiao Jin, Zhiyong Lu, Aidong Zhang",2024-02-20T17:44:06Z,Other,"  While large language models (LLMs) have achieved state-of-the-art performance
on a wide range of medical question answering (QA) tasks, they still face
challenges with hallucinations and outdated knowledge. Retrieval-augmented
generation (RAG) is a promising solution and has been widely adopted. However,
a RAG system can involve multiple flexible components, and there is a lack of
best practices regarding the optimal RAG setting for various medical purposes.
To systematically evaluate such systems, we propose the Medical Information
Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind
benchmark including 7,663 questions from five medical QA datasets. Using
MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt
tokens on 41 combinations of different corpora, retrievers, and backbone LLMs
through the MedRAG toolkit introduced in this work. Overall, MedRAG improves
the accuracy of six different LLMs by up to 18% over chain-of-thought
prompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4-level. Our
results show that the combination of various medical corpora and retrievers
achieves the best performance. In addition, we discovered a log-linear scaling
property and the ""lost-in-the-middle"" effects in medical RAG. We believe our
comprehensive evaluations can serve as practical guidelines for implementing
RAG systems for medicine.
",0
Federated Recommendation via Hybrid Retrieval Augmented Generation,"Huimin Zeng, Zhenrui Yue, Qian Jiang, Dong Wang",2024-03-07T06:38:41Z,Retrieval Augmented Generation,"  Federated Recommendation (FR) emerges as a novel paradigm that enables
privacy-preserving recommendations. However, traditional FR systems usually
represent users/items with discrete identities (IDs), suffering from
performance degradation due to the data sparsity and heterogeneity in FR. On
the other hand, Large Language Models (LLMs) as recommenders have proven
effective across various recommendation scenarios. Yet, LLM-based recommenders
encounter challenges such as low inference efficiency and potential
hallucination, compromising their performance in real-world scenarios. To this
end, we propose GPT-FedRec, a federated recommendation framework leveraging
ChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism.
GPT-FedRec is a two-stage solution. The first stage is a hybrid retrieval
process, mining ID-based user patterns and text-based item features. Next, the
retrieved results are converted into text prompts and fed into GPT for
re-ranking. Our proposed hybrid retrieval mechanism and LLM-based re-rank aims
to extract generalized features from data and exploit pretrained knowledge
within LLM, overcoming data sparsity and heterogeneity in FR. In addition, the
RAG approach also prevents LLM hallucination, improving the recommendation
performance for real-world users. Experimental results on diverse benchmark
datasets demonstrate the superior performance of GPT-FedRec against
state-of-the-art baseline methods.
",0
Loops On Retrieval Augmented Generation (LoRAG),"Ayush Thakur, Rashmi Vashisth",2024-03-18T15:19:17Z,"Retrieval Augmented Generation, RAG","  This paper presents Loops On Retrieval Augmented Generation (LoRAG), a new
framework designed to enhance the quality of retrieval-augmented text
generation through the incorporation of an iterative loop mechanism. The
architecture integrates a generative model, a retrieval mechanism, and a
dynamic loop module, allowing for iterative refinement of the generated text
through interactions with relevant information retrieved from the input
context. Experimental evaluations on benchmark datasets demonstrate that LoRAG
surpasses existing state-of-the-art models in terms of BLEU score, ROUGE score,
and perplexity, showcasing its effectiveness in achieving both coherence and
relevance in generated text. The qualitative assessment further illustrates
LoRAG's capability to produce contextually rich and coherent outputs. This
research contributes valuable insights into the potential of iterative loops in
mitigating challenges in text generation, positioning LoRAG as a promising
advancement in the field.
",0
CPR: Retrieval Augmented Generation for Copyright Protection,"Aditya Golatkar, Alessandro Achille, Luca Zancato, Yu-Xiang Wang, Ashwin Swaminathan, Stefano Soatto",2024-03-27T18:09:55Z,Retrieval Augmented Generation,"  Retrieval Augmented Generation (RAG) is emerging as a flexible and robust
technique to adapt models to private users data without training, to handle
credit attribution, and to allow efficient machine unlearning at scale.
However, RAG techniques for image generation may lead to parts of the retrieved
samples being copied in the model's output. To reduce risks of leaking private
information contained in the retrieved set, we introduce Copy-Protected
generation with Retrieval (CPR), a new method for RAG with strong copyright
protection guarantees in a mixed-private setting for diffusion models.CPR
allows to condition the output of diffusion models on a set of retrieved
images, while also guaranteeing that unique identifiable information about
those example is not exposed in the generated outputs. In particular, it does
so by sampling from a mixture of public (safe) distribution and private (user)
distribution by merging their diffusion scores at inference. We prove that CPR
satisfies Near Access Freeness (NAF) which bounds the amount of information an
attacker may be able to extract from the generated images. We provide two
algorithms for copyright protection, CPR-KL and CPR-Choose. Unlike previously
proposed rejection-sampling-based NAF methods, our methods enable efficient
copyright-protected sampling with a single run of backward diffusion. We show
that our method can be applied to any pre-trained conditional diffusion model,
such as Stable Diffusion or unCLIP. In particular, we empirically show that
applying CPR on top of unCLIP improves quality and text-to-image alignment of
the generated results (81.4 to 83.17 on TIFA benchmark), while enabling credit
attribution, copy-right protection, and deterministic, constant time,
unlearning.
",0
Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey,"Lauren Nicole DeLong, Ramon Fernández Mir, Jacques D. Fleuriot",2023-02-14T17:24:30Z,Neurosymbolic AI,"  Neurosymbolic AI is an increasingly active area of research that combines
symbolic reasoning methods with deep learning to leverage their complementary
benefits. As knowledge graphs are becoming a popular way to represent
heterogeneous and multi-relational data, methods for reasoning on graph
structures have attempted to follow this neurosymbolic paradigm. Traditionally,
such approaches have utilized either rule-based inference or generated
representative numerical embeddings from which patterns could be extracted.
However, several recent studies have attempted to bridge this dichotomy to
generate models that facilitate interpretability, maintain competitive
performance, and integrate expert knowledge. Therefore, we survey methods that
perform neurosymbolic reasoning tasks on knowledge graphs and propose a novel
taxonomy by which we can classify them. Specifically, we propose three major
categories: (1) logically-informed embedding approaches, (2) embedding
approaches with logical constraints, and (3) rule learning approaches.
Alongside the taxonomy, we provide a tabular overview of the approaches and
links to their source code, if available, for more direct comparison. Finally,
we discuss the unique characteristics and limitations of these methods, then
propose several prospective directions toward which this field of research
could evolve.
",0
Neurosymbolic AI for Reasoning on Biomedical Knowledge Graphs,"Lauren Nicole DeLong, Ramon Fernández Mir, Zonglin Ji, Fiona Niamh Coulter Smith, Jacques D. Fleuriot",2023-07-17T11:47:05Z,Neurosymbolic AI,"  Biomedical datasets are often modeled as knowledge graphs (KGs) because they
capture the multi-relational, heterogeneous, and dynamic natures of biomedical
systems. KG completion (KGC), can, therefore, help researchers make predictions
to inform tasks like drug repositioning. While previous approaches for KGC were
either rule-based or embedding-based, hybrid approaches based on neurosymbolic
artificial intelligence are becoming more popular. Many of these methods
possess unique characteristics which make them even better suited toward
biomedical challenges. Here, we survey such approaches with an emphasis on
their utilities and prospective benefits for biomedicine.
",0
"Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability,
  Explainability, and Safety","Manas Gaur, Amit Sheth",2023-12-05T06:13:55Z,Neurosymbolic AI,"  Explainability and Safety engender Trust. These require a model to exhibit
consistency and reliability. To achieve these, it is necessary to use and
analyze data and knowledge with statistical and symbolic AI methods relevant to
the AI application - neither alone will do. Consequently, we argue and seek to
demonstrate that the NeuroSymbolic AI approach is better suited for making AI a
trusted AI system. We present the CREST framework that shows how Consistency,
Reliability, user-level Explainability, and Safety are built on NeuroSymbolic
methods that use data and knowledge to support requirements for critical
applications such as health and well-being. This article focuses on Large
Language Models (LLMs) as the chosen AI system within the CREST framework. LLMs
have garnered substantial attention from researchers due to their versatility
in handling a broad array of natural language processing (NLP) scenarios. For
example, ChatGPT and Google's MedPaLM have emerged as highly promising
platforms for providing information in general and health-related queries,
respectively. Nevertheless, these models remain black boxes despite
incorporating human feedback and instruction-guided tuning. For instance,
ChatGPT can generate unsafe responses despite instituting safety guardrails.
CREST presents a plausible approach harnessing procedural and graph-based
knowledge within a NeuroSymbolic framework to shed light on the challenges
associated with LLMs.
",0
Robust Retrieval Augmented Generation for Zero-shot Slot Filling,"Michael Glass, Gaetano Rossiello, Md Faisal Mahbub Chowdhury, Alfio Gliozzo",2021-08-31T15:51:27Z,Retrieval Augmented Generation,"  Automatically inducing high quality knowledge graphs from a given collection
of documents still remains a challenging problem in AI. One way to make headway
for this problem is through advancements in a related task known as slot
filling. In this task, given an entity query in form of [Entity, Slot, ?], a
system is asked to fill the slot by generating or extracting the missing value
exploiting evidence extracted from relevant passage(s) in the given document
collection. The recent works in the field try to solve this task in an
end-to-end fashion using retrieval-based language models. In this paper, we
present a novel approach to zero-shot slot filling that extends dense passage
retrieval with hard negatives and robust training procedures for retrieval
augmented generation models. Our model reports large improvements on both T-REx
and zsRE slot filling datasets, improving both passage retrieval and slot value
generation, and ranking at the top-1 position in the KILT leaderboard.
Moreover, we demonstrate the robustness of our system showing its domain
adaptation capability on a new variant of the TACRED dataset for slot filling,
through a combination of zero/few-shot learning. We release the source code and
pre-trained models.
",31
End-to-End Table Question Answering via Retrieval-Augmented Generation,"Feifei Pan, Mustafa Canim, Michael Glass, Alfio Gliozzo, James Hendler",2022-03-30T23:30:16Z,Other,"  Most existing end-to-end Table Question Answering (Table QA) models consist
of a two-stage framework with a retriever to select relevant table candidates
from a corpus and a reader to locate the correct answers from table candidates.
Even though the accuracy of the reader models is significantly improved with
the recent transformer-based approaches, the overall performance of such
frameworks still suffers from the poor accuracy of using traditional
information retrieval techniques as retrievers. To alleviate this problem, we
introduce T-RAG, an end-to-end Table QA model, where a non-parametric dense
vector index is fine-tuned jointly with BART, a parametric sequence-to-sequence
model to generate answer tokens. Given any natural language question, T-RAG
utilizes a unified pipeline to automatically search through a table corpus to
directly locate the correct answer from the table cells. We apply T-RAG to
recent open-domain Table QA benchmarks and demonstrate that the fine-tuned
T-RAG model is able to achieve state-of-the-art performance in both the
end-to-end Table QA and the table retrieval tasks.
",0
"Retrieval-Augmented Generative Question Answering for Event Argument
  Extraction","Xinya Du, Heng Ji",2022-11-14T02:00:32Z,Other,"  Event argument extraction has long been studied as a sequential prediction
problem with extractive-based methods, tackling each argument in isolation.
Although recent work proposes generation-based methods to capture
cross-argument dependency, they require generating and post-processing a
complicated target sequence (template). Motivated by these observations and
recent pretrained language models' capabilities of learning from
demonstrations. We propose a retrieval-augmented generative QA model (R-GQA)
for event argument extraction. It retrieves the most similar QA pair and
augments it as prompt to the current example's context, then decodes the
arguments as answers. Our approach outperforms substantially prior methods
across various settings (i.e. fully supervised, domain transfer, and fewshot
learning). Finally, we propose a clustering-based sampling strategy (JointEnc)
and conduct a thorough analysis of how different strategies influence the
few-shot learning performance. The implementations are available at https://
github.com/xinyadu/RGQA
",0
Benchmarking Large Language Models in Retrieval-Augmented Generation,"Jiawei Chen, Hongyu Lin, Xianpei Han, Le Sun",2023-09-04T08:28:44Z,Other,"  Retrieval-Augmented Generation (RAG) is a promising approach for mitigating
the hallucination of large language models (LLMs). However, existing research
lacks rigorous evaluation of the impact of retrieval-augmented generation on
different large language models, which make it challenging to identify the
potential bottlenecks in the capabilities of RAG for different LLMs. In this
paper, we systematically investigate the impact of Retrieval-Augmented
Generation on large language models. We analyze the performance of different
large language models in 4 fundamental abilities required for RAG, including
noise robustness, negative rejection, information integration, and
counterfactual robustness. To this end, we establish Retrieval-Augmented
Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and
Chinese. RGB divides the instances within the benchmark into 4 separate
testbeds based on the aforementioned fundamental abilities required to resolve
the case. Then we evaluate 6 representative LLMs on RGB to diagnose the
challenges of current LLMs when applying RAG. Evaluation reveals that while
LLMs exhibit a certain degree of noise robustness, they still struggle
significantly in terms of negative rejection, information integration, and
dealing with false information. The aforementioned assessment outcomes indicate
that there is still a considerable journey ahead to effectively apply RAG to
LLMs.
",0
"MKRAG: Medical Knowledge Retrieval Augmented Generation for Medical
  Question Answering","Yucheng Shi, Shaochen Xu, Tianze Yang, Zhengliang Liu, Tianming Liu, Quanzheng Li, Xiang Li, Ninghao Liu",2023-09-27T21:26:03Z,"Retrieval Augmented Generation, RAG","  Large Language Models (LLMs), although powerful in general domains, often
perform poorly on domain-specific tasks such as medical question answering
(QA). In addition, LLMs tend to function as ""black-boxes"", making it
challenging to modify their behavior. To address the problem, our work employs
a transparent process of retrieval augmented generation (RAG), aiming to
improve LLM responses without the need for fine-tuning or retraining.
Specifically, we propose a comprehensive retrieval strategy to extract medical
facts from an external knowledge base, and then inject them into the LLM's
query prompt. Focusing on medical QA, we evaluate the impact of different
retrieval models and the number of facts on LLM performance using the
MedQA-SMILE dataset. Notably, our retrieval-augmented Vicuna-7B model exhibited
an accuracy improvement from 44.46% to 48.54%. This work underscores the
potential of RAG to enhance LLM performance, offering a practical approach to
mitigate the challenges posed by black-box LLMs.
",0
RAGLog: Log Anomaly Detection using Retrieval Augmented Generation,"Jonathan Pan, Swee Liang Wong, Yidi Yuan",2023-11-09T10:40:04Z,"Retrieval Augmented Generation, RAG","  The ability to detect log anomalies from system logs is a vital activity
needed to ensure cyber resiliency of systems. It is applied for fault
identification or facilitate cyber investigation and digital forensics.
However, as logs belonging to different systems and components differ
significantly, the challenge to perform such analysis is humanly challenging
from the volume, variety and velocity of logs. This is further complicated by
the lack or unavailability of anomalous log entries to develop trained machine
learning or artificial intelligence models for such purposes. In this research
work, we explore the use of a Retrieval Augmented Large Language Model that
leverages a vector database to detect anomalies from logs. We used a Question
and Answer configuration pipeline. To the best of our knowledge, our experiment
which we called RAGLog is a novel one and the experimental results show much
promise.
",0
Learning to Filter Context for Retrieval-Augmented Generation,"Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, Graham Neubig",2023-11-14T18:41:54Z,Other,"  On-the-fly retrieval of relevant knowledge has proven an essential element of
reliable systems for tasks such as open-domain question answering and fact
verification. However, because retrieval systems are not perfect, generation
models are required to generate outputs given partially or entirely irrelevant
passages. This can cause over- or under-reliance on context, and result in
problems in the generated output such as hallucinations. To alleviate these
problems, we propose FILCO, a method that improves the quality of the context
provided to the generator by (1) identifying useful context based on lexical
and information-theoretic approaches, and (2) training context filtering models
that can filter retrieved contexts at test time. We experiment on six
knowledge-intensive tasks with FLAN-T5 and LLaMa2, and demonstrate that our
method outperforms existing approaches on extractive question answering (QA),
complex multi-hop and long-form QA, fact verification, and dialog generation
tasks. FILCO effectively improves the quality of context, whether or not it
supports the canonical output.
",42
"ARES: An Automated Evaluation Framework for Retrieval-Augmented
  Generation Systems","Jon Saad-Falcon, Omar Khattab, Christopher Potts, Matei Zaharia",2023-11-16T00:39:39Z,Other,"  Evaluating retrieval-augmented generation (RAG) systems traditionally relies
on hand annotations for input queries, passages to retrieve, and responses to
generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating
RAG systems along the dimensions of context relevance, answer faithfulness, and
answer relevance. By creating its own synthetic training data, ARES finetunes
lightweight LM judges to assess the quality of individual RAG components. To
mitigate potential prediction errors, ARES utilizes a small set of
human-annotated datapoints for prediction-powered inference (PPI). Across eight
different knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES
accurately evaluates RAG systems while using only a few hundred human
annotations during evaluation. Furthermore, ARES judges remain effective across
domain shifts, proving accurate even after changing the type of queries and/or
documents used in the evaluated RAG systems. We make our code and datasets
publicly available on Github.
",0
Retrieval Augmented Generation of Symbolic Music with LLMs,"Nicolas Jonason, Luca Casini, Carl Thomé, Bob L. T. Sturm",2023-11-17T08:21:56Z,Retrieval Augmented Generation,"  We explore the use of large language models (LLMs) for music generation using
a retrieval system to select relevant examples. We find promising initial
results for music generation in a dialogue with the user, especially
considering the ease with which such a system can be implemented. The code is
available online.
",0
PaperQA: Retrieval-Augmented Generative Agent for Scientific Research,"Jakub Lála, Odhran O'Donoghue, Aleksandar Shtedritski, Sam Cox, Samuel G. Rodriques, Andrew D. White",2023-12-08T18:50:20Z,Other,"  Large Language Models (LLMs) generalize well across language tasks, but
suffer from hallucinations and uninterpretability, making it difficult to
assess their accuracy without ground-truth. Retrieval-Augmented Generation
(RAG) models have been proposed to reduce hallucinations and provide provenance
for how an answer was generated. Applying such models to the scientific
literature may enable large-scale, systematic processing of scientific
knowledge. We present PaperQA, a RAG agent for answering questions over the
scientific literature. PaperQA is an agent that performs information retrieval
across full-text scientific articles, assesses the relevance of sources and
passages, and uses RAG to provide answers. Viewing this agent as a question
answering model, we find it exceeds performance of existing LLMs and LLM agents
on current science QA benchmarks. To push the field closer to how humans
perform research on scientific literature, we also introduce LitQA, a more
complex benchmark that requires retrieval and synthesis of information from
full-text scientific papers across the literature. Finally, we demonstrate
PaperQA's matches expert human researchers on LitQA.
",0
"RIGHT: Retrieval-augmented Generation for Mainstream Hashtag
  Recommendation","Run-Ze Fan, Yixing Fan, Jiangui Chen, Jiafeng Guo, Ruqing Zhang, Xueqi Cheng",2023-12-16T14:47:03Z,Other,"  Automatic mainstream hashtag recommendation aims to accurately provide users
with concise and popular topical hashtags before publication. Generally,
mainstream hashtag recommendation faces challenges in the comprehensive
difficulty of newly posted tweets in response to new topics, and the accurate
identification of mainstream hashtags beyond semantic correctness. However,
previous retrieval-based methods based on a fixed predefined mainstream hashtag
list excel in producing mainstream hashtags, but fail to understand the
constant flow of up-to-date information. Conversely, generation-based methods
demonstrate a superior ability to comprehend newly posted tweets, but their
capacity is constrained to identifying mainstream hashtags without additional
features. Inspired by the recent success of the retrieval-augmented technique,
in this work, we attempt to adopt this framework to combine the advantages of
both approaches. Meantime, with the help of the generator component, we could
rethink how to further improve the quality of the retriever component at a low
cost. Therefore, we propose RetrIeval-augmented Generative Mainstream HashTag
Recommender (RIGHT), which consists of three components: 1) a retriever seeks
relevant hashtags from the entire tweet-hashtags set; 2) a selector enhances
mainstream identification by introducing global signals; and 3) a generator
incorporates input tweets and selected hashtags to directly generate the
desired hashtags. The experimental results show that our method achieves
significant improvements over state-of-the-art baselines. Moreover, RIGHT can
be easily integrated into large language models, improving the performance of
ChatGPT by more than 10%.
",0
Retrieval-Augmented Generation for Large Language Models: A Survey,"Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang",2023-12-18T07:47:33Z,Other,"  Large Language Models (LLMs) showcase impressive capabilities but encounter
challenges like hallucination, outdated knowledge, and non-transparent,
untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has
emerged as a promising solution by incorporating knowledge from external
databases. This enhances the accuracy and credibility of the generation,
particularly for knowledge-intensive tasks, and allows for continuous knowledge
updates and integration of domain-specific information. RAG synergistically
merges LLMs' intrinsic knowledge with the vast, dynamic repositories of
external databases. This comprehensive review paper offers a detailed
examination of the progression of RAG paradigms, encompassing the Naive RAG,
the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the
tripartite foundation of RAG frameworks, which includes the retrieval, the
generation and the augmentation techniques. The paper highlights the
state-of-the-art technologies embedded in each of these critical components,
providing a profound understanding of the advancements in RAG systems.
Furthermore, this paper introduces up-to-date evaluation framework and
benchmark. At the end, this article delineates the challenges currently faced
and points out prospective avenues for research and development.
",632
"RealGen: Retrieval Augmented Generation for Controllable Traffic
  Scenarios","Wenhao Ding, Yulong Cao, Ding Zhao, Chaowei Xiao, Marco Pavone",2023-12-19T23:11:06Z,Retrieval Augmented Generation,"  Simulation plays a crucial role in the development of autonomous vehicles
(AVs) due to the potential risks associated with real-world testing. Although
significant progress has been made in the visual aspects of simulators,
generating complex behavior among agents remains a formidable challenge. It is
not only imperative to ensure realism in the scenarios generated but also
essential to incorporate preferences and conditions to facilitate controllable
generation for AV training and evaluation. Traditional methods, mainly relying
on memorizing the distribution of training datasets, often fall short in
generating unseen scenarios. Inspired by the success of retrieval augmented
generation in large language models, we present RealGen, a novel
retrieval-based in-context learning framework for traffic scenario generation.
RealGen synthesizes new scenarios by combining behaviors from multiple
retrieved examples in a gradient-free way, which may originate from templates
or tagged scenarios. This in-context learning framework endows versatile
generative capabilities, including the ability to edit scenarios, compose
various behaviors, and produce critical scenarios. Evaluations show that
RealGen offers considerable flexibility and controllability, marking a new
direction in the field of controllable traffic scenario generation. Check our
project website for more information: https://realgen.github.io.
",0
"Seven Failure Points When Engineering a Retrieval Augmented Generation
  System","Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, Mohamed Abdelrazek",2024-01-11T12:04:11Z,Retrieval Augmented Generation,"  Software engineers are increasingly adding semantic search capabilities to
applications using a strategy known as Retrieval Augmented Generation (RAG). A
RAG system involves finding documents that semantically match a query and then
passing the documents to a large language model (LLM) such as ChatGPT to
extract the right answer using an LLM. RAG systems aim to: a) reduce the
problem of hallucinated responses from LLMs, b) link sources/references to
generated responses, and c) remove the need for annotating documents with
meta-data. However, RAG systems suffer from limitations inherent to information
retrieval systems and from reliance on LLMs. In this paper, we present an
experience report on the failure points of RAG systems from three case studies
from separate domains: research, education, and biomedical. We share the
lessons learned and present 7 failure points to consider when designing a RAG
system. The two key takeaways arising from our work are: 1) validation of a RAG
system is only feasible during operation, and 2) the robustness of a RAG system
evolves rather than designed in at the start. We conclude with a list of
potential research directions on RAG systems for the software engineering
community.
",0
"Interactive AI with Retrieval-Augmented Generation for Next Generation
  Networking","Ruichen Zhang, Hongyang Du, Yinqiu Liu, Dusit Niyato, Jiawen Kang, Sumei Sun, Xuemin Shen, H. Vincent Poor",2024-01-21T03:46:00Z,Other,"  With the advance of artificial intelligence (AI), the emergence of Google
Gemini and OpenAI Q* marks the direction towards artificial general
intelligence (AGI). To implement AGI, the concept of interactive AI (IAI) has
been introduced, which can interactively understand and respond not only to
human user input but also to dynamic system and network conditions. In this
article, we explore an integration and enhancement of IAI in networking. We
first comprehensively review recent developments and future perspectives of AI
and then introduce the technology and components of IAI. We then explore the
integration of IAI into the next-generation networks, focusing on how implicit
and explicit interactions can enhance network functionality, improve user
experience, and promote efficient network management. Subsequently, we propose
an IAI-enabled network management and optimization framework, which consists of
environment, perception, action, and brain units. We also design the pluggable
large language model (LLM) module and retrieval augmented generation (RAG)
module to build the knowledge base and contextual memory for decision-making in
the brain unit. We demonstrate the effectiveness of the framework through case
studies. Finally, we discuss potential research directions for IAI-based
networks.
",0
"Revolutionizing Retrieval-Augmented Generation with Enhanced PDF
  Structure Recognition",Demiao Lin,2024-01-23T09:54:36Z,Other,"  With the rapid development of Large Language Models (LLMs),
Retrieval-Augmented Generation (RAG) has become a predominant method in the
field of professional knowledge-based question answering. Presently, major
foundation model companies have opened up Embedding and Chat API interfaces,
and frameworks like LangChain have already integrated the RAG process. It
appears that the key models and steps in RAG have been resolved, leading to the
question: are professional knowledge QA systems now approaching perfection?
This article discovers that current primary methods depend on the premise of
accessing high-quality text corpora. However, since professional documents are
mainly stored in PDFs, the low accuracy of PDF parsing significantly impacts
the effectiveness of professional knowledge-based QA. We conducted an empirical
RAG experiment across hundreds of questions from the corresponding real-world
professional documents. The results show that, ChatDOC, a RAG system equipped
with a panoptic and pinpoint PDF parser, retrieves more accurate and complete
segments, and thus better answers. Empirical experiments show that ChatDOC is
superior to baseline on nearly 47% of questions, ties for 38% of cases, and
falls short on only 15% of cases. It shows that we may revolutionize RAG with
enhanced PDF structure recognition.
",0
"MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop
  Queries","Yixuan Tang, Yi Yang",2024-01-27T11:41:48Z,RAG,"  Retrieval-augmented generation (RAG) augments large language models (LLM) by
retrieving relevant knowledge, showing promising potential in mitigating LLM
hallucinations and enhancing response quality, thereby facilitating the great
adoption of LLMs in practice. However, we find that existing RAG systems are
inadequate in answering multi-hop queries, which require retrieving and
reasoning over multiple pieces of supporting evidence. Furthermore, to our
knowledge, no existing RAG benchmarking dataset focuses on multi-hop queries.
In this paper, we develop a novel dataset, MultiHop-RAG, which consists of a
knowledge base, a large collection of multi-hop queries, their ground-truth
answers, and the associated supporting evidence. We detail the procedure of
building the dataset, utilizing an English news article dataset as the
underlying RAG knowledge base. We demonstrate the benchmarking utility of
MultiHop-RAG in two experiments. The first experiment compares different
embedding models for retrieving evidence for multi-hop queries. In the second
experiment, we examine the capabilities of various state-of-the-art LLMs,
including GPT-4, PaLM, and Llama2-70B, in reasoning and answering multi-hop
queries given the evidence. Both experiments reveal that existing RAG methods
perform unsatisfactorily in retrieving and answering multi-hop queries. We hope
MultiHop-RAG will be a valuable resource for the community in developing
effective RAG systems, thereby facilitating greater adoption of LLMs in
practice. The MultiHop-RAG and implemented RAG system is publicly available at
https://github.com/yixuantt/MultiHop-RAG/.
",0
RAG-Fusion: a New Take on Retrieval-Augmented Generation,Zackary Rackauckas,2024-01-31T22:06:07Z,RAG,"  Infineon has identified a need for engineers, account managers, and customers
to rapidly obtain product information. This problem is traditionally addressed
with retrieval-augmented generation (RAG) chatbots, but in this study, I
evaluated the use of the newly popularized RAG-Fusion method. RAG-Fusion
combines RAG and reciprocal rank fusion (RRF) by generating multiple queries,
reranking them with reciprocal scores and fusing the documents and scores.
Through manually evaluating answers on accuracy, relevance, and
comprehensiveness, I found that RAG-Fusion was able to provide accurate and
comprehensive answers due to the generated queries contextualizing the original
query from various perspectives. However, some answers strayed off topic when
the generated queries' relevance to the original query is insufficient. This
research marks significant progress in artificial intelligence (AI) and natural
language processing (NLP) applications and demonstrates transformations in a
global and multi-industry context.
",0
Financial Report Chunking for Effective Retrieval Augmented Generation,"Antonio Jimeno Yepes, Yao You, Jan Milczek, Sebastian Laverde, Renyu Li",2024-02-05T22:35:42Z,Retrieval Augmented Generation,"  Chunking information is a key step in Retrieval Augmented Generation (RAG).
Current research primarily centers on paragraph-level chunking. This approach
treats all texts as equal and neglects the information contained in the
structure of documents. We propose an expanded approach to chunk documents by
moving beyond mere paragraph-level chunking to chunk primary by structural
element components of documents. Dissecting documents into these constituent
elements creates a new way to chunk documents that yields the best chunk size
without tuning. We introduce a novel framework that evaluates how chunking
based on element types annotated by document understanding models contributes
to the overall context and accuracy of the information retrieved. We also
demonstrate how this approach impacts RAG assisted Question & Answer task
performance. Our research includes a comprehensive analysis of various element
types, their role in effective information retrieval, and the impact they have
on the quality of RAG outputs. Findings support that element type based
chunking largely improve RAG results on financial reporting. Through this
research, we are also able to answer how to uncover highly accurate RAG.
",0
"Prompt Perturbation in Retrieval-Augmented Generation based Large
  Language Models","Zhibo Hu, Chen Wang, Yanfeng Shu,  Helen,  Paik, Liming Zhu",2024-02-11T12:25:41Z,Other,"  The robustness of large language models (LLMs) becomes increasingly important
as their use rapidly grows in a wide range of domains. Retrieval-Augmented
Generation (RAG) is considered as a means to improve the trustworthiness of
text generation from LLMs. However, how the outputs from RAG-based LLMs are
affected by slightly different inputs is not well studied. In this work, we
find that the insertion of even a short prefix to the prompt leads to the
generation of outputs far away from factually correct answers. We
systematically evaluate the effect of such prefixes on RAG by introducing a
novel optimization technique called Gradient Guided Prompt Perturbation (GGPP).
GGPP achieves a high success rate in steering outputs of RAG-based LLMs to
targeted wrong answers. It can also cope with instructions in the prompts
requesting to ignore irrelevant context. We also exploit LLMs' neuron
activation difference between prompts with and without GGPP perturbations to
give a method that improves the robustness of RAG-based LLMs through a highly
effective detector trained on neuron activation triggered by GGPP generated
prompts. Our evaluation on open-sourced LLMs demonstrates the effectiveness of
our methods.
",0
Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning,"Gelei Deng, Yi Liu, Kailong Wang, Yuekang Li, Tianwei Zhang, Yang Liu",2024-02-13T12:40:39Z,Retrieval Augmented Generation,"  Large Language Models~(LLMs) have gained immense popularity and are being
increasingly applied in various domains. Consequently, ensuring the security of
these models is of paramount importance. Jailbreak attacks, which manipulate
LLMs to generate malicious content, are recognized as a significant
vulnerability. While existing research has predominantly focused on direct
jailbreak attacks on LLMs, there has been limited exploration of indirect
methods. The integration of various plugins into LLMs, notably Retrieval
Augmented Generation~(RAG), which enables LLMs to incorporate external
knowledge bases into their response generation such as GPTs, introduces new
avenues for indirect jailbreak attacks.
  To fill this gap, we investigate indirect jailbreak attacks on LLMs,
particularly GPTs, introducing a novel attack vector named Retrieval Augmented
Generation Poisoning. This method, Pandora, exploits the synergy between LLMs
and RAG through prompt manipulation to generate unexpected responses. Pandora
uses maliciously crafted content to influence the RAG process, effectively
initiating jailbreak attacks. Our preliminary tests show that Pandora
successfully conducts jailbreak attacks in four different scenarios, achieving
higher success rates than direct attacks, with 64.3\% for GPT-3.5 and 34.8\%
for GPT-4.
",0
"Unveiling the Magic: Investigating Attention Distillation in
  Retrieval-augmented Generation","Zizhong Li, Haopeng Zhang, Jiawei Zhang",2024-02-19T02:48:44Z,Other,"  Retrieval-augmented generation framework can address the limitations of large
language models by enabling real-time knowledge updates for more accurate
answers. An efficient way in the training phase of retrieval-augmented models
is attention distillation, which uses attention scores as a supervision signal
instead of manually annotated query-document pairs. Despite its growing
popularity, the detailed mechanisms behind the success of attention
distillation remain unexplored, particularly the specific patterns it leverages
to benefit training. In this paper, we address this gap by conducting a
comprehensive review of attention distillation workflow and identifying key
factors influencing the learning quality of retrieval-augmented language
models. We further propose indicators for optimizing models' training methods
and avoiding ineffective training.
",1
MORE: Multi-mOdal REtrieval Augmented Generative Commonsense Reasoning,"Wanqing Cui, Keping Bi, Jiafeng Guo, Xueqi Cheng",2024-02-21T08:54:47Z,Other,"  Since commonsense information has been recorded significantly less frequently
than its existence, language models pre-trained by text generation have
difficulty to learn sufficient commonsense knowledge. Several studies have
leveraged text retrieval to augment the models' commonsense ability. Unlike
text, images capture commonsense information inherently but little effort has
been paid to effectively utilize them. In this work, we propose a novel
Multi-mOdal REtrieval (MORE) augmentation framework, to leverage both text and
images to enhance the commonsense ability of language models. Extensive
experiments on the Common-Gen task have demonstrated the efficacy of MORE based
on the pre-trained models of both single and multiple modalities.
",0
"Improving Assessment of Tutoring Practices using Retrieval-Augmented
  Generation","Zifei FeiFei Han, Jionghao Lin, Ashish Gurung, Danielle R. Thomas, Eason Chen, Conrad Borchers, Shivang Gupta, Kenneth R. Koedinger",2024-02-04T20:42:30Z,Other,"  One-on-one tutoring is an effective instructional method for enhancing
learning, yet its efficacy hinges on tutor competencies. Novice math tutors
often prioritize content-specific guidance, neglecting aspects such as
social-emotional learning. Social-emotional learning promotes equity and
inclusion and nurturing relationships with students, which is crucial for
holistic student development. Assessing the competencies of tutors accurately
and efficiently can drive the development of tailored tutor training programs.
However, evaluating novice tutor ability during real-time tutoring remains
challenging as it typically requires experts-in-the-loop. To address this
challenge, this preliminary study aims to harness Generative Pre-trained
Transformers (GPT), such as GPT-3.5 and GPT-4 models, to automatically assess
tutors' ability of using social-emotional tutoring strategies. Moreover, this
study also reports on the financial dimensions and considerations of employing
these models in real-time and at scale for automated assessment. The current
study examined four prompting strategies: two basic Zero-shot prompt
strategies, Tree of Thought prompt, and Retrieval-Augmented Generator (RAG)
based prompt. The results indicate that the RAG prompt demonstrated more
accurate performance (assessed by the level of hallucination and correctness in
the generated assessment texts) and lower financial costs than the other
strategies evaluated. These findings inform the development of personalized
tutor training interventions to enhance the the educational effectiveness of
tutored learning.
",0
Retrieval-Augmented Generation for AI-Generated Content: A Survey,"Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, Bin Cui",2024-02-29T18:59:01Z,Other,"  Advancements in model algorithms, the growth of foundational models, and
access to high-quality datasets have propelled the evolution of Artificial
Intelligence Generated Content (AIGC). Despite its notable successes, AIGC
still faces hurdles such as updating knowledge, handling long-tail data,
mitigating data leakage, and managing high training and inference costs.
Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to
address such challenges. In particular, RAG introduces the information
retrieval process, which enhances the generation process by retrieving relevant
objects from available data stores, leading to higher accuracy and better
robustness. In this paper, we comprehensively review existing efforts that
integrate RAG technique into AIGC scenarios. We first classify RAG foundations
according to how the retriever augments the generator, distilling the
fundamental abstractions of the augmentation methodologies for various
retrievers and generators. This unified perspective encompasses all RAG
scenarios, illuminating advancements and pivotal technologies that help with
potential future progress. We also summarize additional enhancements methods
for RAG, facilitating effective engineering and implementation of RAG systems.
Then from another view, we survey on practical applications of RAG across
different modalities and tasks, offering valuable references for researchers
and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss
the limitations of current RAG systems, and suggest potential directions for
future research. Github: https://github.com/PKU-DAIR/RAG-Survey.
",0
"Fine Tuning vs. Retrieval Augmented Generation for Less Popular
  Knowledge","Heydar Soudani, Evangelos Kanoulas, Faegheh Hasibi",2024-03-03T08:07:55Z,Retrieval Augmented Generation,"  Language Models (LMs) memorize a vast amount of factual knowledge, exhibiting
strong performance across diverse tasks and domains. However, it has been
observed that the performance diminishes when dealing with less-popular or
low-frequency concepts and entities, for example in domain specific
applications. The two prominent approaches to enhance the performance of LMs on
low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning
(FT) over synthetic data. This paper explores and evaluates the impact of RAG
and FT on customizing LMs in handling low-frequency entities on question
answering tasks. We conduct extensive experiments on twelve LMs of varying size
and type and different fine tuning, data augmentation, and retrieval models.
Our findings indicate that while FT boosts the performance across entities of
varying popularity, RAG surpasses FT by a large margin particularly for least
popular factual knowledge. Additionally, the success of both RAG and FT
approaches is amplified by improving retrieval and data augmentation
techniques. Fine tuning, while beneficial for small LMs, requires extensive
resources. To address this issue, we propose the new Stimulus RAG approach that
surpasses the effectiveness of fine tuning based approaches, thereby
eliminating the need for the costly data augmentation and fine tuning step for
enriching LMs with less popular factual knowledge.
",0
"Towards Comprehensive Vietnamese Retrieval-Augmented Generation and
  Large Language Models","Nguyen Quang Duc, Le Hai Son, Nguyen Duc Nhan, Nguyen Dich Nhat Minh, Le Thanh Huong, Dinh Viet Sang",2024-03-03T21:24:35Z,Other,"  This paper presents our contributions towards advancing the state of
Vietnamese language understanding and generation through the development and
dissemination of open datasets and pre-trained models for Vietnamese
Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs).
",0
"PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System
  Co-design","Wenqi Jiang, Shuai Zhang, Boran Han, Jie Wang, Bernie Wang, Tim Kraska",2024-03-08T21:09:20Z,RAG,"  Retrieval-augmented generation (RAG) can enhance the generation quality of
large language models (LLMs) by incorporating external token databases.
However, retrievals from large databases can constitute a substantial portion
of the overall generation time, particularly when retrievals are periodically
performed to align the retrieved content with the latest states of generation.
In this paper, we introduce PipeRAG, a novel algorithm-system co-design
approach to reduce generation latency and enhance generation quality. PipeRAG
integrates (1) pipeline parallelism to enable concurrent retrieval and
generation processes, (2) flexible retrieval intervals to maximize the
efficiency of pipeline parallelism, and (3) a performance model to
automatically balance retrieval quality and latency based on the generation
states and underlying hardware. Our evaluation shows that, by combining the
three aforementioned methods, PipeRAG achieves up to 2.6$\times$ speedup in
end-to-end generation latency while improving generation quality. These
promising results showcase the effectiveness of co-designing algorithms with
underlying systems, paving the way for the adoption of PipeRAG in future RAG
systems.
",13
"LexDrafter: Terminology Drafting for Legislative Documents using
  Retrieval Augmented Generation","Ashish Chouhan, Michael Gertz",2024-03-24T21:02:35Z,Retrieval Augmented Generation,"  With the increase in legislative documents at the EU, the number of new terms
and their definitions is increasing as well. As per the Joint Practical Guide
of the European Parliament, the Council and the Commission, terms used in legal
documents shall be consistent, and identical concepts shall be expressed
without departing from their meaning in ordinary, legal, or technical language.
Thus, while drafting a new legislative document, having a framework that
provides insights about existing definitions and helps define new terms based
on a document's context will support such harmonized legal definitions across
different regulations and thus avoid ambiguities. In this paper, we present
LexDrafter, a framework that assists in drafting Definitions articles for
legislative documents using retrieval augmented generation (RAG) and existing
term definitions present in different legislative documents. For this,
definition elements are built by extracting definitions from existing
documents. Using definition elements and RAG, a Definitions article can be
suggested on demand for a legislative document that is being drafted. We
demonstrate and evaluate the functionality of LexDrafter using a collection of
EU documents from the energy domain. The code for LexDrafter framework is
available at https://github.com/achouhan93/LexDrafter.
",1
Adversarial Machine Learning for 5G Communications Security,"Yalin E. Sagduyu, Tugba Erpek, Yi Shi",2021-01-07T17:52:17Z,Adversarial Machine Learning,"  Machine learning provides automated means to capture complex dynamics of
wireless spectrum and support better understanding of spectrum resources and
their efficient utilization. As communication systems become smarter with
cognitive radio capabilities empowered by machine learning to perform critical
tasks such as spectrum awareness and spectrum sharing, they also become
susceptible to new vulnerabilities due to the attacks that target the machine
learning applications. This paper identifies the emerging attack surface of
adversarial machine learning and corresponding attacks launched against
wireless communications in the context of 5G systems. The focus is on attacks
against (i) spectrum sharing of 5G communications with incumbent users such as
in the Citizens Broadband Radio Service (CBRS) band and (ii) physical layer
authentication of 5G User Equipment (UE) to support network slicing. For the
first attack, the adversary transmits during data transmission or spectrum
sensing periods to manipulate the signal-level inputs to the deep learning
classifier that is deployed at the Environmental Sensing Capability (ESC) to
support the 5G system. For the second attack, the adversary spoofs wireless
signals with the generative adversarial network (GAN) to infiltrate the
physical layer authentication mechanism based on a deep learning classifier
that is deployed at the 5G base station. Results indicate major vulnerabilities
of 5G systems to adversarial machine learning. To sustain the 5G system
operations in the presence of adversaries, a defense mechanism is presented to
increase the uncertainty of the adversary in training the surrogate model used
for launching its subsequent attacks.
",0
Mental Models of Adversarial Machine Learning,"Lukas Bieringer, Kathrin Grosse, Michael Backes, Battista Biggio, Katharina Krombholz",2021-05-08T16:05:07Z,Adversarial Machine Learning,"  Although machine learning is widely used in practice, little is known about
practitioners' understanding of potential security challenges. In this work, we
close this substantial gap and contribute a qualitative study focusing on
developers' mental models of the machine learning pipeline and potentially
vulnerable components. Similar studies have helped in other security fields to
discover root causes or improve risk communication. Our study reveals two
\facets of practitioners' mental models of machine learning security. Firstly,
practitioners often confuse machine learning security with threats and defences
that are not directly related to machine learning. Secondly, in contrast to
most academic research, our participants perceive security of machine learning
as not solely related to individual models, but rather in the context of entire
workflows that consist of multiple components. Jointly with our additional
findings, these two facets provide a foundation to substantiate mental models
for machine learning security and have implications for the integration of
adversarial machine learning into corporate workflows, \new{decreasing
practitioners' reported uncertainty}, and appropriate regulatory frameworks for
machine learning security.
",0
Markpainting: Adversarial Machine Learning meets Inpainting,"David Khachaturov, Ilia Shumailov, Yiren Zhao, Nicolas Papernot, Ross Anderson",2021-06-01T17:45:52Z,Adversarial Machine Learning,"  Inpainting is a learned interpolation technique that is based on generative
modeling and used to populate masked or missing pieces in an image; it has wide
applications in picture editing and retouching. Recently, inpainting started
being used for watermark removal, raising concerns. In this paper we study how
to manipulate it using our markpainting technique. First, we show how an image
owner with access to an inpainting model can augment their image in such a way
that any attempt to edit it using that model will add arbitrary visible
information. We find that we can target multiple different models
simultaneously with our technique. This can be designed to reconstitute a
watermark if the editor had been trying to remove it. Second, we show that our
markpainting technique is transferable to models that have different
architectures or were trained on different datasets, so watermarks created
using it are difficult for adversaries to remove. Markpainting is novel and can
be used as a manipulation alarm that becomes visible in the event of
inpainting.
",0
Defenses in Adversarial Machine Learning: A Survey,"Baoyuan Wu, Shaokui Wei, Mingli Zhu, Meixi Zheng, Zihao Zhu, Mingda Zhang, Hongrui Chen, Danni Yuan, Li Liu, Qingshan Liu",2023-12-13T15:42:55Z,Adversarial Machine Learning,"  Adversarial phenomenon has been widely observed in machine learning (ML)
systems, especially in those using deep neural networks, describing that ML
systems may produce inconsistent and incomprehensible predictions with humans
at some particular cases. This phenomenon poses a serious security threat to
the practical application of ML systems, and several advanced attack paradigms
have been developed to explore it, mainly including backdoor attacks, weight
attacks, and adversarial examples. For each individual attack paradigm, various
defense paradigms have been developed to improve the model robustness against
the corresponding attack paradigm. However, due to the independence and
diversity of these defense paradigms, it is difficult to examine the overall
robustness of an ML system against different kinds of attacks.This survey aims
to build a systematic review of all existing defense paradigms from a unified
perspective. Specifically, from the life-cycle perspective, we factorize a
complete machine learning system into five stages, including pre-training,
training, post-training, deployment, and inference stages, respectively. Then,
we present a clear taxonomy to categorize and review representative defense
methods at each individual stage. The unified perspective and presented
taxonomies not only facilitate the analysis of the mechanism of each defense
paradigm but also help us to understand connections and differences among
different defense paradigms, which may inspire future research to develop more
advanced, comprehensive defenses.
",0
Logic for Explainable AI,Adnan Darwiche,2023-05-09T04:53:57Z,Explainable AI,"  A central quest in explainable AI relates to understanding the decisions made
by (learned) classifiers. There are three dimensions of this understanding that
have been receiving significant attention in recent years. The first dimension
relates to characterizing conditions on instances that are necessary and
sufficient for decisions, therefore providing abstractions of instances that
can be viewed as the ""reasons behind decisions."" The next dimension relates to
characterizing minimal conditions that are sufficient for a decision, therefore
identifying maximal aspects of the instance that are irrelevant to the
decision. The last dimension relates to characterizing minimal conditions that
are necessary for a decision, therefore identifying minimal perturbations to
the instance that yield alternate decisions. We discuss in this tutorial a
comprehensive, semantical and computational theory of explainability along
these dimensions which is based on some recent developments in symbolic logic.
The tutorial will also discuss how this theory is particularly applicable to
non-symbolic classifiers such as those based on Bayesian networks, decision
trees, random forests and some types of neural networks.
",0
Responsible AI in Healthcare,"Federico Cabitza, Davide Ciucci, Gabriella Pasi, Marco Viviani",2022-02-19T14:48:22Z,Responsible AI,"  This article discusses open problems, implemented solutions, and future
research in the area of responsible AI in healthcare. In particular, we
illustrate two main research themes related to the work of two laboratories
within the Department of Informatics, Systems, and Communication at the
University of Milano-Bicocca. The problems addressed concern, in particular,
{uncertainty in medical data and machine advice}, and the problem of online
health information disorder.
",0
Towards Implementing Responsible AI,"Conrad Sanderson, Qinghua Lu, David Douglas, Xiwei Xu, Liming Zhu, Jon Whittle",2022-05-09T14:59:23Z,Responsible AI,"  As the deployment of artificial intelligence (AI) is changing many fields and
industries, there are concerns about AI systems making decisions and
recommendations without adequately considering various ethical aspects, such as
accountability, reliability, transparency, explainability, contestability,
privacy, and fairness. While many sets of AI ethics principles have been
recently proposed that acknowledge these concerns, such principles are
high-level and do not provide tangible advice on how to develop ethical and
responsible AI systems. To gain insight on the possible implementation of the
principles, we conducted an empirical investigation involving semi-structured
interviews with a cohort of AI practitioners. The salient findings cover four
aspects of AI system design and development, adapting processes used in
software engineering: (i) high-level view, (ii) requirements engineering, (iii)
design and implementation, (iv) deployment and operation.
",0
Explanatory Pluralism in Explainable AI,Yiheng Yao,2021-06-26T09:02:06Z,Explainable AI,"  The increasingly widespread application of AI models motivates increased
demand for explanations from a variety of stakeholders. However, this demand is
ambiguous because there are many types of 'explanation' with different
evaluative criteria. In the spirit of pluralism, I chart a taxonomy of types of
explanation and the associated XAI methods that can address them. When we look
to expose the inner mechanisms of AI models, we develop
Diagnostic-explanations. When we seek to render model output understandable, we
produce Explication-explanations. When we wish to form stable generalizations
of our models, we produce Expectation-explanations. Finally, when we want to
justify the usage of a model, we produce Role-explanations that situate models
within their social context. The motivation for such a pluralistic view stems
from a consideration of causes as manipulable relationships and the different
types of explanations as identifying the relevant points in AI systems we can
intervene upon to affect our desired changes. This paper reduces the ambiguity
in use of the word 'explanation' in the field of XAI, allowing practitioners
and stakeholders a useful template for avoiding equivocation and evaluating XAI
methods and putative explanations.
",3
Counterfactual Evaluation for Explainable AI,"Yingqiang Ge, Shuchang Liu, Zelong Li, Shuyuan Xu, Shijie Geng, Yunqi Li, Juntao Tan, Fei Sun, Yongfeng Zhang",2021-09-05T01:38:49Z,Explainable AI,"  While recent years have witnessed the emergence of various explainable
methods in machine learning, to what degree the explanations really represent
the reasoning process behind the model prediction -- namely, the faithfulness
of explanation -- is still an open problem. One commonly used way to measure
faithfulness is \textit{erasure-based} criteria. Though conceptually simple,
erasure-based criterion could inevitably introduce biases and artifacts. We
propose a new methodology to evaluate the faithfulness of explanations from the
\textit{counterfactual reasoning} perspective: the model should produce
substantially different outputs for the original input and its corresponding
counterfactual edited on a faithful feature. Specially, we introduce two
algorithms to find the proper counterfactuals in both discrete and continuous
scenarios and then use the acquired counterfactuals to measure faithfulness.
Empirical results on several datasets show that compared with existing metrics,
our proposed counterfactual evaluation method can achieve top correlation with
the ground truth under diffe
",0
Adversarial Machine Learning in Text Analysis and Generation,Izzat Alsmadi,2021-01-14T04:37:52Z,Adversarial Machine Learning,"  The research field of adversarial machine learning witnessed a significant
interest in the last few years. A machine learner or model is secure if it can
deliver main objectives with acceptable accuracy, efficiency, etc. while at the
same time, it can resist different types and/or attempts of adversarial
attacks. This paper focuses on studying aspects and research trends in
adversarial machine learning specifically in text analysis and generation. The
paper summarizes main research trends in the field such as GAN algorithms,
models, types of attacks, and defense against those attacks.
",0
"Adversarial Machine Learning Attacks on Condition-Based Maintenance
  Capabilities",Hamidreza Habibollahi Najaf Abadi,2021-01-28T16:34:04Z,Adversarial Machine Learning,"  Condition-based maintenance (CBM) strategies exploit machine learning models
to assess the health status of systems based on the collected data from the
physical environment, while machine learning models are vulnerable to
adversarial attacks. A malicious adversary can manipulate the collected data to
deceive the machine learning model and affect the CBM system's performance.
Adversarial machine learning techniques introduced in the computer vision
domain can be used to make stealthy attacks on CBM systems by adding
perturbation to data to confuse trained models. The stealthy nature causes
difficulty and delay in detection of the attacks. In this paper, adversarial
machine learning in the domain of CBM is introduced. A case study shows how
adversarial machine learning can be used to attack CBM capabilities.
Adversarial samples are crafted using the Fast Gradient Sign method, and the
performance of a CBM system under attack is investigated. The obtained results
reveal that CBM systems are vulnerable to adversarial machine learning attacks
and defense strategies need to be considered.
",0
Segmentation Fault: A Cheap Defense Against Adversarial Machine Learning,"Doha Al Bared, Mohamed Nassar",2021-08-31T04:56:58Z,Adversarial Machine Learning,"  Recently published attacks against deep neural networks (DNNs) have stressed
the importance of methodologies and tools to assess the security risks of using
this technology in critical systems. Efficient techniques for detecting
adversarial machine learning helps establishing trust and boost the adoption of
deep learning in sensitive and security systems. In this paper, we propose a
new technique for defending deep neural network classifiers, and convolutional
ones in particular. Our defense is cheap in the sense that it requires less
computation power despite a small cost to pay in terms of detection accuracy.
The work refers to a recently published technique called ML-LOO. We replace the
costly pixel by pixel leave-one-out approach of ML-LOO by adopting
coarse-grained leave-one-out. We evaluate and compare the efficiency of
different segmentation algorithms for this task. Our results show that a large
gain in efficiency is possible, even though penalized by a marginal decrease in
detection accuracy.
",0
"Contributions to Large Scale Bayesian Inference and Adversarial Machine
  Learning",Víctor Gallego,2021-09-25T23:02:47Z,Other,"  The rampant adoption of ML methodologies has revealed that models are usually
adopted to make decisions without taking into account the uncertainties in
their predictions. More critically, they can be vulnerable to adversarial
examples. Thus, we believe that developing ML systems that take into account
predictive uncertainties and are robust against adversarial examples is a must
for critical, real-world tasks. We start with a case study in retailing. We
propose a robust implementation of the Nerlove-Arrow model using a Bayesian
structural time series model. Its Bayesian nature facilitates incorporating
prior information reflecting the manager's views, which can be updated with
relevant data. However, this case adopted classical Bayesian techniques, such
as the Gibbs sampler. Nowadays, the ML landscape is pervaded with neural
networks and this chapter also surveys current developments in this sub-field.
Then, we tackle the problem of scaling Bayesian inference to complex models and
large data regimes. In the first part, we propose a unifying view of two
different Bayesian inference algorithms, Stochastic Gradient Markov Chain Monte
Carlo (SG-MCMC) and Stein Variational Gradient Descent (SVGD), leading to
improved and efficient novel sampling schemes. In the second part, we develop a
framework to boost the efficiency of Bayesian inference in probabilistic models
by embedding a Markov chain sampler within a variational posterior
approximation. After that, we present an alternative perspective on adversarial
classification based on adversarial risk analysis, and leveraging the scalable
Bayesian approaches from chapter 2. In chapter 4 we turn to reinforcement
learning, introducing Threatened Markov Decision Processes, showing the
benefits of accounting for adversaries in RL while the agent learns.
",0
Adversarial machine learning for protecting against online manipulation,"Stefano Cresci, Marinella Petrocchi, Angelo Spognardi, Stefano Tognazzi",2021-11-23T17:42:45Z,Adversarial Machine Learning,"  Adversarial examples are inputs to a machine learning system that result in
an incorrect output from that system. Attacks launched through this type of
input can cause severe consequences: for example, in the field of image
recognition, a stop signal can be misclassified as a speed limit
indication.However, adversarial examples also represent the fuel for a flurry
of research directions in different domains and applications. Here, we give an
overview of how they can be profitably exploited as powerful tools to build
stronger learning models, capable of better-withstanding attacks, for two
crucial tasks: fake news and social bot detection.
",9
"Addressing Adversarial Machine Learning Attacks in Smart Healthcare
  Perspectives","Arawinkumaar Selvakkumar, Shantanu Pal, Zahra Jadidi",2021-12-16T13:16:46Z,Adversarial Machine Learning,"  Smart healthcare systems are gaining popularity with the rapid development of
intelligent sensors, the Internet of Things (IoT) applications and services,
and wireless communications. However, at the same time, several vulnerabilities
and adversarial attacks make it challenging for a safe and secure smart
healthcare system from a security point of view. Machine learning has been used
widely to develop suitable models to predict and mitigate attacks. Still, the
attacks could trick the machine learning models and misclassify outputs
generated by the model. As a result, it leads to incorrect decisions, for
example, false disease detection and wrong treatment plans for patients. In
this paper, we address the type of adversarial attacks and their impact on
smart healthcare systems. We propose a model to examine how adversarial attacks
impact machine learning classifiers. To test the model, we use a medical image
dataset. Our model can classify medical images with high accuracy. We then
attacked the model with a Fast Gradient Sign Method attack (FGSM) to cause the
model to predict the images and misclassify them inaccurately. Using transfer
learning, we train a VGG-19 model with the medical dataset and later implement
the FGSM to the Convolutional Neural Network (CNN) to examine the significant
impact it causes on the performance and accuracy of the machine learning model.
Our results demonstrate that the adversarial attack misclassifies the images,
causing the model's accuracy rate to drop from 88% to 11%.
",0
"Covert Communications via Adversarial Machine Learning and
  Reconfigurable Intelligent Surfaces","Brian Kim, Tugba Erpek, Yalin E. Sagduyu, Sennur Ulukus",2021-12-21T18:23:57Z,Adversarial Machine Learning,"  By moving from massive antennas to antenna surfaces for software-defined
wireless systems, the reconfigurable intelligent surfaces (RISs) rely on arrays
of unit cells to control the scattering and reflection profiles of signals,
mitigating the propagation loss and multipath attenuation, and thereby
improving the coverage and spectral efficiency. In this paper, covert
communication is considered in the presence of the RIS. While there is an
ongoing transmission boosted by the RIS, both the intended receiver and an
eavesdropper individually try to detect this transmission using their own deep
neural network (DNN) classifiers. The RIS interaction vector is designed by
balancing two (potentially conflicting) objectives of focusing the transmitted
signal to the receiver and keeping the transmitted signal away from the
eavesdropper. To boost covert communications, adversarial perturbations are
added to signals at the transmitter to fool the eavesdropper's classifier while
keeping the effect on the receiver low. Results from different network
topologies show that adversarial perturbation and RIS interaction vector can be
jointly designed to effectively increase the signal detection accuracy at the
receiver while reducing the detection accuracy at the eavesdropper to enable
covert communications.
",12
ROOM: Adversarial Machine Learning Attacks Under Real-Time Constraints,"Amira Guesmi, Khaled N. Khasawneh, Nael Abu-Ghazaleh, Ihsen Alouani",2022-01-05T14:03:26Z,Adversarial Machine Learning,"  Advances in deep learning have enabled a wide range of promising
applications. However, these systems are vulnerable to Adversarial Machine
Learning (AML) attacks; adversarially crafted perturbations to their inputs
could cause them to misclassify. Several state-of-the-art adversarial attacks
have demonstrated that they can reliably fool classifiers making these attacks
a significant threat. Adversarial attack generation algorithms focus primarily
on creating successful examples while controlling the noise magnitude and
distribution to make detection more difficult. The underlying assumption of
these attacks is that the adversarial noise is generated offline, making their
execution time a secondary consideration. However, recently, just-in-time
adversarial attacks where an attacker opportunistically generates adversarial
examples on the fly have been shown to be possible. This paper introduces a new
problem: how do we generate adversarial noise under real-time constraints to
support such real-time adversarial attacks? Understanding this problem improves
our understanding of the threat these attacks pose to real-time systems and
provides security evaluation benchmarks for future defenses. Therefore, we
first conduct a run-time analysis of adversarial generation algorithms.
Universal attacks produce a general attack offline, with no online overhead,
and can be applied to any input; however, their success rate is limited because
of their generality. In contrast, online algorithms, which work on a specific
input, are computationally expensive, making them inappropriate for operation
under time constraints. Thus, we propose ROOM, a novel Real-time Online-Offline
attack construction Model where an offline component serves to warm up the
online algorithm, making it possible to generate highly successful attacks
under time constraints.
",11
"Adversarial Machine Learning Attacks Against Video Anomaly Detection
  Systems","Furkan Mumcu, Keval Doshi, Yasin Yilmaz",2022-04-07T00:57:50Z,Adversarial Machine Learning,"  Anomaly detection in videos is an important computer vision problem with
various applications including automated video surveillance. Although
adversarial attacks on image understanding models have been heavily
investigated, there is not much work on adversarial machine learning targeting
video understanding models and no previous work which focuses on video anomaly
detection. To this end, we investigate an adversarial machine learning attack
against video anomaly detection systems, that can be implemented via an
easy-to-perform cyber-attack. Since surveillance cameras are usually connected
to the server running the anomaly detection model through a wireless network,
they are prone to cyber-attacks targeting the wireless connection. We
demonstrate how Wi-Fi deauthentication attack, a notoriously easy-to-perform
and effective denial-of-service (DoS) attack, can be utilized to generate
adversarial data for video anomaly detection systems. Specifically, we apply
several effects caused by the Wi-Fi deauthentication attack on video quality
(e.g., slow down, freeze, fast forward, low resolution) to the popular
benchmark datasets for video anomaly detection. Our experiments with several
state-of-the-art anomaly detection models show that the attackers can
significantly undermine the reliability of video anomaly detection systems by
causing frequent false alarms and hiding physical anomalies from the
surveillance system.
",0
Synthetic Dataset Generation for Adversarial Machine Learning Research,"Xiruo Liu, Shibani Singh, Cory Cornelius, Colin Busho, Mike Tan, Anindya Paul, Jason Martin",2022-07-21T19:14:44Z,Adversarial Machine Learning,"  Existing adversarial example research focuses on digitally inserted
perturbations on top of existing natural image datasets. This construction of
adversarial examples is not realistic because it may be difficult, or even
impossible, for an attacker to deploy such an attack in the real-world due to
sensing and environmental effects. To better understand adversarial examples
against cyber-physical systems, we propose approximating the real-world through
simulation. In this paper we describe our synthetic dataset generation tool
that enables scalable collection of such a synthetic dataset with realistic
adversarial examples. We use the CARLA simulator to collect such a dataset and
demonstrate simulated attacks that undergo the same environmental transforms
and processing as real-world images. Our tools have been used to collect
datasets to help evaluate the efficacy of adversarial examples, and can be
found at https://github.com/carla-simulator/carla/pull/4992.
",2
"Physics-Guided Adversarial Machine Learning for Aircraft Systems
  Simulation","Houssem Ben Braiek, Thomas Reid, Foutse Khomh",2022-09-07T19:23:45Z,Adversarial Machine Learning,"  In the context of aircraft system performance assessment, deep learning
technologies allow to quickly infer models from experimental measurements, with
less detailed system knowledge than usually required by physics-based modeling.
However, this inexpensive model development also comes with new challenges
regarding model trustworthiness. This work presents a novel approach,
physics-guided adversarial machine learning (ML), that improves the confidence
over the physics consistency of the model. The approach performs, first, a
physics-guided adversarial testing phase to search for test inputs revealing
behavioral system inconsistencies, while still falling within the range of
foreseeable operational conditions. Then, it proceeds with physics-informed
adversarial training to teach the model the system-related physics domain
foreknowledge through iteratively reducing the unwanted output deviations on
the previously-uncovered counterexamples. Empirical evaluation on two aircraft
system performance models shows the effectiveness of our adversarial ML
approach in exposing physical inconsistencies of both models and in improving
their propensity to be consistent with physics domain knowledge.
",0
"Game Theoretic Mixed Experts for Combinational Adversarial Machine
  Learning","Ethan Rathbun, Kaleel Mahmood, Sohaib Ahmad, Caiwen Ding, Marten van Dijk",2022-11-26T21:35:01Z,Other,"  Recent advances in adversarial machine learning have shown that defenses
considered to be robust are actually susceptible to adversarial attacks which
are specifically customized to target their weaknesses. These defenses include
Barrage of Random Transforms (BaRT), Friendly Adversarial Training (FAT), Trash
is Treasure (TiT) and ensemble models made up of Vision Transformers (ViTs),
Big Transfer models and Spiking Neural Networks (SNNs). We first conduct a
transferability analysis, to demonstrate the adversarial examples generated by
customized attacks on one defense, are not often misclassified by another
defense.
  This finding leads to two important questions. First, how can the low
transferability between defenses be utilized in a game theoretic framework to
improve the robustness? Second, how can an adversary within this framework
develop effective multi-model attacks? In this paper, we provide a
game-theoretic framework for ensemble adversarial attacks and defenses. Our
framework is called Game theoretic Mixed Experts (GaME). It is designed to find
the Mixed-Nash strategy for both a detector based and standard defender, when
facing an attacker employing compositional adversarial attacks. We further
propose three new attack algorithms, specifically designed to target defenses
with randomized transformations, multi-model voting schemes, and adversarial
detector architectures. These attacks serve to both strengthen defenses
generated by the GaME framework and verify their robustness against unforeseen
attacks. Overall, our framework and analyses advance the field of adversarial
machine learning by yielding new insights into compositional attack and defense
formulations.
",0
"A Mutation-based Text Generation for Adversarial Machine Learning
  Applications","Jesus Guerrero, Gongbo Liang, Izzat Alsmadi",2022-12-21T04:57:59Z,Adversarial Machine Learning,"  Many natural language related applications involve text generation, created
by humans or machines. While in many of those applications machines support
humans, yet in few others, (e.g. adversarial machine learning, social bots and
trolls) machines try to impersonate humans. In this scope, we proposed and
evaluated several mutation-based text generation approaches. Unlike
machine-based generated text, mutation-based generated text needs human text
samples as inputs. We showed examples of mutation operators but this work can
be extended in many aspects such as proposing new text-based mutation operators
based on the nature of the application.
",0
Chaotic Variational Auto encoder-based Adversarial Machine Learning,"Pavan Venkata Sainadh Reddy, Yelleti Vivek, Gopi Pranay, Vadlamani Ravi",2023-02-25T02:06:15Z,Adversarial Machine Learning,"  Machine Learning (ML) has become the new contrivance in almost every field.
This makes them a target of fraudsters by various adversary attacks, thereby
hindering the performance of ML models. Evasion and Data-Poison-based attacks
are well acclaimed, especially in finance, healthcare, etc. This motivated us
to propose a novel computationally less expensive attack mechanism based on the
adversarial sample generation by Variational Auto Encoder (VAE). It is well
known that Wavelet Neural Network (WNN) is considered computationally efficient
in solving image and audio processing, speech recognition, and time-series
forecasting. This paper proposed VAE-Deep-Wavelet Neural Network
(VAE-Deep-WNN), where Encoder and Decoder employ WNN networks. Further, we
proposed chaotic variants of both VAE with Multi-layer perceptron (MLP) and
Deep-WNN and named them C-VAE-MLP and C-VAE-Deep-WNN, respectively. Here, we
employed a Logistic map to generate random noise in the latent space. In this
paper, we performed VAE-based adversary sample generation and applied it to
various problems related to finance and cybersecurity domain-related problems
such as loan default, credit card fraud, and churn modelling, etc., We
performed both Evasion and Data-Poison attacks on Logistic Regression (LR) and
Decision Tree (DT) models. The results indicated that VAE-Deep-WNN outperformed
the rest in the majority of the datasets and models. However, its chaotic
variant C-VAE-Deep-WNN performed almost similarly to VAE-Deep-WNN in the
majority of the datasets.
",0
"Adversarial Machine Learning and Cybersecurity: Risks, Challenges, and
  Legal Implications","Micah Musser, Andrew Lohn, James X. Dempsey, Jonathan Spring, Ram Shankar Siva Kumar, Brenda Leong, Christina Liaghati, Cindy Martinez, Crystal D. Grant, Daniel Rohrer, Heather Frase, Jonathan Elliott, John Bansemer, Mikel Rodriguez, Mitt Regan, Rumman Chowdhury, Stefan Hermanek",2023-05-23T22:27:53Z,Adversarial Machine Learning,"  In July 2022, the Center for Security and Emerging Technology (CSET) at
Georgetown University and the Program on Geopolitics, Technology, and
Governance at the Stanford Cyber Policy Center convened a workshop of experts
to examine the relationship between vulnerabilities in artificial intelligence
systems and more traditional types of software vulnerabilities. Topics
discussed included the extent to which AI vulnerabilities can be handled under
standard cybersecurity processes, the barriers currently preventing the
accurate sharing of information about AI vulnerabilities, legal issues
associated with adversarial attacks on AI systems, and potential areas where
government support could improve AI vulnerability management and mitigation.
  This report is meant to accomplish two things. First, it provides a
high-level discussion of AI vulnerabilities, including the ways in which they
are disanalogous to other types of vulnerabilities, and the current state of
affairs regarding information sharing and legal oversight of AI
vulnerabilities. Second, it attempts to articulate broad recommendations as
endorsed by the majority of participants at the workshop.
",0
"Adversarial Machine Learning in Latent Representations of Neural
  Networks","Milin Zhang, Mohammad Abdi, Francesco Restuccia",2023-09-29T17:01:29Z,Adversarial Machine Learning,"  Distributed deep neural networks (DNNs) have been shown to reduce the
computational burden of mobile devices and decrease the end-to-end inference
latency in edge computing scenarios. While distributed DNNs have been studied,
to the best of our knowledge the resilience of distributed DNNs to adversarial
action still remains an open problem. In this paper, we fill the existing
research gap by rigorously analyzing the robustness of distributed DNNs against
adversarial action. We cast this problem in the context of information theory
and introduce two new measurements for distortion and robustness. Our
theoretical findings indicate that (i) assuming the same level of information
distortion, latent features are always more robust than input representations;
(ii) the adversarial robustness is jointly determined by the feature dimension
and the generalization capability of the DNN. To test our theoretical findings,
we perform extensive experimental analysis by considering 6 different DNN
architectures, 6 different approaches for distributed DNN and 10 different
adversarial attacks to the ImageNet-1K dataset. Our experimental results
support our theoretical findings by showing that the compressed latent
representations can reduce the success rate of adversarial attacks by 88% in
the best case and by 57% on the average compared to attacks to the input space.
",0
Adversarial Machine Learning-Enabled Anonymization of OpenWiFi Data,"Samhita Kuili, Kareem Dabbour, Irtiza Hasan, Andrea Herscovich, Burak Kantarci, Marcel Chenier, Melike Erol-Kantarci",2024-01-03T04:59:03Z,Adversarial Machine Learning,"  Data privacy and protection through anonymization is a critical issue for
network operators or data owners before it is forwarded for other possible use
of data. With the adoption of Artificial Intelligence (AI), data anonymization
augments the likelihood of covering up necessary sensitive information;
preventing data leakage and information loss. OpenWiFi networks are vulnerable
to any adversary who is trying to gain access or knowledge on traffic
regardless of the knowledge possessed by data owners. The odds for discovery of
actual traffic information is addressed by applied conditional tabular
generative adversarial network (CTGAN). CTGAN yields synthetic data; which
disguises as actual data but fostering hidden acute information of actual data.
In this paper, the similarity assessment of synthetic with actual data is
showcased in terms of clustering algorithms followed by a comparison of
performance for unsupervised cluster validation metrics. A well-known
algorithm, K-means outperforms other algorithms in terms of similarity
assessment of synthetic data over real data while achieving nearest scores
0.634, 23714.57, and 0.598 as Silhouette, Calinski and Harabasz and Davies
Bouldin metric respectively. On exploiting a comparative analysis in validation
scores among several algorithms, K-means forms the epitome of unsupervised
clustering algorithms ensuring explicit usage of synthetic data at the same
time a replacement for real data. Hence, the experimental results aim to show
the viability of using CTGAN-generated synthetic data in lieu of publishing
anonymized data to be utilized in various applications.
",0
"Advancing Hyperspectral Targeted Alpha Therapy with Adversarial Machine
  Learning","Jim Zhao, Greg Leadman",2024-03-11T20:36:35Z,Other,"  Targeted Alpha Therapy (TAT) has emerged as a promising modality for the
treatment of various malignancies, leveraging the high linear energy transfer
(LET) and short range of alpha particles to selectively irradiate cancer cells
while sparing healthy tissue. Monitoring and optimizing TAT delivery is crucial
for its clinical success. Hyper-spectral Single Photon Imaging (HSPI) presents
a novel and versatile approach for the real-time assessment of TAT in vivo.
This study introduces a comprehensive framework for HSPI in TAT, encompassing
spectral unmixing, quantitative dosimetry, and spatiotemporal visualization. We
report the development of a dedicated HSPI system tailored to alpha-emitting
radionuclides, enabling the simultaneous acquisition of high-resolution
spectral data and single-photon localization. Utilizing advanced spectral
unmixing algorithms, we demonstrate the discrimination of alpha-induced
scintillation from background fluorescence, facilitating precise alpha particle
tracking with adversarial machine learning.
",0
"Towards Sustainable SecureML: Quantifying Carbon Footprint of
  Adversarial Machine Learning","Syed Mhamudul Hasan, Abdur R. Shahid, Ahmed Imteaj",2024-03-27T21:02:15Z,Adversarial Machine Learning,"  The widespread adoption of machine learning (ML) across various industries
has raised sustainability concerns due to its substantial energy usage and
carbon emissions. This issue becomes more pressing in adversarial ML, which
focuses on enhancing model security against different network-based attacks.
Implementing defenses in ML systems often necessitates additional computational
resources and network security measures, exacerbating their environmental
impacts. In this paper, we pioneer the first investigation into adversarial
ML's carbon footprint, providing empirical evidence connecting greater model
robustness to higher emissions. Addressing the critical need to quantify this
trade-off, we introduce the Robustness Carbon Trade-off Index (RCTI). This
novel metric, inspired by economic elasticity principles, captures the
sensitivity of carbon emissions to changes in adversarial robustness. We
demonstrate the RCTI through an experiment involving evasion attacks, analyzing
the interplay between robustness against attacks, performance, and carbon
emissions.
",0
"A Neural Lambda Calculus: Neurosymbolic AI meets the foundations of
  computing and functional programming","João Flach, Luis C. Lamb",2023-04-18T20:30:16Z,Neurosymbolic AI,"  Over the last decades, deep neural networks based-models became the dominant
paradigm in machine learning. Further, the use of artificial neural networks in
symbolic learning has been seen as increasingly relevant recently. To study the
capabilities of neural networks in the symbolic AI domain, researchers have
explored the ability of deep neural networks to learn mathematical
constructions, such as addition and multiplication, logic inference, such as
theorem provers, and even the execution of computer programs. The latter is
known to be too complex a task for neural networks. Therefore, the results were
not always successful, and often required the introduction of biased elements
in the learning process, in addition to restricting the scope of possible
programs to be executed. In this work, we will analyze the ability of neural
networks to learn how to execute programs as a whole. To do so, we propose a
different approach. Instead of using an imperative programming language, with
complex structures, we use the Lambda Calculus ({\lambda}-Calculus), a simple,
but Turing-Complete mathematical formalism, which serves as the basis for
modern functional programming languages and is at the heart of computability
theory. We will introduce the use of integrated neural learning and lambda
calculi formalization. Finally, we explore execution of a program in
{\lambda}-Calculus is based on reductions, we will show that it is enough to
learn how to perform these reductions so that we can execute any program.
Keywords: Machine Learning, Lambda Calculus, Neurosymbolic AI, Neural Networks,
Transformer Model, Sequence-to-Sequence Models, Computational Models
",0
"Continual Reasoning: Non-Monotonic Reasoning in Neurosymbolic AI using
  Continual Learning","Sofoklis Kyriakopoulos, Artur S. d'Avila Garcez",2023-05-03T15:11:34Z,Neurosymbolic AI,"  Despite the extensive investment and impressive recent progress at reasoning
by similarity, deep learning continues to struggle with more complex forms of
reasoning such as non-monotonic and commonsense reasoning. Non-monotonicity is
a property of non-classical reasoning typically seen in commonsense reasoning,
whereby a reasoning system is allowed (differently from classical logic) to
jump to conclusions which may be retracted later, when new information becomes
available. Neural-symbolic systems such as Logic Tensor Networks (LTN) have
been shown to be effective at enabling deep neural networks to achieve
reasoning capabilities. In this paper, we show that by combining a
neural-symbolic system with methods from continual learning, LTN can obtain a
higher level of accuracy when addressing non-monotonic reasoning tasks.
Continual learning is added to LTNs by adopting a curriculum of learning from
knowledge and data with recall. We call this process Continual Reasoning, a new
methodology for the application of neural-symbolic systems to reasoning tasks.
Continual Reasoning is applied to a prototypical non-monotonic reasoning
problem as well as other reasoning examples. Experimentation is conducted to
compare and analyze the effects that different curriculum choices may have on
overall learning and reasoning results. Results indicate significant
improvement on the prototypical non-monotonic reasoning problem and a promising
outlook for the proposed approach on statistical relational learning examples.
",0
"NeSy4VRD: A Multifaceted Resource for Neurosymbolic AI Research using
  Knowledge Graphs in Visual Relationship Detection","David Herron, Ernesto Jiménez-Ruiz, Giacomo Tarroni, Tillman Weyde",2023-05-22T17:28:25Z,Neurosymbolic AI,"  NeSy4VRD is a multifaceted resource designed to support the development of
neurosymbolic AI (NeSy) research. NeSy4VRD re-establishes public access to the
images of the VRD dataset and couples them with an extensively revised,
quality-improved version of the VRD visual relationship annotations. Crucially,
NeSy4VRD provides a well-aligned, companion OWL ontology that describes the
dataset domain.It comes with open source infrastructure that provides
comprehensive support for extensibility of the annotations (which, in turn,
facilitates extensibility of the ontology), and open source code for loading
the annotations to/from a knowledge graph. We are contributing NeSy4VRD to the
computer vision, NeSy and Semantic Web communities to help foster more NeSy
research using OWL-based knowledge graphs.
",0
AI Alignment: A Comprehensive Survey,"Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, Fanzhi Zeng, Kwan Yee Ng, Juntao Dai, Xuehai Pan, Aidan O'Gara, Yingshan Lei, Hua Xu, Brian Tse, Jie Fu, Stephen McAleer, Yaodong Yang, Yizhou Wang, Song-Chun Zhu, Yike Guo, Wen Gao",2023-10-30T15:52:15Z,AI Alignment,"  AI alignment aims to make AI systems behave in line with human intentions and
values. As AI systems grow more capable, so do risks from misalignment. To
provide a comprehensive and up-to-date overview of the alignment field, in this
survey, we delve into the core concepts, methodology, and practice of
alignment. First, we identify four principles as the key objectives of AI
alignment: Robustness, Interpretability, Controllability, and Ethicality
(RICE). Guided by these four principles, we outline the landscape of current
alignment research and decompose them into two key components: forward
alignment and backward alignment. The former aims to make AI systems aligned
via alignment training, while the latter aims to gain evidence about the
systems' alignment and govern them appropriately to avoid exacerbating
misalignment risks. On forward alignment, we discuss techniques for learning
from feedback and learning under distribution shift. On backward alignment, we
discuss assurance techniques and governance practices.
  We also release and continually update the website (www.alignmentsurvey.com)
which features tutorials, collections of papers, blog posts, and other
resources.
",109
"Efficient Retrieval Augmented Generation from Unstructured Knowledge for
  Task-Oriented Dialog","David Thulke, Nico Daheim, Christian Dugast, Hermann Ney",2021-02-09T04:50:35Z,Retrieval Augmented Generation,"  This paper summarizes our work on the first track of the ninth Dialog System
Technology Challenge (DSTC 9), ""Beyond Domain APIs: Task-oriented
Conversational Modeling with Unstructured Knowledge Access"". The goal of the
task is to generate responses to user turns in a task-oriented dialog that
require knowledge from unstructured documents. The task is divided into three
subtasks: detection, selection and generation. In order to be compute
efficient, we formulate the selection problem in terms of hierarchical
classification steps. We achieve our best results with this model.
Alternatively, we employ siamese sequence embedding models, referred to as
Dense Knowledge Retrieval, to retrieve relevant documents. This method further
reduces the computation time by a factor of more than 100x at the cost of
degradation in R@1 of 5-6% compared to the first model. Then for either
approach, we use Retrieval Augmented Generation to generate responses based on
multiple selected snippets and we show how the method can be used to fine-tune
trained embeddings.
",0
"MuRAG: Multimodal Retrieval-Augmented Generator for Open Question
  Answering over Images and Text","Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, William W. Cohen",2022-10-06T13:58:03Z,RAG,"  While language Models store a massive amount of world knowledge implicitly in
their parameters, even very large models often fail to encode information about
rare entities and events, while incurring huge computational costs. Recently,
retrieval-augmented models, such as REALM, RAG, and RETRO, have incorporated
world knowledge into language generation by leveraging an external
non-parametric index and have demonstrated impressive performance with
constrained model sizes. However, these methods are restricted to retrieving
only textual knowledge, neglecting the ubiquitous amount of knowledge in other
modalities like images -- much of which contains information not covered by any
text. To address this limitation, we propose the first Multimodal
Retrieval-Augmented Transformer (MuRAG), which accesses an external
non-parametric multimodal memory to augment language generation. MuRAG is
pre-trained with a mixture of large-scale image-text and text-only corpora
using a joint contrastive and generative loss. We perform experiments on two
different datasets that require retrieving and reasoning over both images and
text to answer a given query: WebQA, and MultimodalQA. Our results show that
MuRAG achieves state-of-the-art accuracy, outperforming existing models by
10-20\% absolute on both datasets and under both distractor and full-wiki
settings.
",0
"Retrieval Augmented Generation and Representative Vector Summarization
  for large unstructured textual data in Medical Education","S. S. Manathunga, Y. A. Illangasekara",2023-08-01T12:04:50Z,Retrieval Augmented Generation,"  Large Language Models are increasingly being used for various tasks including
content generation and as chatbots. Despite their impressive performances in
general tasks, LLMs need to be aligned when applying for domain specific tasks
to mitigate the problems of hallucination and producing harmful answers.
Retrieval Augmented Generation (RAG) allows to easily attach and manipulate a
non-parametric knowledgebases to LLMs. Applications of RAG in the field of
medical education are discussed in this paper. A combined extractive and
abstractive summarization method for large unstructured textual data using
representative vectors is proposed.
",0
"Modeling Uncertainty and Using Post-fusion as Fallback Improves
  Retrieval Augmented Generation with LLMs","Ye Liu, Semih Yavuz, Rui Meng, Meghana Moorthy, Shafiq Joty, Caiming Xiong, Yingbo Zhou",2023-08-24T05:26:54Z,Retrieval Augmented Generation,"  The integration of retrieved passages and large language models (LLMs), such
as ChatGPTs, has significantly contributed to improving open-domain question
answering. However, there is still a lack of exploration regarding the optimal
approach for incorporating retrieved passages into the answer generation
process. This paper aims to fill this gap by investigating different methods of
combining retrieved passages with LLMs to enhance answer generation. We begin
by examining the limitations of a commonly-used concatenation approach.
Surprisingly, this approach often results in generating ""unknown"" outputs, even
when the correct document is among the top-k retrieved passages. To address
this issue, we explore four alternative strategies for integrating the
retrieved passages with the LLMs. These strategies include two single-round
methods that utilize chain-of-thought reasoning and two multi-round strategies
that incorporate feedback loops. Through comprehensive analyses and
experiments, we provide insightful observations on how to effectively leverage
retrieved passages to enhance the answer generation capability of LLMs.
",0
"Retrieval-augmented Generation to Improve Math Question-Answering:
  Trade-offs Between Groundedness and Human Preference","Zachary Levonian, Chenglu Li, Wangda Zhu, Anoushka Gade, Owen Henkel, Millie-Ellen Postle, Wanli Xing",2023-10-04T22:09:28Z,Other,"  For middle-school math students, interactive question-answering (QA) with
tutors is an effective way to learn. The flexibility and emergent capabilities
of generative large language models (LLMs) has led to a surge of interest in
automating portions of the tutoring process - including interactive QA to
support conceptual discussion of mathematical concepts. However, LLM responses
to math questions can be incorrect or mismatched to the educational context -
such as being misaligned with a school's curriculum. One potential solution is
retrieval-augmented generation (RAG), which involves incorporating a vetted
external knowledge source in the LLM prompt to increase response quality. In
this paper, we designed prompts that retrieve and use content from a
high-quality open-source math textbook to generate responses to real student
questions. We evaluate the efficacy of this RAG system for middle-school
algebra and geometry QA by administering a multi-condition survey, finding that
humans prefer responses generated using RAG, but not when responses are too
grounded in the textbook content. We argue that while RAG is able to improve
response quality, designers of math QA systems must consider trade-offs between
generating responses preferred by students and responses closely matched to
specific educational resources.
",0
"FloodBrain: Flood Disaster Reporting by Web-based Retrieval Augmented
  Generation with an LLM","Grace Colverd, Paul Darm, Leonard Silverberg, Noah Kasmanoff",2023-11-05T08:34:26Z,Other,"  Fast disaster impact reporting is crucial in planning humanitarian
assistance. Large Language Models (LLMs) are well known for their ability to
write coherent text and fulfill a variety of tasks relevant to impact
reporting, such as question answering or text summarization. However, LLMs are
constrained by the knowledge within their training data and are prone to
generating inaccurate, or ""hallucinated"", information. To address this, we
introduce a sophisticated pipeline embodied in our tool FloodBrain
(floodbrain.com), specialized in generating flood disaster impact reports by
extracting and curating information from the web. Our pipeline assimilates
information from web search results to produce detailed and accurate reports on
flood events. We test different LLMs as backbones in our tool and compare their
generated reports to human-written reports on different metrics. Similar to
other studies, we find a notable correlation between the scores assigned by
GPT-4 and the scores given by human evaluators when comparing our generated
reports to human-authored ones. Additionally, we conduct an ablation study to
test our single pipeline components and their relevancy for the final reports.
With our tool, we aim to advance the use of LLMs for disaster impact reporting
and reduce the time for coordination of humanitarian efforts in the wake of
flood disasters.
",0
"Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale Memory for
  Retrieval Augmented Generation",Eric Melz,2023-11-07T18:03:23Z,"Retrieval Augmented Generation, RAG","  Large Language Models (LLMs) are smart but forgetful. Recent studies, (e.g.,
(Bubeck et al., 2023)) on modern LLMs have shown that they are capable of
performing amazing tasks typically necessitating human-level intelligence.
However, unlike humans, frozen LLMs do not improve over time; they neither
acquire new knowledge nor learn from their successes or failures. Some
approaches to improving the intelligence of LLMs include fine-tuning models
based on problem-solving performance (Zelikman et al., 2022), and building
bigger and more sophisticated models (Bubeck et al., 2023). However, these
methods have the drawback of requiring substantial data and computational
resources to retrain existing models. In this paper, we explore the use of
Retrieval Augmented Generation, also known as RAG (Lewis et al., 2021) to
improve problem-solving performance. We propose ARM-RAG (Auxiliary Rationale
Memory for Retrieval Augmented Generation), a system that learns from its
successes without incurring high training costs. We demonstrate that the
storage and subsequent retrieval of reasoning chains have a positive influence
on performance in grade-school math problems.
",0
"KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI
  Integration using Retrieval-Augmented Generation","Suad Alshammari, Lama Basalelah, Walaa Abu Rukbah, Ali Alsuhibani, Dayanjan S. Wijesinghe",2023-11-07T19:27:28Z,Other,"  Academic researchers face challenges keeping up with exponentially growing
published findings in their field. Performing comprehensive literature reviews
to synthesize knowledge is time-consuming and labor-intensive using manual
approaches. Recent advances in artificial intelligence provide promising
solutions, yet many require coding expertise, limiting accessibility.
KNIMEZoBot represents an innovative integration of Zotero, OpenAI, and the
KNIME visual programming platform to automate literature review tasks for users
with no coding experience. By leveraging KNIME's intuitive graphical interface,
researchers can create workflows to search their Zotero libraries and utilize
OpenAI models to extract key information without coding. Users simply provide
API keys and configure settings through a user-friendly interface in a locally
stored copy of the workflow. KNIMEZoBot then allows asking natural language
questions via a chatbot and retrieves relevant passages from papers to generate
synthesized answers. This system has significant potential to expedite
literature reviews for researchers unfamiliar with coding by automating
retrieval and analysis of publications in personal Zotero libraries. KNIMEZoBot
demonstrates how thoughtfully designed AI tools can expand accessibility and
accelerate knowledge building across diverse research domains.
",0
"Large Language Models with Retrieval-Augmented Generation for Zero-Shot
  Disease Phenotyping","Will E. Thompson, David M. Vidmar, Jessica K. De Freitas, John M. Pfeifer, Brandon K. Fornwalt, Ruijun Chen, Gabriel Altay, Kabir Manghnani, Andrew C. Nelsen, Kellie Morland, Martin C. Stumpe, Riccardo Miotto",2023-12-11T15:45:27Z,Other,"  Identifying disease phenotypes from electronic health records (EHRs) is
critical for numerous secondary uses. Manually encoding physician knowledge
into rules is particularly challenging for rare diseases due to inadequate EHR
coding, necessitating review of clinical notes. Large language models (LLMs)
offer promise in text understanding but may not efficiently handle real-world
clinical documentation. We propose a zero-shot LLM-based method enriched by
retrieval-augmented generation and MapReduce, which pre-identifies
disease-related text snippets to be used in parallel as queries for the LLM to
establish diagnosis. We show that this method as applied to pulmonary
hypertension (PH), a rare disease characterized by elevated arterial pressures
in the lungs, significantly outperforms physician logic rules ($F_1$ score of
0.62 vs. 0.75). This method has the potential to enhance rare disease cohort
identification, expanding the scope of robust clinical research and care gap
identification.
",0
"NoMIRACL: Knowing When You Don't Know for Robust Multilingual
  Retrieval-Augmented Generation","Nandan Thakur, Luiz Bonifacio, Xinyu Zhang, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Boxing Chen, Mehdi Rezagholizadeh, Jimmy Lin",2023-12-18T17:18:04Z,Other,"  Retrieval-augmented generation (RAG) grounds large language model (LLM)
output by leveraging external knowledge sources to reduce factual
hallucinations. However, prior works lack a comprehensive evaluation of
different language families, making it challenging to evaluate LLM robustness
against errors in external retrieved knowledge. To overcome this, we establish
NoMIRACL, a human-annotated dataset for evaluating LLM robustness in RAG across
18 typologically diverse languages. NoMIRACL includes both a non-relevant and a
relevant subset. Queries in the non-relevant subset contain passages judged as
non-relevant, whereas queries in the relevant subset include at least a single
judged relevant passage. We measure LLM robustness using two metrics: (i)
hallucination rate, measuring model tendency to hallucinate an answer, when the
answer is not present in passages in the non-relevant subset, and (ii) error
rate, measuring model inaccuracy to recognize relevant passages in the relevant
subset. In our work, we measure robustness for a wide variety of
multilingual-focused LLMs and observe that most of the models struggle to
balance the two capacities. Models such as LLAMA-2, Orca-2, and FLAN-T5 observe
more than an 88% hallucination rate on the non-relevant subset, whereas,
Mistral overall hallucinates less, but can achieve up to a 74.9% error rate on
the relevant subset. Overall, GPT-4 is observed to provide the best tradeoff on
both subsets, highlighting future work necessary to improve LLM robustness.
",0
"Advancing TTP Analysis: Harnessing the Power of Large Language Models
  with Retrieval Augmented Generation","Reza Fayyazi, Rozhina Taghdimi, Shanchieh Jay Yang",2023-12-30T16:56:24Z,Retrieval Augmented Generation,"  Tactics, Techniques, and Procedures (TTPs) outline the methods attackers use
to exploit vulnerabilities. The interpretation of TTPs in the MITRE ATT&CK
framework can be challenging for cybersecurity practitioners due to presumed
expertise and complex dependencies. Meanwhile, advancements with Large Language
Models (LLMs) have led to recent surge in studies exploring its uses in
cybersecurity operations. It is, however, unclear how LLMs can be used in an
efficient and proper way to provide accurate responses for critical domains
such as cybersecurity. This leads us to investigate how to better use two types
of LLMs: small-scale encoder-only (e.g., RoBERTa) and larger decoder-only
(e.g., GPT-3.5) LLMs to comprehend and summarize TTPs with the intended
purposes (i.e., tactics) of a cyberattack procedure. This work studies and
compares the uses of supervised fine-tuning (SFT) of encoder-only LLMs vs.
Retrieval Augmented Generation (RAG) for decoder-only LLMs (without
fine-tuning). Both SFT and RAG techniques presumably enhance the LLMs with
relevant contexts for each cyberattack procedure. Our studies show decoder-only
LLMs with RAG achieves better performance than encoder-only models with SFT,
particularly when directly relevant context is extracted by RAG. The
decoder-only results could suffer low `Precision' while achieving high
`Recall'. Our findings further highlight a counter-intuitive observation that
more generic prompts tend to yield better predictions of cyberattack tactics
than those that are more specifically tailored.
",0
"Question-Answering Based Summarization of Electronic Health Records
  using Retrieval Augmented Generation","Walid Saba, Suzanne Wendelken, James. Shanahan",2024-01-03T00:09:34Z,Retrieval Augmented Generation,"  Summarization of electronic health records (EHRs) can substantially minimize
'screen time' for both patients as well as medical personnel. In recent years
summarization of EHRs have employed machine learning pipelines using state of
the art neural models. However, these models have produced less than adequate
results that are attributed to the difficulty of obtaining sufficient annotated
data for training. Moreover, the requirement to consider the entire content of
an EHR in summarization has resulted in poor performance due to the fact that
attention mechanisms in modern large language models (LLMs) adds a quadratic
complexity in terms of the size of the input. We propose here a method that
mitigates these shortcomings by combining semantic search, retrieval augmented
generation (RAG) and question-answering using the latest LLMs. In our approach
summarization is the extraction of answers to specific questions that are
deemed important by subject-matter experts (SMEs). Our approach is quite
efficient; requires minimal to no training; does not suffer from the
'hallucination' problem of LLMs; and it ensures diversity, since the summary
will not have repeated content but diverse answers to specific questions.
",0
"Concurrent Brainstorming & Hypothesis Satisfying: An Iterative Framework
  for Enhanced Retrieval-Augmented Generation (R2CBR3H-SR)",Arash Shahmansoori,2024-01-03T17:01:44Z,Other,"  Addressing the complexity of comprehensive information retrieval, this study
introduces an innovative, iterative retrieval-augmented generation system. Our
approach uniquely integrates a vector-space driven re-ranking mechanism with
concurrent brainstorming to expedite the retrieval of highly relevant
documents, thereby streamlining the generation of potential queries. This sets
the stage for our novel hybrid process, which synergistically combines
hypothesis formulation with satisfying decision-making strategy to determine
content adequacy, leveraging a chain of thought-based prompting technique. This
unified hypothesize-satisfied phase intelligently distills information to
ascertain whether user queries have been satisfactorily addressed. Upon
reaching this criterion, the system refines its output into a concise
representation, maximizing conceptual density with minimal verbosity. The
iterative nature of the workflow enhances process efficiency and accuracy.
Crucially, the concurrency within the brainstorming phase significantly
accelerates recursive operations, facilitating rapid convergence to solution
satisfaction. Compared to conventional methods, our system demonstrates a
marked improvement in computational time and cost-effectiveness. This research
advances the state-of-the-art in intelligent retrieval systems, setting a new
benchmark for resource-efficient information extraction and abstraction in
knowledge-intensive applications.
",0
"Prompt-RAG: Pioneering Vector Embedding-Free Retrieval-Augmented
  Generation in Niche Domains, Exemplified by Korean Medicine","Bongsu Kang, Jundong Kim, Tae-Rim Yun, Chang-Eop Kim",2024-01-20T14:59:43Z,RAG,"  We propose a natural language prompt-based retrieval augmented generation
(Prompt-RAG), a novel approach to enhance the performance of generative large
language models (LLMs) in niche domains. Conventional RAG methods mostly
require vector embeddings, yet the suitability of generic LLM-based embedding
representations for specialized domains remains uncertain. To explore and
exemplify this point, we compared vector embeddings from Korean Medicine (KM)
and Conventional Medicine (CM) documents, finding that KM document embeddings
correlated more with token overlaps and less with human-assessed document
relatedness, in contrast to CM embeddings. Prompt-RAG, distinct from
conventional RAG models, operates without the need for embedding vectors. Its
performance was assessed through a Question-Answering (QA) chatbot application,
where responses were evaluated for relevance, readability, and informativeness.
The results showed that Prompt-RAG outperformed existing models, including
ChatGPT and conventional vector embedding-based RAGs, in terms of relevance and
informativeness. Despite challenges like content structuring and response
latency, the advancements in LLMs are expected to encourage the use of
Prompt-RAG, making it a promising tool for other domains in need of RAG
methods.
",4
"UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for
  Personalized Dialogue Systems","Hongru Wang, Wenyu Huang, Yang Deng, Rui Wang, Zezhong Wang, Yufei Wang, Fei Mi, Jeff Z. Pan, Kam-Fai Wong",2024-01-24T06:50:20Z,RAG,"  Large Language Models (LLMs) has shown exceptional capabilities in many
natual language understanding and generation tasks. However, the
personalization issue still remains a much-coveted property, especially when it
comes to the multiple sources involved in the dialogue system. To better plan
and incorporate the use of multiple sources in generating personalized
response, we firstly decompose it into three sub-tasks: Knowledge Source
Selection, Knowledge Retrieval, and Response Generation. We then propose a
novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG)
Specifically, we unify these three sub-tasks with different formulations into
the same sequence-to-sequence paradigm during the training, to adaptively
retrieve evidences and evaluate the relevance on-demand using special tokens,
called acting tokens and evaluation tokens. Enabling language models to
generate acting tokens facilitates interaction with various knowledge sources,
allowing them to adapt their behavior to diverse task requirements. Meanwhile,
evaluation tokens gauge the relevance score between the dialogue context and
the retrieved evidence. In addition, we carefully design a self-refinement
mechanism to iteratively refine the generated response considering 1) the
consistency scores between the generated response and retrieved evidence; and
2) the relevance scores. Experiments on two personalized datasets (DuLeMon and
KBP) show that UniMS-RAG achieves state-of-the-art performance on the knowledge
source selection and response generation task with itself as a retriever in a
unified manner. Extensive analyses and discussions are provided for shedding
some new perspectives for personalized dialogue systems.
",0
"CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented
  Generation of Large Language Models","Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huanyong Liu, Tong Xu, Enhong Chen",2024-01-30T14:25:32Z,RAG,"  Retrieval-Augmented Generation (RAG) is a technique that enhances the
capabilities of large language models (LLMs) by incorporating external
knowledge sources. This method addresses common LLM limitations, including
outdated information and the tendency to produce inaccurate ""hallucinated""
content. However, the evaluation of RAG systems is challenging, as existing
benchmarks are limited in scope and diversity. Most of the current benchmarks
predominantly assess question-answering applications, overlooking the broader
spectrum of situations where RAG could prove advantageous. Moreover, they only
evaluate the performance of the LLM component of the RAG pipeline in the
experiments, and neglect the influence of the retrieval component and the
external knowledge database. To address these issues, this paper constructs a
large-scale and more comprehensive benchmark, and evaluates all the components
of RAG systems in various RAG application scenarios. Specifically, we have
categorized the range of RAG applications into four distinct types-Create,
Read, Update, and Delete (CRUD), each representing a unique use case. ""Create""
refers to scenarios requiring the generation of original, varied content.
""Read"" involves responding to intricate questions in knowledge-intensive
situations. ""Update"" focuses on revising and rectifying inaccuracies or
inconsistencies in pre-existing texts. ""Delete"" pertains to the task of
summarizing extensive texts into more concise forms. For each of these CRUD
categories, we have developed comprehensive datasets to evaluate the
performance of RAG systems. We also analyze the effects of various components
of the RAG system, such as the retriever, the context length, the knowledge
base construction, and the LLM. Finally, we provide useful insights for
optimizing the RAG technology for different scenarios.
",0
"Development and Testing of Retrieval Augmented Generation in Large
  Language Models -- A Case Study Report","YuHe Ke, Liyuan Jin, Kabilan Elangovan, Hairil Rizal Abdullah, Nan Liu, Alex Tiong Heng Sia, Chai Rick Soh, Joshua Yi Min Tung, Jasmine Chiat Ling Ong, Daniel Shu Wei Ting",2024-01-29T06:49:53Z,Retrieval Augmented Generation,"  Purpose: Large Language Models (LLMs) hold significant promise for medical
applications. Retrieval Augmented Generation (RAG) emerges as a promising
approach for customizing domain knowledge in LLMs. This case study presents the
development and evaluation of an LLM-RAG pipeline tailored for healthcare,
focusing specifically on preoperative medicine.
  Methods: We developed an LLM-RAG model using 35 preoperative guidelines and
tested it against human-generated responses, with a total of 1260 responses
evaluated. The RAG process involved converting clinical documents into text
using Python-based frameworks like LangChain and Llamaindex, and processing
these texts into chunks for embedding and retrieval. Vector storage techniques
and selected embedding models to optimize data retrieval, using Pinecone for
vector storage with a dimensionality of 1536 and cosine similarity for loss
metrics. Human-generated answers, provided by junior doctors, were used as a
comparison.
  Results: The LLM-RAG model generated answers within an average of 15-20
seconds, significantly faster than the 10 minutes typically required by humans.
Among the basic LLMs, GPT4.0 exhibited the best accuracy of 80.1%. This
accuracy was further increased to 91.4% when the model was enhanced with RAG.
Compared to the human-generated instructions, which had an accuracy of 86.3%,
the performance of the GPT4.0 RAG model demonstrated non-inferiority (p=0.610).
  Conclusions: In this case study, we demonstrated a LLM-RAG model for
healthcare implementation. The pipeline shows the advantages of grounded
knowledge, upgradability, and scalability as important aspects of healthcare
LLM deployment.
",0
"List-aware Reranking-Truncation Joint Model for Search and
  Retrieval-augmented Generation","Shicheng Xu, Liang Pang, Jun Xu, Huawei Shen, Xueqi Cheng",2024-02-05T06:52:53Z,Other,"  The results of information retrieval (IR) are usually presented in the form
of a ranked list of candidate documents, such as web search for humans and
retrieval-augmented generation for large language models (LLMs). List-aware
retrieval aims to capture the list-level contextual features to return a better
list, mainly including reranking and truncation. Reranking finely re-scores the
documents in the list. Truncation dynamically determines the cut-off point of
the ranked list to achieve the trade-off between overall relevance and avoiding
misinformation from irrelevant documents. Previous studies treat them as two
separate tasks and model them separately. However, the separation is not
optimal. First, it is hard to share the contextual information of the ranking
list between the two tasks. Second, the separate pipeline usually meets the
error accumulation problem, where the small error from the reranking stage can
largely affect the truncation stage. To solve these problems, we propose a
Reranking-Truncation joint model (GenRT) that can perform the two tasks
concurrently. GenRT integrates reranking and truncation via generative paradigm
based on encoder-decoder architecture. We also design the novel loss functions
for joint optimization to make the model learn both tasks. Sharing parameters
by the joint model is conducive to making full use of the common modeling
information of the two tasks. Besides, the two tasks are performed concurrently
and co-optimized to solve the error accumulation problem between separate
stages. Experiments on public learning-to-rank benchmarks and open-domain Q\&A
tasks show that our method achieves SOTA performance on both reranking and
truncation tasks for web search and retrieval-augmented LLMs.
",0
"Enhancing Textbook Question Answering Task with Large Language Models
  and Retrieval Augmented Generation","Hessa Abdulrahman Alawwad, Areej Alhothali, Usman Naseem, Ali Alkhathlan, Amani Jamal",2024-02-05T11:58:56Z,Retrieval Augmented Generation,"  Textbook question answering (TQA) is a challenging task in artificial
intelligence due to the complex nature of context and multimodal data. Although
previous research has significantly improved the task, there are still some
limitations including the models' weak reasoning and inability to capture
contextual information in the lengthy context. The introduction of large
language models (LLMs) has revolutionized the field of AI, however, directly
applying LLMs often leads to inaccurate answers. This paper proposes a
methodology that handle the out-of-domain scenario in TQA where concepts are
spread across different lessons by incorporating the retrieval augmented
generation (RAG) technique and utilize transfer learning to handle the long
context and enhance reasoning abilities. Through supervised fine-tuning of the
LLM model Llama-2 and the incorporation of RAG, our architecture outperforms
the baseline, achieving a 4.12% accuracy improvement on validation set and
9.84% on test set for non-diagram multiple-choice questions.
",0
"G-Retriever: Retrieval-Augmented Generation for Textual Graph
  Understanding and Question Answering","Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V. Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, Bryan Hooi",2024-02-12T13:13:04Z,Other,"  Given a graph with textual attributes, we enable users to `chat with their
graph': that is, to ask questions about the graph using a conversational
interface. In response to a user's questions, our method provides textual
replies and highlights the relevant parts of the graph. While existing works
integrate large language models (LLMs) and graph neural networks (GNNs) in
various ways, they mostly focus on either conventional graph tasks (such as
node, edge, and graph classification), or on answering simple graph queries on
small or synthetic graphs. In contrast, we develop a flexible
question-answering framework targeting real-world textual graphs, applicable to
multiple applications including scene graph understanding, common sense
reasoning, and knowledge graph reasoning. Toward this goal, we first develop a
Graph Question Answering (GraphQA) benchmark with data collected from different
tasks. Then, we propose our G-Retriever method, introducing the first
retrieval-augmented generation (RAG) approach for general textual graphs, which
can be fine-tuned to enhance graph understanding via soft prompting. To resist
hallucination and to allow for textual graphs that greatly exceed the LLM's
context window size, G-Retriever performs RAG over a graph by formulating this
task as a Prize-Collecting Steiner Tree optimization problem. Empirical
evaluations show that our method outperforms baselines on textual graph tasks
from multiple domains, scales well with larger graph sizes, and mitigates
hallucination.~\footnote{Our codes and datasets are available at:
\url{https://github.com/XiaoxinHe/G-Retriever}}
",0
"CyberMetric: A Benchmark Dataset based on Retrieval-Augmented Generation
  for Evaluating LLMs in Cybersecurity Knowledge","Norbert Tihanyi, Mohamed Amine Ferrag, Ridhi Jain, Tamas Bisztray, Merouane Debbah",2024-02-12T14:53:28Z,Other,"  Large Language Models (LLMs) are increasingly used across various domains,
from software development to cyber threat intelligence. Understanding all the
different fields of cybersecurity, which includes topics such as cryptography,
reverse engineering, and risk assessment, poses a challenge even for human
experts. To accurately test the general knowledge of LLMs in cybersecurity, the
research community needs a diverse, accurate, and up-to-date dataset. To
address this gap, we present CyberMetric-80, CyberMetric-500, CyberMetric-2000,
and CyberMetric-10000, which are multiple-choice Q&A benchmark datasets
comprising 80, 500, 2000, and 10,000 questions respectively. By utilizing
GPT-3.5 and Retrieval-Augmented Generation (RAG), we collected documents,
including NIST standards, research papers, publicly accessible books, RFCs, and
other publications in the cybersecurity domain, to generate questions, each
with four possible answers. The results underwent several rounds of error
checking and refinement. Human experts invested over 200 hours validating the
questions and solutions to ensure their accuracy and relevance, and to filter
out any questions unrelated to cybersecurity. We have evaluated and compared 25
state-of-the-art LLM models on the CyberMetric datasets. In addition to our
primary goal of evaluating LLMs, we involved 30 human participants to solve
CyberMetric-80 in a closed-book scenario. The results can serve as a reference
for comparing the general cybersecurity knowledge of humans and LLMs. The
findings revealed that GPT-4o, GPT-4-turbo, Mixtral-8x7B-Instruct,
Falcon-180B-Chat, and GEMINI-pro 1.0 were the best-performing LLMs.
Additionally, the top LLMs were more accurate than humans on CyberMetric-80,
although highly experienced human experts still outperformed small models such
as Llama-3-8B, Phi-2 or Gemma-7b.
",0
"PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented
  Generation of Large Language Models","Wei Zou, Runpeng Geng, Binghui Wang, Jinyuan Jia",2024-02-12T18:28:36Z,RAG,"  Large language models (LLMs) have achieved remarkable success due to their
exceptional generative capabilities. Despite their success, they also have
inherent limitations such as a lack of up-to-date knowledge and hallucination.
Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to
mitigate these limitations. The key idea of RAG is to ground the answer
generation of an LLM on external knowledge retrieved from a knowledge database.
Existing studies mainly focus on improving the accuracy or efficiency of RAG,
leaving its security largely unexplored. We aim to bridge the gap in this work.
We find that the knowledge database in a RAG system introduces a new and
practical attack surface. Based on this attack surface, we propose PoisonedRAG,
the first knowledge corruption attack to RAG, where an attacker could inject a
few malicious texts into the knowledge database of a RAG system to induce an
LLM to generate an attacker-chosen target answer for an attacker-chosen target
question. We formulate knowledge corruption attacks as an optimization problem,
whose solution is a set of malicious texts. Depending on the background
knowledge (e.g., black-box and white-box settings) of an attacker on a RAG
system, we propose two solutions to solve the optimization problem,
respectively. Our results show PoisonedRAG could achieve a 90% attack success
rate when injecting five malicious texts for each target question into a
knowledge database with millions of texts. We also evaluate several defenses
and our results show they are insufficient to defend against PoisonedRAG,
highlighting the need for new defenses.
",0
"FeB4RAG: Evaluating Federated Search in the Context of Retrieval
  Augmented Generation","Shuai Wang, Ekaterina Khramtsova, Shengyao Zhuang, Guido Zuccon",2024-02-19T07:06:52Z,RAG,"  Federated search systems aggregate results from multiple search engines,
selecting appropriate sources to enhance result quality and align with user
intent. With the increasing uptake of Retrieval-Augmented Generation (RAG)
pipelines, federated search can play a pivotal role in sourcing relevant
information across heterogeneous data sources to generate informed responses.
However, existing datasets, such as those developed in the past TREC FedWeb
tracks, predate the RAG paradigm shift and lack representation of modern
information retrieval challenges. To bridge this gap, we present FeB4RAG, a
novel dataset specifically designed for federated search within RAG frameworks.
This dataset, derived from 16 sub-collections of the widely used \beir
benchmarking collection, includes 790 information requests (akin to
conversational queries) tailored for chatbot applications, along with top
results returned by each resource and associated LLM-derived relevance
judgements. Additionally, to support the need for this collection, we
demonstrate the impact on response generation of a high quality federated
search system for RAG compared to a naive approach to federated search. We do
so by comparing answers generated through the RAG pipeline through a
qualitative side-by-side comparison. Our collection fosters and supports the
development and evaluation of new federated search methods, especially in the
context of RAG pipelines.
",0
"Causal Graph Discovery with Retrieval-Augmented Generation based Large
  Language Models","Yuzhe Zhang, Yipeng Zhang, Yidong Gan, Lina Yao, Chen Wang",2024-02-23T13:02:10Z,Other,"  Causal graph recovery is traditionally done using statistical
estimation-based methods or based on individual's knowledge about variables of
interests. They often suffer from data collection biases and limitations of
individuals' knowledge. The advance of large language models (LLMs) provides
opportunities to address these problems. We propose a novel method that
leverages LLMs to deduce causal relationships in general causal graph recovery
tasks. This method leverages knowledge compressed in LLMs and knowledge LLMs
extracted from scientific publication database as well as experiment data about
factors of interest to achieve this goal. Our method gives a prompting strategy
to extract associational relationships among those factors and a mechanism to
perform causality verification for these associations. Comparing to other
LLM-based methods that directly instruct LLMs to do the highly complex causal
reasoning, our method shows clear advantage on causal graph quality on
benchmark datasets. More importantly, as causality among some factors may
change as new research results emerge, our method show sensitivity to new
evidence in the literature and can provide useful information for updating
causal graphs accordingly.
",5
"RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for
  Short-form Open-Domain Question Answering","Zihan Zhang, Meng Fang, Ling Chen",2024-02-26T09:59:04Z,Other,"  Adaptive retrieval-augmented generation (ARAG) aims to dynamically determine
the necessity of retrieval for queries instead of retrieving indiscriminately
to enhance the efficiency and relevance of the sourced information. However,
previous works largely overlook the evaluation of ARAG approaches, leading to
their effectiveness being understudied. This work presents a benchmark,
RetrievalQA, comprising 1,271 short-form questions covering new world and
long-tail knowledge. The knowledge necessary to answer the questions is absent
from LLMs; therefore, external information must be retrieved to answer
correctly. This makes RetrievalQA a suitable testbed to evaluate existing ARAG
methods. We observe that calibration-based methods heavily rely on threshold
tuning, while vanilla prompting is inadequate for guiding LLMs to make reliable
retrieval decisions. Based on our findings, we propose Time-Aware Adaptive
Retrieval (TA-ARE), a simple yet effective method that helps LLMs assess the
necessity of retrieval without calibration or additional training. The dataset
and code will be available at https://github.com/hyintell/RetrievalQA
",0
"Follow My Instruction and Spill the Beans: Scalable Data Extraction from
  Retrieval-Augmented Generation Systems","Zhenting Qi, Hanlin Zhang, Eric Xing, Sham Kakade, Himabindu Lakkaraju",2024-02-27T19:08:05Z,Other,"  Retrieval-Augmented Generation (RAG) improves pre-trained models by
incorporating external knowledge at test time to enable customized adaptation.
We study the risk of datastore leakage in Retrieval-In-Context RAG Language
Models (LMs). We show that an adversary can exploit LMs' instruction-following
capabilities to easily extract text data verbatim from the datastore of RAG
systems built with instruction-tuned LMs via prompt injection. The
vulnerability exists for a wide range of modern LMs that span Llama2,
Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the
exploitability exacerbates as the model size scales up. We also study multiple
effects of RAG setup on the extractability of data, indicating that following
unexpected instructions to regurgitate data can be an outcome of failure in
effectively utilizing contexts for modern LMs, and further show that such
vulnerability can be greatly mitigated by position bias elimination strategies.
Extending our study to production RAG models GPTs, we design an attack that can
cause datastore leakage with a 100% success rate on 25 randomly selected
customized GPTs with at most 2 queries, and we extract text data verbatim at a
rate of 41% from a book of 77,000 words and 3% from a corpus of 1,569,000 words
by prompting the GPTs with only 100 queries generated by themselves.
",0
"Unsupervised Information Refinement Training of Large Language Models
  for Retrieval-Augmented Generation","Shicheng Xu, Liang Pang, Mo Yu, Fandong Meng, Huawei Shen, Xueqi Cheng, Jie Zhou",2024-02-28T08:24:38Z,Other,"  Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
incorporating additional information from retrieval. However, studies have
shown that LLMs still face challenges in effectively using the retrieved
information, even ignoring it or being misled by it. The key reason is that the
training of LLMs does not clearly make LLMs learn how to utilize input
retrieved texts with varied quality. In this paper, we propose a novel
perspective that considers the role of LLMs in RAG as ``Information Refiner'',
which means that regardless of correctness, completeness, or usefulness of
retrieved texts, LLMs can consistently integrate knowledge within the retrieved
texts and model parameters to generate the texts that are more concise,
accurate, and complete than the retrieved texts. To this end, we propose an
information refinement training method named InFO-RAG that optimizes LLMs for
RAG in an unsupervised manner. InFO-RAG is low-cost and general across various
tasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse
tasks including Question Answering, Slot-Filling, Language Modeling, Dialogue,
and Code Generation show that InFO-RAG improves the performance of LLaMA2 by an
average of 9.39\% relative points. InFO-RAG also shows advantages in in-context
learning and robustness of RAG.
",0
"Retrieval Augmented Generation Systems: Automatic Dataset Creation,
  Evaluation and Boolean Agent Setup","Tristan Kenneweg, Philip Kenneweg, Barbara Hammer",2024-02-26T12:56:17Z,Retrieval Augmented Generation,"  Retrieval Augmented Generation (RAG) systems have seen huge popularity in
augmenting Large-Language Model (LLM) outputs with domain specific and time
sensitive data. Very recently a shift is happening from simple RAG setups that
query a vector database for additional information with every user input to
more sophisticated forms of RAG. However, different concrete approaches compete
on mostly anecdotal evidence at the moment. In this paper we present a rigorous
dataset creation and evaluation workflow to quantitatively compare different
RAG strategies. We use a dataset created this way for the development and
evaluation of a boolean agent RAG setup: A system in which a LLM can decide
whether to query a vector database or not, thus saving tokens on questions that
can be answered with internal knowledge. We publish our code and generated
dataset online.
",0
"Detecting Hallucination and Coverage Errors in Retrieval Augmented
  Generation for Controversial Topics","Tyler A. Chang, Katrin Tomanek, Jessica Hoffmann, Nithum Thain, Erin van Liemt, Kathleen Meier-Hellstern, Lucas Dixon",2024-03-13T18:47:00Z,RAG,"  We explore a strategy to handle controversial topics in LLM-based chatbots
based on Wikipedia's Neutral Point of View (NPOV) principle: acknowledge the
absence of a single true answer and surface multiple perspectives. We frame
this as retrieval augmented generation, where perspectives are retrieved from a
knowledge base and the LLM is tasked with generating a fluent and faithful
response from the given perspectives. As a starting point, we use a
deterministic retrieval system and then focus on common LLM failure modes that
arise during this approach to text generation, namely hallucination and
coverage errors. We propose and evaluate three methods to detect such errors
based on (1) word-overlap, (2) salience, and (3) LLM-based classifiers. Our
results demonstrate that LLM-based classifiers, even when trained only on
synthetic errors, achieve high error detection performance, with ROC AUC scores
of 95.3% for hallucination and 90.5% for coverage error detection on
unambiguous error cases. We show that when no training data is available, our
other methods still yield good results on hallucination (84.0%) and coverage
error (85.2%) detection.
",0
"DRAGIN: Dynamic Retrieval Augmented Generation based on the Information
  Needs of Large Language Models","Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, Yiqun Liu",2024-03-15T07:45:37Z,"Retrieval Augmented Generation, RAG","  Dynamic retrieval augmented generation (RAG) paradigm actively decides when
and what to retrieve during the text generation process of Large Language
Models (LLMs). There are two key elements of this paradigm: identifying the
optimal moment to activate the retrieval module (deciding when to retrieve) and
crafting the appropriate query once retrieval is triggered (determining what to
retrieve). However, current dynamic RAG methods fall short in both aspects.
Firstly, the strategies for deciding when to retrieve often rely on static
rules. Moreover, the strategies for deciding what to retrieve typically limit
themselves to the LLM's most recent sentence or the last few tokens, while the
LLM's real-time information needs may span across the entire context. To
overcome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic
Retrieval Augmented Generation based on the real-time Information Needs of
LLMs. Our framework is specifically designed to make decisions on when and what
to retrieve based on the LLM's real-time information needs during the text
generation process. We evaluate DRAGIN along with existing methods
comprehensively over 4 knowledge-intensive generation datasets. Experimental
results show that DRAGIN achieves superior performance on all tasks,
demonstrating the effectiveness of our method. We have open-sourced all the
code, data, and models in GitHub: https://github.com/oneal2000/DRAGIN/tree/main
",5
"Img2Loc: Revisiting Image Geolocalization using Multi-modality
  Foundation Models and Image-based Retrieval-Augmented Generation","Zhongliang Zhou, Jielu Zhang, Zihan Guan, Mengxuan Hu, Ni Lao, Lan Mu, Sheng Li, Gengchen Mai",2024-03-28T17:07:02Z,Other,"  Geolocating precise locations from images presents a challenging problem in
computer vision and information retrieval.Traditional methods typically employ
either classification, which dividing the Earth surface into grid cells and
classifying images accordingly, or retrieval, which identifying locations by
matching images with a database of image-location pairs. However,
classification-based approaches are limited by the cell size and cannot yield
precise predictions, while retrieval-based systems usually suffer from poor
search quality and inadequate coverage of the global landscape at varied scale
and aggregation levels. To overcome these drawbacks, we present Img2Loc, a
novel system that redefines image geolocalization as a text generation task.
This is achieved using cutting-edge large multi-modality models like GPT4V or
LLaVA with retrieval augmented generation. Img2Loc first employs CLIP-based
representations to generate an image-based coordinate query database. It then
uniquely combines query results with images itself, forming elaborate prompts
customized for LMMs. When tested on benchmark datasets such as Im2GPS3k and
YFCC4k, Img2Loc not only surpasses the performance of previous state-of-the-art
models but does so without any model training.
",0
Ethics and Responsible AI Deployment,"Petar Radanliev, Omar Santos",2023-11-12T13:32:46Z,Responsible AI,"  As Artificial Intelligence (AI) becomes more prevalent, protecting personal
privacy is a critical ethical issue that must be addressed. This article
explores the need for ethical AI systems that safeguard individual privacy
while complying with ethical standards. By taking a multidisciplinary approach,
the research examines innovative algorithmic techniques such as differential
privacy, homomorphic encryption, federated learning, international regulatory
frameworks, and ethical guidelines. The study concludes that these algorithms
effectively enhance privacy protection while balancing the utility of AI with
the need to protect personal data. The article emphasises the importance of a
comprehensive approach that combines technological innovation with ethical and
regulatory strategies to harness the power of AI in a way that respects and
protects individual privacy.
",7
Explainable AI in Credit Risk Management,"Branka Hadji Misheva, Joerg Osterrieder, Ali Hirsa, Onkar Kulkarni, Stephen Fung Lin",2021-03-01T12:23:20Z,Explainable AI,"  Artificial Intelligence (AI) has created the single biggest technology
revolution the world has ever seen. For the finance sector, it provides great
opportunities to enhance customer experience, democratize financial services,
ensure consumer protection and significantly improve risk management. While it
is easier than ever to run state-of-the-art machine learning models, designing
and implementing systems that support real-world finance applications have been
challenging. In large part because they lack transparency and explainability
which are important factors in establishing reliable technology and the
research on this topic with a specific focus on applications in credit risk
management. In this paper, we implement two advanced post-hoc model agnostic
explainability techniques called Local Interpretable Model Agnostic
Explanations (LIME) and SHapley Additive exPlanations (SHAP) to machine
learning (ML)-based credit scoring models applied to the open-access data set
offered by the US-based P2P Lending Platform, Lending Club. Specifically, we
use LIME to explain instances locally and SHAP to get both local and global
explanations. We discuss the results in detail and present multiple comparison
scenarios by using various kernels available for explaining graphs generated
using SHAP values. We also discuss the practical challenges associated with the
implementation of these state-of-art eXplainabale AI (XAI) methods and document
them for future reference. We have made an effort to document every technical
aspect of this research, while at the same time providing a general summary of
the conclusions.
",49
"Explainable AI, but explainable to whom?","Julie Gerlings, Millie Søndergaard Jensen, Arisa Shollo",2021-06-10T07:47:33Z,Explainable AI,"  Advances in AI technologies have resulted in superior levels of AI-based
model performance. However, this has also led to a greater degree of model
complexity, resulting in 'black box' models. In response to the AI black box
problem, the field of explainable AI (xAI) has emerged with the aim of
providing explanations catered to human understanding, trust, and transparency.
Yet, we still have a limited understanding of how xAI addresses the need for
explainable AI in the context of healthcare. Our research explores the
differing explanation needs amongst stakeholders during the development of an
AI-system for classifying COVID-19 patients for the ICU. We demonstrate that
there is a constellation of stakeholders who have different explanation needs,
not just the 'user'. Further, the findings demonstrate how the need for xAI
emerges through concerns associated with specific stakeholder groups i.e., the
development team, subject matter experts, decision makers, and the audience.
Our findings contribute to the expansion of xAI by highlighting that different
stakeholders have different explanation needs. From a practical perspective,
the study provides insights on how AI systems can be adjusted to support
different stakeholders needs, ensuring better implementation and operation in a
healthcare context.
",0
Explainable AI for Natural Adversarial Images,"Tomas Folke, ZhaoBin Li, Ravi B. Sojitra, Scott Cheng-Hsin Yang, Patrick Shafto",2021-06-16T20:19:04Z,Explainable AI,"  Adversarial images highlight how vulnerable modern image classifiers are to
perturbations outside of their training set. Human oversight might mitigate
this weakness, but depends on humans understanding the AI well enough to
predict when it is likely to make a mistake. In previous work we have found
that humans tend to assume that the AI's decision process mirrors their own.
Here we evaluate if methods from explainable AI can disrupt this assumption to
help participants predict AI classifications for adversarial and standard
images. We find that both saliency maps and examples facilitate catching AI
errors, but their effects are not additive, and saliency maps are more
effective than examples.
",3
Exploring deterministic frequency deviations with explainable AI,"Johannes Kruse, Benjamin Schäfer, Dirk Witthaut",2021-06-14T08:30:33Z,Explainable AI,"  Deterministic frequency deviations (DFDs) critically affect power grid
frequency quality and power system stability. A better understanding of these
events is urgently needed as frequency deviations have been growing in the
European grid in recent years. DFDs are partially explained by the rapid
adjustment of power generation following the intervals of electricity trading,
but this intuitive picture fails especially before and around noonday. In this
article, we provide a detailed analysis of DFDs and their relation to external
features using methods from explainable Artificial Intelligence. We establish a
machine learning model that well describes the daily cycle of DFDs and
elucidate key interdependencies using SHapley Additive exPlanations (SHAP).
Thereby, we identify solar ramps as critical to explain patterns in the Rate of
Change of Frequency (RoCoF).
",0
Explainable AI: current status and future directions,"Prashant Gohel, Priyanka Singh, Manoranjan Mohanty",2021-07-12T08:42:19Z,Explainable AI,"  Explainable Artificial Intelligence (XAI) is an emerging area of research in
the field of Artificial Intelligence (AI). XAI can explain how AI obtained a
particular solution (e.g., classification or object detection) and can also
answer other ""wh"" questions. This explainability is not possible in traditional
AI. Explainability is essential for critical applications, such as defense,
health care, law and order, and autonomous driving vehicles, etc, where the
know-how is required for trust and transparency. A number of XAI techniques so
far have been purposed for such applications. This paper provides an overview
of these techniques from a multimedia (i.e., text, image, audio, and video)
point of view. The advantages and shortcomings of these techniques have been
discussed, and pointers to some future directions have also been provided.
",0
Knowledge-intensive Language Understanding for Explainable AI,"Amit Sheth, Manas Gaur, Kaushik Roy, Keyur Faldu",2021-08-02T21:12:30Z,Explainable AI,"  AI systems have seen significant adoption in various domains. At the same
time, further adoption in some domains is hindered by inability to fully trust
an AI system that it will not harm a human. Besides the concerns for fairness,
privacy, transparency, and explainability are key to developing trusts in AI
systems. As stated in describing trustworthy AI ""Trust comes through
understanding. How AI-led decisions are made and what determining factors were
included are crucial to understand."" The subarea of explaining AI systems has
come to be known as XAI. Multiple aspects of an AI system can be explained;
these include biases that the data might have, lack of data points in a
particular region of the example space, fairness of gathering the data, feature
importances, etc. However, besides these, it is critical to have human-centered
explanations that are directly related to decision-making similar to how a
domain expert makes decisions based on ""domain knowledge,"" that also include
well-established, peer-validated explicit guidelines. To understand and
validate an AI system's outcomes (such as classification, recommendations,
predictions), that lead to developing trust in the AI system, it is necessary
to involve explicit domain knowledge that humans understand and use.
",0
Toward Explainable AI for Regression Models,"Simon Letzgus, Patrick Wagner, Jonas Lederer, Wojciech Samek, Klaus-Robert Müller, Gregoire Montavon",2021-12-21T18:09:42Z,Explainable AI,"  In addition to the impressive predictive power of machine learning (ML)
models, more recently, explanation methods have emerged that enable an
interpretation of complex non-linear learning models such as deep neural
networks. Gaining a better understanding is especially important e.g. for
safety-critical ML applications or medical diagnostics etc. While such
Explainable AI (XAI) techniques have reached significant popularity for
classifiers, so far little attention has been devoted to XAI for regression
models (XAIR). In this review, we clarify the fundamental conceptual
differences of XAI for regression and classification tasks, establish novel
theoretical insights and analysis for XAIR, provide demonstrations of XAIR on
genuine practical regression problems, and finally discuss the challenges
remaining for the field.
",0
Cryptocurrency Valuation: An Explainable AI Approach,"Yulin Liu, Luyao Zhang",2022-01-30T19:01:23Z,Explainable AI,"  Currently, there are no convincing proxies for the fundamentals of
cryptocurrency assets. We propose a new market-to-fundamental ratio, the
price-to-utility (PU) ratio, utilizing unique blockchain accounting methods. We
then proxy various existing fundamental-to-market ratios by Bitcoin historical
data and find they have little predictive power for short-term bitcoin returns.
However, PU ratio effectively predicts long-term bitcoin returns than
alternative methods. Furthermore, we verify the explainability of PU ratio
using machine learning. Finally, we present an automated trading strategy
advised by the PU ratio that outperforms the conventional buy-and-hold and
market-timing strategies. Our research contributes to explainable AI in finance
from three facets: First, our market-to-fundamental ratio is based on classic
monetary theory and the unique UTXO model of Bitcoin accounting rather than ad
hoc; Second, the empirical evidence testifies the buy-low and sell-high
implications of the ratio; Finally, we distribute the trading algorithms as
open-source software via Python Package Index for future research, which is
exceptional in finance research.
",0
Explainable AI through the Learning of Arguments,"Jonas Bei, David Pomerenke, Lukas Schreiner, Sepideh Sharbaf, Pieter Collins, Nico Roos",2022-02-01T12:52:30Z,Explainable AI,"  Learning arguments is highly relevant to the field of explainable artificial
intelligence. It is a family of symbolic machine learning techniques that is
particularly human-interpretable. These techniques learn a set of arguments as
an intermediate representation. Arguments are small rules with exceptions that
can be chained to larger arguments for making predictions or decisions. We
investigate the learning of arguments, specifically the learning of arguments
from a 'case model' proposed by Verheij [34]. The case model in Verheij's
approach are cases or scenarios in a legal setting. The number of cases in a
case model are relatively low. Here, we investigate whether Verheij's approach
can be used for learning arguments from other types of data sets with a much
larger number of instances. We compare the learning of arguments from a case
model with the HeRO algorithm [15] and learning a decision tree.
",0
Deep Learning Reproducibility and Explainable AI (XAI),"A. -M. Leventi-Peetz, T. Östreich",2022-02-23T12:06:20Z,Explainable AI,"  The nondeterminism of Deep Learning (DL) training algorithms and its
influence on the explainability of neural network (NN) models are investigated
in this work with the help of image classification examples. To discuss the
issue, two convolutional neural networks (CNN) have been trained and their
results compared. The comparison serves the exploration of the feasibility of
creating deterministic, robust DL models and deterministic explainable
artificial intelligence (XAI) in practice. Successes and limitation of all here
carried out efforts are described in detail. The source code of the attained
deterministic models has been listed in this work. Reproducibility is indexed
as a development-phase-component of the Model Governance Framework, proposed by
the EU within their excellence in AI approach. Furthermore, reproducibility is
a requirement for establishing causality for the interpretation of model
results and building of trust towards the overwhelming expansion of AI systems
applications. Problems that have to be solved on the way to reproducibility and
ways to deal with some of them, are examined in this work.
",0
Explainable AI via Learning to Optimize,"Howard Heaton, Samy Wu Fung",2022-04-29T15:57:03Z,Explainable AI,"  Indecipherable black boxes are common in machine learning (ML), but
applications increasingly require explainable artificial intelligence (XAI).
The core of XAI is to establish transparent and interpretable data-driven
algorithms. This work provides concrete tools for XAI in situations where prior
knowledge must be encoded and untrustworthy inferences flagged. We use the
""learn to optimize"" (L2O) methodology wherein each inference solves a
data-driven optimization problem. Our L2O models are straightforward to
implement, directly encode prior knowledge, and yield theoretical guarantees
(e.g. satisfaction of constraints). We also propose use of interpretable
certificates to verify whether model inferences are trustworthy. Numerical
examples are provided in the applications of dictionary-based signal recovery,
CT imaging, and arbitrage trading of cryptoassets. Code and additional
documentation can be found at https://xai-l2o.research.typal.academy.
",12
OmniXAI: A Library for Explainable AI,"Wenzhuo Yang, Hung Le, Tanmay Laud, Silvio Savarese, Steven C. H. Hoi",2022-06-01T11:35:37Z,Explainable AI,"  We introduce OmniXAI (short for Omni eXplainable AI), an open-source Python
library of eXplainable AI (XAI), which offers omni-way explainable AI
capabilities and various interpretable machine learning techniques to address
the pain points of understanding and interpreting the decisions made by machine
learning (ML) in practice. OmniXAI aims to be a one-stop comprehensive library
that makes explainable AI easy for data scientists, ML researchers and
practitioners who need explanation for various types of data, models and
explanation methods at different stages of ML process (data exploration,
feature engineering, model development, evaluation, and decision-making, etc).
In particular, our library includes a rich family of explanation methods
integrated in a unified interface, which supports multiple data types (tabular
data, images, texts, time-series), multiple types of ML models (traditional ML
in Scikit-learn and deep learning models in PyTorch/TensorFlow), and a range of
diverse explanation methods including ""model-specific"" and ""model-agnostic""
ones (such as feature-attribution explanation, counterfactual explanation,
gradient-based explanation, etc). For practitioners, the library provides an
easy-to-use unified interface to generate the explanations for their
applications by only writing a few lines of codes, and also a GUI dashboard for
visualization of different explanations for more insights about decisions. In
this technical report, we present OmniXAI's design principles, system
architectures, and major functionalities, and also demonstrate several example
use cases across different types of data, tasks, and models.
",0
Explainable AI for High Energy Physics,"Mark S. Neubauer, Avik Roy",2022-06-14T06:46:15Z,Explainable AI,"  Neural Networks are ubiquitous in high energy physics research. However,
these highly nonlinear parameterized functions are treated as \textit{black
boxes}- whose inner workings to convey information and build the desired
input-output relationship are often intractable. Explainable AI (xAI) methods
can be useful in determining a neural model's relationship with data toward
making it \textit{interpretable} by establishing a quantitative and tractable
relationship between the input and the model's output. In this letter of
interest, we explore the potential of using xAI methods in the context of
problems in high energy physics.
",8
EMaP: Explainable AI with Manifold-based Perturbations,"Minh N. Vu, Huy Q. Mai, My T. Thai",2022-09-18T02:43:50Z,Explainable AI,"  In the last few years, many explanation methods based on the perturbations of
input data have been introduced to improve our understanding of decisions made
by black-box models. The goal of this work is to introduce a novel perturbation
scheme so that more faithful and robust explanations can be obtained. Our study
focuses on the impact of perturbing directions on the data topology. We show
that perturbing along the orthogonal directions of the input manifold better
preserves the data topology, both in the worst-case analysis of the discrete
Gromov-Hausdorff distance and in the average-case analysis via persistent
homology. From those results, we introduce EMaP algorithm, realizing the
orthogonal perturbation scheme. Our experiments show that EMaP not only
improves the explainers' performance but also helps them overcome a
recently-developed attack against perturbation-based methods.
",0
eXplainable AI for Quantum Machine Learning,"Patrick Steinmüller, Tobias Schulz, Ferdinand Graf, Daniel Herr",2022-11-02T19:20:56Z,Explainable AI,"  Parametrized Quantum Circuits (PQCs) enable a novel method for machine
learning (ML). However, from a computational point of view they present a
challenge to existing eXplainable AI (xAI) methods. On the one hand,
measurements on quantum circuits introduce probabilistic errors which impact
the convergence of these methods. On the other hand, the phase space of a
quantum circuit expands exponentially with the number of qubits, complicating
efforts to execute xAI methods in polynomial time. In this paper we will
discuss the performance of established xAI methods, such as Baseline SHAP and
Integrated Gradients. Using the internal mechanics of PQCs we study ways to
speed up their computation.
",0
Trends in Explainable AI (XAI) Literature,Alon Jacovi,2023-01-13T08:36:56Z,Explainable AI,"  The XAI literature is decentralized, both in terminology and in publication
venues, but recent years saw the community converge around keywords that make
it possible to more reliably discover papers automatically. We use keyword
search using the SemanticScholar API and manual curation to collect a
well-formatted and reasonably comprehensive set of 5199 XAI papers, available
at https://github.com/alonjacovi/XAI-Scholar . We use this collection to
clarify and visualize trends about the size and scope of the literature,
citation trends, cross-field trends, and collaboration trends. Overall, XAI is
becoming increasingly multidisciplinary, with relative growth in papers
belonging to increasingly diverse (non-CS) scientific fields, increasing
cross-field collaborative authorship, increasing cross-field citation activity.
The collection can additionally be used as a paper discovery engine, by
retrieving XAI literature which is cited according to specific constraints (for
example, papers that are influential outside of their field, or influential to
non-XAI research).
",0
ExplainableFold: Understanding AlphaFold Prediction with Explainable AI,"Juntao Tan, Yongfeng Zhang",2023-01-27T15:06:03Z,Explainable AI,"  This paper presents ExplainableFold, an explainable AI framework for protein
structure prediction. Despite the success of AI-based methods such as AlphaFold
in this field, the underlying reasons for their predictions remain unclear due
to the black-box nature of deep learning models. To address this, we propose a
counterfactual learning framework inspired by biological principles to generate
counterfactual explanations for protein structure prediction, enabling a
dry-lab experimentation approach. Our experimental results demonstrate the
ability of ExplainableFold to generate high-quality explanations for
AlphaFold's predictions, providing near-experimental understanding of the
effects of amino acids on 3D protein structure. This framework has the
potential to facilitate a deeper understanding of protein structures.
",3
Deep-BIAS: Detecting Structural Bias using Explainable AI,"Bas van Stein, Diederick Vermetten, Fabio Caraffini, Anna V. Kononova",2023-04-04T15:21:15Z,Explainable AI,"  Evaluating the performance of heuristic optimisation algorithms is essential
to determine how well they perform under various conditions. Recently, the BIAS
toolbox was introduced as a behaviour benchmark to detect structural bias (SB)
in search algorithms. The toolbox can be used to identify biases in existing
algorithms, as well as to test for bias in newly developed algorithms. In this
article, we introduce a novel and explainable deep-learning expansion of the
BIAS toolbox, called Deep-BIAS. Where the original toolbox uses 39 statistical
tests and a Random Forest model to predict the existence and type of SB, the
Deep-BIAS method uses a trained deep-learning model to immediately detect the
strength and type of SB based on the raw performance distributions. Through a
series of experiments with a variety of structurally biased scenarios, we
demonstrate the effectiveness of Deep-BIAS. We also present the results of
using the toolbox on 336 state-of-the-art optimisation algorithms, which showed
the presence of various types of structural bias, particularly towards the
centre of the objective space or exhibiting discretisation behaviour. The
Deep-BIAS method outperforms the BIAS toolbox both in detecting bias and for
classifying the type of SB. Furthermore, explanations can be derived using XAI
techniques.
",0
Monetizing Explainable AI: A Double-edged Sword,"Travis Greene, Sofie Goethals, David Martens, Galit Shmueli",2023-03-27T15:50:41Z,Explainable AI,"  Algorithms used by organizations increasingly wield power in society as they
decide the allocation of key resources and basic goods. In order to promote
fairer, juster, and more transparent uses of such decision-making power,
explainable artificial intelligence (XAI) aims to provide insights into the
logic of algorithmic decision-making. Despite much research on the topic,
consumer-facing applications of XAI remain rare. A central reason may be that a
viable platform-based monetization strategy for this new technology has yet to
be found. We introduce and describe a novel monetization strategy for fusing
algorithmic explanations with programmatic advertising via an explanation
platform. We claim the explanation platform represents a new,
socially-impactful, and profitable form of human-algorithm interaction and
estimate its potential for revenue generation in the high-risk domains of
finance, hiring, and education. We then consider possible undesirable and
unintended effects of monetizing XAI and simulate these scenarios using
real-world credit lending data. Ultimately, we argue that monetizing XAI may be
a double-edged sword: while monetization may incentivize industry adoption of
XAI in a variety of consumer applications, it may also conflict with the
original legal and ethical justifications for developing XAI. We conclude by
discussing whether there may be ways to responsibly and democratically harness
the potential of monetized XAI to provide greater consumer access to
algorithmic explanations.
",0
"Explaining AI in Finance: Past, Present, Prospects",Barry Quinn,2023-06-05T10:59:53Z,Other,"  This paper explores the journey of AI in finance, with a particular focus on
the crucial role and potential of Explainable AI (XAI). We trace AI's evolution
from early statistical methods to sophisticated machine learning, highlighting
XAI's role in popular financial applications. The paper underscores the
superior interpretability of methods like Shapley values compared to
traditional linear regression in complex financial scenarios. It emphasizes the
necessity of further XAI research, given forthcoming EU regulations. The paper
demonstrates, through simulations, that XAI enhances trust in AI systems,
fostering more responsible decision-making within finance.
",0
Explainable AI using expressive Boolean formulas,"Gili Rosenberg, J. Kyle Brubaker, Martin J. A. Schuetz, Grant Salton, Zhihuai Zhu, Elton Yechao Zhu, Serdar Kadıoğlu, Sima E. Borujeni, Helmut G. Katzgraber",2023-06-06T19:18:46Z,Explainable AI,"  We propose and implement an interpretable machine learning classification
model for Explainable AI (XAI) based on expressive Boolean formulas. Potential
applications include credit scoring and diagnosis of medical conditions. The
Boolean formula defines a rule with tunable complexity (or interpretability),
according to which input data are classified. Such a formula can include any
operator that can be applied to one or more Boolean variables, thus providing
higher expressivity compared to more rigid rule-based and tree-based
approaches. The classifier is trained using native local optimization
techniques, efficiently searching the space of feasible formulas. Shallow rules
can be determined by fast Integer Linear Programming (ILP) or Quadratic
Unconstrained Binary Optimization (QUBO) solvers, potentially powered by
special purpose hardware or quantum devices. We combine the expressivity and
efficiency of the native local optimizer with the fast operation of these
devices by executing non-local moves that optimize over subtrees of the full
Boolean formula. We provide extensive numerical benchmarking results featuring
several baselines on well-known public datasets. Based on the results, we find
that the native local rule classifier is generally competitive with the other
classifiers. The addition of non-local moves achieves similar results with
fewer iterations, and therefore using specialized or quantum hardware could
lead to a speedup by fast proposal of non-local moves.
",0
Is Task-Agnostic Explainable AI a Myth?,Alicja Chaszczewicz,2023-07-13T07:48:04Z,Explainable AI,"  Our work serves as a framework for unifying the challenges of contemporary
explainable AI (XAI). We demonstrate that while XAI methods provide
supplementary and potentially useful output for machine learning models,
researchers and decision-makers should be mindful of their conceptual and
technical limitations, which frequently result in these methods themselves
becoming black boxes. We examine three XAI research avenues spanning image,
textual, and graph data, covering saliency, attention, and graph-type
explainers. Despite the varying contexts and timeframes of the mentioned cases,
the same persistent roadblocks emerge, highlighting the need for a conceptual
breakthrough in the field to address the challenge of compatibility between XAI
methods and application tasks.
",0
Towards eXplainable AI for Mobility Data Science,"Anahid Jalali, Anita Graser, Clemens Heistracher",2023-07-17T13:06:33Z,Explainable AI,"  This paper presents our ongoing work towards XAI for Mobility Data Science
applications, focusing on explainable models that can learn from dense
trajectory data, such as GPS tracks of vehicles and vessels using temporal
graph neural networks (GNNs) and counterfactuals. We review the existing GeoXAI
studies, argue the need for comprehensible explanations with human-centered
approaches, and outline a research path toward XAI for Mobility Data Science.
",0
Precise Benchmarking of Explainable AI Attribution Methods,"Rafaël Brandt, Daan Raatjens, Georgi Gaydadjiev",2023-08-06T17:03:32Z,Explainable AI,"  The rationale behind a deep learning model's output is often difficult to
understand by humans. EXplainable AI (XAI) aims at solving this by developing
methods that improve interpretability and explainability of machine learning
models. Reliable evaluation metrics are needed to assess and compare different
XAI methods. We propose a novel evaluation approach for benchmarking
state-of-the-art XAI attribution methods. Our proposal consists of a synthetic
classification model accompanied by its derived ground truth explanations
allowing high precision representation of input nodes contributions. We also
propose new high-fidelity metrics to quantify the difference between
explanations of the investigated XAI method and those derived from the
synthetic model. Our metrics allow assessment of explanations in terms of
precision and recall separately. Also, we propose metrics to independently
evaluate negative or positive contributions of inputs. Our proposal provides
deeper insights into XAI methods output. We investigate our proposal by
constructing a synthetic convolutional image classification model and
benchmarking several widely used XAI attribution methods using our evaluation
approach. We compare our results with established prior XAI evaluation metrics.
By deriving the ground truth directly from the constructed model in our method,
we ensure the absence of bias, e.g., subjective either based on the training
set. Our experimental results provide novel insights into the performance of
Guided-Backprop and Smoothgrad XAI methods that are widely in use. Both have
good precision and recall scores among positively contributing pixels (0.7,
0.76 and 0.7, 0.77, respectively), but poor precision scores among negatively
contributing pixels (0.44, 0.61 and 0.47, 0.75, resp.). The recall scores in
the latter case remain close. We show that our metrics are among the fastest in
terms of execution time.
",0
A Comprehensive Review on Financial Explainable AI,"Wei Jie Yeo, Wihan van der Heever, Rui Mao, Erik Cambria, Ranjan Satapathy, Gianmarco Mengaldo",2023-09-21T10:30:49Z,Explainable AI,"  The success of artificial intelligence (AI), and deep learning models in
particular, has led to their widespread adoption across various industries due
to their ability to process huge amounts of data and learn complex patterns.
However, due to their lack of explainability, there are significant concerns
regarding their use in critical sectors, such as finance and healthcare, where
decision-making transparency is of paramount importance. In this paper, we
provide a comparative survey of methods that aim to improve the explainability
of deep learning models within the context of finance. We categorize the
collection of explainable AI methods according to their corresponding
characteristics, and we review the concerns and challenges of adopting
explainable AI methods, together with future directions we deemed appropriate
and important.
",0
Does Explainable AI Have Moral Value?,"Joshua L. M. Brand, Luca Nannini",2023-11-05T15:59:27Z,Explainable AI,"  Explainable AI (XAI) aims to bridge the gap between complex algorithmic
systems and human stakeholders. Current discourse often examines XAI in
isolation as either a technological tool, user interface, or policy mechanism.
This paper proposes a unifying ethical framework grounded in moral duties and
the concept of reciprocity. We argue that XAI should be appreciated not merely
as a right, but as part of our moral duties that helps sustain a reciprocal
relationship between humans affected by AI systems. This is because, we argue,
explanations help sustain constitutive symmetry and agency in AI-led
decision-making processes. We then assess leading XAI communities and reveal
gaps between the ideal of reciprocity and practical feasibility. Machine
learning offers useful techniques but overlooks evaluation and adoption
challenges. Human-computer interaction provides preliminary insights but
oversimplifies organizational contexts. Policies espouse accountability but
lack technical nuance. Synthesizing these views exposes barriers to
implementable, ethical XAI. Still, positioning XAI as a moral duty transcends
rights-based discourse to capture a more robust and complete moral picture.
This paper provides an accessible, detailed analysis elucidating the moral
value of explainability.
",0
Root Causing Prediction Anomalies Using Explainable AI,"Ramanathan Vishnampet, Rajesh Shenoy, Jianhui Chen, Anuj Gupta",2024-03-04T19:38:50Z,Explainable AI,"  This paper presents a novel application of explainable AI (XAI) for
root-causing performance degradation in machine learning models that learn
continuously from user engagement data. In such systems a single feature
corruption can cause cascading feature, label and concept drifts. We have
successfully applied this technique to improve the reliability of models used
in personalized advertising. Performance degradation in such systems manifest
as prediction anomalies in the models. These models are typically trained
continuously using features that are produced by hundreds of real time data
processing pipelines or derived from other upstream models. A failure in any of
these pipelines or an instability in any of the upstream models can cause
feature corruption, causing the model's predicted output to deviate from the
actual output and the training data to become corrupted. The causal
relationship between the features and the predicted output is complex, and
root-causing is challenging due to the scale and dynamism of the system. We
demonstrate how temporal shifts in the global feature importance distribution
can effectively isolate the cause of a prediction anomaly, with better recall
than model-to-feature correlation methods. The technique appears to be
effective even when approximating the local feature importance using a simple
perturbation-based method, and aggregating over a few thousand examples. We
have found this technique to be a model-agnostic, cheap and effective way to
monitor complex data pipelines in production and have deployed a system for
continuously analyzing the global feature importance distribution of
continuously trained models.
",0
"Prompt Generate Train (PGT): Few-shot Domain Adaption of Retrieval
  Augmented Generation Models for Open Book Question-Answering",C. S. Krishna,2023-07-12T04:44:31Z,Other,"  We propose a framework - Prompt, Generate, Train (PGT) - to efficiently
develop a generative question-answering model for open-book question-answering
over a proprietary collection of text documents. The framework adapts a
retriever augmented generation (RAG) model to the target domain using
supervised fine-tuning and reinforcement learning with synthetic feedback in a
few-shot setting. This, we hypothesize, will yield an aligned, uncertainty
calibrated model that is competitive with GPT-4 based in-context retrieval
augmented generation in generating relevant answers at lower serving costs. The
framework's synthetic generation pipeline will generate synthetic training data
comprising <passage, question, answer> tuples using an open-source LLM and a
novel consistency filtering scheme. The pipeline will be designed to generate
both abstractive and extractive questions that span the entire corpus. The
framework proposes to fine-tune a smaller RAG model comprising a dense
retriever (ColBERTv2) and a smaller sized LLM on the synthetic dataset. In
parallel, the framework will train a Reward model to score domain grounded
answers higher than hallucinated answers using an a priori relevance ordering
of synthetically assembled samples. In the next phase, the framework will align
the RAG model with the target domain using reinforcement learning (Proximal
Policy Optimization). This step may improve the RAG model's ability to generate
grounded answers and ignore out of domain questions. In the final phase, the
framework will calibrate the model's uncertainty for extractive
question-answers.
",0
"Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented
  Generation and Soft-Prompting for Non-Specialist LLM Users","Jennifer Dodgson, Lin Nanzheng, Julian Peh, Akira Rafhael Janson Pattirane, Alfath Daryl Alhajir, Eko Ridho Dinarto, Joseph Lim, Syed Danyal Ahmad",2023-11-10T07:13:06Z,Other,"  Research into methods for improving the performance of large language models
(LLMs) through fine-tuning, retrieval-augmented generation (RAG) and
soft-prompting has tended to focus on the use of highly technical or high-cost
techniques, making many of the newly discovered approaches comparatively
inaccessible to non-technical users. In this paper we tested an unmodified
version of GPT 3.5, a fine-tuned version, and the same unmodified model when
given access to a vectorised RAG database, both in isolation and in combination
with a basic, non-algorithmic soft prompt. In each case we tested the model's
ability to answer a set of 100 questions relating primarily to events that
occurred after September 2021 (the point at which GPT 3.5's training data set
ends). We found that if commercial platforms are used and default settings are
applied with no iteration in order to establish a baseline set of outputs, a
fine-tuned model outperforms GPT 3.5 Turbo, while the RAG approach
out-performed both. The application of a soft prompt significantly improved the
performance of each approach.
",4
"Investigating the performance of Retrieval-Augmented Generation and
  fine-tuning for the development of AI-driven knowledge-based systems","Robert Lakatos, Peter Pollner, Andras Hajdu, Tamas Joo",2024-03-12T21:06:31Z,Other,"  The development of generative large language models (G-LLM) opened up new
opportunities for the development of new types of knowledge-based systems
similar to ChatGPT, Bing, or Gemini. Fine-tuning (FN) and Retrieval-Augmented
Generation (RAG) are the techniques that can be used to implement domain
adaptation for the development of G-LLM-based knowledge systems. In our study,
using ROUGE, BLEU, METEOR scores, and cosine similarity, we compare and examine
the performance of RAG and FN for the GPT-J-6B, OPT-6.7B, LlaMA, LlaMA-2
language models. Based on measurements shown on different datasets, we
demonstrate that RAG-based constructions are more efficient than models
produced with FN. We point out that connecting RAG and FN is not trivial,
because connecting FN models with RAG can cause a decrease in performance.
Furthermore, we outline a simple RAG-based architecture which, on average,
outperforms the FN models by 16% in terms of the ROGUE score, 15% in the case
of the BLEU score, and 53% based on the cosine similarity. This shows the
significant advantage of RAG over FN in terms of hallucination, which is not
offset by the fact that the average 8% better METEOR score of FN models
indicates greater creativity compared to RAG.
",0
Foundational Moral Values for AI Alignment,"Betty Li Hou, Brian Patrick Green",2023-11-28T18:11:24Z,AI Alignment,"  Solving the AI alignment problem requires having clear, defensible values
towards which AI systems can align. Currently, targets for alignment remain
underspecified and do not seem to be built from a philosophically robust
structure. We begin the discussion of this problem by presenting five core,
foundational values, drawn from moral philosophy and built on the requisites
for human existence: survival, sustainable intergenerational existence,
society, education, and truth. We show that these values not only provide a
clearer direction for technical alignment work, but also serve as a framework
to highlight threats and opportunities from AI systems to both obtain and
sustain these values.
",0
"Can Explainable AI Explain Unfairness? A Framework for Evaluating
  Explainable AI","Kiana Alikhademi, Brianna Richardson, Emma Drobina, Juan E. Gilbert",2021-06-14T15:14:03Z,Explainable AI,"  Many ML models are opaque to humans, producing decisions too complex for
humans to easily understand. In response, explainable artificial intelligence
(XAI) tools that analyze the inner workings of a model have been created.
Despite these tools' strength in translating model behavior, critiques have
raised concerns about the impact of XAI tools as a tool for `fairwashing` by
misleading users into trusting biased or incorrect models. In this paper, we
created a framework for evaluating explainable AI tools with respect to their
capabilities for detecting and addressing issues of bias and fairness as well
as their capacity to communicate these results to their users clearly. We found
that despite their capabilities in simplifying and explaining model behavior,
many prominent XAI tools lack features that could be critical in detecting
bias. Developers can use our framework to suggest modifications needed in their
toolkits to reduce issues likes fairwashing.
",0
"Explainable AI is Dead, Long Live Explainable AI! Hypothesis-driven
  decision support",Tim Miller,2023-02-24T01:33:25Z,Explainable AI,"  In this paper, we argue for a paradigm shift from the current model of
explainable artificial intelligence (XAI), which may be counter-productive to
better human decision making. In early decision support systems, we assumed
that we could give people recommendations and that they would consider them,
and then follow them when required. However, research found that people often
ignore recommendations because they do not trust them; or perhaps even worse,
people follow them blindly, even when the recommendations are wrong.
Explainable artificial intelligence mitigates this by helping people to
understand how and why models give certain recommendations. However, recent
research shows that people do not always engage with explainability tools
enough to help improve decision making. The assumption that people will engage
with recommendations and explanations has proven to be unfounded. We argue this
is because we have failed to account for two things. First, recommendations
(and their explanations) take control from human decision makers, limiting
their agency. Second, giving recommendations and explanations does not align
with the cognitive processes employed by people making decisions. This position
paper proposes a new conceptual framework called Evaluative AI for explainable
decision support. This is a machine-in-the-loop paradigm in which decision
support tools provide evidence for and against decisions made by people, rather
than provide recommendations to accept or reject. We argue that this mitigates
issues of over- and under-reliance on decision support tools, and better
leverages human expertise in decision making.
",0
"Adversarial Machine Learning in Wireless Communications using RF Data: A
  Review","Damilola Adesina, Chung-Chu Hsieh, Yalin E. Sagduyu, Lijun Qian",2020-12-28T18:11:43Z,Adversarial Machine Learning,"  Machine learning (ML) provides effective means to learn from spectrum data
and solve complex tasks involved in wireless communications. Supported by
recent advances in computational resources and algorithmic designs, deep
learning (DL) has found success in performing various wireless communication
tasks such as signal recognition, spectrum sensing and waveform design.
However, ML in general and DL in particular have been found vulnerable to
manipulations thus giving rise to a field of study called adversarial machine
learning (AML). Although AML has been extensively studied in other data domains
such as computer vision and natural language processing, research for AML in
the wireless communications domain is still in its early stage. This paper
presents a comprehensive review of the latest research efforts focused on AML
in wireless communications while accounting for the unique characteristics of
wireless systems. First, the background of AML attacks on deep neural networks
is discussed and a taxonomy of AML attack types is provided. Various methods of
generating adversarial examples and attack mechanisms are also described. In
addition, an holistic survey of existing research on AML attacks for various
wireless communication problems as well as the corresponding defense mechanisms
in the wireless domain are presented. Finally, as new attacks and defense
techniques are developed, recent research trends and the overarching future
outlook for AML for next-generation wireless communications are discussed.
",0
"Improving DGA-Based Malicious Domain Classifiers for Malware Defense
  with Adversarial Machine Learning","Ibrahim Yilmaz, Ambareen Siraj, Denis Ulybyshev",2021-01-02T22:04:22Z,Adversarial Machine Learning,"  Domain Generation Algorithms (DGAs) are used by adversaries to establish
Command and Control (C\&C) server communications during cyber attacks.
Blacklists of known/identified C\&C domains are often used as one of the
defense mechanisms. However, since blacklists are static and generated by
signature-based approaches, they can neither keep up nor detect
never-seen-before malicious domain names. Due to this shortcoming of blacklist
domain checking, machine learning algorithms have been used to address the
problem to some extent. However, when training is performed with limited
datasets, the algorithms are likely to fail in detecting new DGA variants. To
mitigate this weakness, we successfully applied a DGA-based malicious domain
classifier using the Long Short-Term Memory (LSTM) method with a novel feature
engineering technique. Our model's performance shows a higher level of accuracy
compared to a previously reported model from prior research. Additionally, we
propose a new method using adversarial machine learning to generate
never-before-seen malware-related domain families that can be used to
illustrate the shortcomings of machine learning algorithms in this regard.
Next, we augment the training dataset with new samples such that it makes
training of the machine learning models more effective in detecting
never-before-seen malicious domain name variants. Finally, to protect
blacklists of malicious domain names from disclosure and tampering, we devise
secure data containers that store blacklists and guarantee their protection
against adversarial access and modifications.
",0
"Privacy Protection of Grid Users Data with Blockchain and Adversarial
  Machine Learning","Ibrahim Yilmaz, Kavish Kapoor, Ambareen Siraj, Mahmoud Abouyoussef",2021-01-15T21:54:55Z,Other,"  Utilities around the world are reported to invest a total of around 30
billion over the next few years for installation of more than 300 million smart
meters, replacing traditional analog meters [1]. By mid-decade, with full
country wide deployment, there will be almost 1.3 billion smart meters in place
[1]. Collection of fine grained energy usage data by these smart meters
provides numerous advantages such as energy savings for customers with use of
demand optimization, a billing system of higher accuracy with dynamic pricing
programs, bidirectional information exchange ability between end-users for
better consumer-operator interaction, and so on. However, all these perks
associated with fine grained energy usage data collection threaten the privacy
of users. With this technology, customers' personal data such as sleeping
cycle, number of occupants, and even type and number of appliances stream into
the hands of the utility companies and can be subject to misuse. This research
paper addresses privacy violation of consumers' energy usage data collected
from smart meters and provides a novel solution for the privacy protection
while allowing benefits of energy data analytics. First, we demonstrate the
successful application of occupancy detection attacks using a deep neural
network method that yields high accuracy results. We then introduce Adversarial
Machine Learning Occupancy Detection Avoidance with Blockchain (AMLODA-B)
framework as a counter-attack by deploying an algorithm based on the Long Short
Term Memory (LSTM) model into the standardized smart metering infrastructure to
prevent leakage of consumers personal information. Our privacy-aware approach
protects consumers' privacy without compromising the correctness of billing and
preserves operational efficiency without use of authoritative intermediaries.
",0
"Adversarial Machine Learning for Flooding Attacks on 5G Radio Access
  Network Slicing","Yi Shi, Yalin E. Sagduyu",2021-01-21T17:05:31Z,Adversarial Machine Learning,"  Network slicing manages network resources as virtual resource blocks (RBs)
for the 5G Radio Access Network (RAN). Each communication request comes with
quality of experience (QoE) requirements such as throughput and
latency/deadline, which can be met by assigning RBs, communication power, and
processing power to the request. For a completed request, the achieved reward
is measured by the weight (priority) of this request. Then, the reward is
maximized over time by allocating resources, e.g., with reinforcement learning
(RL). In this paper, we introduce a novel flooding attack on 5G network
slicing, where an adversary generates fake network slicing requests to consume
the 5G RAN resources that would be otherwise available to real requests. The
adversary observes the spectrum and builds a surrogate model on the network
slicing algorithm through RL that decides on how to craft fake requests to
minimize the reward of real requests over time. We show that the portion of the
reward achieved by real requests may be much less than the reward that would be
achieved when there was no attack. We also show that this flooding attack is
more effective than other benchmark attacks such as random fake requests and
fake requests with the minimum resource requirement (lowest QoE requirement).
Fake requests may be detected due to their fixed weight. As an attack
enhancement, we present schemes to randomize weights of fake requests and show
that it is still possible to reduce the reward of real requests while
maintaining the balance on weight distributions.
",0
"Adversarial Machine Learning Security Problems for 6G: mmWave Beam
  Prediction Use-Case","Evren Catak, Ferhat Ozgur Catak, Arild Moldsvor",2021-03-12T13:42:25Z,Adversarial Machine Learning,"  6G is the next generation for the communication systems. In recent years,
machine learning algorithms have been applied widely in various fields such as
health, transportation, and the autonomous car. The predictive algorithms will
be used in 6G problems. With the rapid developments of deep learning
techniques, it is critical to take the security concern into account to apply
the algorithms. While machine learning offers significant advantages for 6G, AI
models' security is ignored. Since it has many applications in the real world,
security is a vital part of the algorithms. This paper has proposed a
mitigation method for adversarial attacks against proposed 6G machine learning
models for the millimeter-wave (mmWave) beam prediction with adversarial
learning. The main idea behind adversarial attacks against machine learning
models is to produce faulty results by manipulating trained deep learning
models for 6G applications for mmWave beam prediction use case. We have also
presented the adversarial learning mitigation method's performance for 6G
security in millimeter-wave beam prediction application with fast gradient sign
method attack. The mean square errors of the defended model and undefended
model are very close.
",0
"HASI: Hardware-Accelerated Stochastic Inference, A Defense Against
  Adversarial Machine Learning Attacks","Mohammad Hossein Samavatian, Saikat Majumdar, Kristin Barber, Radu Teodorescu",2021-06-09T14:31:28Z,Adversarial Machine Learning,"  Deep Neural Networks (DNNs) are employed in an increasing number of
applications, some of which are safety critical. Unfortunately, DNNs are known
to be vulnerable to so-called adversarial attacks that manipulate inputs to
cause incorrect results that can be beneficial to an attacker or damaging to
the victim. Multiple defenses have been proposed to increase the robustness of
DNNs. In general, these defenses have high overhead, some require
attack-specific re-training of the model or careful tuning to adapt to
different attacks.
  This paper presents HASI, a hardware-accelerated defense that uses a process
we call stochastic inference to detect adversarial inputs. We show that by
carefully injecting noise into the model at inference time, we can
differentiate adversarial inputs from benign ones. HASI uses the output
distribution characteristics of noisy inference compared to a non-noisy
reference to detect adversarial inputs. We show an adversarial detection rate
of 86% when applied to VGG16 and 93% when applied to ResNet50, which exceeds
the detection rate of the state of the art approaches, with a much lower
overhead. We demonstrate two software/hardware-accelerated co-designs, which
reduces the performance impact of stochastic inference to 1.58X-2X relative to
the unprotected baseline, compared to 15X-20X overhead for a software-only GPU
implementation.
",0
"Adversarial Machine Learning for Cybersecurity and Computer Vision:
  Current Developments and Challenges",Bowei Xi,2021-06-30T03:05:58Z,Adversarial Machine Learning,"  We provide a comprehensive overview of adversarial machine learning focusing
on two application domains, i.e., cybersecurity and computer vision. Research
in adversarial machine learning addresses a significant threat to the wide
application of machine learning techniques -- they are vulnerable to carefully
crafted attacks from malicious adversaries. For example, deep neural networks
fail to correctly classify adversarial images, which are generated by adding
imperceptible perturbations to clean images.We first discuss three main
categories of attacks against machine learning techniques -- poisoning attacks,
evasion attacks, and privacy attacks. Then the corresponding defense approaches
are introduced along with the weakness and limitations of the existing defense
approaches. We notice adversarial samples in cybersecurity and computer vision
are fundamentally different. While adversarial samples in cybersecurity often
have different properties/distributions compared with training data,
adversarial images in computer vision are created with minor input
perturbations. This further complicates the development of robust learning
techniques, because a robust learning technique must withstand different types
of attacks.
",0
"Using Undervolting as an On-Device Defense Against Adversarial Machine
  Learning Attacks","Saikat Majumdar, Mohammad Hossein Samavatian, Kristin Barber, Radu Teodorescu",2021-07-20T23:21:04Z,Other,"  Deep neural network (DNN) classifiers are powerful tools that drive a broad
spectrum of important applications, from image recognition to autonomous
vehicles. Unfortunately, DNNs are known to be vulnerable to adversarial attacks
that affect virtually all state-of-the-art models. These attacks make small
imperceptible modifications to inputs that are sufficient to induce the DNNs to
produce the wrong classification.
  In this paper we propose a novel, lightweight adversarial correction and/or
detection mechanism for image classifiers that relies on undervolting (running
a chip at a voltage that is slightly below its safe margin). We propose using
controlled undervolting of the chip running the inference process in order to
introduce a limited number of compute errors. We show that these errors disrupt
the adversarial input in a way that can be used either to correct the
classification or detect the input as adversarial. We evaluate the proposed
solution in an FPGA design and through software simulation. We evaluate 10
attacks and show average detection rates of 77% and 90% on two popular DNNs.
",0
"Adversarial Machine Learning In Network Intrusion Detection Domain: A
  Systematic Review","Huda Ali Alatwi, Charles Morisset",2021-12-06T19:10:23Z,Adversarial Machine Learning,"  Due to their massive success in various domains, deep learning techniques are
increasingly used to design network intrusion detection solutions that detect
and mitigate unknown and known attacks with high accuracy detection rates and
minimal feature engineering. However, it has been found that deep learning
models are vulnerable to data instances that can mislead the model to make
incorrect classification decisions so-called (adversarial examples). Such
vulnerability allows attackers to target NIDSs by adding small crafty
perturbations to the malicious traffic to evade detection and disrupt the
system's critical functionalities. The problem of deep adversarial learning has
been extensively studied in the computer vision domain; however, it is still an
area of open research in network security applications. Therefore, this survey
explores the researches that employ different aspects of adversarial machine
learning in the area of network intrusion detection in order to provide
directions for potential solutions. First, the surveyed studies are categorized
based on their contribution to generating adversarial examples, evaluating the
robustness of ML-based NIDs towards adversarial examples, and defending these
models against such attacks. Second, we highlight the characteristics
identified in the surveyed research. Furthermore, we discuss the applicability
of the existing generic adversarial attacks for the NIDS domain, the
feasibility of launching the proposed attacks in real-world scenarios, and the
limitations of the existing mitigation solutions.
",18
"Adversarial Machine Learning Threat Analysis and Remediation in Open
  Radio Access Network (O-RAN)","Edan Habler, Ron Bitton, Dan Avraham, Dudu Mimran, Eitan Klevansky, Oleg Brodt, Heiko Lehmann, Yuval Elovici, Asaf Shabtai",2022-01-16T17:01:38Z,Adversarial Machine Learning,"  O-RAN is a new, open, adaptive, and intelligent RAN architecture. Motivated
by the success of artificial intelligence in other domains, O-RAN strives to
leverage machine learning (ML) to automatically and efficiently manage network
resources in diverse use cases such as traffic steering, quality of experience
prediction, and anomaly detection. Unfortunately, it has been shown that
ML-based systems are vulnerable to an attack technique referred to as
adversarial machine learning (AML). This special kind of attack has already
been demonstrated in recent studies and in multiple domains. In this paper, we
present a systematic AML threat analysis for O-RAN. We start by reviewing
relevant ML use cases and analyzing the different ML workflow deployment
scenarios in O-RAN. Then, we define the threat model, identifying potential
adversaries, enumerating their adversarial capabilities, and analyzing their
main goals. Next, we explore the various AML threats associated with O-RAN and
review a large number of attacks that can be performed to realize these threats
and demonstrate an AML attack on a traffic steering model. In addition, we
analyze and propose various AML countermeasures for mitigating the identified
threats. Finally, based on the identified AML threats and countermeasures, we
present a methodology and a tool for performing risk assessment for AML attacks
for a specific ML use case in O-RAN.
",0
"Physics-aware Complex-valued Adversarial Machine Learning in
  Reconfigurable Diffractive All-optical Neural Network","Ruiyang Chen, Yingjie Li, Minhan Lou, Jichao Fan, Yingheng Tang, Berardi Sensale-Rodriguez, Cunxi Yu, Weilu Gao",2022-03-09T17:09:41Z,Adversarial Machine Learning,"  Diffractive optical neural networks have shown promising advantages over
electronic circuits for accelerating modern machine learning (ML) algorithms.
However, it is challenging to achieve fully programmable all-optical
implementation and rapid hardware deployment. Furthermore, understanding the
threat of adversarial ML in such system becomes crucial for real-world
applications, which remains unexplored. Here, we demonstrate a large-scale,
cost-effective, complex-valued, and reconfigurable diffractive all-optical
neural networks system in the visible range based on cascaded transmissive
twisted nematic liquid crystal spatial light modulators. With the assist of
categorical reparameterization, we create a physics-aware training framework
for the fast and accurate deployment of computer-trained models onto optical
hardware. Furthermore, we theoretically analyze and experimentally demonstrate
physics-aware adversarial attacks onto the system, which are generated from a
complex-valued gradient-based algorithm. The detailed adversarial robustness
comparison with conventional multiple layer perceptrons and convolutional
neural networks features a distinct statistical adversarial property in
diffractive optical neural networks. Our full stack of software and hardware
provides new opportunities of employing diffractive optics in a variety of ML
tasks and enabling the research on optical adversarial ML.
",0
"DNNShield: Dynamic Randomized Model Sparsification, A Defense Against
  Adversarial Machine Learning","Mohammad Hossein Samavatian, Saikat Majumdar, Kristin Barber, Radu Teodorescu",2022-07-31T19:29:44Z,Adversarial Machine Learning,"  DNNs are known to be vulnerable to so-called adversarial attacks that
manipulate inputs to cause incorrect results that can be beneficial to an
attacker or damaging to the victim. Recent works have proposed approximate
computation as a defense mechanism against machine learning attacks. We show
that these approaches, while successful for a range of inputs, are insufficient
to address stronger, high-confidence adversarial attacks. To address this, we
propose DNNSHIELD, a hardware-accelerated defense that adapts the strength of
the response to the confidence of the adversarial input. Our approach relies on
dynamic and random sparsification of the DNN model to achieve inference
approximation efficiently and with fine-grain control over the approximation
error. DNNSHIELD uses the output distribution characteristics of sparsified
inference compared to a dense reference to detect adversarial inputs. We show
an adversarial detection rate of 86% when applied to VGG16 and 88% when applied
to ResNet50, which exceeds the detection rate of the state of the art
approaches, with a much lower overhead. We demonstrate a
software/hardware-accelerated FPGA prototype, which reduces the performance
impact of DNNSHIELD relative to software-only CPU and GPU implementations.
",0
"Adversarial Machine Learning-Based Anticipation of Threats Against
  Vehicle-to-Microgrid Services","Ahmed Omara, Burak Kantarci",2022-08-09T23:25:32Z,Adversarial Machine Learning,"  In this paper, we study the expanding attack surface of Adversarial Machine
Learning (AML) and the potential attacks against Vehicle-to-Microgrid (V2M)
services. We present an anticipatory study of a multi-stage gray-box attack
that can achieve a comparable result to a white-box attack. Adversaries aim to
deceive the targeted Machine Learning (ML) classifier at the network edge to
misclassify the incoming energy requests from microgrids. With an inference
attack, an adversary can collect real-time data from the communication between
smart microgrids and a 5G gNodeB to train a surrogate (i.e., shadow) model of
the targeted classifier at the edge. To anticipate the associated impact of an
adversary's capability to collect real-time data instances, we study five
different cases, each representing different amounts of real-time data
instances collected by an adversary. Out of six ML models trained on the
complete dataset, K-Nearest Neighbour (K-NN) is selected as the surrogate
model, and through simulations, we demonstrate that the multi-stage gray-box
attack is able to mislead the ML classifier and cause an Evasion Increase Rate
(EIR) up to 73.2% using 40% less data than what a white-box attack needs to
achieve a similar EIR.
",0
"Gamma-convergence of a nonlocal perimeter arising in adversarial machine
  learning","Leon Bungert, Kerrek Stinson",2022-11-28T11:17:10Z,Other,"  In this paper we prove Gamma-convergence of a nonlocal perimeter of Minkowski
type to a local anisotropic perimeter. The nonlocal model describes the
regularizing effect of adversarial training in binary classifications. The
energy essentially depends on the interaction between two distributions
modelling likelihoods for the associated classes. We overcome typical strict
regularity assumptions for the distributions by only assuming that they have
bounded $BV$ densities. In the natural topology coming from compactness, we
prove Gamma-convergence to a weighted perimeter with weight determined by an
anisotropic function of the two densities. Despite being local, this sharp
interface limit reflects classification stability with respect to adversarial
perturbations. We further apply our results to deduce Gamma-convergence of the
associated total variations, to study the asymptotics of adversarial training,
and to prove Gamma-convergence of graph discretizations for the nonlocal
perimeter.
",0
"Adversarial Machine Learning and Defense Game for NextG Signal
  Classification with Deep Learning",Yalin E. Sagduyu,2022-12-22T15:13:03Z,Adversarial Machine Learning,"  This paper presents a game-theoretic framework to study the interactions of
attack and defense for deep learning-based NextG signal classification. NextG
systems such as the one envisioned for a massive number of IoT devices can
employ deep neural networks (DNNs) for various tasks such as user equipment
identification, physical layer authentication, and detection of incumbent users
(such as in the Citizens Broadband Radio Service (CBRS) band). By training
another DNN as the surrogate model, an adversary can launch an inference
(exploratory) attack to learn the behavior of the victim model, predict
successful operation modes (e.g., channel access), and jam them. A defense
mechanism can increase the adversary's uncertainty by introducing controlled
errors in the victim model's decisions (i.e., poisoning the adversary's
training data). This defense is effective against an attack but reduces the
performance when there is no attack. The interactions between the defender and
the adversary are formulated as a non-cooperative game, where the defender
selects the probability of defending or the defense level itself (i.e., the
ratio of falsified decisions) and the adversary selects the probability of
attacking. The defender's objective is to maximize its reward (e.g., throughput
or transmission success ratio), whereas the adversary's objective is to
minimize this reward and its attack cost. The Nash equilibrium strategies are
determined as operation modes such that no player can unilaterally improve its
utility given the other's strategy is fixed. A fictitious play is formulated
for each player to play the game repeatedly in response to the empirical
frequency of the opponent's actions. The performance in Nash equilibrium is
compared to the fixed attack and defense cases, and the resilience of NextG
signal classification against attacks is quantified.
",0
"Attacks in Adversarial Machine Learning: A Systematic Survey from the
  Life-cycle Perspective","Baoyuan Wu, Zihao Zhu, Li Liu, Qingshan Liu, Zhaofeng He, Siwei Lyu",2023-02-19T02:12:21Z,Adversarial Machine Learning,"  Adversarial machine learning (AML) studies the adversarial phenomenon of
machine learning, which may make inconsistent or unexpected predictions with
humans. Some paradigms have been recently developed to explore this adversarial
phenomenon occurring at different stages of a machine learning system, such as
backdoor attack occurring at the pre-training, in-training and inference stage;
weight attack occurring at the post-training, deployment and inference stage;
adversarial attack occurring at the inference stage. However, although these
adversarial paradigms share a common goal, their developments are almost
independent, and there is still no big picture of AML. In this work, we aim to
provide a unified perspective to the AML community to systematically review the
overall progress of this field. We firstly provide a general definition about
AML, and then propose a unified mathematical framework to covering existing
attack paradigms. According to the proposed unified framework, we build a full
taxonomy to systematically categorize and review existing representative
methods for each paradigm. Besides, using this unified framework, it is easy to
figure out the connections and differences among different attack paradigms,
which may inspire future researchers to develop more advanced attack paradigms.
Finally, to facilitate the viewing of the built taxonomy and the related
literature in adversarial machine learning, we further provide a website, \ie,
\url{http://adversarial-ml.com}, where the taxonomies and literature will be
continuously updated.
",17
"A reading survey on adversarial machine learning: Adversarial attacks
  and their understanding",Shashank Kotyan,2023-08-07T07:37:26Z,Adversarial Machine Learning,"  Deep Learning has empowered us to train neural networks for complex data with
high performance. However, with the growing research, several vulnerabilities
in neural networks have been exposed. A particular branch of research,
Adversarial Machine Learning, exploits and understands some of the
vulnerabilities that cause the neural networks to misclassify for near original
input. A class of algorithms called adversarial attacks is proposed to make the
neural networks misclassify for various tasks in different domains. With the
extensive and growing research in adversarial attacks, it is crucial to
understand the classification of adversarial attacks. This will help us
understand the vulnerabilities in a systematic order and help us to mitigate
the effects of adversarial attacks. This article provides a survey of existing
adversarial attacks and their understanding based on different perspectives. We
also provide a brief overview of existing adversarial defences and their
limitations in mitigating the effect of adversarial attacks. Further, we
conclude with a discussion on the future research directions in the field of
adversarial machine learning.
",0
"On the Computational Entanglement of Distant Features in Adversarial
  Machine Learning","YenLung Lai, Xingbo Dong, Zhe Jin",2023-09-27T14:09:15Z,Other,"  In this research, we introduce the concept of ""computational entanglement,"" a
phenomenon observed in overparameterized feedforward linear networks that
enables the network to achieve zero loss by fitting random noise, even on
previously unseen test samples. Analyzing this behavior through spacetime
diagrams reveals its connection to length contraction, where both training and
test samples converge toward a shared normalized point within a flat Riemannian
manifold. Moreover, we present a novel application of computational
entanglement in transforming a worst-case adversarial examples-inputs that are
highly non-robust and uninterpretable to human observers-into outputs that are
both recognizable and robust. This provides new insights into the behavior of
non-robust features in adversarial example generation, underscoring the
critical role of computational entanglement in enhancing model robustness and
advancing our understanding of neural networks in adversarial contexts.
",0
"Adversarial Machine Learning for Social Good: Reframing the Adversary as
  an Ally","Shawqi Al-Maliki, Adnan Qayyum, Hassan Ali, Mohamed Abdallah, Junaid Qadir, Dinh Thai Hoang, Dusit Niyato, Ala Al-Fuqaha",2023-10-05T15:49:04Z,Adversarial Machine Learning,"  Deep Neural Networks (DNNs) have been the driving force behind many of the
recent advances in machine learning. However, research has shown that DNNs are
vulnerable to adversarial examples -- input samples that have been perturbed to
force DNN-based models to make errors. As a result, Adversarial Machine
Learning (AdvML) has gained a lot of attention, and researchers have
investigated these vulnerabilities in various settings and modalities. In
addition, DNNs have also been found to incorporate embedded bias and often
produce unexplainable predictions, which can result in anti-social AI
applications. The emergence of new AI technologies that leverage Large Language
Models (LLMs), such as ChatGPT and GPT-4, increases the risk of producing
anti-social applications at scale. AdvML for Social Good (AdvML4G) is an
emerging field that repurposes the AdvML bug to invent pro-social applications.
Regulators, practitioners, and researchers should collaborate to encourage the
development of pro-social applications and hinder the development of
anti-social ones. In this work, we provide the first comprehensive review of
the emerging field of AdvML4G. This paper encompasses a taxonomy that
highlights the emergence of AdvML4G, a discussion of the differences and
similarities between AdvML4G and AdvML, a taxonomy covering social good-related
concepts and aspects, an exploration of the motivations behind the emergence of
AdvML4G at the intersection of ML4G and AdvML, and an extensive summary of the
works that utilize AdvML4G as an auxiliary tool for innovating pro-social
applications. Finally, we elaborate upon various challenges and open research
issues that require significant attention from the research community.
",1
"FRAUDability: Estimating Users' Susceptibility to Financial Fraud Using
  Adversarial Machine Learning","Chen Doytshman, Satoru Momiyama, Inderjeet Singh, Yuval Elovici, Asaf Shabtai",2023-12-02T18:33:05Z,Adversarial Machine Learning,"  In recent years, financial fraud detection systems have become very efficient
at detecting fraud, which is a major threat faced by e-commerce platforms. Such
systems often include machine learning-based algorithms aimed at detecting and
reporting fraudulent activity. In this paper, we examine the application of
adversarial learning based ranking techniques in the fraud detection domain and
propose FRAUDability, a method for the estimation of a financial fraud
detection system's performance for every user. We are motivated by the
assumption that ""not all users are created equal"" -- while some users are well
protected by fraud detection algorithms, others tend to pose a challenge to
such systems. The proposed method produces scores, namely ""fraudability
scores,"" which are numerical estimations of a fraud detection system's ability
to detect financial fraud for a specific user, given his/her unique activity in
the financial system. Our fraudability scores enable those tasked with
defending users in a financial platform to focus their attention and resources
on users with high fraudability scores to better protect them. We validate our
method using a real e-commerce platform's dataset and demonstrate the
application of fraudability scores from the attacker's perspective, on the
platform, and more specifically, on the fraud detection systems used by the
e-commerce enterprise. We show that the scores can also help attackers increase
their financial profit by 54%, by engaging solely with users with high
fraudability scores, avoiding those users whose spending habits enable more
accurate fraud detection.
",0
Causal Learning for Socially Responsible AI,"Lu Cheng, Ahmadreza Mosallanezhad, Paras Sheth, Huan Liu",2021-04-25T22:09:11Z,Responsible AI,"  There have been increasing concerns about Artificial Intelligence (AI) due to
its unfathomable potential power. To make AI address ethical challenges and
shun undesirable outcomes, researchers proposed to develop socially responsible
AI (SRAI). One of these approaches is causal learning (CL). We survey
state-of-the-art methods of CL for SRAI. We begin by examining the seven CL
tools to enhance the social responsibility of AI, then review how existing
works have succeeded using these tools to tackle issues in developing SRAI such
as fairness. The goal of this survey is to bring forefront the potentials and
promises of CL for SRAI.
",0
AI and Ethics -- Operationalising Responsible AI,"Liming Zhu, Xiwei Xu, Qinghua Lu, Guido Governatori, Jon Whittle",2021-05-19T00:55:40Z,Responsible AI,"  In the last few years, AI continues demonstrating its positive impact on
society while sometimes with ethically questionable consequences. Building and
maintaining public trust in AI has been identified as the key to successful and
sustainable innovation. This chapter discusses the challenges related to
operationalizing ethical AI principles and presents an integrated view that
covers high-level ethical AI principles, the general notion of
trust/trustworthiness, and product/process support in the context of
responsible AI, which helps improve both trust and trustworthiness of AI for a
wider set of stakeholders.
",32
Tools and Practices for Responsible AI Engineering,"Ryan Soklaski, Justin Goodwin, Olivia Brown, Michael Yee, Jason Matterer",2022-01-14T19:47:46Z,Responsible AI,"  Responsible Artificial Intelligence (AI) - the practice of developing,
evaluating, and maintaining accurate AI systems that also exhibit essential
properties such as robustness and explainability - represents a multifaceted
challenge that often stretches standard machine learning tooling, frameworks,
and testing methods beyond their limits. In this paper, we present two new
software libraries - hydra-zen and the rAI-toolbox - that address critical
needs for responsible AI engineering. hydra-zen dramatically simplifies the
process of making complex AI applications configurable, and their behaviors
reproducible. The rAI-toolbox is designed to enable methods for evaluating and
enhancing the robustness of AI-models in a way that is scalable and that
composes naturally with other popular ML frameworks. We describe the design
principles and methodologies that make these tools effective, including the use
of property-based testing to bolster the reliability of the tools themselves.
Finally, we demonstrate the composability and flexibility of the tools by
showing how various use cases from adversarial robustness and explainable AI
can be concisely implemented with familiar APIs.
",0
Five Ps: Leverage Zones Towards Responsible AI,"Ehsan Nabavi, Chris Browne",2022-04-20T04:20:14Z,"RAG, Responsible AI","  There is a growing debate amongst academics and practitioners on whether
interventions made, thus far, towards Responsible AI would have been enough to
engage with root causes of AI problems. Failure to effect meaningful changes in
this system could see these initiatives to not reach their potential and lead
to the concept becoming another buzzword for companies to use in their
marketing campaigns. We propose that there is an opportunity to improve the
extent to which interventions are understood to be effective in their
contribution to the change required for Responsible AI. Using the notions of
leverage zones adapted from the 'Systems Thinking' literature, we suggest a
novel approach to evaluate the effectiveness of interventions, to focus on
those that may bring about the real change that is needed. In this paper we
argue that insights from using this perspective demonstrate that the majority
of current initiatives taken by various actors in the field, focus on low-order
interventions, such as short-term fixes, tweaking algorithms and updating
parameters, absent from higher-order interventions, such as redefining the
system's foundational structures that govern those parameters, or challenging
the underlying purpose upon which those structures are built and developed in
the first place(high-leverage). This paper presents a conceptual framework
called the Five Ps to identify interventions towards Responsible AI and
provides a scaffold for transdisciplinary question asking to improve outcomes
towards Responsible AI.
",0
Towards Responsible AI for Financial Transactions,"Charl Maree, Jan Erik Modal, Christian W. Omlin",2022-06-06T08:29:47Z,Responsible AI,"  The application of AI in finance is increasingly dependent on the principles
of responsible AI. These principles - explainability, fairness, privacy,
accountability, transparency and soundness form the basis for trust in future
AI systems. In this study, we address the first principle by providing an
explanation for a deep neural network that is trained on a mixture of
numerical, categorical and textual inputs for financial transaction
classification. The explanation is achieved through (1) a feature importance
analysis using Shapley additive explanations (SHAP) and (2) a hybrid approach
of text clustering and decision tree classifiers. We then test the robustness
of the model by exposing it to a targeted evasion attack, leveraging the
knowledge we gained about the model through the extracted explanation.
",0
Tailoring Requirements Engineering for Responsible AI,"Walid Maalej, Yen Dieu Pham, Larissa Chazette",2023-02-21T16:48:59Z,Responsible AI,"  Requirements Engineering (RE) is the discipline for identifying, analyzing,
as well as ensuring the implementation and delivery of user, technical, and
societal requirements. Recently reported issues concerning the acceptance of
Artificial Intelligence (AI) solutions after deployment, e.g. in the medical,
automotive, or scientific domains, stress the importance of RE for designing
and delivering Responsible AI systems. In this paper, we argue that RE should
not only be carefully conducted but also tailored for Responsible AI. We
outline related challenges for research and practice.
",0
A Pathway Towards Responsible AI Generated Content,"Chen Chen, Jie Fu, Lingjuan Lyu",2023-03-02T14:58:40Z,Responsible AI,"  AI Generated Content (AIGC) has received tremendous attention within the past
few years, with content generated in the format of image, text, audio, video,
etc. Meanwhile, AIGC has become a double-edged sword and recently received much
criticism regarding its responsible usage. In this article, we focus on 8 main
concerns that may hinder the healthy development and deployment of AIGC in
practice, including risks from (1) privacy; (2) bias, toxicity, misinformation;
(3) intellectual property (IP); (4) robustness; (5) open source and
explanation; (6) technology abuse; (7) consent, credit, and compensation; (8)
environment. Additionally, we provide insights into the promising directions
for tackling these risks while constructing generative models, enabling AIGC to
be used more responsibly to truly benefit society.
",58
Beyond XAI:Obstacles Towards Responsible AI,Yulu Pi,2023-09-07T11:08:14Z,Responsible AI,"  The rapidly advancing domain of Explainable Artificial Intelligence (XAI) has
sparked significant interests in developing techniques to make AI systems more
transparent and understandable. Nevertheless, in real-world contexts, the
methods of explainability and their evaluation strategies present numerous
limitations.Moreover, the scope of responsible AI extends beyond just
explainability. In this paper, we explore these limitations and discuss their
implications in a boarder context of responsible AI when considering other
important aspects, including privacy, fairness and contestability.
",0
Responsible AI (RAI) Games and Ensembles,"Yash Gupta, Runtian Zhai, Arun Suggala, Pradeep Ravikumar",2023-10-28T22:17:30Z,Responsible AI,"  Several recent works have studied the societal effects of AI; these include
issues such as fairness, robustness, and safety. In many of these objectives, a
learner seeks to minimize its worst-case loss over a set of predefined
distributions (known as uncertainty sets), with usual examples being perturbed
versions of the empirical distribution. In other words, aforementioned problems
can be written as min-max problems over these uncertainty sets. In this work,
we provide a general framework for studying these problems, which we refer to
as Responsible AI (RAI) games. We provide two classes of algorithms for solving
these games: (a) game-play based algorithms, and (b) greedy stagewise
estimation algorithms. The former class is motivated by online learning and
game theory, whereas the latter class is motivated by the classical statistical
literature on boosting, and regression. We empirically demonstrate the
applicability and competitive performance of our techniques for solving several
RAI problems, particularly around subpopulation shift.
",0
Responsible AI Research Needs Impact Statements Too,"Alexandra Olteanu, Michael Ekstrand, Carlos Castillo, Jina Suh",2023-11-20T14:02:28Z,Responsible AI,"  All types of research, development, and policy work can have unintended,
adverse consequences - work in responsible artificial intelligence (RAI),
ethical AI, or ethics in AI is no exception.
",7
Responsible AI Governance: A Systematic Literature Review,"Amna Batool, Didar Zowghi, Muneera Bano",2023-12-18T05:22:36Z,Responsible AI,"  As artificial intelligence transforms a wide range of sectors and drives
innovation, it also introduces complex challenges concerning ethics,
transparency, bias, and fairness. The imperative for integrating Responsible AI
(RAI) principles within governance frameworks is paramount to mitigate these
emerging risks. While there are many solutions for AI governance, significant
questions remain about their effectiveness in practice. Addressing this
knowledge gap, this paper aims to examine the existing literature on AI
Governance. The focus of this study is to analyse the literature to answer key
questions: WHO is accountable for AI systems' governance, WHAT elements are
being governed, WHEN governance occurs within the AI development life cycle,
and HOW it is executed through various mechanisms like frameworks, tools,
standards, policies, or models. Employing a systematic literature review
methodology, a rigorous search and selection process has been employed. This
effort resulted in the identification of 61 relevant articles on the subject of
AI Governance. Out of the 61 studies analysed, only 5 provided complete
responses to all questions. The findings from this review aid research in
formulating more holistic and comprehensive Responsible AI (RAI) governance
frameworks. This study highlights important role of AI governance on various
levels specially organisational in establishing effective and responsible AI
practices. The findings of this study provides a foundational basis for future
research and development of comprehensive governance models that align with RAI
principles.
",0
Adversarial AI in Insurance: Pervasiveness and Resilience,"Elisa Luciano, Matteo Cattaneo, Ron Kenett",2023-01-17T08:49:54Z,Adversarial AI,"  The rapid and dynamic pace of Artificial Intelligence (AI) and Machine
Learning (ML) is revolutionizing the insurance sector. AI offers significant,
very much welcome advantages to insurance companies, and is fundamental to
their customer-centricity strategy. It also poses challenges, in the project
and implementation phase. Among those, we study Adversarial Attacks, which
consist of the creation of modified input data to deceive an AI system and
produce false outputs. We provide examples of attacks on insurance AI
applications, categorize them, and argue on defence methods and precautionary
systems, considering that they can involve few-shot and zero-shot
multilabelling. A related topic, with growing interest, is the validation and
verification of systems incorporating AI and ML components. These topics are
discussed in various sections of this paper.
",0
"AI Alignment Dialogues: An Interactive Approach to AI Alignment in
  Support Agents","Pei-Yu Chen, Myrthe L. Tielman, Dirk K. J. Heylen, Catholijn M. Jonker, M. Birna van Riemsdijk",2023-01-16T13:19:53Z,AI Alignment,"  AI alignment is about ensuring AI systems only pursue goals and activities
that are beneficial to humans. Most of the current approach to AI alignment is
to learn what humans value from their behavioural data. This paper proposes a
different way of looking at the notion of alignment, namely by introducing AI
Alignment Dialogues: dialogues with which users and agents try to achieve and
maintain alignment via interaction. We argue that alignment dialogues have a
number of advantages in comparison to data-driven approaches, especially for
behaviour support agents, which aim to support users in achieving their desired
future behaviours rather than their current behaviours. The advantages of
alignment dialogues include allowing the users to directly convey higher-level
concepts to the agent, and making the agent more transparent and trustworthy.
In this paper we outline the concept and high-level structure of alignment
dialogues. Moreover, we conducted a qualitative focus group user study from
which we developed a model that describes how alignment dialogues affect users,
and created design suggestions for AI alignment dialogues. Through this we
establish foundations for AI alignment dialogues and shed light on what
requires further development and research.
",0
ALTO: An Efficient Network Orchestrator for Compound AI Systems,"Keshav Santhanam, Deepti Raghavan, Muhammad Shahir Rahman, Thejas Venkatesh, Neha Kunjal, Pratiksha Thaker, Philip Levis, Matei Zaharia",2024-03-07T08:30:26Z,Compound AI Systems,"  We present ALTO, a network orchestrator for efficiently serving compound AI
systems such as pipelines of language models. ALTO achieves high throughput and
low latency by taking advantage of an optimization opportunity specific to
generative language models: streaming intermediate outputs. As language models
produce outputs token by token, ALTO exposes opportunities to stream
intermediate outputs between stages when possible. We highlight two new
challenges of correctness and load balancing which emerge when streaming
intermediate data across distributed pipeline stage instances. We also motivate
the need for an aggregation-aware routing interface and distributed
prompt-aware scheduling to address these challenges. We demonstrate the impact
of ALTO's partial output streaming on a complex chatbot verification pipeline,
increasing throughput by up to 3x for a fixed latency target of 4 seconds /
request while also reducing tail latency by 1.8x compared to a baseline serving
approach.
",0
"Responsible-AI-by-Design: a Pattern Collection for Designing Responsible
  AI Systems","Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle",2022-03-02T07:30:03Z,Other,"  Although AI has significant potential to transform society, there are serious
concerns about its ability to behave and make decisions responsibly. Many
ethical regulations, principles, and guidelines for responsible AI have been
issued recently. However, these principles are high-level and difficult to put
into practice. In the meantime much effort has been put into responsible AI
from the algorithm perspective, but they are limited to a small subset of
ethical principles amenable to mathematical analysis. Responsible AI issues go
beyond data and algorithms and are often at the system-level crosscutting many
system components and the entire software engineering lifecycle. Based on the
result of a systematic literature review, this paper identifies one missing
element as the system-level guidance - how to design the architecture of
responsible AI systems. We present a summary of design patterns that can be
embedded into the AI systems as product features to contribute to
responsible-AI-by-design.
",0
"Investigating ADR mechanisms with knowledge graph mining and explainable
  AI","Emmanuel Bresso, Pierre Monnin, Cédric Bousquet, François-Elie Calvier, Ndeye-Coumba Ndiaye, Nadine Petitpain, Malika Smaïl-Tabbone, Adrien Coulet",2020-12-16T16:59:25Z,Other,"  Adverse Drug Reactions (ADRs) are characterized within randomized clinical
trials and postmarketing pharmacovigilance, but their molecular mechanism
remains unknown in most cases. Aside from clinical trials, many elements of
knowledge about drug ingredients are available in open-access knowledge graphs.
In addition, drug classifications that label drugs as either causative or not
for several ADRs, have been established. We propose to mine knowledge graphs
for identifying biomolecular features that may enable reproducing automatically
expert classifications that distinguish drug causative or not for a given type
of ADR. In an explainable AI perspective, we explore simple classification
techniques such as Decision Trees and Classification Rules because they provide
human-readable models, which explain the classification itself, but may also
provide elements of explanation for molecular mechanisms behind ADRs. In
summary, we mine a knowledge graph for features; we train classifiers at
distinguishing, drugs associated or not with ADRs; we isolate features that are
both efficient in reproducing expert classifications and interpretable by
experts (i.e., Gene Ontology terms, drug targets, or pathway names); and we
manually evaluate how they may be explanatory. Extracted features reproduce
with a good fidelity classifications of drugs causative or not for DILI and
SCAR. Experts fully agreed that 73% and 38% of the most discriminative features
are possibly explanatory for DILI and SCAR, respectively; and partially agreed
(2/3) for 90% and 77% of them. Knowledge graphs provide diverse features to
enable simple and explainable models to distinguish between drugs that are
causative or not for ADRs. In addition to explaining classifications, most
discriminative features appear to be good candidates for investigating ADR
mechanisms further.
",0
EUCA: the End-User-Centered Explainable AI Framework,"Weina Jin, Jianyu Fan, Diane Gromala, Philippe Pasquier, Ghassan Hamarneh",2021-02-04T06:39:31Z,Explainable AI,"  The ability to explain decisions to end-users is a necessity to deploy AI as
critical decision support. Yet making AI explainable to non-technical end-users
is a relatively ignored and challenging problem. To bridge the gap, we first
identify twelve end-user-friendly explanatory forms that do not require
technical knowledge to comprehend, including feature-, example-, and rule-based
explanations. We then instantiate the explanatory forms as prototyping cards in
four AI-assisted critical decision-making tasks, and conduct a user study to
co-design low-fidelity prototypes with 32 layperson participants. The results
confirm the relevance of using explanatory forms as building blocks of
explanations, and identify their proprieties - pros, cons, applicable
explanation goals, and design implications. The explanatory forms, their
proprieties, and prototyping supports (including a suggested prototyping
process, design templates and exemplars, and associated algorithms to actualize
explanatory forms) constitute the End-User-Centered explainable AI framework
EUCA, and is available at http://weinajin.github.io/end-user-xai . It serves as
a practical prototyping toolkit for HCI/AI practitioners and researchers to
understand user requirements and build end-user-centered explainable AI.
",14
VitrAI -- Applying Explainable AI in the Real World,"Marc Hanussek, Falko Kötter, Maximilien Kintz, Jens Drawehn",2021-02-12T13:44:39Z,Explainable AI,"  With recent progress in the field of Explainable Artificial Intelligence
(XAI) and increasing use in practice, the need for an evaluation of different
XAI methods and their explanation quality in practical usage scenarios arises.
For this purpose, we present VitrAI, which is a web-based service with the goal
of uniformly demonstrating four different XAI algorithms in the context of
three real life scenarios and evaluating their performance and
comprehensibility for humans. This work reveals practical obstacles when
adopting XAI methods and gives qualitative estimates on how well different
approaches perform in said scenarios.
",2
"Explaining Network Intrusion Detection System Using Explainable AI
  Framework","Shraddha Mane, Dattaraj Rao",2021-03-12T07:15:09Z,Explainable AI,"  Cybersecurity is a domain where the data distribution is constantly changing
with attackers exploring newer patterns to attack cyber infrastructure.
Intrusion detection system is one of the important layers in cyber safety in
today's world. Machine learning based network intrusion detection systems
started showing effective results in recent years. With deep learning models,
detection rates of network intrusion detection system are improved. More
accurate the model, more the complexity and hence less the interpretability.
Deep neural networks are complex and hard to interpret which makes difficult to
use them in production as reasons behind their decisions are unknown. In this
paper, we have used deep neural network for network intrusion detection and
also proposed explainable AI framework to add transparency at every stage of
machine learning pipeline. This is done by leveraging Explainable AI algorithms
which focus on making ML models less of black boxes by providing explanations
as to why a prediction is made. Explanations give us measurable factors as to
what features influence the prediction of a cyberattack and to what degree.
These explanations are generated from SHAP, LIME, Contrastive Explanations
Method, ProtoDash and Boolean Decision Rules via Column Generation. We apply
these approaches to NSL KDD dataset for intrusion detection system and
demonstrate results.
",0
Focused LRP: Explainable AI for Face Morphing Attack Detection,"Clemens Seibold, Anna Hilsmann, Peter Eisert",2021-03-26T19:05:01Z,Explainable AI,"  The task of detecting morphed face images has become highly relevant in
recent years to ensure the security of automatic verification systems based on
facial images, e.g. automated border control gates. Detection methods based on
Deep Neural Networks (DNN) have been shown to be very suitable to this end.
However, they do not provide transparency in the decision making and it is not
clear how they distinguish between genuine and morphed face images. This is
particularly relevant for systems intended to assist a human operator, who
should be able to understand the reasoning. In this paper, we tackle this
problem and present Focused Layer-wise Relevance Propagation (FLRP). This
framework explains to a human inspector on a precise pixel level, which image
regions are used by a Deep Neural Network to distinguish between a genuine and
a morphed face image. Additionally, we propose another framework to objectively
analyze the quality of our method and compare FLRP to other DNN
interpretability methods. This evaluation framework is based on removing
detected artifacts and analyzing the influence of these changes on the decision
of the DNN. Especially, if the DNN is uncertain in its decision or even
incorrect, FLRP performs much better in highlighting visible artifacts compared
to other methods.
",0
Question-Driven Design Process for Explainable AI User Experiences,"Q. Vera Liao, Milena Pribić, Jaesik Han, Sarah Miller, Daby Sow",2021-04-08T02:51:36Z,Explainable AI,"  A pervasive design issue of AI systems is their explainability--how to
provide appropriate information to help users understand the AI. The technical
field of explainable AI (XAI) has produced a rich toolbox of techniques.
Designers are now tasked with the challenges of how to select the most suitable
XAI techniques and translate them into UX solutions. Informed by our previous
work studying design challenges around XAI UX, this work proposes a design
process to tackle these challenges. We review our and related prior work to
identify requirements that the process should fulfill, and accordingly, propose
a Question-Driven Design Process that grounds the user needs, choices of XAI
techniques, design, and evaluation of XAI UX all in the user questions. We
provide a mapping guide between prototypical user questions and exemplars of
XAI techniques to reframe the technical space of XAI, also serving as boundary
objects to support collaboration between designers and AI engineers. We
demonstrate it with a use case of designing XAI for healthcare adverse events
prediction, and discuss lessons learned for tackling design challenges of AI
systems.
",0
Order in the Court: Explainable AI Methods Prone to Disagreement,"Michael Neely, Stefan F. Schouten, Maurits J. R. Bleeker, Ana Lucic",2021-05-07T14:27:37Z,Explainable AI,"  By computing the rank correlation between attention weights and
feature-additive explanation methods, previous analyses either invalidate or
support the role of attention-based explanations as a faithful and plausible
measure of salience. To investigate whether this approach is appropriate, we
compare LIME, Integrated Gradients, DeepLIFT, Grad-SHAP, Deep-SHAP, and
attention-based explanations, applied to two neural architectures trained on
single- and pair-sequence language tasks. In most cases, we find that none of
our chosen methods agree. Based on our empirical observations and theoretical
objections, we conclude that rank correlation does not measure the quality of
feature-additive methods. Practitioners should instead use the numerous and
rigorous diagnostic methods proposed by the community.
",0
XAI Handbook: Towards a Unified Framework for Explainable AI,"Sebastian Palacio, Adriano Lucieri, Mohsin Munir, Jörn Hees, Sheraz Ahmed, Andreas Dengel",2021-05-14T07:28:21Z,Explainable AI,"  The field of explainable AI (XAI) has quickly become a thriving and prolific
community. However, a silent, recurrent and acknowledged issue in this area is
the lack of consensus regarding its terminology. In particular, each new
contribution seems to rely on its own (and often intuitive) version of terms
like ""explanation"" and ""interpretation"". Such disarray encumbers the
consolidation of advances in the field towards the fulfillment of scientific
and regulatory demands e.g., when comparing methods or establishing their
compliance with respect to biases and fairness constraints. We propose a
theoretical framework that not only provides concrete definitions for these
terms, but it also outlines all steps necessary to produce explanations and
interpretations. The framework also allows for existing contributions to be
re-contextualized such that their scope can be measured, thus making them
comparable to other methods. We show that this framework is compliant with
desiderata on explanations, on interpretability and on evaluation metrics. We
present a use-case showing how the framework can be used to compare LIME, SHAP
and MDNet, establishing their advantages and shortcomings. Finally, we discuss
relevant trends in XAI as well as recommendations for future work, all from the
standpoint of our framework.
",0
"Evaluating the Correctness of Explainable AI Algorithms for
  Classification","Orcun Yalcin, Xiuyi Fan, Siyuan Liu",2021-05-20T13:36:41Z,Explainable AI,"  Explainable AI has attracted much research attention in recent years with
feature attribution algorithms, which compute ""feature importance"" in
predictions, becoming increasingly popular. However, there is little analysis
of the validity of these algorithms as there is no ""ground truth"" in the
existing datasets to validate their correctness. In this work, we develop a
method to quantitatively evaluate the correctness of XAI algorithms by creating
datasets with known explanation ground truth. To this end, we focus on the
binary classification problems. String datasets are constructed using formal
language derived from a grammar. A string is positive if and only if a certain
property is fulfilled. Symbols serving as explanation ground truth in a
positive string are part of an explanation if and only if they contributes to
fulfilling the property. Two popular feature attribution explainers, Local
Interpretable Model-agnostic Explanations (LIME) and SHapley Additive
exPlanations (SHAP), are used in our experiments.We show that: (1)
classification accuracy is positively correlated with explanation accuracy; (2)
SHAP provides more accurate explanations than LIME; (3) explanation accuracy is
negatively correlated with dataset complexity.
",14
"Zero-shot learning approach to adaptive Cybersecurity using Explainable
  AI","Dattaraj Rao, Shraddha Mane",2021-06-21T06:29:13Z,Other,"  Cybersecurity is a domain where there is constant change in patterns of
attack, and we need ways to make our Cybersecurity systems more adaptive to
handle new attacks and categorize for appropriate action. We present a novel
approach to handle the alarm flooding problem faced by Cybersecurity systems
like security information and event management (SIEM) and intrusion detection
(IDS). We apply a zero-shot learning method to machine learning (ML) by
leveraging explanations for predictions of anomalies generated by a ML model.
This approach has huge potential to auto detect alarm labels generated in SIEM
and associate them with specific attack types. In this approach, without any
prior knowledge of attack, we try to identify it, decipher the features that
contribute to classification and try to bucketize the attack in a specific
category - using explainable AI. Explanations give us measurable factors as to
what features influence the prediction of a cyber-attack and to what degree.
These explanations generated based on game-theory are used to allocate credit
to specific features based on their influence on a specific prediction. Using
this allocation of credit, we propose a novel zero-shot approach to categorize
novel attacks into specific new classes based on feature influence. The
resulting system demonstrated will get good at separating attack traffic from
normal flow and auto-generate a label for attacks based on features that
contribute to the attack. These auto-generated labels can be presented to SIEM
analyst and are intuitive enough to figure out the nature of attack. We apply
this approach to a network flow dataset and demonstrate results for specific
attack types like ip sweep, denial of service, remote to local, etc.
  Paper was presented at the first Conference on Deployable AI at IIT-Madras in
June 2021.
",0
Explainable AI Enabled Inspection of Business Process Prediction Models,"Chun Ouyang, Renuka Sindhgatta, Catarina Moreira",2021-07-16T06:51:18Z,Explainable AI,"  Modern data analytics underpinned by machine learning techniques has become a
key enabler to the automation of data-led decision making. As an important
branch of state-of-the-art data analytics, business process predictions are
also faced with a challenge in regard to the lack of explanation to the
reasoning and decision by the underlying `black-box' prediction models. With
the development of interpretable machine learning techniques, explanations can
be generated for a black-box model, making it possible for (human) users to
access the reasoning behind machine learned predictions. In this paper, we aim
to present an approach that allows us to use model explanations to investigate
certain reasoning applied by machine learned predictions and detect potential
issues with the underlying methods thus enhancing trust in business process
prediction models. A novel contribution of our approach is the proposal of
model inspection that leverages both the explanations generated by
interpretable machine learning mechanisms and the contextual or domain
knowledge extracted from event logs that record historical process execution.
Findings drawn from this work are expected to serve as a key input to
developing model reliability metrics and evaluation in the context of business
process predictions.
",4
A Checklist for Explainable AI in the Insurance Domain,"Olivier Koster, Ruud Kosman, Joost Visser",2021-07-18T10:19:04Z,Explainable AI,"  Artificial intelligence (AI) is a powerful tool to accomplish a great many
tasks. This exciting branch of technology is being adopted increasingly across
varying sectors, including the insurance domain. With that power arise several
complications. One of which is a lack of transparency and explainability of an
algorithm for experts and non-experts alike. This brings into question both the
usefulness as well as the accuracy of the algorithm, coupled with an added
difficulty to assess potential biases within the data or the model. In this
paper, we investigate the current usage of AI algorithms in the Dutch insurance
industry and the adoption of explainable artificial intelligence (XAI)
techniques. Armed with this knowledge we design a checklist for insurance
companies that should help assure quality standards regarding XAI and a solid
foundation for cooperation between organisations. This checklist extends an
existing checklist of SIVI, the standardisation institute for digital
cooperation and innovation in Dutch insurance.
",0
Secondary control activation analysed and predicted with explainable AI,"Johannes Kruse, Benjamin Schäfer, Dirk Witthaut",2021-09-10T11:39:53Z,Explainable AI,"  The transition to a renewable energy system poses challenges for power grid
operation and stability. Secondary control is key in restoring the power system
to its reference following a disturbance. Underestimating the necessary control
capacity may require emergency measures, such as load shedding. Hence, a solid
understanding of the emerging risks and the driving factors of control is
needed. In this contribution, we establish an explainable machine learning
model for the activation of secondary control power in Germany. Training
gradient boosted trees, we obtain an accurate description of control
activation. Using SHapely Additive exPlanation (SHAP) values, we investigate
the dependency between control activation and external features such as the
generation mix, forecasting errors, and electricity market data. Thereby, our
analysis reveals drivers that lead to high reserve requirements in the German
power system. Our transparent approach, utilizing open data and making machine
learning models interpretable, opens new scientific discovery avenues.
",0
Explainability Pitfalls: Beyond Dark Patterns in Explainable AI,"Upol Ehsan, Mark O. Riedl",2021-09-26T03:05:50Z,Explainable AI,"  To make Explainable AI (XAI) systems trustworthy, understanding harmful
effects is just as important as producing well-designed explanations. In this
paper, we address an important yet unarticulated type of negative effect in
XAI. We introduce explainability pitfalls(EPs), unanticipated negative
downstream effects from AI explanations manifesting even when there is no
intention to manipulate users. EPs are different from, yet related to, dark
patterns, which are intentionally deceptive practices. We articulate the
concept of EPs by demarcating it from dark patterns and highlighting the
challenges arising from uncertainties around pitfalls. We situate and
operationalize the concept using a case study that showcases how, despite best
intentions, unsuspecting negative effects such as unwarranted trust in
numerical explanations can emerge. We propose proactive and preventative
strategies to address EPs at three interconnected levels: research, design, and
organizational.
",0
Human-Centered Explainable AI (XAI): From Algorithms to User Experiences,"Q. Vera Liao, Kush R. Varshney",2021-10-20T21:33:46Z,Explainable AI,"  In recent years, the field of explainable AI (XAI) has produced a vast
collection of algorithms, providing a useful toolbox for researchers and
practitioners to build XAI applications. With the rich application
opportunities, explainability is believed to have moved beyond a demand by data
scientists or researchers to comprehend the models they develop, to an
essential requirement for people to trust and adopt AI deployed in numerous
domains. However, explainability is an inherently human-centric property and
the field is starting to embrace human-centered approaches. Human-computer
interaction (HCI) research and user experience (UX) design in this area are
becoming increasingly important. In this chapter, we begin with a high-level
overview of the technical landscape of XAI algorithms, then selectively survey
our own and other recent HCI works that take human-centered approaches to
design, evaluate, and provide conceptual and methodological tools for XAI. We
ask the question ""what are human-centered approaches doing for XAI"" and
highlight three roles that they play in shaping XAI technologies by helping
navigate, assess and expand the XAI toolbox: to drive technical choices by
users' explainability needs, to uncover pitfalls of existing XAI methods and
inform new methods, and to provide conceptual frameworks for human-compatible
XAI.
",0
"ProtoShotXAI: Using Prototypical Few-Shot Architecture for Explainable
  AI","Samuel Hess, Gregory Ditzler",2021-10-22T05:24:52Z,Other,"  Unexplainable black-box models create scenarios where anomalies cause
deleterious responses, thus creating unacceptable risks. These risks have
motivated the field of eXplainable Artificial Intelligence (XAI) to improve
trust by evaluating local interpretability in black-box neural networks.
Unfortunately, the ground truth is unavailable for the model's decision, so
evaluation is limited to qualitative assessment. Further, interpretability may
lead to inaccurate conclusions about the model or a false sense of trust. We
propose to improve XAI from the vantage point of the user's trust by exploring
a black-box model's latent feature space. We present an approach, ProtoShotXAI,
that uses a Prototypical few-shot network to explore the contrastive manifold
between nonlinear features of different classes. A user explores the manifold
by perturbing the input features of a query sample and recording the response
for a subset of exemplars from any class. Our approach is the first locally
interpretable XAI model that can be extended to, and demonstrated on, few-shot
networks. We compare ProtoShotXAI to the state-of-the-art XAI approaches on
MNIST, Omniglot, and ImageNet to demonstrate, both quantitatively and
qualitatively, that ProtoShotXAI provides more flexibility for model
exploration. Finally, ProtoShotXAI also demonstrates novel explainabilty and
detectabilty on adversarial samples.
",0
From Kepler to Newton: Explainable AI for Science,"Zelong Li, Jianchao Ji, Yongfeng Zhang",2021-11-24T00:45:27Z,Explainable AI,"  The Observation--Hypothesis--Prediction--Experimentation loop paradigm for
scientific research has been practiced by researchers for years towards
scientific discoveries. However, with data explosion in both mega-scale and
milli-scale scientific research, it has been sometimes very difficult to
manually analyze the data and propose new hypotheses to drive the cycle for
scientific discovery. In this paper, we discuss the role of Explainable AI in
scientific discovery process by demonstrating an Explainable AI-based paradigm
for science discovery. The key is to use Explainable AI to help derive data or
model interpretations, hypotheses, as well as scientific discoveries or
insights. We show how computational and data-intensive methodology -- together
with experimental and theoretical methodology -- can be seamlessly integrated
for scientific research. To demonstrate the AI-based science discovery process,
and to pay our respect to some of the greatest minds in human history, we show
how Kepler's laws of planetary motion and Newton's law of universal gravitation
can be rediscovered by (Explainable) AI based on Tycho Brahe's astronomical
observation data, whose works were leading the scientific revolution in the
16-17th century. This work also highlights the important role of Explainable AI
(as compared to Blackbox AI) in science discovery to help humans prevent or
better prepare for the possible technological singularity that may happen in
the future, since science is not only about the know how, but also the know
why. Presentation of the work is available at
https://slideslive.com/38986142/from-kepler-to-newton-explainable-ai-for-science-discovery.
",0
Towards Relatable Explainable AI with the Perceptual Process,"Wencan Zhang, Brian Y. Lim",2021-12-28T05:48:53Z,Explainable AI,"  Machine learning models need to provide contrastive explanations, since
people often seek to understand why a puzzling prediction occurred instead of
some expected outcome. Current contrastive explanations are rudimentary
comparisons between examples or raw features, which remain difficult to
interpret, since they lack semantic meaning. We argue that explanations must be
more relatable to other concepts, hypotheticals, and associations. Inspired by
the perceptual process from cognitive psychology, we propose the XAI Perceptual
Processing Framework and RexNet model for relatable explainable AI with
Contrastive Saliency, Counterfactual Synthetic, and Contrastive Cues
explanations. We investigated the application of vocal emotion recognition, and
implemented a modular multi-task deep neural network to predict and explain
emotions from speech. From think-aloud and controlled studies, we found that
counterfactual explanations were useful and further enhanced with semantic
cues, but not saliency explanations. This work provides insights into providing
and evaluating relatable contrastive explainable AI for perception
applications.
",50
"From Explanations to Segmentation: Using Explainable AI for Image
  Segmentation","Clemens Seibold, Johannes Künzel, Anna Hilsmann, Peter Eisert",2022-02-01T10:26:10Z,Explainable AI,"  The new era of image segmentation leveraging the power of Deep Neural Nets
(DNNs) comes with a price tag: to train a neural network for pixel-wise
segmentation, a large amount of training samples has to be manually labeled on
pixel-precision. In this work, we address this by following an indirect
solution. We build upon the advances of the Explainable AI (XAI) community and
extract a pixel-wise binary segmentation from the output of the Layer-wise
Relevance Propagation (LRP) explaining the decision of a classification
network. We show that we achieve similar results compared to an established
U-Net segmentation architecture, while the generation of the training data is
significantly simplified. The proposed method can be trained in a weakly
supervised fashion, as the training samples must be only labeled on
image-level, at the same time enabling the output of a segmentation mask. This
makes it especially applicable to a wider range of real applications where
tedious pixel-level labelling is often not possible.
",0
Assessing Gender Bias in Predictive Algorithms using eXplainable AI,"Cristina Manresa-Yee, Silvia Ramis",2022-03-19T07:47:45Z,Explainable AI,"  Predictive algorithms have a powerful potential to offer benefits in areas as
varied as medicine or education. However, these algorithms and the data they
use are built by humans, consequently, they can inherit the bias and prejudices
present in humans. The outcomes can systematically repeat errors that create
unfair results, which can even lead to situations of discrimination (e.g.
gender, social or racial). In order to illustrate how important is to count
with a diverse training dataset to avoid bias, we manipulate a well-known
facial expression recognition dataset to explore gender bias and discuss its
implications.
",8
On the Influence of Explainable AI on Automation Bias,"Max Schemmer, Niklas Kühl, Carina Benz, Gerhard Satzger",2022-04-19T12:54:23Z,Explainable AI,"  Artificial intelligence (AI) is gaining momentum, and its importance for the
future of work in many areas, such as medicine and banking, is continuously
rising. However, insights on the effective collaboration of humans and AI are
still rare. Typically, AI supports humans in decision-making by addressing
human limitations. However, it may also evoke human bias, especially in the
form of automation bias as an over-reliance on AI advice. We aim to shed light
on the potential to influence automation bias by explainable AI (XAI). In this
pre-test, we derive a research model and describe our study design.
Subsequentially, we conduct an online experiment with regard to hotel review
classifications and discuss first results. We expect our research to contribute
to the design and development of safe hybrid intelligence systems.
",0
ExMo: Explainable AI Model using Inverse Frequency Decision Rules,"Pradip Mainali, Ismini Psychoula, Fabien A. P. Petitcolas",2022-05-20T09:36:49Z,Explainable AI,"  In this paper, we present a novel method to compute decision rules to build a
more accurate interpretable machine learning model, denoted as ExMo. The ExMo
interpretable machine learning model consists of a list of IF...THEN...
statements with a decision rule in the condition. This way, ExMo naturally
provides an explanation for a prediction using the decision rule that was
triggered. ExMo uses a new approach to extract decision rules from the training
data using term frequency-inverse document frequency (TF-IDF) features. With
TF-IDF, decision rules with feature values that are more relevant to each class
are extracted. Hence, the decision rules obtained by ExMo can distinguish the
positive and negative classes better than the decision rules used in the
existing Bayesian Rule List (BRL) algorithm, obtained using the frequent
pattern mining approach. The paper also shows that ExMo learns a qualitatively
better model than BRL. Furthermore, ExMo demonstrates that the textual
explanation can be provided in a human-friendly way so that the explanation can
be easily understood by non-expert users. We validate ExMo on several datasets
with different sizes to evaluate its efficacy. Experimental validation on a
real-world fraud detection application shows that ExMo is 20% more accurate
than BRL and that it achieves accuracy similar to those of deep learning
models.
",0
Is explainable AI a race against model complexity?,Advait Sarkar,2022-05-17T09:57:25Z,Explainable AI,"  Explaining the behaviour of intelligent systems will get increasingly and
perhaps intractably challenging as models grow in size and complexity. We may
not be able to expect an explanation for every prediction made by a brain-scale
model, nor can we expect explanations to remain objective or apolitical. Our
functionalist understanding of these models is of less advantage than we might
assume. Models precede explanations, and can be useful even when both model and
explanation are incorrect. Explainability may never win the race against
complexity, but this is less problematic than it seems.
",0
Why we do need Explainable AI for Healthcare,"Giovanni Cinà, Tabea Röber, Rob Goedhart, Ilker Birbil",2022-06-30T15:35:50Z,Explainable AI,"  The recent spike in certified Artificial Intelligence (AI) tools for
healthcare has renewed the debate around adoption of this technology. One
thread of such debate concerns Explainable AI and its promise to render AI
devices more transparent and trustworthy. A few voices active in the medical AI
space have expressed concerns on the reliability of Explainable AI techniques,
questioning their use and inclusion in guidelines and standards. Revisiting
such criticisms, this article offers a balanced and comprehensive perspective
on the utility of Explainable AI, focusing on the specificity of clinical
applications of AI and placing them in the context of healthcare interventions.
Against its detractors and despite valid concerns, we argue that the
Explainable AI research program is still central to human-machine interaction
and ultimately our main tool against loss of control, a danger that cannot be
prevented by rigorous clinical validation alone.
",0
Towards Smart Fake News Detection Through Explainable AI,"Athira A B, S D Madhu Kumar, Anu Mary Chacko",2022-07-23T10:48:45Z,Explainable AI,"  People now see social media sites as their sole source of information due to
their popularity. The Majority of people get their news through social media.
At the same time, fake news has grown exponentially on social media platforms
in recent years. Several artificial intelligence-based solutions for detecting
fake news have shown promising results. On the other hand, these detection
systems lack explanation capabilities, i.e., the ability to explain why they
made a prediction. This paper highlights the current state of the art in
explainable fake news detection. We discuss the pitfalls in the current
explainable AI-based fake news detection models and present our ongoing
research on multi-modal explainable fake news detection model.
",1
Explainable AI based Glaucoma Detection using Transfer Learning and LIME,"Touhidul Islam Chayan, Anita Islam, Eftykhar Rahman, Md. Tanzim Reza, Tasnim Sakib Apon, MD. Golam Rabiul Alam",2022-10-07T05:36:33Z,Explainable AI,"  Glaucoma is the second driving reason for partial or complete blindness among
all the visual deficiencies which mainly occurs because of excessive pressure
in the eye due to anxiety or depression which damages the optic nerve and
creates complications in vision. Traditional glaucoma screening is a
time-consuming process that necessitates the medical professionals' constant
attention, and even so time to time due to the time constrains and pressure
they fail to classify correctly that leads to wrong treatment. Numerous efforts
have been made to automate the entire glaucoma classification procedure
however, these existing models in general have a black box characteristics that
prevents users from understanding the key reasons behind the prediction and
thus medical practitioners generally can not rely on these system. In this
article after comparing with various pre-trained models, we propose a transfer
learning model that is able to classify Glaucoma with 94.71\% accuracy. In
addition, we have utilized Local Interpretable Model-Agnostic
Explanations(LIME) that introduces explainability in our system. This
improvement enables medical professionals obtain important and comprehensive
information that aid them in making judgments. It also lessen the opacity and
fragility of the traditional deep learning models.
",0
"Utilizing Explainable AI for improving the Performance of Neural
  Networks","Huawei Sun, Lorenzo Servadei, Hao Feng, Michael Stephan, Robert Wille, Avik Santra",2022-10-07T09:39:20Z,Explainable AI,"  Nowadays, deep neural networks are widely used in a variety of fields that
have a direct impact on society. Although those models typically show
outstanding performance, they have been used for a long time as black boxes. To
address this, Explainable Artificial Intelligence (XAI) has been developing as
a field that aims to improve the transparency of the model and increase their
trustworthiness. We propose a retraining pipeline that consistently improves
the model predictions starting from XAI and utilizing state-of-the-art
techniques. To do that, we use the XAI results, namely SHapley Additive
exPlanations (SHAP) values, to give specific training weights to the data
samples. This leads to an improved training of the model and, consequently,
better performance. In order to benchmark our method, we evaluate it on both
real-life and public datasets. First, we perform the method on a radar-based
people counting scenario. Afterward, we test it on the CIFAR-10, a public
Computer Vision dataset. Experiments using the SHAP-based retraining approach
achieve a 4% more accuracy w.r.t. the standard equal weight retraining for
people counting tasks. Moreover, on the CIFAR-10, our SHAP-based weighting
strategy ends up with a 3% accuracy rate than the training procedure with equal
weighted samples.
",0
Seamful XAI: Operationalizing Seamful Design in Explainable AI,"Upol Ehsan, Q. Vera Liao, Samir Passi, Mark O. Riedl, Hal Daume III",2022-11-12T21:54:05Z,Explainable AI,"  Mistakes in AI systems are inevitable, arising from both technical
limitations and sociotechnical gaps. While black-boxing AI systems can make the
user experience seamless, hiding the seams risks disempowering users to
mitigate fallouts from AI mistakes. Instead of hiding these AI imperfections,
can we leverage them to help the user? While Explainable AI (XAI) has
predominantly tackled algorithmic opaqueness, we propose that seamful design
can foster AI explainability by revealing and leveraging sociotechnical and
infrastructural mismatches. We introduce the concept of Seamful XAI by (1)
conceptually transferring ""seams"" to the AI context and (2) developing a design
process that helps stakeholders anticipate and design with seams. We explore
this process with 43 AI practitioners and real end-users, using a
scenario-based co-design activity informed by real-world use cases. We found
that the Seamful XAI design process helped users foresee AI harms, identify
underlying reasons (seams), locate them in the AI's lifecycle, learn how to
leverage seamful information to improve XAI and user agency. We share empirical
insights, implications, and reflections on how this process can help
practitioners anticipate and craft seams in AI, how seamfulness can improve
explainability, empower end-users, and facilitate Responsible AI.
",0
Bengali Handwritten Digit Recognition using CNN with Explainable AI,"Md Tanvir Rouf Shawon, Raihan Tanvir, Md. Golam Rabiul Alam",2022-12-23T04:40:20Z,Explainable AI,"  Handwritten character recognition is a hot topic for research nowadays. If we
can convert a handwritten piece of paper into a text-searchable document using
the Optical Character Recognition (OCR) technique, we can easily understand the
content and do not need to read the handwritten document. OCR in the English
language is very common, but in the Bengali language, it is very hard to find a
good quality OCR application. If we can merge machine learning and deep
learning with OCR, it could be a huge contribution to this field. Various
researchers have proposed a number of strategies for recognizing Bengali
handwritten characters. A lot of ML algorithms and deep neural networks were
used in their work, but the explanations of their models are not available. In
our work, we have used various machine learning algorithms and CNN to recognize
handwritten Bengali digits. We have got acceptable accuracy from some ML
models, and CNN has given us great testing accuracy. Grad-CAM was used as an
XAI method on our CNN model, which gave us insights into the model and helped
us detect the origin of interest for recognizing a digit from an image.
",0
"Explainable AI for Bioinformatics: Methods, Tools, and Applications","Md. Rezaul Karim, Tanhim Islam, Oya Beyan, Christoph Lange, Michael Cochez, Dietrich Rebholz-Schuhmann, Stefan Decker",2022-12-25T21:00:36Z,Explainable AI,"  Artificial intelligence (AI) systems utilizing deep neural networks (DNNs)
and machine learning (ML) algorithms are widely used for solving important
problems in bioinformatics, biomedical informatics, and precision medicine.
However, complex DNNs or ML models, which are often perceived as opaque and
black-box, can make it difficult to understand the reasoning behind their
decisions. This lack of transparency can be a challenge for both end-users and
decision-makers, as well as AI developers. Additionally, in sensitive areas
like healthcare, explainability and accountability are not only desirable but
also legally required for AI systems that can have a significant impact on
human lives. Fairness is another growing concern, as algorithmic decisions
should not show bias or discrimination towards certain groups or individuals
based on sensitive attributes. Explainable artificial intelligence (XAI) aims
to overcome the opaqueness of black-box models and provide transparency in how
AI systems make decisions. Interpretable ML models can explain how they make
predictions and the factors that influence their outcomes. However, most
state-of-the-art interpretable ML methods are domain-agnostic and evolved from
fields like computer vision, automated reasoning, or statistics, making direct
application to bioinformatics problems challenging without customization and
domain-specific adaptation. In this paper, we discuss the importance of
explainability in the context of bioinformatics, provide an overview of
model-specific and model-agnostic interpretable ML methods and tools, and
outline their potential caveats and drawbacks. Besides, we discuss how to
customize existing interpretable ML methods for bioinformatics problems.
Nevertheless, we demonstrate how XAI methods can improve transparency through
case studies in bioimaging, cancer genomics, and text mining.
",0
"Towards Reconciling Usability and Usefulness of Explainable AI
  Methodologies","Pradyumna Tambwekar, Matthew Gombolay",2023-01-13T01:08:49Z,Explainable AI,"  Interactive Artificial Intelligence (AI) agents are becoming increasingly
prevalent in society. However, application of such systems without
understanding them can be problematic. Black-box AI systems can lead to
liability and accountability issues when they produce an incorrect decision.
Explainable AI (XAI) seeks to bridge the knowledge gap, between developers and
end-users, by offering insights into how an AI algorithm functions. Many modern
algorithms focus on making the AI model ""transparent"", i.e. unveil the inherent
functionality of the agent in a simpler format. However, these approaches do
not cater to end-users of these systems, as users may not possess the requisite
knowledge to understand these explanations in a reasonable amount of time.
Therefore, to be able to develop suitable XAI methods, we need to understand
the factors which influence subjective perception and objective usability. In
this paper, we present a novel user-study which studies four differing XAI
modalities commonly employed in prior work for explaining AI behavior, i.e.
Decision Trees, Text, Programs. We study these XAI modalities in the context of
explaining the actions of a self-driving car on a highway, as driving is an
easily understandable real-world task and self-driving cars is a keen area of
interest within the AI community. Our findings highlight internal consistency
issues wherein participants perceived language explanations to be significantly
more usable, however participants were better able to objectively understand
the decision making process of the car through a decision tree explanation. Our
work also provides further evidence of importance of integrating user-specific
and situational criteria into the design of XAI systems. Our findings show that
factors such as computer science experience, and watching the car succeed or
fail can impact the perception and usefulness of the explanation.
",0
Selective Explanations: Leveraging Human Input to Align Explainable AI,"Vivian Lai, Yiming Zhang, Chacha Chen, Q. Vera Liao, Chenhao Tan",2023-01-23T19:00:02Z,"RAG, Explainable AI","  While a vast collection of explainable AI (XAI) algorithms have been
developed in recent years, they are often criticized for significant gaps with
how humans produce and consume explanations. As a result, current XAI
techniques are often found to be hard to use and lack effectiveness. In this
work, we attempt to close these gaps by making AI explanations selective -- a
fundamental property of human explanations -- by selectively presenting a
subset from a large set of model reasons based on what aligns with the
recipient's preferences. We propose a general framework for generating
selective explanations by leveraging human input on a small sample. This
framework opens up a rich design space that accounts for different selectivity
goals, types of input, and more. As a showcase, we use a decision-support task
to explore selective explanations based on what the decision-maker would
consider relevant to the decision task. We conducted two experimental studies
to examine three out of a broader possible set of paradigms based on our
proposed framework: in Study 1, we ask the participants to provide their own
input to generate selective explanations, with either open-ended or
critique-based input. In Study 2, we show participants selective explanations
based on input from a panel of similar users (annotators). Our experiments
demonstrate the promise of selective explanations in reducing over-reliance on
AI and improving decision outcomes and subjective perceptions of the AI, but
also paint a nuanced picture that attributes some of these positive effects to
the opportunity to provide one's own input to augment AI explanations. Overall,
our work proposes a novel XAI framework inspired by human communication
behaviors and demonstrates its potentials to encourage future work to better
align AI explanations with human production and consumption of explanations.
",0
Towards Modelling and Verification of Social Explainable AI,"Damian Kurpiewski, Wojciech Jamroga, Teofil Sidoruk",2023-02-02T12:48:31Z,Explainable AI,"  Social Explainable AI (SAI) is a new direction in artificial intelligence
that emphasises decentralisation, transparency, social context, and focus on
the human users. SAI research is still at an early stage. Consequently, it
concentrates on delivering the intended functionalities, but largely ignores
the possibility of unwelcome behaviours due to malicious or erroneous activity.
We propose that, in order to capture the breadth of relevant aspects, one can
use models and logics of strategic ability, that have been developed in
multi-agent systems. Using the STV model checker, we take the first step
towards the formal modelling and verification of SAI environments, in
particular of their resistance to various types of attacks by compromised AI
modules.
",0
"LiteVR: Interpretable and Lightweight Cybersickness Detection using
  Explainable AI","Ripan Kumar Kundu, Rifatul Islam, John Quarles, Khaza Anuarul Hoque",2023-02-05T21:51:12Z,Explainable AI,"  Cybersickness is a common ailment associated with virtual reality (VR) user
experiences. Several automated methods exist based on machine learning (ML) and
deep learning (DL) to detect cybersickness. However, most of these
cybersickness detection methods are perceived as computationally intensive and
black-box methods. Thus, those techniques are neither trustworthy nor practical
for deploying on standalone energy-constrained VR head-mounted devices (HMDs).
In this work, we present an explainable artificial intelligence (XAI)-based
framework, LiteVR, for cybersickness detection, explaining the model's outcome
and reducing the feature dimensions and overall computational costs. First, we
develop three cybersickness DL models based on long-term short-term memory
(LSTM), gated recurrent unit (GRU), and multilayer perceptron (MLP). Then, we
employed a post-hoc explanation, such as SHapley Additive Explanations (SHAP),
to explain the results and extract the most dominant features of cybersickness.
Finally, we retrain the DL models with the reduced number of features. Our
results show that eye-tracking features are the most dominant for cybersickness
detection. Furthermore, based on the XAI-based feature ranking and
dimensionality reduction, we significantly reduce the model's size by up to
4.3x, training time by up to 5.6x, and its inference time by up to 3.8x, with
higher cybersickness detection accuracy and low regression error (i.e., on Fast
Motion Scale (FMS)). Our proposed lite LSTM model obtained an accuracy of 94%
in classifying cybersickness and regressing (i.e., FMS 1-10) with a Root Mean
Square Error (RMSE) of 0.30, which outperforms the state-of-the-art. Our
proposed LiteVR framework can help researchers and practitioners analyze,
detect, and deploy their DL-based cybersickness detection models in standalone
VR HMDs.
",0
"A Systematic Literature Review of Explainable AI for Software
  Engineering","Ahmad Haji Mohammadkhani, Nitin Sai Bommi, Mariem Daboussi, Onkar Sabnis, Chakkrit Tantithamthavorn, Hadi Hemmati",2023-02-13T02:59:41Z,Explainable AI,"  Context: In recent years, leveraging machine learning (ML) techniques has
become one of the main solutions to tackle many software engineering (SE)
tasks, in research studies (ML4SE). This has been achieved by utilizing
state-of-the-art models that tend to be more complex and black-box, which is
led to less explainable solutions that reduce trust and uptake of ML4SE
solutions by professionals in the industry.
  Objective: One potential remedy is to offer explainable AI (XAI) methods to
provide the missing explainability. In this paper, we aim to explore to what
extent XAI has been studied in the SE community (XAI4SE) and provide a
comprehensive view of the current state-of-the-art as well as challenge and
roadmap for future work.
  Method: We conduct a systematic literature review on 24 (out of 869 primary
studies that were selected by keyword search) most relevant published studies
in XAI4SE. We have three research questions that were answered by meta-analysis
of the collected data per paper.
  Results: Our study reveals that among the identified studies, software
maintenance (\%68) and particularly defect prediction has the highest share on
the SE stages and tasks being studied. Additionally, we found that XAI methods
were mainly applied to classic ML models rather than more complex models. We
also noticed a clear lack of standard evaluation metrics for XAI methods in the
literature which has caused confusion among researchers and a lack of
benchmarks for comparisons.
  Conclusions: XAI has been identified as a helpful tool by most studies, which
we cover in the systematic review. However, XAI4SE is a relatively new domain
with a lot of untouched potentials, including the SE tasks to help with, the
ML4SE methods to explain, and the types of explanations to offer. This study
encourages the researchers to work on the identified challenges and roadmap
reported in the paper.
",0
Towards Trust of Explainable AI in Thyroid Nodule Diagnosis,"Truong Thanh Hung Nguyen, Van Binh Truong, Vo Thanh Khang Nguyen, Quoc Hung Cao, Quoc Khanh Nguyen",2023-03-08T17:18:13Z,Explainable AI,"  The ability to explain the prediction of deep learning models to end-users is
an important feature to leverage the power of artificial intelligence (AI) for
the medical decision-making process, which is usually considered
non-transparent and challenging to comprehend. In this paper, we apply
state-of-the-art eXplainable artificial intelligence (XAI) methods to explain
the prediction of the black-box AI models in the thyroid nodule diagnosis
application. We propose new statistic-based XAI methods, namely Kernel Density
Estimation and Density map, to explain the case of no nodule detected. XAI
methods' performances are considered under a qualitative and quantitative
comparison as feedback to improve the data quality and the model performance.
Finally, we survey to assess doctors' and patients' trust in XAI explanations
of the model's decisions on thyroid nodule images.
",0
Explainable AI for Time Series via Virtual Inspection Layers,"Johanna Vielhaben, Sebastian Lapuschkin, Grégoire Montavon, Wojciech Samek",2023-03-11T10:20:47Z,Explainable AI,"  The field of eXplainable Artificial Intelligence (XAI) has greatly advanced
in recent years, but progress has mainly been made in computer vision and
natural language processing. For time series, where the input is often not
interpretable, only limited research on XAI is available. In this work, we put
forward a virtual inspection layer, that transforms the time series to an
interpretable representation and allows to propagate relevance attributions to
this representation via local XAI methods like layer-wise relevance propagation
(LRP). In this way, we extend the applicability of a family of XAI methods to
domains (e.g. speech) where the input is only interpretable after a
transformation. Here, we focus on the Fourier transformation which is
prominently applied in the interpretation of time series and LRP and refer to
our method as DFT-LRP. We demonstrate the usefulness of DFT-LRP in various time
series classification settings like audio and electronic health records. We
showcase how DFT-LRP reveals differences in the classification strategies of
models trained in different domains (e.g., time vs. frequency domain) or helps
to discover how models act on spurious correlations in the data.
",0
"Sensitive Region-based Metamorphic Testing Framework using Explainable
  AI","Yuma Torikoshi, Yasuharu Nishi, Juichi Takahashi",2023-03-14T01:56:15Z,Other,"  Deep Learning (DL) is one of the most popular research topics in machine
learning and DL-driven image recognition systems have developed rapidly. Recent
research has employed metamorphic testing (MT) to detect misclassified images.
Most of them discuss metamorphic relations (MR), with limited attention given
to which regions should be transformed. We focus on the fact that there are
sensitive regions where even small transformations can easily change the
prediction results and propose an MT framework that efficiently tests for
regions prone to misclassification by transforming these sensitive regions. Our
evaluation demonstrated that the sensitive regions can be specified by
Explainable AI (XAI) and our framework effectively detects faults.
",0
"Smart ROI Detection for Alzheimer's disease prediction using explainable
  AI","Atefe Aghaei, Mohsen Ebrahimi Moghaddam",2023-03-18T11:58:56Z,Other,"  Purpose Predicting the progression of MCI to Alzheimer's disease is an
important step in reducing the progression of the disease. Therefore, many
methods have been introduced for this task based on deep learning. Among these
approaches, the methods based on ROIs are in a good position in terms of
accuracy and complexity. In these techniques, some specific parts of the brain
are extracted as ROI manually for all of the patients. Extracting ROI manually
is time-consuming and its results depend on human expertness and precision.
Method To overcome these limitations, we propose a novel smart method for
detecting ROIs automatically based on Explainable AI using Grad-Cam and a 3DCNN
model that extracts ROIs per patient. After extracting the ROIs automatically,
Alzheimer's disease is predicted using extracted ROI-based 3D CNN. Results We
implement our method on 176 MCI patients of the famous ADNI dataset and obtain
remarkable results compared to the state-of-the-art methods. The accuracy
acquired using 5-fold cross-validation is 98.6 and the AUC is 1. We also
compare the results of the ROI-based method with the whole brain-based method.
The results show that the performance is impressively increased. Conclusion The
experimental results show that the proposed smart ROI extraction, which
extracts the ROIs automatically, performs well for Alzheimer's disease
prediction. The proposed method can also be used for Alzheimer's disease
classification and diagnosis.
",0
Interpretable Bangla Sarcasm Detection using BERT and Explainable AI,"Ramisa Anan, Tasnim Sakib Apon, Zeba Tahsin Hossain, Elizabeth Antora Modhu, Sudipta Mondal, MD. Golam Rabiul Alam",2023-03-22T17:35:35Z,Explainable AI,"  A positive phrase or a sentence with an underlying negative motive is usually
defined as sarcasm that is widely used in today's social media platforms such
as Facebook, Twitter, Reddit, etc. In recent times active users in social media
platforms are increasing dramatically which raises the need for an automated
NLP-based system that can be utilized in various tasks such as determining
market demand, sentiment analysis, threat detection, etc. However, since
sarcasm usually implies the opposite meaning and its detection is frequently a
challenging issue, data meaning extraction through an NLP-based model becomes
more complicated. As a result, there has been a lot of study on sarcasm
detection in English over the past several years, and there's been a noticeable
improvement and yet sarcasm detection in the Bangla language's state remains
the same. In this article, we present a BERT-based system that can achieve
99.60\% while the utilized traditional machine learning algorithms are only
capable of achieving 89.93\%. Additionally, we have employed Local
Interpretable Model-Agnostic Explanations that introduce explainability to our
system. Moreover, we have utilized a newly collected bangla sarcasm dataset,
BanglaSarc that was constructed specifically for the evaluation of this study.
This dataset consists of fresh records of sarcastic and non-sarcastic comments,
the majority of which are acquired from Facebook and YouTube comment sections.
",0
XAIR: A Framework of Explainable AI in Augmented Reality,"Xuhai Xu, Mengjie Yu, Tanya R. Jonker, Kashyap Todi, Feiyu Lu, Xun Qian, João Marcelo Evangelista Belo, Tianyi Wang, Michelle Li, Aran Mun, Te-Yen Wu, Junxiao Shen, Ting Zhang, Narine Kokhlikyan, Fulton Wang, Paul Sorenson, Sophie Kahyun Kim, Hrvoje Benko",2023-03-28T20:14:29Z,Explainable AI,"  Explainable AI (XAI) has established itself as an important component of
AI-driven interactive systems. With Augmented Reality (AR) becoming more
integrated in daily lives, the role of XAI also becomes essential in AR because
end-users will frequently interact with intelligent services. However, it is
unclear how to design effective XAI experiences for AR. We propose XAIR, a
design framework that addresses ""when"", ""what"", and ""how"" to provide
explanations of AI output in AR. The framework was based on a
multi-disciplinary literature review of XAI and HCI research, a large-scale
survey probing 500+ end-users' preferences for AR-based explanations, and three
workshops with 12 experts collecting their insights about XAI design in AR.
XAIR's utility and effectiveness was verified via a study with 10 designers and
another study with 12 end-users. XAIR can provide guidelines for designers,
inspiring them to identify new design opportunities and achieve effective XAI
designs in AR.
",0
"Towards Explainable AI Writing Assistants for Non-native English
  Speakers","Yewon Kim, Mina Lee, Donghwi Kim, Sung-Ju Lee",2023-04-05T17:51:36Z,Explainable AI,"  We highlight the challenges faced by non-native speakers when using AI
writing assistants to paraphrase text. Through an interview study with 15
non-native English speakers (NNESs) with varying levels of English proficiency,
we observe that they face difficulties in assessing paraphrased texts generated
by AI writing assistants, largely due to the lack of explanations accompanying
the suggested paraphrases. Furthermore, we examine their strategies to assess
AI-generated texts in the absence of such explanations. Drawing on the needs of
NNESs identified in our interview, we propose four potential user interfaces to
enhance the writing experience of NNESs using AI writing assistants. The
proposed designs focus on incorporating explanations to better support NNESs in
understanding and evaluating the AI-generated paraphrasing suggestions.
",0
Explainable AI And Visual Reasoning: Insights From Radiology,"Robert Kaufman, David Kirsh",2023-04-06T18:30:27Z,Explainable AI,"  Why do explainable AI (XAI) explanations in radiology, despite their promise
of transparency, still fail to gain human trust? Current XAI approaches provide
justification for predictions, however, these do not meet practitioners' needs.
These XAI explanations lack intuitive coverage of the evidentiary basis for a
given classification, posing a significant barrier to adoption. We posit that
XAI explanations that mirror human processes of reasoning and justification
with evidence may be more useful and trustworthy than traditional visual
explanations like heat maps. Using a radiology case study, we demonstrate how
radiology practitioners get other practitioners to see a diagnostic
conclusion's validity. Machine-learned classifications lack this evidentiary
grounding and consequently fail to elicit trust and adoption by potential
users. Insights from this study may generalize to guiding principles for
human-centered explanation design based on human reasoning and justification of
evidence.
",5
Towards a Praxis for Intercultural Ethics in Explainable AI,Chinasa T. Okolo,2023-04-24T07:15:58Z,Explainable AI,"  Explainable AI (XAI) is often promoted with the idea of helping users
understand how machine learning models function and produce predictions. Still,
most of these benefits are reserved for those with specialized domain
knowledge, such as machine learning developers. Recent research has argued that
making AI explainable can be a viable way of making AI more useful in
real-world contexts, especially within low-resource domains in the Global
South. While AI has transcended borders, a limited amount of work focuses on
democratizing the concept of explainable AI to the ""majority world"", leaving
much room to explore and develop new approaches within this space that cater to
the distinct needs of users within culturally and socially-diverse regions.
This article introduces the concept of an intercultural ethics approach to AI
explainability. It examines how cultural nuances impact the adoption and use of
technology, the factors that impede how technical concepts such as AI are
explained, and how integrating an intercultural ethics approach in the
development of XAI can improve user understanding and facilitate efficient
usage of these methods.
",0
Categorical Foundations of Explainable AI: A Unifying Theory,"Pietro Barbiero, Stefano Fioravanti, Francesco Giannini, Alberto Tonda, Pietro Lio, Elena Di Lavore",2023-04-27T11:10:16Z,Explainable AI,"  Explainable AI (XAI) aims to address the human need for safe and reliable AI
systems. However, numerous surveys emphasize the absence of a sound
mathematical formalization of key XAI notions -- remarkably including the term
""explanation"" which still lacks a precise definition. To bridge this gap, this
paper presents the first mathematically rigorous definitions of key XAI notions
and processes, using the well-funded formalism of Category theory. We show that
our categorical framework allows to: (i) model existing learning schemes and
architectures, (ii) formally define the term ""explanation"", (iii) establish a
theoretical basis for XAI taxonomies, and (iv) analyze commonly overlooked
aspects of explaining methods. As a consequence, our categorical framework
promotes the ethical and secure deployment of AI technologies as it represents
a significant step towards a sound theoretical foundation of explainable AI.
",0
"Explainable AI and Machine Learning Towards Human Gait Deterioration
  Analysis",Abdullah Alharthi,2023-06-12T14:53:00Z,Explainable AI,"  Gait analysis, an expanding research area, employs non invasive sensors and
machine learning techniques for a range of applicatio ns. In this study, we
concentrate on gait analysis for detecting cognitive decline in Parkinson's
disease (PD) and under dual task conditions. Using convolutional neural
networks (CNNs) and explainable machine learning, we objectively analyze gait
data and associate findings with clinically relevant biomarkers. This is
accomplished by connecting machine learning outputs to decisions based on human
visual observations or derived quantitative gait parameters, which are tested
and routinely implemented in curr ent healthcare practice. Our analysis of gait
deterioration due to cognitive decline in PD enables robust results using the
proposed methods for assessing PD severity from ground reaction force (GRF)
data. We achieved classification accuracies of 98% F1 sc ores for each
PhysioNet.org dataset and 95.5% F1 scores for the combined PhysioNet dataset.
By linking clinically observable features to the model outputs, we demonstrate
the impact of PD severity on gait. Furthermore, we explore the significance of
cognit ive load in healthy gait analysis, resulting in robust classification
accuracies of 100% F1 scores for subject identity verification. We also
identify weaker features crucial for model predictions using Layer Wise
Relevance Propagation. A notable finding o f this study reveals that cognitive
deterioration's effect on gait influences body balance and foot landing/lifting
dynamics in both classification cases: cognitive load in healthy gait and
cognitive decline in PD gait.
",0
"Unlocking Sales Growth: Account Prioritization Engine with Explainable
  AI","Suvendu Jena, Jilei Yang, Fangfang Tan",2023-06-12T23:42:08Z,Other,"  B2B sales requires effective prediction of customer growth, identification of
upsell potential, and mitigation of churn risks. LinkedIn sales representatives
traditionally relied on intuition and fragmented data signals to assess
customer performance. This resulted in significant time investment in data
understanding as well as strategy formulation and under-investment in active
selling. To overcome this challenge, we developed a data product called Account
Prioritizer, an intelligent sales account prioritization engine. It uses
machine learning recommendation models and integrated account-level explanation
algorithms within the sales CRM to automate the manual process of sales book
prioritization. A successful A/B test demonstrated that the Account Prioritizer
generated a substantial +8.08% increase in renewal bookings for the LinkedIn
Business.
",0
Detection of Sensor-To-Sensor Variations using Explainable AI,"Sarah Seifi, Sebastian A. Schober, Cecilia Carbonelli, Lorenzo Servadei, Robert Wille",2023-06-19T11:00:54Z,Explainable AI,"  With the growing concern for air quality and its impact on human health,
interest in environmental gas monitoring has increased. However,
chemi-resistive gas sensing devices are plagued by issues of sensor
reproducibility during manufacturing. This study proposes a novel approach for
detecting sensor-to-sensor variations in sensing devices using the explainable
AI (XAI) method of SHapley Additive exPlanations (SHAP). This is achieved by
identifying sensors that contribute the most to environmental gas concentration
estimation via machine learning, and measuring the similarity of feature
rankings between sensors to flag deviations or outliers. The methodology is
tested using artificial and realistic Ozone concentration profiles to train a
Gated Recurrent Unit (GRU) model. Two applications were explored in the study:
the detection of wrong behaviors of sensors in the train dataset, and the
detection of deviations in the test dataset. By training the GRU with the
pruned train dataset, we could reduce computational costs while improving the
model performance. Overall, the results show that our approach improves the
understanding of sensor behavior, successfully detects sensor deviations down
to 5-10% from the normal behavior, and leads to more efficient model
preparation and calibration. Our method provides a novel solution for
identifying deviating sensors, linking inconsistencies in hardware to
sensor-to-sensor variations in the manufacturing process on an AI model-level.
",0
Towards Explainable AI for Channel Estimation in Wireless Communications,"Abdul Karim Gizzini, Yahia Medjahdi, Ali J. Ghandour, Laurent Clavier",2023-07-03T11:51:00Z,Explainable AI,"  Research into 6G networks has been initiated to support a variety of critical
artificial intelligence (AI) assisted applications such as autonomous driving.
In such applications, AI-based decisions should be performed in a real-time
manner. These decisions include resource allocation, localization, channel
estimation, etc. Considering the black-box nature of existing AI-based models,
it is highly challenging to understand and trust the decision-making behavior
of such models. Therefore, explaining the logic behind those models through
explainable AI (XAI) techniques is essential for their employment in critical
applications. This manuscript proposes a novel XAI-based channel estimation
(XAI-CHEST) scheme that provides detailed reasonable interpretability of the
deep learning (DL) models that are employed in doubly-selective channel
estimation. The aim of the proposed XAI-CHEST scheme is to identify the
relevant model inputs by inducing high noise on the irrelevant ones. As a
result, the behavior of the studied DL-based channel estimators can be further
analyzed and evaluated based on the generated interpretations. Simulation
results show that the proposed XAI-CHEST scheme provides valid interpretations
of the DL-based channel estimators for different scenarios.
",0
CoVid-19 Detection leveraging Vision Transformers and Explainable AI,"Pangoth Santhosh Kumar, Kundrapu Supriya, Mallikharjuna Rao K, Taraka Satya Krishna Teja Malisetti",2023-07-29T17:45:27Z,"RAG, Explainable AI","  Lung disease is a common health problem in many parts of the world. It is a
significant risk to people health and quality of life all across the globe
since it is responsible for five of the top thirty leading causes of death.
Among them are COVID 19, pneumonia, and tuberculosis, to name just a few. It is
critical to diagnose lung diseases in their early stages. Several different
models including machine learning and image processing have been developed for
this purpose. The earlier a condition is diagnosed, the better the patient
chances of making a full recovery and surviving into the long term. Thanks to
deep learning algorithms, there is significant promise for the autonomous,
rapid, and accurate identification of lung diseases based on medical imaging.
Several different deep learning strategies, including convolutional neural
networks (CNN), vanilla neural networks, visual geometry group based networks
(VGG), and capsule networks , are used for the goal of making lung disease
forecasts. The standard CNN has a poor performance when dealing with rotated,
tilted, or other aberrant picture orientations. As a result of this, within the
scope of this study, we have suggested a vision transformer based approach end
to end framework for the diagnosis of lung disorders. In the architecture, data
augmentation, training of the suggested models, and evaluation of the models
are all included. For the purpose of detecting lung diseases such as pneumonia,
Covid 19, lung opacity, and others, a specialised Compact Convolution
Transformers (CCT) model have been tested and evaluated on datasets such as the
Covid 19 Radiography Database. The model has achieved a better accuracy for
both its training and validation purposes on the Covid 19 Radiography Database.
",0
"Explainable AI in Orthopedics: Challenges, Opportunities, and Prospects","Soheyla Amirian, Luke A. Carlson, Matthew F. Gong, Ines Lohse, Kurt R. Weiss, Johannes F. Plate, Ahmad P. Tafti",2023-08-09T04:15:10Z,Explainable AI,"  While artificial intelligence (AI) has made many successful applications in
various domains, its adoption in healthcare lags a little bit behind other
high-stakes settings. Several factors contribute to this slower uptake,
including regulatory frameworks, patient privacy concerns, and data
heterogeneity. However, one significant challenge that impedes the
implementation of AI in healthcare, particularly in orthopedics, is the lack of
explainability and interpretability around AI models. Addressing the challenge
of explainable AI (XAI) in orthopedics requires developing AI models and
algorithms that prioritize transparency and interpretability, allowing
clinicians, surgeons, and patients to understand the contributing factors
behind any AI-powered predictive or descriptive models. The current
contribution outlines several key challenges and opportunities that manifest in
XAI in orthopedic practice. This work emphasizes the need for interdisciplinary
collaborations between AI practitioners, orthopedic specialists, and regulatory
entities to establish standards and guidelines for the adoption of XAI in
orthopedics.
",1
Explainable AI applications in the Medical Domain: a systematic review,"Nicoletta Prentzas, Antonis Kakas, Constantinos S. Pattichis",2023-08-10T08:12:17Z,Explainable AI,"  Artificial Intelligence in Medicine has made significant progress with
emerging applications in medical imaging, patient care, and other areas. While
these applications have proven successful in retrospective studies, very few of
them were applied in practice.The field of Medical AI faces various challenges,
in terms of building user trust, complying with regulations, using data
ethically.Explainable AI (XAI) aims to enable humans understand AI and trust
its results. This paper presents a literature review on the recent developments
of XAI solutions for medical decision support, based on a representative sample
of 198 articles published in recent years. The systematic synthesis of the
relevant articles resulted in several findings. (1) model-agnostic XAI
techniques were mostly employed in these solutions, (2) deep learning models
are utilized more than other types of machine learning models, (3)
explainability was applied to promote trust, but very few works reported the
physicians participation in the loop, (4) visual and interactive user interface
is more useful in understanding the explanation and the recommendation of the
system. More research is needed in collaboration between medical and AI
experts, that could guide the development of suitable frameworks for the
design, implementation, and evaluation of XAI solutions in medicine.
",0
"Towards a Comprehensive Human-Centred Evaluation Framework for
  Explainable AI","Ivania Donoso-Guzmán, Jeroen Ooge, Denis Parra, Katrien Verbert",2023-07-31T09:20:16Z,Explainable AI,"  While research on explainable AI (XAI) is booming and explanation techniques
have proven promising in many application domains, standardised human-centred
evaluation procedures are still missing. In addition, current evaluation
procedures do not assess XAI methods holistically in the sense that they do not
treat explanations' effects on humans as a complex user experience. To tackle
this challenge, we propose to adapt the User-Centric Evaluation Framework used
in recommender systems: we integrate explanation aspects, summarise explanation
properties, indicate relations between them, and categorise metrics that
measure these properties. With this comprehensive evaluation framework, we hope
to contribute to the human-centred standardisation of XAI evaluation.
",0
Explainable AI for tool wear prediction in turning,"Saleh Valizadeh Sotubadi, Rui Liu, Vinh Neguyen",2023-08-17T03:36:13Z,Explainable AI,"  This research aims develop an Explainable Artificial Intelligence (XAI)
framework to facilitate human-understandable solutions for tool wear prediction
during turning. A random forest algorithm was used as the supervised Machine
Learning (ML) classifier for training and binary classification using
acceleration, acoustics, temperature, and spindle speed during the orthogonal
tube turning process as input features. The ML classifier was used to predict
the condition of the tool after the cutting process, which was determined in a
binary class form indicating if the cutting tool was available or failed. After
the training process, the Shapley criterion was used to explain the predictions
of the trained ML classifier. Specifically, the significance of each input
feature in the decision-making and classification was identified to explain the
reasoning of the ML classifier predictions. After implementing the Shapley
criterion on all testing datasets, the tool temperature was identified as the
most significant feature in determining the classification of available versus
failed cutting tools. Hence, this research demonstrates capability of XAI to
provide machining operators the ability to diagnose and understand complex ML
classifiers in prediction of tool wear.
",0
"XIMAGENET-12: An Explainable AI Benchmark Dataset for Model Robustness
  Evaluation","Qiang Li, Dan Zhang, Shengzhao Lei, Xun Zhao, Porawit Kamnoedboon, WeiWei Li, Junhao Dong, Shuyan Li",2023-10-12T10:17:40Z,Explainable AI,"  Despite the promising performance of existing visual models on public
benchmarks, the critical assessment of their robustness for real-world
applications remains an ongoing challenge. To bridge this gap, we propose an
explainable visual dataset, XIMAGENET-12, to evaluate the robustness of visual
models. XIMAGENET-12 consists of over 200K images with 15,410 manual semantic
annotations. Specifically, we deliberately selected 12 categories from
ImageNet, representing objects commonly encountered in practical life. To
simulate real-world situations, we incorporated six diverse scenarios, such as
overexposure, blurring, and color changes, etc. We further develop a
quantitative criterion for robustness assessment, allowing for a nuanced
understanding of how visual models perform under varying conditions, notably in
relation to the background. We make the XIMAGENET-12 dataset and its
corresponding code openly accessible at
\url{https://sites.google.com/view/ximagenet-12/home}. We expect the
introduction of the XIMAGENET-12 dataset will empower researchers to thoroughly
evaluate the robustness of their visual models under challenging conditions.
",1
A Critical Survey on Fairness Benefits of Explainable AI,"Luca Deck, Jakob Schoeffer, Maria De-Arteaga, Niklas Kühl",2023-10-15T08:17:45Z,Explainable AI,"  In this critical survey, we analyze typical claims on the relationship
between explainable AI (XAI) and fairness to disentangle the multidimensional
relationship between these two concepts. Based on a systematic literature
review and a subsequent qualitative content analysis, we identify seven
archetypal claims from 175 scientific articles on the alleged fairness benefits
of XAI. We present crucial caveats with respect to these claims and provide an
entry point for future discussions around the potentials and limitations of XAI
for specific fairness desiderata. Importantly, we notice that claims are often
(i) vague and simplistic, (ii) lacking normative grounding, or (iii) poorly
aligned with the actual capabilities of XAI. We suggest to conceive XAI not as
an ethical panacea but as one of many tools to approach the multidimensional,
sociotechnical challenge of algorithmic fairness. Moreover, when making a claim
about XAI and fairness, we emphasize the need to be more specific about what
kind of XAI method is used, which fairness desideratum it refers to, how
exactly it enables fairness, and who is the stakeholder that benefits from XAI.
",0
Medical Image Denosing via Explainable AI Feature Preserving Loss,"Guanfang Dong, Anup Basu",2023-10-31T00:37:03Z,Explainable AI,"  Denoising algorithms play a crucial role in medical image processing and
analysis. However, classical denoising algorithms often ignore explanatory and
critical medical features preservation, which may lead to misdiagnosis and
legal liabilities. In this work, we propose a new denoising method for medical
images that not only efficiently removes various types of noise, but also
preserves key medical features throughout the process. To achieve this goal, we
utilize a gradient-based eXplainable Artificial Intelligence (XAI) approach to
design a feature preserving loss function. Our feature preserving loss function
is motivated by the characteristic that gradient-based XAI is sensitive to
noise. Through backpropagation, medical image features before and after
denoising can be kept consistent. We conducted extensive experiments on three
available medical image datasets, including synthesized 13 different types of
noise and artifacts. The experimental results demonstrate the superiority of
our method in terms of denoising performance, model explainability, and
generalization.
",0
Unveiling The Factors of Aesthetic Preferences with Explainable AI,"Derya Soydaner, Johan Wagemans",2023-11-24T11:06:22Z,Explainable AI,"  The allure of aesthetic appeal in images captivates our senses, yet the
underlying intricacies of aesthetic preferences remain elusive. In this study,
we pioneer a novel perspective by utilizing several different machine learning
(ML) models that focus on aesthetic attributes known to influence preferences.
Our models process these attributes as inputs to predict the aesthetic scores
of images. Moreover, to delve deeper and obtain interpretable explanations
regarding the factors driving aesthetic preferences, we utilize the popular
Explainable AI (XAI) technique known as SHapley Additive exPlanations (SHAP).
Our methodology compares the performance of various ML models, including Random
Forest, XGBoost, Support Vector Regression, and Multilayer Perceptron, in
accurately predicting aesthetic scores, and consistently observing results in
conjunction with SHAP. We conduct experiments on three image aesthetic
benchmarks, namely Aesthetics with Attributes Database (AADB), Explainable
Visual Aesthetics (EVA), and Personalized image Aesthetics database with Rich
Attributes (PARA), providing insights into the roles of attributes and their
interactions. Finally, our study presents ML models for aesthetics research,
alongside the introduction of XAI. Our aim is to shed light on the complex
nature of aesthetic preferences in images through ML and to provide a deeper
understanding of the attributes that influence aesthetic judgements.
",1
FM-G-CAM: A Holistic Approach for Explainable AI in Computer Vision,"Ravidu Suien Rammuni Silva, Jordan J. Bird",2023-12-10T19:33:40Z,Explainable AI,"  Explainability is an aspect of modern AI that is vital for impact and
usability in the real world. The main objective of this paper is to emphasise
the need to understand the predictions of Computer Vision models, specifically
Convolutional Neural Network (CNN) based models. Existing methods of explaining
CNN predictions are mostly based on Gradient-weighted Class Activation Maps
(Grad-CAM) and solely focus on a single target class. We show that from the
point of the target class selection, we make an assumption on the prediction
process, hence neglecting a large portion of the predictor CNN model's thinking
process. In this paper, we present an exhaustive methodology called Fused
Multi-class Gradient-weighted Class Activation Map (FM-G-CAM) that considers
multiple top predicted classes, which provides a holistic explanation of the
predictor CNN's thinking rationale. We also provide a detailed and
comprehensive mathematical and algorithmic description of our method.
Furthermore, along with a concise comparison of existing methods, we compare
FM-G-CAM with Grad-CAM, highlighting its benefits through real-world practical
use cases. Finally, we present an open-source Python library with FM-G-CAM
implementation to conveniently generate saliency maps for CNN-based model
predictions.
",0
"KAXAI: An Integrated Environment for Knowledge Analysis and Explainable
  AI","Saikat Barua, Sifat Momen",2023-12-30T10:20:47Z,Other,"  In order to fully harness the potential of machine learning, it is crucial to
establish a system that renders the field more accessible and less daunting for
individuals who may not possess a comprehensive understanding of its
intricacies. The paper describes the design of a system that integrates AutoML,
XAI, and synthetic data generation to provide a great UX design for users. The
system allows users to navigate and harness the power of machine learning while
abstracting its complexities and providing high usability. The paper proposes
two novel classifiers, Logistic Regression Forest and Support Vector Tree, for
enhanced model performance, achieving 96\% accuracy on a diabetes dataset and
93\% on a survey dataset. The paper also introduces a model-dependent local
interpreter called MEDLEY and evaluates its interpretation against LIME,
Greedy, and Parzen. Additionally, the paper introduces LLM-based synthetic data
generation, library-based data generation, and enhancing the original dataset
with GAN. The findings on synthetic data suggest that enhancing the original
dataset with GAN is the most reliable way to generate synthetic data, as
evidenced by KS tests, standard deviation, and feature importance. The authors
also found that GAN works best for quantitative datasets.
",0
XAI for All: Can Large Language Models Simplify Explainable AI?,"Philip Mavrepis, Georgios Makridis, Georgios Fatouros, Vasileios Koukos, Maria Margarita Separdani, Dimosthenis Kyriazis",2024-01-23T21:47:12Z,Explainable AI,"  The field of Explainable Artificial Intelligence (XAI) often focuses on users
with a strong technical background, making it challenging for non-experts to
understand XAI methods. This paper presents ""x-[plAIn]"", a new approach to make
XAI more accessible to a wider audience through a custom Large Language Model
(LLM), developed using ChatGPT Builder. Our goal was to design a model that can
generate clear, concise summaries of various XAI methods, tailored for
different audiences, including business professionals and academics. The key
feature of our model is its ability to adapt explanations to match each
audience group's knowledge level and interests. Our approach still offers
timely insights, facilitating the decision-making process by the end users.
Results from our use-case studies show that our model is effective in providing
easy-to-understand, audience-specific explanations, regardless of the XAI
method used. This adaptability improves the accessibility of XAI, bridging the
gap between complex AI technologies and their practical applications. Our
findings indicate a promising direction for LLMs in making advanced AI concepts
more accessible to a diverse range of users.
",6
Explainable AI for survival analysis: a median-SHAP approach,"Lucile Ter-Minassian, Sahra Ghalebikesabi, Karla Diaz-Ordaz, Chris Holmes",2024-01-30T20:47:50Z,Explainable AI,"  With the adoption of machine learning into routine clinical practice comes
the need for Explainable AI methods tailored to medical applications. Shapley
values have sparked wide interest for locally explaining models. Here, we
demonstrate their interpretation strongly depends on both the summary statistic
and the estimator for it, which in turn define what we identify as an 'anchor
point'. We show that the convention of using a mean anchor point may generate
misleading interpretations for survival analysis and introduce median-SHAP, a
method for explaining black-box models predicting individual survival times.
",0
"User Decision Guidance with Selective Explanation Presentation from
  Explainable-AI","Yosuke Fukuchi, Seiji Yamada",2024-02-28T03:21:25Z,Other,"  This paper addresses the challenge of selecting explanations for XAI
(Explainable AI)-based Intelligent Decision Support Systems (IDSSs). IDSSs have
shown promise in improving user decisions through XAI-generated explanations
along with AI predictions, and the development of XAI made it possible to
generate a variety of such explanations. However, how IDSSs should select
explanations to enhance user decision-making remains an open question. This
paper proposes X-Selector, a method for selectively presenting XAI
explanations. It enables IDSSs to strategically guide users to an AI-suggested
decision by predicting the impact of different combinations of explanations on
a user's decision and selecting the combination that is expected to minimize
the discrepancy between an AI suggestion and a user decision. We compared the
efficacy of X-Selector with two naive strategies (all possible explanations and
explanations only for the most likely prediction) and two baselines (no
explanation and no AI support). The results suggest the potential of X-Selector
to guide users to AI-suggested decisions and improve task performance under the
condition of a high AI accuracy.
",0
"User Characteristics in Explainable AI: The Rabbit Hole of
  Personalization?","Robert Nimmo, Marios Constantinides, Ke Zhou, Daniele Quercia, Simone Stumpf",2024-02-29T21:36:05Z,Explainable AI,"  As Artificial Intelligence (AI) becomes ubiquitous, the need for Explainable
AI (XAI) has become critical for transparency and trust among users. A
significant challenge in XAI is catering to diverse users, such as data
scientists, domain experts, and end-users. Recent research has started to
investigate how users' characteristics impact interactions with and user
experience of explanations, with a view to personalizing XAI. However, are we
heading down a rabbit hole by focusing on unimportant details? Our research
aimed to investigate how user characteristics are related to using,
understanding, and trusting an AI system that provides explanations. Our
empirical study with 149 participants who interacted with an XAI system that
flagged inappropriate comments showed that very few user characteristics
mattered; only age and the personality trait openness influenced actual
understanding. Our work provides evidence to reorient user-focused XAI research
and question the pursuit of personalized XAI based on fine-grained user
characteristics.
",0
"An Explainable AI Framework for Artificial Intelligence of Medical
  Things","Al Amin, Kamrul Hasan, Saleh Zein-Sabatto, Deo Chimba, Imtiaz Ahmed, Tariqul Islam",2024-03-07T01:08:41Z,Explainable AI,"  The healthcare industry has been revolutionized by the convergence of
Artificial Intelligence of Medical Things (AIoMT), allowing advanced
data-driven solutions to improve healthcare systems. With the increasing
complexity of Artificial Intelligence (AI) models, the need for Explainable
Artificial Intelligence (XAI) techniques become paramount, particularly in the
medical domain, where transparent and interpretable decision-making becomes
crucial. Therefore, in this work, we leverage a custom XAI framework,
incorporating techniques such as Local Interpretable Model-Agnostic
Explanations (LIME), SHapley Additive exPlanations (SHAP), and
Gradient-weighted Class Activation Mapping (Grad-Cam), explicitly designed for
the domain of AIoMT. The proposed framework enhances the effectiveness of
strategic healthcare methods and aims to instill trust and promote
understanding in AI-driven medical applications. Moreover, we utilize a
majority voting technique that aggregates predictions from multiple
convolutional neural networks (CNNs) and leverages their collective
intelligence to make robust and accurate decisions in the healthcare system.
Building upon this decision-making process, we apply the XAI framework to brain
tumor detection as a use case demonstrating accurate and transparent diagnosis.
Evaluation results underscore the exceptional performance of the XAI framework,
achieving high precision, recall, and F1 scores with a training accuracy of 99%
and a validation accuracy of 98%. Combining advanced XAI techniques with
ensemble-based deep-learning (DL) methodologies allows for precise and reliable
brain tumor diagnoses as an application of AIoMT.
",0
Cultural Bias in Explainable AI Research: A Systematic Analysis,"Uwe Peters, Mary Carman",2024-02-28T19:30:32Z,Explainable AI,"  For synergistic interactions between humans and artificial intelligence (AI)
systems, AI outputs often need to be explainable to people. Explainable AI
(XAI) systems are commonly tested in human user studies. However, whether XAI
researchers consider potential cultural differences in human explanatory needs
remains unexplored. We highlight psychological research that found significant
differences in human explanations between many people from Western, commonly
individualist countries and people from non-Western, often collectivist
countries. We argue that XAI research currently overlooks these variations and
that many popular XAI designs implicitly and problematically assume that
Western explanatory needs are shared cross-culturally. Additionally, we
systematically reviewed over 200 XAI user studies and found that most studies
did not consider relevant cultural variations, sampled only Western
populations, but drew conclusions about human-XAI interactions more generally.
We also analyzed over 30 literature reviews of XAI studies. Most reviews did
not mention cultural differences in explanatory needs or flag overly broad
cross-cultural extrapolations of XAI user study results. Combined, our analyses
provide evidence of a cultural bias toward Western populations in XAI research,
highlighting an important knowledge gap regarding how culturally diverse users
may respond to widely used XAI systems that future work can and should address.
",0
Red Teaming Models for Hyperspectral Image Analysis Using Explainable AI,"Vladimir Zaigrajew, Hubert Baniecki, Lukasz Tulczyjew, Agata M. Wijata, Jakub Nalepa, Nicolas Longépé, Przemyslaw Biecek",2024-03-12T18:28:32Z,Explainable AI,"  Remote sensing (RS) applications in the space domain demand machine learning
(ML) models that are reliable, robust, and quality-assured, making red teaming
a vital approach for identifying and exposing potential flaws and biases. Since
both fields advance independently, there is a notable gap in integrating red
teaming strategies into RS. This paper introduces a methodology for examining
ML models operating on hyperspectral images within the HYPERVIEW challenge,
focusing on soil parameters' estimation. We use post-hoc explanation methods
from the Explainable AI (XAI) domain to critically assess the best performing
model that won the HYPERVIEW challenge and served as an inspiration for the
model deployed on board the INTUITION-1 hyperspectral mission. Our approach
effectively red teams the model by pinpointing and validating key shortcomings,
constructing a model that achieves comparable performance using just 1% of the
input features and a mere up to 5% performance loss. Additionally, we propose a
novel way of visualizing explanations that integrate domain-specific
information about hyperspectral bands (wavelengths) and data transformations to
better suit interpreting models for hyperspectral image analysis.
",0
Gradient based Feature Attribution in Explainable AI: A Technical Review,"Yongjie Wang, Tong Zhang, Xu Guo, Zhiqi Shen",2024-03-15T15:49:31Z,Explainable AI,"  The surge in black-box AI models has prompted the need to explain the
internal mechanism and justify their reliability, especially in high-stakes
applications, such as healthcare and autonomous driving. Due to the lack of a
rigorous definition of explainable AI (XAI), a plethora of research related to
explainability, interpretability, and transparency has been developed to
explain and analyze the model from various perspectives. Consequently, with an
exhaustive list of papers, it becomes challenging to have a comprehensive
overview of XAI research from all aspects. Considering the popularity of neural
networks in AI research, we narrow our focus to a specific area of XAI
research: gradient based explanations, which can be directly adopted for neural
network models. In this review, we systematically explore gradient based
explanation methods to date and introduce a novel taxonomy to categorize them
into four distinct classes. Then, we present the essence of technique details
in chronological order and underscore the evolution of algorithms. Next, we
introduce both human and quantitative evaluations to measure algorithm
performance. More importantly, we demonstrate the general challenges in XAI and
specific challenges in gradient based explanations. We hope that this survey
can help researchers understand state-of-the-art progress and their
corresponding disadvantages, which could spark their interest in addressing
these issues in future work.
",0
Leaf-Based Plant Disease Detection and Explainable AI,"Saurav Sagar, Mohammed Javed, David S Doermann",2023-12-17T03:40:12Z,Explainable AI,"  The agricultural sector plays an essential role in the economic growth of a
country. Specifically, in an Indian context, it is the critical source of
livelihood for millions of people living in rural areas. Plant Disease is one
of the significant factors affecting the agricultural sector. Plants get
infected with diseases for various reasons, including synthetic fertilizers,
archaic practices, environmental conditions, etc., which impact the farm yield
and subsequently hinder the economy. To address this issue, researchers have
explored many applications based on AI and Machine Learning techniques to
detect plant diseases. This research survey provides a comprehensive
understanding of common plant leaf diseases, evaluates traditional and deep
learning techniques for disease detection, and summarizes available datasets.
It also explores Explainable AI (XAI) to enhance the interpretability of deep
learning models' decisions for end-users. By consolidating this knowledge, the
survey offers valuable insights to researchers, practitioners, and stakeholders
in the agricultural sector, fostering the development of efficient and
transparent solutions for combating plant diseases and promoting sustainable
agricultural practices.
",0
"How to Build an AI Tutor that Can Adapt to Any Course and Provide
  Accurate Answers Using Large Language Model and Retrieval-Augmented
  Generation",Chenxi Dong,2023-11-29T15:02:46Z,Other,"  This paper proposes a low-code solution to build an AI tutor that leverages
advanced AI techniques to provide accurate and contextually relevant responses
in a personalized learning environment. The OpenAI Assistants API allows AI
Tutor to easily embed, store, retrieve, and manage files and chat history,
enabling a low-code solution. Large Language Models (LLMs) and
Retrieval-Augmented Generation (RAG) technology generate sophisticated answers
based on course-specific materials. The application efficiently organizes and
retrieves relevant information through vector embedding and similarity-based
retrieval algorithms. The AI Tutor prototype demonstrates its ability to
generate relevant, accurate answers with source citations. It represents a
significant advancement in technology-enhanced tutoring systems, democratizing
access to high-quality, customized educational support in higher education.
",0
A Multi-Level Framework for the AI Alignment Problem,"Betty Li Hou, Brian Patrick Green",2023-01-10T01:09:07Z,AI Alignment,"  AI alignment considers how we can encode AI systems in a way that is
compatible with human values. The normative side of this problem asks what
moral values or principles, if any, we should encode in AI. To this end, we
present a framework to consider the question at four levels: Individual,
Organizational, National, and Global. We aim to illustrate how AI alignment is
made up of value alignment problems at each of these levels, where values at
each level affect the others and effects can flow in either direction. We
outline key questions and considerations of each level and demonstrate an
application of this framework to the topic of AI content moderation.
",0
"Methodological reflections for AI alignment research using human
  feedback","Thilo Hagendorff, Sarah Fabi",2022-12-22T14:27:33Z,AI Alignment,"  The field of artificial intelligence (AI) alignment aims to investigate
whether AI technologies align with human interests and values and function in a
safe and ethical manner. AI alignment is particularly relevant for large
language models (LLMs), which have the potential to exhibit unintended behavior
due to their ability to learn and adapt in ways that are difficult to predict.
In this paper, we discuss methodological challenges for the alignment problem
specifically in the context of LLMs trained to summarize texts. In particular,
we focus on methods for collecting reliable human feedback on summaries to
train a reward model which in turn improves the summarization model. We
conclude by suggesting specific improvements in the experimental design of
alignment studies for LLMs' summarization capabilities.
",0
"Decolonial AI Alignment: Openness, Viśe\d{s}a-Dharma, and Including
  Excluded Knowledges",Kush R. Varshney,2023-09-10T14:04:21Z,AI Alignment,"  Prior work has explicated the coloniality of artificial intelligence (AI)
development and deployment through mechanisms such as extractivism, automation,
sociological essentialism, surveillance, and containment. However, that work
has not engaged much with alignment: teaching behaviors to a large language
model (LLM) in line with desired values, and has not considered a mechanism
that arises within that process: moral absolutism -- a part of the coloniality
of knowledge. Colonialism has a history of altering the beliefs and values of
colonized peoples; in this paper, I argue that this history is recapitulated in
current LLM alignment practices and technologies. Furthermore, I suggest that
AI alignment be decolonialized using three forms of openness: openness of
models, openness to society, and openness to excluded knowledges. This
suggested approach to decolonial AI alignment uses ideas from the argumentative
moral philosophical tradition of Hinduism, which has been described as an
open-source religion. One concept used is vi\'{s}e\d{s}a-dharma, or particular
context-specific notions of right and wrong. At the end of the paper, I provide
a suggested reference architecture to work toward the proposed framework.
",0
"AI Alignment and Social Choice: Fundamental Limitations and Policy
  Implications",Abhilash Mishra,2023-10-24T17:59:04Z,AI Alignment,"  Aligning AI agents to human intentions and values is a key bottleneck in
building safe and deployable AI applications. But whose values should AI agents
be aligned with? Reinforcement learning with human feedback (RLHF) has emerged
as the key framework for AI alignment. RLHF uses feedback from human
reinforcers to fine-tune outputs; all widely deployed large language models
(LLMs) use RLHF to align their outputs to human values. It is critical to
understand the limitations of RLHF and consider policy challenges arising from
these limitations. In this paper, we investigate a specific challenge in
building RLHF systems that respect democratic norms. Building on impossibility
results in social choice theory, we show that, under fairly broad assumptions,
there is no unique voting protocol to universally align AI systems using RLHF
through democratic processes. Further, we show that aligning AI agents with the
values of all individuals will always violate certain private ethical
preferences of an individual user i.e., universal AI alignment using RLHF is
impossible. We discuss policy implications for the governance of AI systems
built using RLHF: first, the need for mandating transparent voting rules to
hold model builders accountable. Second, the need for model builders to focus
on developing AI agents that are narrowly aligned to specific user groups.
",0
Social Contract AI: Aligning AI Assistants with Implicit Group Norms,"Jan-Philipp Fränken, Sam Kwok, Peixuan Ye, Kanishk Gandhi, Dilip Arumugam, Jared Moore, Alex Tamkin, Tobias Gerstenberg, Noah D. Goodman",2023-10-26T20:27:03Z,Other,"  We explore the idea of aligning an AI assistant by inverting a model of
users' (unknown) preferences from observed interactions. To validate our
proposal, we run proof-of-concept simulations in the economic ultimatum game,
formalizing user preferences as policies that guide the actions of simulated
players. We find that the AI assistant accurately aligns its behavior to match
standard policies from the economic literature (e.g., selfish, altruistic).
However, the assistant's learned policies lack robustness and exhibit limited
generalization in an out-of-distribution setting when confronted with a
currency (e.g., grams of medicine) that was not included in the assistant's
training distribution. Additionally, we find that when there is inconsistency
in the relationship between language use and an unknown policy (e.g., an
altruistic policy combined with rude language), the assistant's learning of the
policy is slowed. Overall, our preliminary results suggest that developing
simulation frameworks in which AI assistants need to infer preferences from
diverse users can provide a valuable approach for studying practical alignment
questions.
",0
"Interactive AI Alignment: Specification, Process, and Evaluation
  Alignment","Michael Terry, Chinmay Kulkarni, Martin Wattenberg, Lucas Dixon, Meredith Ringel Morris",2023-10-23T14:33:11Z,AI Alignment,"  Modern AI enables a high-level, declarative form of interaction: Users
describe the intended outcome they wish an AI to produce, but do not actually
create the outcome themselves. In contrast, in traditional user interfaces,
users invoke specific operations to create the desired outcome. This paper
revisits the basic input-output interaction cycle in light of this declarative
style of interaction, and connects concepts in AI alignment to define three
objectives for interactive alignment of AI: specification alignment (aligning
on what to do), process alignment (aligning on how to do it), and evaluation
alignment (assisting users in verifying and understanding what was produced).
Using existing systems as examples, we show how these user-centered views of AI
alignment can be used descriptively, prescriptively, and as an evaluative aid.
",0
"Kantian Deontology Meets AI Alignment: Towards Morally Grounded Fairness
  Metrics","Carlos Mougan, Joshua Brand",2023-11-09T09:16:02Z,"AI Alignment, Ontology","  Deontological ethics, specifically understood through Immanuel Kant, provides
a moral framework that emphasizes the importance of duties and principles,
rather than the consequences of action. Understanding that despite the
prominence of deontology, it is currently an overlooked approach in fairness
metrics, this paper explores the compatibility of a Kantian deontological
framework in fairness metrics, part of the AI alignment field. We revisit
Kant's critique of utilitarianism, which is the primary approach in AI fairness
metrics and argue that fairness principles should align with the Kantian
deontological framework. By integrating Kantian ethics into AI alignment, we
not only bring in a widely-accepted prominent moral theory but also strive for
a more morally grounded AI landscape that better balances outcomes and
procedures in pursuit of fairness and justice.
",0
Case Repositories: Towards Case-Based Reasoning for AI Alignment,"K. J. Kevin Feng, Quan Ze Chen, Inyoung Cheong, King Xia, Amy X. Zhang",2023-11-18T02:02:40Z,AI Alignment,"  Case studies commonly form the pedagogical backbone in law, ethics, and many
other domains that face complex and ambiguous societal questions informed by
human values. Similar complexities and ambiguities arise when we consider how
AI should be aligned in practice: when faced with vast quantities of diverse
(and sometimes conflicting) values from different individuals and communities,
with whose values is AI to align, and how should AI do so? We propose a
complementary approach to constitutional AI alignment, grounded in ideas from
case-based reasoning (CBR), that focuses on the construction of policies
through judgments on a set of cases. We present a process to assemble such a
case repository by: 1) gathering a set of ``seed'' cases -- questions one may
ask an AI system -- in a particular domain, 2) eliciting domain-specific key
dimensions for cases through workshops with domain experts, 3) using LLMs to
generate variations of cases not seen in the wild, and 4) engaging with the
public to judge and improve cases. We then discuss how such a case repository
could assist in AI alignment, both through directly acting as precedents to
ground acceptable behaviors, and as a medium for individuals and communities to
engage in moral reasoning around AI.
",0
Toward Human-AI Alignment in Large-Scale Multi-Player Games,"Sugandha Sharma, Guy Davidson, Khimya Khetarpal, Anssi Kanervisto, Udit Arora, Katja Hofmann, Ida Momennejad",2024-02-05T22:55:33Z,AI Alignment,"  Achieving human-AI alignment in complex multi-agent games is crucial for
creating trustworthy AI agents that enhance gameplay. We propose a method to
evaluate this alignment using an interpretable task-sets framework, focusing on
high-level behavioral tasks instead of low-level policies. Our approach has
three components. First, we analyze extensive human gameplay data from Xbox's
Bleeding Edge (100K+ games), uncovering behavioral patterns in a complex task
space. This task space serves as a basis set for a behavior manifold capturing
interpretable axes: fight-flight, explore-exploit, and solo-multi-agent.
Second, we train an AI agent to play Bleeding Edge using a Generative
Pretrained Causal Transformer and measure its behavior. Third, we project human
and AI gameplay to the proposed behavior manifold to compare and contrast. This
allows us to interpret differences in policy as higher-level behavioral
concepts, e.g., we find that while human players exhibit variability in
fight-flight and explore-exploit behavior, AI players tend towards uniformity.
Furthermore, AI agents predominantly engage in solo play, while humans often
engage in cooperative and competitive multi-agent patterns. These stark
differences underscore the need for interpretable evaluation, design, and
integration of AI in human-aligned applications. Our study advances the
alignment discussion in AI and especially generative AI research, offering a
measurable framework for interpretable human-agent alignment in multiplayer
gaming.
",0
Inverse Reinforcement Learning without Reinforcement Learning,"Gokul Swamy, Sanjiban Choudhury, J. Andrew Bagnell, Zhiwei Steven Wu",2023-03-26T04:35:53Z,Reinforcement Learning,"  Inverse Reinforcement Learning (IRL) is a powerful set of techniques for
imitation learning that aims to learn a reward function that rationalizes
expert demonstrations. Unfortunately, traditional IRL methods suffer from a
computational weakness: they require repeatedly solving a hard reinforcement
learning (RL) problem as a subroutine. This is counter-intuitive from the
viewpoint of reductions: we have reduced the easier problem of imitation
learning to repeatedly solving the harder problem of RL. Another thread of work
has proved that access to the side-information of the distribution of states
where a strong policy spends time can dramatically reduce the sample and
computational complexities of solving an RL problem. In this work, we
demonstrate for the first time a more informed imitation learning reduction
where we utilize the state distribution of the expert to alleviate the global
exploration component of the RL subroutine, providing an exponential speedup in
theory. In practice, we find that we are able to significantly speed up the
prior art on continuous control tasks.
",0
"Socially Responsible AI Algorithms: Issues, Purposes, and Challenges","Lu Cheng, Kush R. Varshney, Huan Liu",2021-01-01T17:34:42Z,Responsible AI,"  In the current era, people and society have grown increasingly reliant on
artificial intelligence (AI) technologies. AI has the potential to drive us
towards a future in which all of humanity flourishes. It also comes with
substantial risks for oppression and calamity. Discussions about whether we
should (re)trust AI have repeatedly emerged in recent years and in many
quarters, including industry, academia, healthcare, services, and so on.
Technologists and AI researchers have a responsibility to develop trustworthy
AI systems. They have responded with great effort to design more responsible AI
algorithms. However, existing technical solutions are narrow in scope and have
been primarily directed towards algorithms for scoring or classification tasks,
with an emphasis on fairness and unwanted bias. To build long-lasting trust
between AI and human beings, we argue that the key is to think beyond
algorithmic fairness and connect major aspects of AI that potentially cause
AI's indifferent behavior. In this survey, we provide a systematic framework of
Socially Responsible AI Algorithms that aims to examine the subjects of AI
indifference and the need for socially responsible AI algorithms, define the
objectives, and introduce the means by which we may achieve these objectives.
We further discuss how to leverage this framework to improve societal
well-being through protection, information, and prevention/mitigation.
",0
Responsible AI Challenges in End-to-end Machine Learning,"Steven Euijong Whang, Ki Hyun Tae, Yuji Roh, Geon Heo",2021-01-15T04:55:03Z,Responsible AI,"  Responsible AI is becoming critical as AI is widely used in our everyday
lives. Many companies that deploy AI publicly state that when training a model,
we not only need to improve its accuracy, but also need to guarantee that the
model does not discriminate against users (fairness), is resilient to noisy or
poisoned data (robustness), is explainable, and more. In addition, these
objectives are not only relevant to model training, but to all steps of
end-to-end machine learning, which include data collection, data cleaning and
validation, model training, model evaluation, and model management and serving.
Finally, responsible AI is conceptually challenging, and supporting all the
objectives must be as easy as possible. We thus propose three key research
directions towards this vision - depth, breadth, and usability - to measure
progress and introduce our ongoing research. First, responsible AI must be
deeply supported where multiple objectives like fairness and robust must be
handled together. To this end, we propose FR-Train, a holistic framework for
fair and robust model training in the presence of data bias and poisoning.
Second, responsible AI must be broadly supported, preferably in all steps of
machine learning. Currently we focus on the data pre-processing steps and
propose Slice Tuner, a selective data acquisition framework for training fair
and accurate models, and MLClean, a data cleaning framework that also improves
fairness and robustness. Finally, responsible AI must be usable where the
techniques must be easy to deploy and actionable. We propose FairBatch, a batch
selection approach for fairness that is effective and simple to use, and Slice
Finder, a model evaluation tool that automatically finds problematic slices. We
believe we scratched the surface of responsible AI for end-to-end machine
learning and suggest research challenges moving forward.
",6
Making Responsible AI the Norm rather than the Exception,Abhishek Gupta,2021-01-28T06:39:01Z,Responsible AI,"  This report prepared by the Montreal AI Ethics Institute provides
recommendations in response to the National Security Commission on Artificial
Intelligence (NSCAI) Key Considerations for Responsible Development and
Fielding of Artificial Intelligence document. The report centres on the idea
that Responsible AI should be made the Norm rather than an Exception. It does
so by utilizing the guiding principles of: (1) alleviating friction in existing
workflows, (2) empowering stakeholders to get buy-in, and (3) conducting an
effective translation of abstract standards into actionable engineering
practices. After providing some overarching comments on the document from the
NSCAI, the report dives into the primary contribution of an actionable
framework to help operationalize the ideas presented in the document from the
NSCAI. The framework consists of: (1) a learning, knowledge, and information
exchange (LKIE), (2) the Three Ways of Responsible AI, (3) an
empirically-driven risk-prioritization matrix, and (4) achieving the right
level of complexity. All components reinforce each other to move from
principles to practice in service of making Responsible AI the norm rather than
the exception.
",0
A Decentralized Approach towards Responsible AI in Social Ecosystems,Wenjing Chu,2021-02-12T06:33:42Z,Responsible AI,"  For AI technology to fulfill its full promises, we must have effective means
to ensure Responsible AI behavior and curtail potential irresponsible use,
e.g., in areas of privacy protection, human autonomy, robustness, and
prevention of biases and discrimination in automated decision making. Recent
literature in the field has identified serious shortcomings of narrow
technology focused and formalism-oriented research and has proposed an
interdisciplinary approach that brings the social context into the scope of
study. In this paper, we take a sociotechnical approach to propose a more
expansive framework of thinking about the Responsible AI challenges in both
technical and social context. Effective solutions need to bridge the gap
between a technical system with the social system that it will be deployed to.
To this end, we propose human agency and regulation as main mechanisms of
intervention and propose a decentralized computational infrastructure, or a set
of public utilities, as the computational means to bridge this gap. A
decentralized infrastructure is uniquely suited for meeting this challenge and
enable technical solutions and social institutions in a mutually reinforcing
dynamic to achieve Responsible AI goals. Our approach is novel in its
sociotechnical approach and its aim in tackling the structural issues that
cannot be solved within the narrow confines of AI technical research. We then
explore possible features of the proposed infrastructure and discuss how it may
help solve example problems recently studied in the field.
",0
Responsible AI: Gender bias assessment in emotion recognition,"Artem Domnich, Gholamreza Anbarjafari",2021-03-21T17:00:21Z,Responsible AI,"  Rapid development of artificial intelligence (AI) systems amplify many
concerns in society. These AI algorithms inherit different biases from humans
due to mysterious operational flow and because of that it is becoming adverse
in usage. As a result, researchers have started to address the issue by
investigating deeper in the direction towards Responsible and Explainable AI.
Among variety of applications of AI, facial expression recognition might not be
the most important one, yet is considered as a valuable part of human-AI
interaction. Evolution of facial expression recognition from the feature based
methods to deep learning drastically improve quality of such algorithms. This
research work aims to study a gender bias in deep learning methods for facial
expression recognition by investigating six distinct neural networks, training
them, and further analysed on the presence of bias, according to the three
definition of fairness. The main outcomes show which models are gender biased,
which are not and how gender of subject affects its emotion recognition. More
biased neural networks show bigger accuracy gap in emotion recognition between
male and female test sets. Furthermore, this trend keeps for true positive and
false positive rates. In addition, due to the nature of the research, we can
observe which types of emotions are better classified for men and which for
women. Since the topic of biases in facial expression recognition is not well
studied, a spectrum of continuation of this research is truly extensive, and
may comprise detail analysis of state-of-the-art methods, as well as targeting
other biases.
",0
Measurement as governance in and for responsible AI,Abigail Z. Jacobs,2021-09-13T01:04:22Z,Responsible AI,"  Measurement of social phenomena is everywhere, unavoidably, in sociotechnical
systems. This is not (only) an academic point: Fairness-related harms emerge
when there is a mismatch in the measurement process between the thing we
purport to be measuring and the thing we actually measure. However, the
measurement process -- where social, cultural, and political values are
implicitly encoded in sociotechnical systems -- is almost always obscured.
Furthermore, this obscured process is where important governance decisions are
encoded: governance about which systems are fair, which individuals belong in
which categories, and so on. We can then use the language of measurement, and
the tools of construct validity and reliability, to uncover hidden governance
decisions. In particular, we highlight two types of construct validity, content
validity and consequential validity, that are useful to elicit and characterize
the feedback loops between the measurement, social construction, and
enforcement of social categories. We then explore the constructs of fairness,
robustness, and responsibility in the context of governance in and for
responsible AI. Together, these perspectives help us unpack how measurement
acts as a hidden governance process in sociotechnical systems. Understanding
measurement as governance supports a richer understanding of the governance
processes already happening in AI -- responsible or otherwise -- revealing
paths to more effective interventions.
",13
"Towards a Responsible AI Development Lifecycle: Lessons From Information
  Security",Erick Galinkin,2022-03-06T13:03:58Z,Responsible AI,"  Legislation and public sentiment throughout the world have promoted fairness
metrics, explainability, and interpretability as prescriptions for the
responsible development of ethical artificial intelligence systems. Despite the
importance of these three pillars in the foundation of the field, they can be
challenging to operationalize and attempts to solve the problems in production
environments often feel Sisyphean. This difficulty stems from a number of
factors: fairness metrics are computationally difficult to incorporate into
training and rarely alleviate all of the harms perpetrated by these systems.
Interpretability and explainability can be gamed to appear fair, may
inadvertently reduce the privacy of personal information contained in training
data, and increase user confidence in predictions -- even when the explanations
are wrong. In this work, we propose a framework for responsibly developing
artificial intelligence systems by incorporating lessons from the field of
information security and the secure development lifecycle to overcome
challenges associated with protecting users in adversarial settings. In
particular, we propose leveraging the concepts of threat modeling, design
review, penetration testing, and incident response in the context of developing
AI systems as ways to resolve shortcomings in the aforementioned methods.
",0
Towards a Roadmap on Software Engineering for Responsible AI,"Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Zhenchang Xing",2022-03-09T07:01:32Z,Responsible AI,"  Although AI is transforming the world, there are serious concerns about its
ability to behave and make decisions responsibly. Many ethical regulations,
principles, and frameworks for responsible AI have been issued recently.
However, they are high level and difficult to put into practice. On the other
hand, most AI researchers focus on algorithmic solutions, while the responsible
AI challenges actually crosscut the entire engineering lifecycle and components
of AI systems. To close the gap in operationalizing responsible AI, this paper
aims to develop a roadmap on software engineering for responsible AI. The
roadmap focuses on (i) establishing multi-level governance for responsible AI
systems, (ii) setting up the development processes incorporating
process-oriented practices for responsible AI systems, and (iii) building
responsible-AI-by-design into AI systems through system-level architectural
style, patterns and techniques.
",0
"Data Cards: Purposeful and Transparent Dataset Documentation for
  Responsible AI","Mahima Pushkarna, Andrew Zaldivar, Oddur Kjartansson",2022-04-03T13:49:36Z,Responsible AI,"  As research and industry moves towards large-scale models capable of numerous
downstream tasks, the complexity of understanding multi-modal datasets that
give nuance to models rapidly increases. A clear and thorough understanding of
a dataset's origins, development, intent, ethical considerations and evolution
becomes a necessary step for the responsible and informed deployment of models,
especially those in people-facing contexts and high-risk domains. However, the
burden of this understanding often falls on the intelligibility, conciseness,
and comprehensiveness of the documentation. It requires consistency and
comparability across the documentation of all datasets involved, and as such
documentation must be treated as a user-centric product in and of itself. In
this paper, we propose Data Cards for fostering transparent, purposeful and
human-centered documentation of datasets within the practical contexts of
industry and research. Data Cards are structured summaries of essential facts
about various aspects of ML datasets needed by stakeholders across a dataset's
lifecycle for responsible AI development. These summaries provide explanations
of processes and rationales that shape the data and consequently the models,
such as upstream sources, data collection and annotation methods; training and
evaluation methods, intended use; or decisions affecting model performance. We
also present frameworks that ground Data Cards in real-world utility and
human-centricity. Using two case studies, we report on desirable
characteristics that support adoption across domains, organizational
structures, and audience groups. Finally, we present lessons learned from
deploying over 20 Data Cards.
",0
How Different Groups Prioritize Ethical Values for Responsible AI,"Maurice Jakesch, Zana Buçinca, Saleema Amershi, Alexandra Olteanu",2022-05-16T14:39:37Z,Responsible AI,"  Private companies, public sector organizations, and academic groups have
outlined ethical values they consider important for responsible artificial
intelligence technologies. While their recommendations converge on a set of
central values, little is known about the values a more representative public
would find important for the AI technologies they interact with and might be
affected by. We conducted a survey examining how individuals perceive and
prioritize responsible AI values across three groups: a representative sample
of the US population (N=743), a sample of crowdworkers (N=755), and a sample of
AI practitioners (N=175). Our results empirically confirm a common concern: AI
practitioners' value priorities differ from those of the general public.
Compared to the US-representative sample, AI practitioners appear to consider
responsible AI values as less important and emphasize a different set of
values. In contrast, self-identified women and black respondents found
responsible AI values more important than other groups. Surprisingly, more
liberal-leaning participants, rather than participants reporting experiences
with discrimination, were more likely to prioritize fairness than other groups.
Our findings highlight the importance of paying attention to who gets to define
responsible AI.
",0
Roadmap Towards Responsible AI in Crisis Resilience Management,"Cheng-Chun Lee, Tina Comes, Megan Finn, Ali Mostafavi",2022-07-20T04:39:29Z,Responsible AI,"  Novel data sensing and AI technologies are finding practical use in the
analysis of crisis resilience, revealing the need to consider how responsible
artificial intelligence (AI) practices can mitigate harmful outcomes and
protect vulnerable populations. In this paper, we present a responsible AI
roadmap that is embedded in the Crisis Information Management Circle. This
roadmap includes six propositions to highlight and address important challenges
and considerations specifically related to responsible AI for crisis resilience
management. We cover a wide spectrum of interwoven challenges and
considerations pertaining to the responsible collection, analysis, sharing, and
use of information such as equity, fairness, biases, explainability and
transparency, accountability, privacy and security, inter-organizational
coordination, and public engagement. Through examining issues around AI systems
for crisis resilience management, we dissect the inherent complexities of
information management and decision-making in crises and highlight the urgency
of responsible AI research and practice. The ideas laid out in this paper are
the first attempt in establishing a roadmap for researchers, practitioners,
developers, emergency managers, humanitarian organizations, and public
officials to address important considerations for responsible AI pertaining to
crisis resilience management.
",0
A Human Rights-Based Approach to Responsible AI,"Vinodkumar Prabhakaran, Margaret Mitchell, Timnit Gebru, Iason Gabriel",2022-10-06T04:07:53Z,Responsible AI,"  Research on fairness, accountability, transparency and ethics of AI-based
interventions in society has gained much-needed momentum in recent years.
However it lacks an explicit alignment with a set of normative values and
principles that guide this research and interventions. Rather, an implicit
consensus is often assumed to hold for the values we impart into our models -
something that is at odds with the pluralistic world we live in. In this paper,
we put forth the doctrine of universal human rights as a set of globally
salient and cross-culturally recognized set of values that can serve as a
grounding framework for explicit value alignment in responsible AI - and
discuss its efficacy as a framework for civil society partnership and
participation. We argue that a human rights framework orients the research in
this space away from the machines and the risks of their biases, and towards
humans and the risks to their rights, essentially helping to center the
conversation around who is harmed, what harms they face, and how those harms
may be mitigated.
",0
"Implementing Responsible AI: Tensions and Trade-Offs Between Ethics
  Aspects","Conrad Sanderson, David Douglas, Qinghua Lu",2023-04-17T13:43:13Z,Responsible AI,"  Many sets of ethics principles for responsible AI have been proposed to allay
concerns about misuse and abuse of AI/ML systems. The underlying aspects of
such sets of principles include privacy, accuracy, fairness, robustness,
explainability, and transparency. However, there are potential tensions between
these aspects that pose difficulties for AI/ML developers seeking to follow
these principles. For example, increasing the accuracy of an AI/ML system may
reduce its explainability. As part of the ongoing effort to operationalise the
principles into practice, in this work we compile and discuss a catalogue of 10
notable tensions, trade-offs and other interactions between the underlying
aspects. We primarily focus on two-sided interactions, drawing on support
spread across a diverse literature. This catalogue can be helpful in raising
awareness of the possible interactions between aspects of ethics principles, as
well as facilitating well-supported judgements by the designers and developers
of AI/ML systems.
",6
"Collect, Measure, Repeat: Reliability Factors for Responsible AI Data
  Collection","Oana Inel, Tim Draws, Lora Aroyo",2023-08-22T18:01:27Z,Responsible AI,"  The rapid entry of machine learning approaches in our daily activities and
high-stakes domains demands transparency and scrutiny of their fairness and
reliability. To help gauge machine learning models' robustness, research
typically focuses on the massive datasets used for their deployment, e.g.,
creating and maintaining documentation for understanding their origin, process
of development, and ethical considerations. However, data collection for AI is
still typically a one-off practice, and oftentimes datasets collected for a
certain purpose or application are reused for a different problem.
Additionally, dataset annotations may not be representative over time, contain
ambiguous or erroneous annotations, or be unable to generalize across issues or
domains. Recent research has shown these practices might lead to unfair,
biased, or inaccurate outcomes. We argue that data collection for AI should be
performed in a responsible manner where the quality of the data is thoroughly
scrutinized and measured through a systematic set of appropriate metrics. In
this paper, we propose a Responsible AI (RAI) methodology designed to guide the
data collection with a set of metrics for an iterative in-depth analysis of the
factors influencing the quality and reliability} of the generated data. We
propose a granular set of measurements to inform on the internal reliability of
a dataset and its external stability over time. We validate our approach across
nine existing datasets and annotation tasks and four content modalities. This
approach impacts the assessment of data robustness used for AI applied in the
real world, where diversity of users and content is eminent. Furthermore, it
deals with fairness and accountability aspects in data collection by providing
systematic and transparent quality analysis for data collections.
",0
RAI4IoE: Responsible AI for Enabling the Internet of Energy,"Minhui Xue, Surya Nepal, Ling Liu, Subbu Sethuvenkatraman, Xingliang Yuan, Carsten Rudolph, Ruoxi Sun, Greg Eisenhauer",2023-09-20T23:45:54Z,Responsible AI,"  This paper plans to develop an Equitable and Responsible AI framework with
enabling techniques and algorithms for the Internet of Energy (IoE), in short,
RAI4IoE. The energy sector is going through substantial changes fueled by two
key drivers: building a zero-carbon energy sector and the digital
transformation of the energy infrastructure. We expect to see the convergence
of these two drivers resulting in the IoE, where renewable distributed energy
resources (DERs), such as electric cars, storage batteries, wind turbines and
photovoltaics (PV), can be connected and integrated for reliable energy
distribution by leveraging advanced 5G-6G networks and AI technology. This
allows DER owners as prosumers to participate in the energy market and derive
economic incentives. DERs are inherently asset-driven and face equitable
challenges (i.e., fair, diverse and inclusive). Without equitable access,
privileged individuals, groups and organizations can participate and benefit at
the cost of disadvantaged groups. The real-time management of DER resources not
only brings out the equity problem to the IoE, it also collects highly
sensitive location, time, activity dependent data, which requires to be handled
responsibly (e.g., privacy, security and safety), for AI-enhanced predictions,
optimization and prioritization services, and automated management of flexible
resources. The vision of our project is to ensure equitable participation of
the community members and responsible use of their data in IoE so that it could
reap the benefits of advances in AI to provide safe, reliable and sustainable
energy services.
",4
Steering Responsible AI: A Case for Algorithmic Pluralism,Stefaan G. Verhulst,2023-11-20T18:45:04Z,Responsible AI,"  In this paper, I examine questions surrounding AI neutrality through the
prism of existing literature and scholarship about mediation and media
pluralism. Such traditions, I argue, provide a valuable theoretical framework
for how we should approach the (likely) impending era of AI mediation. In
particular, I suggest examining further the notion of algorithmic pluralism.
Contrasting this notion to the dominant idea of algorithmic transparency, I
seek to describe what algorithmic pluralism may be, and present both its
opportunities and challenges. Implemented thoughtfully and responsibly, I
argue, Algorithmic or AI pluralism has the potential to sustain the diversity,
multiplicity, and inclusiveness that are so vital to democracy.
",0
Investigating Responsible AI for Scientific Research: An Empirical Study,"Muneera Bano, Didar Zowghi, Pip Shea, Georgina Ibarra",2023-12-15T06:40:27Z,Responsible AI,"  Scientific research organizations that are developing and deploying
Artificial Intelligence (AI) systems are at the intersection of technological
progress and ethical considerations. The push for Responsible AI (RAI) in such
institutions underscores the increasing emphasis on integrating ethical
considerations within AI design and development, championing core values like
fairness, accountability, and transparency. For scientific research
organizations, prioritizing these practices is paramount not just for
mitigating biases and ensuring inclusivity, but also for fostering trust in AI
systems among both users and broader stakeholders. In this paper, we explore
the practices at a research organization concerning RAI practices, aiming to
assess the awareness and preparedness regarding the ethical risks inherent in
AI design and development. We have adopted a mixed-method research approach,
utilising a comprehensive survey combined with follow-up in-depth interviews
with selected participants from AI-related projects. Our results have revealed
certain knowledge gaps concerning ethical, responsible, and inclusive AI, with
limitations in awareness of the available AI ethics frameworks. This revealed
an overarching underestimation of the ethical risks that AI technologies can
present, especially when implemented without proper guidelines and governance.
Our findings reveal the need for a holistic and multi-tiered strategy to uplift
capabilities and better support science research teams for responsible,
ethical, and inclusive AI development and deployment.
",0
"Toward Responsible AI Use: Considerations for Sustainability Impact
  Assessment","Eva Thelisson, Grzegorz Mika, Quentin Schneiter, Kirtan Padh, Himanshu Verma",2023-12-19T09:38:56Z,Responsible AI,"  As AI/ML models, including Large Language Models, continue to scale with
massive datasets, so does their consumption of undeniably limited natural
resources, and impact on society. In this collaboration between AI,
Sustainability, HCI and legal researchers, we aim to enable a transition to
sustainable AI development by enabling stakeholders across the AI value chain
to assess and quantitfy the environmental and societal impact of AI. We present
the ESG Digital and Green Index (DGI), which offers a dashboard for assessing a
company's performance in achieving sustainability targets. This includes
monitoring the efficiency and sustainable use of limited natural resources
related to AI technologies (water, electricity, etc). It also addresses the
societal and governance challenges related to AI. The DGI creates incentives
for companies to align their pathway with the Sustainable Development Goals
(SDGs). The value, challenges and limitations of our methodology and findings
are discussed in the paper.
",0
Resolving Ethics Trade-offs in Implementing Responsible AI,"Conrad Sanderson, Emma Schleiger, David Douglas, Petra Kuhnert, Qinghua Lu",2024-01-16T04:14:23Z,Responsible AI,"  While the operationalisation of high-level AI ethics principles into
practical AI/ML systems has made progress, there is still a theory-practice gap
in managing tensions between the underlying AI ethics aspects. We cover five
approaches for addressing the tensions via trade-offs, ranging from rudimentary
to complex. The approaches differ in the types of considered context, scope,
methods for measuring contexts, and degree of justification. None of the
approaches is likely to be appropriate for all organisations, systems, or
applications. To address this, we propose a framework which consists of: (i)
proactive identification of tensions, (ii) prioritisation and weighting of
ethics aspects, (iii) justification and documentation of trade-off decisions.
The proposed framework aims to facilitate the implementation of well-rounded
AI/ML systems that are appropriate for potential regulatory requirements.
",0
"Farsight: Fostering Responsible AI Awareness During AI Application
  Prototyping","Zijie J. Wang, Chinmay Kulkarni, Lauren Wilcox, Michael Terry, Michael Madaio",2024-02-23T14:38:05Z,Responsible AI,"  Prompt-based interfaces for Large Language Models (LLMs) have made
prototyping and building AI-powered applications easier than ever before.
However, identifying potential harms that may arise from AI applications
remains a challenge, particularly during prompt-based prototyping. To address
this, we present Farsight, a novel in situ interactive tool that helps people
identify potential harms from the AI applications they are prototyping. Based
on a user's prompt, Farsight highlights news articles about relevant AI
incidents and allows users to explore and edit LLM-generated use cases,
stakeholders, and harms. We report design insights from a co-design study with
10 AI prototypers and findings from a user study with 42 AI prototypers. After
using Farsight, AI prototypers in our user study are better able to
independently identify potential harms associated with a prompt and find our
tool more useful and usable than existing resources. Their qualitative feedback
also highlights that Farsight encourages them to focus on end-users and think
beyond immediate harms. We discuss these findings and reflect on their
implications for designing AI prototyping experiences that meaningfully engage
with AI harms. Farsight is publicly accessible at:
https://PAIR-code.github.io/farsight.
",0
"Guidelines for Integrating Value Sensitive Design in Responsible AI
  Toolkits","Malak Sadek, Marios Constantinides, Daniele Quercia, Céline Mougenot",2024-02-29T21:49:38Z,Responsible AI,"  Value Sensitive Design (VSD) is a framework for integrating human values
throughout the technology design process. In parallel, Responsible AI (RAI)
advocates for the development of systems aligning with ethical values, such as
fairness and transparency. In this study, we posit that a VSD approach is not
only compatible, but also advantageous to the development of RAI toolkits. To
empirically assess this hypothesis, we conducted four workshops involving 17
early-career AI researchers. Our aim was to establish links between VSD and RAI
values while examining how existing toolkits incorporate VSD principles in
their design. Our findings show that collaborative and educational design
features within these toolkits, including illustrative examples and open-ended
cues, facilitate an understanding of human and ethical values, and empower
researchers to incorporate values into AI systems. Drawing on these insights,
we formulated six design guidelines for integrating VSD values into the
development of RAI toolkits.
",3
Evolving Reinforcement Learning Algorithms,"John D. Co-Reyes, Yingjie Miao, Daiyi Peng, Esteban Real, Sergey Levine, Quoc V. Le, Honglak Lee, Aleksandra Faust",2021-01-08T18:55:07Z,Reinforcement Learning,"  We propose a method for meta-learning reinforcement learning algorithms by
searching over the space of computational graphs which compute the loss
function for a value-based model-free RL agent to optimize. The learned
algorithms are domain-agnostic and can generalize to new environments not seen
during training. Our method can both learn from scratch and bootstrap off known
existing algorithms, like DQN, enabling interpretable modifications which
improve performance. Learning from scratch on simple classical control and
gridworld tasks, our method rediscovers the temporal-difference (TD) algorithm.
Bootstrapped from DQN, we highlight two learned algorithms which obtain good
generalization performance over other classical control tasks, gridworld type
tasks, and Atari games. The analysis of the learned algorithm behavior shows
resemblance to recently proposed RL algorithms that address overestimation in
value-based methods.
",0
Domain Adversarial Reinforcement Learning,"Bonnie Li, Vincent François-Lavet, Thang Doan, Joelle Pineau",2021-02-14T07:58:41Z,Reinforcement Learning,"  We consider the problem of generalization in reinforcement learning where
visual aspects of the observations might differ, e.g. when there are different
backgrounds or change in contrast, brightness, etc. We assume that our agent
has access to only a few of the MDPs from the MDP distribution during training.
The performance of the agent is then reported on new unknown test domains drawn
from the distribution (e.g. unseen backgrounds). For this ""zero-shot RL"" task,
we enforce invariance of the learned representations to visual domains via a
domain adversarial optimization process. We empirically show that this approach
allows achieving a significant generalization improvement to new unseen
domains.
",21
Safe Distributional Reinforcement Learning,"Jianyi Zhang, Paul Weng",2021-02-26T13:03:27Z,Reinforcement Learning,"  Safety in reinforcement learning (RL) is a key property in both training and
execution in many domains such as autonomous driving or finance. In this paper,
we formalize it with a constrained RL formulation in the distributional RL
setting. Our general model accepts various definitions of safety(e.g., bounds
on expected performance, CVaR, variance, or probability of reaching bad
states). To ensure safety during learning, we extend a safe policy optimization
method to solve our problem. The distributional RL perspective leads to a more
efficient algorithm while additionally catering for natural safe constraints.
We empirically validate our propositions on artificial and real domains against
appropriate state-of-the-art safe RL algorithms.
",0
Reinforcement Learning Beyond Expectation,"Bhaskar Ramasubramanian, Luyao Niu, Andrew Clark, Radha Poovendran",2021-03-29T20:35:25Z,Reinforcement Learning,"  The inputs and preferences of human users are important considerations in
situations where these users interact with autonomous cyber or cyber-physical
systems. In these scenarios, one is often interested in aligning behaviors of
the system with the preferences of one or more human users. Cumulative prospect
theory (CPT) is a paradigm that has been empirically shown to model a tendency
of humans to view gains and losses differently. In this paper, we consider a
setting where an autonomous agent has to learn behaviors in an unknown
environment. In traditional reinforcement learning, these behaviors are learned
through repeated interactions with the environment by optimizing an expected
utility. In order to endow the agent with the ability to closely mimic the
behavior of human users, we optimize a CPT-based cost. We introduce the notion
of the CPT-value of an action taken in a state, and establish the convergence
of an iterative dynamic programming-based approach to estimate this quantity.
We develop two algorithms to enable agents to learn policies to optimize the
CPT-vale, and evaluate these algorithms in environments where a target state
has to be reached while avoiding obstacles. We demonstrate that behaviors of
the agent learned using these algorithms are better aligned with that of a
human user who might be placed in the same environment, and is significantly
improved over a baseline that optimizes an expected utility.
",1
Heuristic-Guided Reinforcement Learning,"Ching-An Cheng, Andrey Kolobov, Adith Swaminathan",2021-06-05T00:04:09Z,Reinforcement Learning,"  We provide a framework for accelerating reinforcement learning (RL)
algorithms by heuristics constructed from domain knowledge or offline data.
Tabula rasa RL algorithms require environment interactions or computation that
scales with the horizon of the sequential decision-making task. Using our
framework, we show how heuristic-guided RL induces a much shorter-horizon
subproblem that provably solves the original task. Our framework can be viewed
as a horizon-based regularization for controlling bias and variance in RL under
a finite interaction budget. On the theoretical side, we characterize
properties of a good heuristic and its impact on RL acceleration. In
particular, we introduce the novel concept of an improvable heuristic, a
heuristic that allows an RL agent to extrapolate beyond its prior knowledge. On
the empirical side, we instantiate our framework to accelerate several
state-of-the-art algorithms in simulated robotic control tasks and procedurally
generated games. Our framework complements the rich literature on warm-starting
RL with expert demonstrations or exploratory datasets, and introduces a
principled method for injecting prior knowledge into RL.
",0
Offline Inverse Reinforcement Learning,"Firas Jarboui, Vianney Perchet",2021-06-09T13:44:06Z,Reinforcement Learning,"  The objective of offline RL is to learn optimal policies when a fixed
exploratory demonstrations data-set is available and sampling additional
observations is impossible (typically if this operation is either costly or
rises ethical questions). In order to solve this problem, off the shelf
approaches require a properly defined cost function (or its evaluation on the
provided data-set), which are seldom available in practice. To circumvent this
issue, a reasonable alternative is to query an expert for few optimal
demonstrations in addition to the exploratory data-set. The objective is then
to learn an optimal policy w.r.t. the expert's latent cost function. Current
solutions either solve a behaviour cloning problem (which does not leverage the
exploratory data) or a reinforced imitation learning problem (using a fixed
cost function that discriminates available exploratory trajectories from expert
ones). Inspired by the success of IRL techniques in achieving state of the art
imitation performances in online settings, we exploit GAN based data
augmentation procedures to construct the first offline IRL algorithm. The
obtained policies outperformed the aforementioned solutions on multiple OpenAI
gym environments.
",17
Density Constrained Reinforcement Learning,"Zengyi Qin, Yuxiao Chen, Chuchu Fan",2021-06-24T04:22:03Z,Reinforcement Learning,"  We study constrained reinforcement learning (CRL) from a novel perspective by
setting constraints directly on state density functions, rather than the value
functions considered by previous works. State density has a clear physical and
mathematical interpretation, and is able to express a wide variety of
constraints such as resource limits and safety requirements. Density
constraints can also avoid the time-consuming process of designing and tuning
cost functions required by value function-based constraints to encode system
specifications. We leverage the duality between density functions and Q
functions to develop an effective algorithm to solve the density constrained RL
problem optimally and the constrains are guaranteed to be satisfied. We prove
that the proposed algorithm converges to a near-optimal solution with a bounded
error even when the policy update is imperfect. We use a set of comprehensive
experiments to demonstrate the advantages of our approach over state-of-the-art
CRL methods, with a wide range of density constrained tasks as well as standard
CRL benchmarks such as Safety-Gym.
",32
Temporal Shift Reinforcement Learning,"Deepak George Thomas, Tichakorn Wongpiromsarn, Ali Jannesari",2021-09-05T18:47:13Z,Reinforcement Learning,"  The function approximators employed by traditional image-based Deep
Reinforcement Learning (DRL) algorithms usually lack a temporal learning
component and instead focus on learning the spatial component. We propose a
technique, Temporal Shift Reinforcement Learning (TSRL), wherein both temporal,
as well as spatial components are jointly learned. Moreover, TSRL does not
require additional parameters to perform temporal learning. We show that TSRL
outperforms the commonly used frame stacking heuristic on both of the Atari
environments we test on while beating the SOTA for one of them. This
investigation has implications in the robotics as well as sequential
decision-making domains.
",0
Wasserstein Unsupervised Reinforcement Learning,"Shuncheng He, Yuhang Jiang, Hongchang Zhang, Jianzhun Shao, Xiangyang Ji",2021-10-15T08:41:51Z,Reinforcement Learning,"  Unsupervised reinforcement learning aims to train agents to learn a handful
of policies or skills in environments without external reward. These
pre-trained policies can accelerate learning when endowed with external reward,
and can also be used as primitive options in hierarchical reinforcement
learning. Conventional approaches of unsupervised skill discovery feed a latent
variable to the agent and shed its empowerment on agent's behavior by mutual
information (MI) maximization. However, the policies learned by MI-based
methods cannot sufficiently explore the state space, despite they can be
successfully identified from each other. Therefore we propose a new framework
Wasserstein unsupervised reinforcement learning (WURL) where we directly
maximize the distance of state distributions induced by different policies.
Additionally, we overcome difficulties in simultaneously training N(N >2)
policies, and amortizing the overall reward to each step. Experiments show
policies learned by our approach outperform MI-based methods on the metric of
Wasserstein distance while keeping high discriminability. Furthermore, the
agents trained by WURL can sufficiently explore the state space in mazes and
MuJoCo tasks and the pre-trained policies can be applied to downstream tasks by
hierarchical learning.
",0
Godot Reinforcement Learning Agents,"Edward Beeching, Jilles Debangoye, Olivier Simonin, Christian Wolf",2021-12-07T11:24:34Z,Reinforcement Learning,"  We present Godot Reinforcement Learning (RL) Agents, an open-source interface
for developing environments and agents in the Godot Game Engine. The Godot RL
Agents interface allows the design, creation and learning of agent behaviors in
challenging 2D and 3D environments with various on-policy and off-policy Deep
RL algorithms. We provide a standard Gym interface, with wrappers for learning
in the Ray RLlib and Stable Baselines RL frameworks. This allows users access
to over 20 state of the art on-policy, off-policy and multi-agent RL
algorithms. The framework is a versatile tool that allows researchers and game
designers the ability to create environments with discrete, continuous and
mixed action spaces. The interface is relatively performant, with 12k
interactions per second on a high end laptop computer, when parallized on 4 CPU
cores. An overview video is available here: https://youtu.be/g1MlZSFqIj4
",0
Reinforcement Learning Textbook,Sergey Ivanov,2022-01-19T15:54:39Z,Reinforcement Learning,"  This textbook covers principles behind main modern deep reinforcement
learning algorithms that achieved breakthrough results in many domains from
game AI to robotics. All required theory is explained with proofs using unified
notation and emphasize on the differences between different types of algorithms
and the reasons why they are constructed the way they are.
",5
Group-Agent Reinforcement Learning,"Kaiyue Wu, Xiao-Jun Zeng",2022-02-10T16:40:59Z,Reinforcement Learning,"  It can largely benefit the reinforcement learning (RL) process of each agent
if multiple geographically distributed agents perform their separate RL tasks
cooperatively. Different from multi-agent reinforcement learning (MARL) where
multiple agents are in a common environment and should learn to cooperate or
compete with each other, in this case each agent has its separate environment
and only communicates with others to share knowledge without any cooperative or
competitive behaviour as a learning outcome. In fact, this scenario exists
widely in real life whose concept can be utilised in many applications, but is
not well understood yet and not well formulated. As the first effort, we
propose group-agent system for RL as a formulation of this scenario and the
third type of RL system with respect to single-agent and multi-agent systems.
We then propose a distributed RL framework called DDAL (Decentralised
Distributed Asynchronous Learning) designed for group-agent reinforcement
learning (GARL). We show through experiments that DDAL achieved desirable
performance with very stable training and has good scalability.
",0
Branching Reinforcement Learning,"Yihan Du, Wei Chen",2022-02-16T11:19:03Z,Reinforcement Learning,"  In this paper, we propose a novel Branching Reinforcement Learning (Branching
RL) model, and investigate both Regret Minimization (RM) and Reward-Free
Exploration (RFE) metrics for this model. Unlike standard RL where the
trajectory of each episode is a single $H$-step path, branching RL allows an
agent to take multiple base actions in a state such that transitions branch out
to multiple successor states correspondingly, and thus it generates a
tree-structured trajectory. This model finds important applications in
hierarchical recommendation systems and online advertising. For branching RL,
we establish new Bellman equations and key lemmas, i.e., branching value
difference lemma and branching law of total variance, and also bound the total
variance by only $O(H^2)$ under an exponentially-large trajectory. For RM and
RFE metrics, we propose computationally efficient algorithms BranchVI and
BranchRFE, respectively, and derive nearly matching upper and lower bounds. Our
results are only polynomial in problem parameters despite exponentially-large
trajectories.
",0
Retrieval-Augmented Reinforcement Learning,"Anirudh Goyal, Abram L. Friesen, Andrea Banino, Theophane Weber, Nan Rosemary Ke, Adria Puigdomenech Badia, Arthur Guez, Mehdi Mirza, Peter C. Humphreys, Ksenia Konyushkova, Laurent Sifre, Michal Valko, Simon Osindero, Timothy Lillicrap, Nicolas Heess, Charles Blundell",2022-02-17T02:44:05Z,Reinforcement Learning,"  Most deep reinforcement learning (RL) algorithms distill experience into
parametric behavior policies or value functions via gradient updates. While
effective, this approach has several disadvantages: (1) it is computationally
expensive, (2) it can take many updates to integrate experiences into the
parametric model, (3) experiences that are not fully integrated do not
appropriately influence the agent's behavior, and (4) behavior is limited by
the capacity of the model. In this paper we explore an alternative paradigm in
which we train a network to map a dataset of past experiences to optimal
behavior. Specifically, we augment an RL agent with a retrieval process
(parameterized as a neural network) that has direct access to a dataset of
experiences. This dataset can come from the agent's past experiences, expert
demonstrations, or any other relevant source. The retrieval process is trained
to retrieve information from the dataset that may be useful in the current
context, to help the agent achieve its goal faster and more efficiently. he
proposed method facilitates learning agents that at test-time can condition
their behavior on the entire dataset and not only the current state, or current
trajectory. We integrate our method into two different RL agents: an offline
DQN agent and an online R2D2 agent. In offline multi-task problems, we show
that the retrieval-augmented DQN agent avoids task interference and learns
faster than the baseline DQN agent. On Atari, we show that retrieval-augmented
R2D2 learns significantly faster than the baseline R2D2 agent and achieves
higher scores. We run extensive ablations to measure the contributions of the
components of our proposed method.
",0
Jump-Start Reinforcement Learning,"Ikechukwu Uchendu, Ted Xiao, Yao Lu, Banghua Zhu, Mengyuan Yan, Joséphine Simon, Matthew Bennice, Chuyuan Fu, Cong Ma, Jiantao Jiao, Sergey Levine, Karol Hausman",2022-04-05T17:25:22Z,Reinforcement Learning,"  Reinforcement learning (RL) provides a theoretical framework for continuously
improving an agent's behavior via trial and error. However, efficiently
learning policies from scratch can be very difficult, particularly for tasks
with exploration challenges. In such settings, it might be desirable to
initialize RL with an existing policy, offline data, or demonstrations.
However, naively performing such initialization in RL often works poorly,
especially for value-based methods. In this paper, we present a meta algorithm
that can use offline data, demonstrations, or a pre-existing policy to
initialize an RL policy, and is compatible with any RL approach. In particular,
we propose Jump-Start Reinforcement Learning (JSRL), an algorithm that employs
two policies to solve tasks: a guide-policy, and an exploration-policy. By
using the guide-policy to form a curriculum of starting states for the
exploration-policy, we are able to efficiently improve performance on a set of
simulated robotic tasks. We show via experiments that JSRL is able to
significantly outperform existing imitation and reinforcement learning
algorithms, particularly in the small-data regime. In addition, we provide an
upper bound on the sample complexity of JSRL and show that with the help of a
guide-policy, one can improve the sample complexity for non-optimism
exploration methods from exponential in horizon to polynomial.
",0
$q$-Munchausen Reinforcement Learning,"Lingwei Zhu, Zheng Chen, Eiji Uchibe, Takamitsu Matsubara",2022-05-16T06:26:10Z,Reinforcement Learning,"  The recently successful Munchausen Reinforcement Learning (M-RL) features
implicit Kullback-Leibler (KL) regularization by augmenting the reward function
with logarithm of the current stochastic policy. Though significant improvement
has been shown with the Boltzmann softmax policy, when the Tsallis sparsemax
policy is considered, the augmentation leads to a flat learning curve for
almost every problem considered. We show that it is due to the mismatch between
the conventional logarithm and the non-logarithmic (generalized) nature of
Tsallis entropy. Drawing inspiration from the Tsallis statistics literature, we
propose to correct the mismatch of M-RL with the help of
$q$-logarithm/exponential functions. The proposed formulation leads to implicit
Tsallis KL regularization under the maximum Tsallis entropy framework. We show
such formulation of M-RL again achieves superior performance on benchmark
problems and sheds light on more general M-RL with various entropic indices
$q$.
",0
Reachability Constrained Reinforcement Learning,"Dongjie Yu, Haitong Ma, Shengbo Eben Li, Jianyu Chen",2022-05-16T09:32:45Z,Reinforcement Learning,"  Constrained reinforcement learning (CRL) has gained significant interest
recently, since safety constraints satisfaction is critical for real-world
problems. However, existing CRL methods constraining discounted cumulative
costs generally lack rigorous definition and guarantee of safety. In contrast,
in the safe control research, safety is defined as persistently satisfying
certain state constraints. Such persistent safety is possible only on a subset
of the state space, called feasible set, where an optimal largest feasible set
exists for a given environment. Recent studies incorporate feasible sets into
CRL with energy-based methods such as control barrier function (CBF), safety
index (SI), and leverage prior conservative estimations of feasible sets, which
harms the performance of the learned policy. To deal with this problem, this
paper proposes the reachability CRL (RCRL) method by using reachability
analysis to establish the novel self-consistency condition and characterize the
feasible sets. The feasible sets are represented by the safety value function,
which is used as the constraint in CRL. We use the multi-time scale stochastic
approximation theory to prove that the proposed algorithm converges to a local
optimum, where the largest feasible set can be guaranteed. Empirical results on
different benchmarks validate the learned feasible set, the policy performance,
and constraint satisfaction of RCRL, compared to CRL and safe control
baselines.
",0
Federated Offline Reinforcement Learning,"Doudou Zhou, Yufeng Zhang, Aaron Sonabend-W, Zhaoran Wang, Junwei Lu, Tianxi Cai",2022-06-11T18:03:26Z,Reinforcement Learning,"  Evidence-based or data-driven dynamic treatment regimes are essential for
personalized medicine, which can benefit from offline reinforcement learning
(RL). Although massive healthcare data are available across medical
institutions, they are prohibited from sharing due to privacy constraints.
Besides, heterogeneity exists in different sites. As a result, federated
offline RL algorithms are necessary and promising to deal with the problems. In
this paper, we propose a multi-site Markov decision process model that allows
for both homogeneous and heterogeneous effects across sites. The proposed model
makes the analysis of the site-level features possible. We design the first
federated policy optimization algorithm for offline RL with sample complexity.
The proposed algorithm is communication-efficient, which requires only a single
round of communication interaction by exchanging summary statistics. We give a
theoretical guarantee for the proposed algorithm, where the suboptimality for
the learned policies is comparable to the rate as if data is not distributed.
Extensive simulations demonstrate the effectiveness of the proposed algorithm.
The method is applied to a sepsis dataset in multiple sites to illustrate its
use in clinical settings.
",0
Recursive Reinforcement Learning,"Ernst Moritz Hahn, Mateo Perez, Sven Schewe, Fabio Somenzi, Ashutosh Trivedi, Dominik Wojtczak",2022-06-23T00:29:42Z,Reinforcement Learning,"  Recursion is the fundamental paradigm to finitely describe potentially
infinite objects. As state-of-the-art reinforcement learning (RL) algorithms
cannot directly reason about recursion, they must rely on the practitioner's
ingenuity in designing a suitable ""flat"" representation of the environment. The
resulting manual feature constructions and approximations are cumbersome and
error-prone; their lack of transparency hampers scalability. To overcome these
challenges, we develop RL algorithms capable of computing optimal policies in
environments described as a collection of Markov decision processes (MDPs) that
can recursively invoke one another. Each constituent MDP is characterized by
several entry and exit points that correspond to input and output values of
these invocations. These recursive MDPs (or RMDPs) are expressively equivalent
to probabilistic pushdown systems (with call-stack playing the role of the
pushdown stack), and can model probabilistic programs with recursive procedural
calls. We introduce Recursive Q-learning -- a model-free RL algorithm for RMDPs
-- and prove that it converges for finite, single-exit and deterministic
multi-exit RMDPs under mild assumptions.
",136
Performative Reinforcement Learning,"Debmalya Mandal, Stelios Triantafyllou, Goran Radanovic",2022-06-30T18:26:03Z,Reinforcement Learning,"  We introduce the framework of performative reinforcement learning where the
policy chosen by the learner affects the underlying reward and transition
dynamics of the environment. Following the recent literature on performative
prediction~\cite{Perdomo et. al., 2020}, we introduce the concept of
performatively stable policy. We then consider a regularized version of the
reinforcement learning problem and show that repeatedly optimizing this
objective converges to a performatively stable policy under reasonable
assumptions on the transition dynamics. Our proof utilizes the dual perspective
of the reinforcement learning problem and may be of independent interest in
analyzing the convergence of other algorithms with decision-dependent
environments. We then extend our results for the setting where the learner just
performs gradient ascent steps instead of fully optimizing the objective, and
for the setting where the learner has access to a finite number of trajectories
from the changed environment. For both settings, we leverage the dual
formulation of performative reinforcement learning and establish convergence to
a stable solution. Finally, through extensive experiments on a grid-world
environment, we demonstrate the dependence of convergence on various parameters
e.g. regularization, smoothness, and the number of samples.
",0
Lifelong Inverse Reinforcement Learning,"Jorge A. Mendez, Shashank Shivkumar, Eric Eaton",2022-07-01T14:36:02Z,Reinforcement Learning,"  Methods for learning from demonstration (LfD) have shown success in acquiring
behavior policies by imitating a user. However, even for a single task, LfD may
require numerous demonstrations. For versatile agents that must learn many
tasks via demonstration, this process would substantially burden the user if
each task were learned in isolation. To address this challenge, we introduce
the novel problem of lifelong learning from demonstration, which allows the
agent to continually build upon knowledge learned from previously demonstrated
tasks to accelerate the learning of new tasks, reducing the amount of
demonstrations required. As one solution to this problem, we propose the first
lifelong learning approach to inverse reinforcement learning, which learns
consecutive tasks via demonstration, continually transferring knowledge between
tasks to improve performance.
",0
Entropy Augmented Reinforcement Learning,Jianfei Ma,2022-08-19T13:09:32Z,Reinforcement Learning,"  Deep reinforcement learning was instigated with the presence of trust region
methods, being scalable and efficient. However, the pessimism of such
algorithms, among which it forces to constrain in a trust region by all means,
has been proven to suppress the exploration and harm the performance.
Exploratory algorithm such as SAC, while utilizes the entropy to encourage
exploration, implicitly optimizing another objective yet. We first observed
this inconsistency, and therefore put forward an analogous augmentation
technique, which combines well with the on-policy algorithms, when a value
critic is involved. Surprisingly, the proposed method consistently satisfies
the soft policy improvement theorem, while being more extensible. As the
analysis advises, it is crucial to control the temperature coefficient to
balance the exploration and exploitation. Empirical tests on MuJoCo benchmark
tasks show that the agent is heartened towards higher reward regions, and
enjoys a finer performance. Furthermore, we verify the exploration bonus of our
method on a set of custom environments.
",0
Socially Fair Reinforcement Learning,"Debmalya Mandal, Jiarui Gan",2022-08-26T11:01:55Z,Reinforcement Learning,"  We consider the problem of episodic reinforcement learning where there are
multiple stakeholders with different reward functions. Our goal is to output a
policy that is socially fair with respect to different reward functions. Prior
works have proposed different objectives that a fair policy must optimize
including minimum welfare, and generalized Gini welfare. We first take an
axiomatic view of the problem, and propose four axioms that any such fair
objective must satisfy. We show that the Nash social welfare is the unique
objective that uniquely satisfies all four objectives, whereas prior objectives
fail to satisfy all four axioms. We then consider the learning version of the
problem where the underlying model i.e. Markov decision process is unknown. We
consider the problem of minimizing regret with respect to the fair policies
maximizing three different fair objectives -- minimum welfare, generalized Gini
welfare, and Nash social welfare. Based on optimistic planning, we propose a
generic learning algorithm and derive its regret bound with respect to the
three different policies. For the objective of Nash social welfare, we also
derive a lower bound in regret that grows exponentially with $n$, the number of
agents. Finally, we show that for the objective of minimum welfare, one can
improve regret by a factor of $O(H)$ for a weaker notion of regret.
",0
Style-Agnostic Reinforcement Learning,"Juyong Lee, Seokjun Ahn, Jaesik Park",2022-08-31T13:45:00Z,Reinforcement Learning,"  We present a novel method of learning style-agnostic representation using
both style transfer and adversarial learning in the reinforcement learning
framework. The style, here, refers to task-irrelevant details such as the color
of the background in the images, where generalizing the learned policy across
environments with different styles is still a challenge. Focusing on learning
style-agnostic representations, our method trains the actor with diverse image
styles generated from an inherent adversarial style perturbation generator,
which plays a min-max game between the actor and the generator, without
demanding expert knowledge for data augmentation or additional class labels for
adversarial training. We verify that our method achieves competitive or better
performances than the state-of-the-art approaches on Procgen and Distracting
Control Suite benchmarks, and further investigate the features extracted from
our model, showing that the model better captures the invariants and is less
distracted by the shifted style. The code is available at
https://github.com/POSTECH-CVLab/style-agnostic-RL.
",0
Robust Constrained Reinforcement Learning,"Yue Wang, Fei Miao, Shaofeng Zou",2022-09-14T18:29:02Z,Reinforcement Learning,"  Constrained reinforcement learning is to maximize the expected reward subject
to constraints on utilities/costs. However, the training environment may not be
the same as the test one, due to, e.g., modeling error, adversarial attack,
non-stationarity, resulting in severe performance degradation and more
importantly constraint violation. We propose a framework of robust constrained
reinforcement learning under model uncertainty, where the MDP is not fixed but
lies in some uncertainty set, the goal is to guarantee that constraints on
utilities/costs are satisfied for all MDPs in the uncertainty set, and to
maximize the worst-case reward performance over the uncertainty set. We design
a robust primal-dual approach, and further theoretically develop guarantee on
its convergence, complexity and robust feasibility. We then investigate a
concrete example of $\delta$-contamination uncertainty set, design an online
and model-free algorithm and theoretically characterize its sample complexity.
",0
Understanding reinforcement learned crowds,"Ariel Kwiatkowski, Vicky Kalogeiton, Julien Pettré, Marie-Paule Cani",2022-09-19T20:47:49Z,Other,"  Simulating trajectories of virtual crowds is a commonly encountered task in
Computer Graphics. Several recent works have applied Reinforcement Learning
methods to animate virtual agents, however they often make different design
choices when it comes to the fundamental simulation setup. Each of these
choices comes with a reasonable justification for its use, so it is not obvious
what is their real impact, and how they affect the results. In this work, we
analyze some of these arbitrary choices in terms of their impact on the
learning performance, as well as the quality of the resulting simulation
measured in terms of the energy efficiency. We perform a theoretical analysis
of the properties of the reward function design, and empirically evaluate the
impact of using certain observation and action spaces on a variety of
scenarios, with the reward function and energy usage as metrics. We show that
directly using the neighboring agents' information as observation generally
outperforms the more widely used raycasting. Similarly, using nonholonomic
controls with egocentric observations tends to produce more efficient behaviors
than holonomic controls with absolute observations. Each of these choices has a
significant, and potentially nontrivial impact on the results, and so
researchers should be mindful about choosing and reporting them in their work.
",0
Hyperbolic Deep Reinforcement Learning,"Edoardo Cetin, Benjamin Chamberlain, Michael Bronstein, Jonathan J Hunt",2022-10-04T12:03:04Z,Reinforcement Learning,"  We propose a new class of deep reinforcement learning (RL) algorithms that
model latent representations in hyperbolic space. Sequential decision-making
requires reasoning about the possible future consequences of current behavior.
Consequently, capturing the relationship between key evolving features for a
given task is conducive to recovering effective policies. To this end,
hyperbolic geometry provides deep RL models with a natural basis to precisely
encode this inherently hierarchical information. However, applying existing
methodologies from the hyperbolic deep learning literature leads to fatal
optimization instabilities due to the non-stationarity and variance
characterizing RL gradient estimators. Hence, we design a new general method
that counteracts such optimization challenges and enables stable end-to-end
learning with deep hyperbolic representations. We empirically validate our
framework by applying it to popular on-policy and off-policy RL algorithms on
the Procgen and Atari 100K benchmarks, attaining near universal performance and
generalization benefits. Given its natural fit, we hope future RL research will
consider hyperbolic representations as a standard tool.
",0
Reinforcement Learning for ConnectX,"Sheel Shah, Shubham Gupta",2022-10-15T11:38:19Z,Reinforcement Learning,"  ConnectX is a two-player game that generalizes the popular game Connect 4.
The objective is to get X coins across a row, column, or diagonal of an M x N
board. The first player to do so wins the game. The parameters (M, N, X) are
allowed to change in each game, making ConnectX a novel and challenging
problem. In this paper, we present our work on the implementation and
modification of various reinforcement learning algorithms to play ConnectX.
",0
Opportunistic Episodic Reinforcement Learning,"Xiaoxiao Wang, Nader Bouacida, Xueying Guo, Xin Liu",2022-10-24T18:02:33Z,Reinforcement Learning,"  In this paper, we propose and study opportunistic reinforcement learning - a
new variant of reinforcement learning problems where the regret of selecting a
suboptimal action varies under an external environmental condition known as the
variation factor. When the variation factor is low, so is the regret of
selecting a suboptimal action and vice versa. Our intuition is to exploit more
when the variation factor is high, and explore more when the variation factor
is low. We demonstrate the benefit of this novel framework for finite-horizon
episodic MDPs by designing and evaluating OppUCRL2 and OppPSRL algorithms. Our
algorithms dynamically balance the exploration-exploitation trade-off for
reinforcement learning by introducing variation factor-dependent optimism to
guide exploration. We establish an $\tilde{O}(HS \sqrt{AT})$ regret bound for
the OppUCRL2 algorithm and show through simulations that both OppUCRL2 and
OppPSRL algorithm outperform their original corresponding algorithms.
",0
Doubly Inhomogeneous Reinforcement Learning,"Liyuan Hu, Mengbing Li, Chengchun Shi, Zhenke Wu, Piotr Fryzlewicz",2022-11-08T03:41:14Z,Reinforcement Learning,"  This paper studies reinforcement learning (RL) in doubly inhomogeneous
environments under temporal non-stationarity and subject heterogeneity. In a
number of applications, it is commonplace to encounter datasets generated by
system dynamics that may change over time and population, challenging
high-quality sequential decision making. Nonetheless, most existing RL
solutions require either temporal stationarity or subject homogeneity, which
would result in sub-optimal policies if both assumptions were violated. To
address both challenges simultaneously, we propose an original algorithm to
determine the ``best data chunks"" that display similar dynamics over time and
across individuals for policy learning, which alternates between most recent
change point detection and cluster identification. Our method is general, and
works with a wide range of clustering and change point detection algorithms. It
is multiply robust in the sense that it takes multiple initial estimators as
input and only requires one of them to be consistent. Moreover, by borrowing
information over time and population, it allows us to detect weaker signals and
has better convergence properties when compared to applying the clustering
algorithm per time or the change point detection algorithm per subject.
Empirically, we demonstrate the usefulness of our method through extensive
simulations and a real data application.
",0
Backward Curriculum Reinforcement Learning,KyungMin Ko,2022-12-29T08:23:39Z,Reinforcement Learning,"  Current reinforcement learning algorithms train an agent using
forward-generated trajectories, which provide little guidance so that the agent
can explore as much as possible. While realizing the value of reinforcement
learning results from sufficient exploration, this approach leads to a
trade-off in losing sample efficiency, an essential factor impacting algorithm
performance. Previous tasks use reward-shaping techniques and network structure
modification to increase sample efficiency. However, these methods require many
steps to implement. In this work, we propose novel backward curriculum
reinforcement learning that begins training the agent using the backward
trajectory of the episode instead of the original forward trajectory. This
approach provides the agent with a strong reward signal, enabling more
sample-efficient learning. Moreover, our method only requires a minor change in
the algorithm of reversing the order of the trajectory before agent training,
allowing a straightforward application to any state-of-the-art algorithm.
",0
Internally Rewarded Reinforcement Learning,"Mengdi Li, Xufeng Zhao, Jae Hee Lee, Cornelius Weber, Stefan Wermter",2023-02-01T06:25:46Z,Reinforcement Learning,"  We study a class of reinforcement learning problems where the reward signals
for policy learning are generated by an internal reward model that is dependent
on and jointly optimized with the policy. This interdependence between the
policy and the reward model leads to an unstable learning process because
reward signals from an immature reward model are noisy and impede policy
learning, and conversely, an under-optimized policy impedes reward estimation
learning. We call this learning setting $\textit{Internally Rewarded
Reinforcement Learning}$ (IRRL) as the reward is not provided directly by the
environment but $\textit{internally}$ by a reward model. In this paper, we
formally formulate IRRL and present a class of problems that belong to IRRL. We
theoretically derive and empirically analyze the effect of the reward function
in IRRL and based on these analyses propose the clipped linear reward function.
Experimental results show that the proposed reward function can consistently
stabilize the training process by reducing the impact of reward noise, which
leads to faster convergence and higher performance compared with baselines in
diverse tasks.
",0
Post Reinforcement Learning Inference,"Vasilis Syrgkanis, Ruohan Zhan",2023-02-17T12:53:15Z,Reinforcement Learning,"  We consider estimation and inference using data collected from reinforcement
learning algorithms. These algorithms, characterized by their adaptive
experimentation, interact with individual units over multiple stages,
dynamically adjusting their strategies based on previous interactions. Our goal
is to evaluate a counterfactual policy post-data collection and estimate
structural parameters, like dynamic treatment effects, which can be used for
credit assignment and determining the effect of earlier actions on final
outcomes. Such parameters of interest can be framed as solutions to moment
equations, but not minimizers of a population loss function, leading to
Z-estimation approaches for static data. However, in the adaptive data
collection environment of reinforcement learning, where algorithms deploy
nonstationary behavior policies, standard estimators do not achieve asymptotic
normality due to the fluctuating variance. We propose a weighted Z-estimation
approach with carefully designed adaptive weights to stabilize the time-varying
estimation variance. We identify proper weighting schemes to restore the
consistency and asymptotic normality of the weighted Z-estimators for target
parameters, which allows for hypothesis testing and constructing uniform
confidence regions. Primary applications include dynamic treatment effect
estimation and dynamic off-policy evaluation.
",0
Minimax-Bayes Reinforcement Learning,"Thomas Kleine Buening, Christos Dimitrakakis, Hannes Eriksson, Divya Grover, Emilio Jorge",2023-02-21T17:10:21Z,Reinforcement Learning,"  While the Bayesian decision-theoretic framework offers an elegant solution to
the problem of decision making under uncertainty, one question is how to
appropriately select the prior distribution. One idea is to employ a worst-case
prior. However, this is not as easy to specify in sequential decision making as
in simple statistical estimation problems. This paper studies (sometimes
approximate) minimax-Bayes solutions for various reinforcement learning
problems to gain insights into the properties of the corresponding priors and
policies. We find that while the worst-case prior depends on the setting, the
corresponding minimax policies are more robust than those that assume a
standard (i.e. uniform) prior.
",0
Feudal Graph Reinforcement Learning,"Tommaso Marzi, Arshjot Khehra, Andrea Cini, Cesare Alippi",2023-04-11T09:51:13Z,Reinforcement Learning,"  Graph-based representations and message-passing modular policies constitute
prominent approaches to tackling composable control problems in reinforcement
learning (RL). However, as shown by recent graph deep learning literature, such
local message-passing operators can create information bottlenecks and hinder
global coordination. The issue becomes more serious in tasks requiring
high-level planning. In this work, we propose a novel methodology, named Feudal
Graph Reinforcement Learning (FGRL), that addresses such challenges by relying
on hierarchical RL and a pyramidal message-passing architecture. In particular,
FGRL defines a hierarchy of policies where high-level commands are propagated
from the top of the hierarchy down through a layered graph structure. The
bottom layers mimic the morphology of the physical system, while the upper
layers correspond to higher-order sub-modules. The resulting agents are then
characterized by a committee of policies where actions at a certain level set
goals for the level below, thus implementing a hierarchical decision-making
structure that can naturally implement task decomposition. We evaluate the
proposed framework on a graph clustering problem and MuJoCo locomotion tasks;
simulation results show that FGRL compares favorably against relevant
baselines. Furthermore, an in-depth analysis of the command propagation
mechanism provides evidence that the introduced message-passing scheme favors
learning hierarchical decision-making policies.
",0
Heterogeneous-Agent Reinforcement Learning,"Yifan Zhong, Jakub Grudzien Kuba, Xidong Feng, Siyi Hu, Jiaming Ji, Yaodong Yang",2023-04-19T05:08:02Z,Reinforcement Learning,"  The necessity for cooperation among intelligent machines has popularised
cooperative multi-agent reinforcement learning (MARL) in AI research. However,
many research endeavours heavily rely on parameter sharing among agents, which
confines them to only homogeneous-agent setting and leads to training
instability and lack of convergence guarantees. To achieve effective
cooperation in the general heterogeneous-agent setting, we propose
Heterogeneous-Agent Reinforcement Learning (HARL) algorithms that resolve the
aforementioned issues. Central to our findings are the multi-agent advantage
decomposition lemma and the sequential update scheme. Based on these, we
develop the provably correct Heterogeneous-Agent Trust Region Learning (HATRL),
and derive HATRPO and HAPPO by tractable approximations. Furthermore, we
discover a novel framework named Heterogeneous-Agent Mirror Learning (HAML),
which strengthens theoretical guarantees for HATRPO and HAPPO and provides a
general template for cooperative MARL algorithmic designs. We prove that all
algorithms derived from HAML inherently enjoy monotonic improvement of joint
return and convergence to Nash Equilibrium. As its natural outcome, HAML
validates more novel algorithms in addition to HATRPO and HAPPO, including
HAA2C, HADDPG, and HATD3, which generally outperform their existing
MA-counterparts. We comprehensively test HARL algorithms on six challenging
benchmarks and demonstrate their superior effectiveness and stability for
coordinating heterogeneous agents compared to strong baselines such as MAPPO
and QMIX.
",0
Two-Memory Reinforcement Learning,"Zhao Yang, Thomas. M. Moerland, Mike Preuss, Aske Plaat",2023-04-20T05:39:25Z,Reinforcement Learning,"  While deep reinforcement learning has shown important empirical success, it
tends to learn relatively slow due to slow propagation of rewards information
and slow update of parametric neural networks. Non-parametric episodic memory,
on the other hand, provides a faster learning alternative that does not require
representation learning and uses maximum episodic return as state-action values
for action selection. Episodic memory and reinforcement learning both have
their own strengths and weaknesses. Notably, humans can leverage multiple
memory systems concurrently during learning and benefit from all of them. In
this work, we propose a method called Two-Memory reinforcement learning agent
(2M) that combines episodic memory and reinforcement learning to distill both
of their strengths. The 2M agent exploits the speed of the episodic memory part
and the optimality and the generalization capacity of the reinforcement
learning part to complement each other. Our experiments demonstrate that the 2M
agent is more data efficient and outperforms both pure episodic memory and pure
reinforcement learning, as well as a state-of-the-art memory-augmented RL
agent. Moreover, the proposed approach provides a general framework that can be
used to combine any episodic memory agent with other off-policy reinforcement
learning algorithms.
",0
Replicable Reinforcement Learning,"Eric Eaton, Marcel Hussing, Michael Kearns, Jessica Sorrell",2023-05-24T16:05:15Z,Reinforcement Learning,"  The replicability crisis in the social, behavioral, and data sciences has led
to the formulation of algorithm frameworks for replicability -- i.e., a
requirement that an algorithm produce identical outputs (with high probability)
when run on two different samples from the same underlying distribution. While
still in its infancy, provably replicable algorithms have been developed for
many fundamental tasks in machine learning and statistics, including
statistical query learning, the heavy hitters problem, and distribution
testing. In this work we initiate the study of replicable reinforcement
learning, providing a provably replicable algorithm for parallel value
iteration, and a provably replicable version of R-max in the episodic setting.
These are the first formal replicability results for control problems, which
present different challenges for replication than batch learning settings.
",0
Replicability in Reinforcement Learning,"Amin Karbasi, Grigoris Velegkas, Lin F. Yang, Felix Zhou",2023-05-31T05:16:23Z,Reinforcement Learning,"  We initiate the mathematical study of replicability as an algorithmic
property in the context of reinforcement learning (RL). We focus on the
fundamental setting of discounted tabular MDPs with access to a generative
model. Inspired by Impagliazzo et al. [2022], we say that an RL algorithm is
replicable if, with high probability, it outputs the exact same policy after
two executions on i.i.d. samples drawn from the generator when its internal
randomness is the same. We first provide an efficient $\rho$-replicable
algorithm for $(\varepsilon, \delta)$-optimal policy estimation with sample and
time complexity $\widetilde
O\left(\frac{N^3\cdot\log(1/\delta)}{(1-\gamma)^5\cdot\varepsilon^2\cdot\rho^2}\right)$,
where $N$ is the number of state-action pairs. Next, for the subclass of
deterministic algorithms, we provide a lower bound of order
$\Omega\left(\frac{N^3}{(1-\gamma)^3\cdot\varepsilon^2\cdot\rho^2}\right)$.
Then, we study a relaxed version of replicability proposed by Kalavasis et al.
[2023] called TV indistinguishability. We design a computationally efficient TV
indistinguishable algorithm for policy estimation whose sample complexity is
$\widetilde
O\left(\frac{N^2\cdot\log(1/\delta)}{(1-\gamma)^5\cdot\varepsilon^2\cdot\rho^2}\right)$.
At the cost of $\exp(N)$ running time, we transform these TV indistinguishable
algorithms to $\rho$-replicable ones without increasing their sample
complexity. Finally, we introduce the notion of approximate-replicability where
we only require that two outputted policies are close under an appropriate
statistical divergence (e.g., Renyi) and show an improved sample complexity of
$\widetilde
O\left(\frac{N\cdot\log(1/\delta)}{(1-\gamma)^5\cdot\varepsilon^2\cdot\rho^2}\right)$.
",0
Representation-Driven Reinforcement Learning,"Ofir Nabati, Guy Tennenholtz, Shie Mannor",2023-05-31T14:59:12Z,Reinforcement Learning,"  We present a representation-driven framework for reinforcement learning. By
representing policies as estimates of their expected values, we leverage
techniques from contextual bandits to guide exploration and exploitation.
Particularly, embedding a policy network into a linear feature space allows us
to reframe the exploration-exploitation problem as a
representation-exploitation problem, where good policy representations enable
optimal exploration. We demonstrate the effectiveness of this framework through
its application to evolutionary and policy gradient-based approaches, leading
to significantly improved performance compared to traditional methods. Our
framework provides a new perspective on reinforcement learning, highlighting
the importance of policy representation in determining optimal
exploration-exploitation strategies.
",0
Submodular Reinforcement Learning,"Manish Prajapat, Mojmír Mutný, Melanie N. Zeilinger, Andreas Krause",2023-07-25T09:46:02Z,Reinforcement Learning,"  In reinforcement learning (RL), rewards of states are typically considered
additive, and following the Markov assumption, they are $\textit{independent}$
of states visited previously. In many important applications, such as coverage
control, experiment design and informative path planning, rewards naturally
have diminishing returns, i.e., their value decreases in light of similar
states visited previously. To tackle this, we propose $\textit{submodular RL}$
(SubRL), a paradigm which seeks to optimize more general, non-additive (and
history-dependent) rewards modelled via submodular set functions which capture
diminishing returns. Unfortunately, in general, even in tabular settings, we
show that the resulting optimization problem is hard to approximate. On the
other hand, motivated by the success of greedy algorithms in classical
submodular optimization, we propose SubPO, a simple policy gradient-based
algorithm for SubRL that handles non-additive rewards by greedily maximizing
marginal gains. Indeed, under some assumptions on the underlying Markov
Decision Process (MDP), SubPO recovers optimal constant factor approximations
of submodular bandits. Moreover, we derive a natural policy gradient approach
for locally optimizing SubRL instances even in large state- and action- spaces.
We showcase the versatility of our approach by applying SubPO to several
applications, such as biodiversity monitoring, Bayesian experiment design,
informative path planning, and coverage maximization. Our results demonstrate
sample efficiency, as well as scalability to high-dimensional state-action
spaces.
",8
Rating-based Reinforcement Learning,"Devin White, Mingkang Wu, Ellen Novoseller, Vernon J. Lawhern, Nicholas Waytowich, Yongcan Cao",2023-07-30T23:54:22Z,Reinforcement Learning,"  This paper develops a novel rating-based reinforcement learning approach that
uses human ratings to obtain human guidance in reinforcement learning.
Different from the existing preference-based and ranking-based reinforcement
learning paradigms, based on human relative preferences over sample pairs, the
proposed rating-based reinforcement learning approach is based on human
evaluation of individual trajectories without relative comparisons between
sample pairs. The rating-based reinforcement learning approach builds on a new
prediction model for human ratings and a novel multi-class loss function. We
conduct several experimental studies based on synthetic ratings and real human
ratings to evaluate the effectiveness and benefits of the new rating-based
reinforcement learning approach.
",0
Dyadic Reinforcement Learning,"Shuangning Li, Lluis Salvat Niell, Sung Won Choi, Inbal Nahum-Shani, Guy Shani, Susan Murphy",2023-08-15T15:43:12Z,Reinforcement Learning,"  Mobile health aims to enhance health outcomes by delivering interventions to
individuals as they go about their daily life. The involvement of care partners
and social support networks often proves crucial in helping individuals
managing burdensome medical conditions. This presents opportunities in mobile
health to design interventions that target the dyadic relationship -- the
relationship between a target person and their care partner -- with the aim of
enhancing social support. In this paper, we develop dyadic RL, an online
reinforcement learning algorithm designed to personalize intervention delivery
based on contextual factors and past responses of a target person and their
care partner. Here, multiple sets of interventions impact the dyad across
multiple time intervals. The developed dyadic RL is Bayesian and hierarchical.
We formally introduce the problem setup, develop dyadic RL and establish a
regret bound. We demonstrate dyadic RL's empirical performance through
simulation studies on both toy scenarios and on a realistic test bed
constructed from data collected in a mobile health study.
",0
Cyclophobic Reinforcement Learning,"Stefan Sylvius Wagner, Peter Arndt, Jan Robine, Stefan Harmeling",2023-08-30T09:38:44Z,Reinforcement Learning,"  In environments with sparse rewards, finding a good inductive bias for
exploration is crucial to the agent's success. However, there are two competing
goals: novelty search and systematic exploration. While existing approaches
such as curiosity-driven exploration find novelty, they sometimes do not
systematically explore the whole state space, akin to depth-first-search vs
breadth-first-search. In this paper, we propose a new intrinsic reward that is
cyclophobic, i.e., it does not reward novelty, but punishes redundancy by
avoiding cycles. Augmenting the cyclophobic intrinsic reward with a sequence of
hierarchical representations based on the agent's cropped observations we are
able to achieve excellent results in the MiniGrid and MiniHack environments.
Both are particularly hard, as they require complex interactions with different
objects in order to be solved. Detailed comparisons with previous approaches
and thorough ablation studies show that our newly proposed cyclophobic
reinforcement learning is more sample efficient than other state of the art
methods in a variety of tasks.
",0
Multicopy Reinforcement Learning Agents,"Alicia P. Wolfe, Oliver Diamond, Brigitte Goeler-Slough, Remi Feuerman, Magdalena Kisielinska, Victoria Manfredi",2023-09-19T20:03:17Z,Reinforcement Learning,"  This paper examines a novel type of multi-agent problem, in which an agent
makes multiple identical copies of itself in order to achieve a single agent
task better or more efficiently. This strategy improves performance if the
environment is noisy and the task is sometimes unachievable by a single agent
copy. We propose a learning algorithm for this multicopy problem which takes
advantage of the structure of the value function to efficiently learn how to
balance the advantages and costs of adding additional copies.
",0
Delays in Reinforcement Learning,Pierre Liotet,2023-09-20T07:04:46Z,Reinforcement Learning,"  Delays are inherent to most dynamical systems. Besides shifting the process
in time, they can significantly affect their performance. For this reason, it
is usually valuable to study the delay and account for it. Because they are
dynamical systems, it is of no surprise that sequential decision-making
problems such as Markov decision processes (MDP) can also be affected by
delays. These processes are the foundational framework of reinforcement
learning (RL), a paradigm whose goal is to create artificial agents capable of
learning to maximise their utility by interacting with their environment.
  RL has achieved strong, sometimes astonishing, empirical results, but delays
are seldom explicitly accounted for. The understanding of the impact of delay
on the MDP is limited. In this dissertation, we propose to study the delay in
the agent's observation of the state of the environment or in the execution of
the agent's actions. We will repeatedly change our point of view on the problem
to reveal some of its structure and peculiarities. A wide spectrum of delays
will be considered, and potential solutions will be presented. This
dissertation also aims to draw links between celebrated frameworks of the RL
literature and the one of delays.
",0
Maximum diffusion reinforcement learning,"Thomas A. Berrueta, Allison Pinosky, Todd D. Murphey",2023-09-26T22:14:56Z,Reinforcement Learning,"  Robots and animals both experience the world through their bodies and senses.
Their embodiment constrains their experiences, ensuring they unfold
continuously in space and time. As a result, the experiences of embodied agents
are intrinsically correlated. Correlations create fundamental challenges for
machine learning, as most techniques rely on the assumption that data are
independent and identically distributed. In reinforcement learning, where data
are directly collected from an agent's sequential experiences, violations of
this assumption are often unavoidable. Here, we derive a method that overcomes
this issue by exploiting the statistical mechanics of ergodic processes, which
we term maximum diffusion reinforcement learning. By decorrelating agent
experiences, our approach provably enables single-shot learning in continuous
deployments over the course of individual task attempts. Moreover, we prove our
approach generalizes well-known maximum entropy techniques, and robustly
exceeds state-of-the-art performance across popular benchmarks. Our results at
the nexus of physics, learning, and control form a foundation for transparent
and reliable decision-making in embodied reinforcement learning agents.
",0
Imitation Bootstrapped Reinforcement Learning,"Hengyuan Hu, Suvir Mirchandani, Dorsa Sadigh",2023-11-03T19:03:20Z,Reinforcement Learning,"  Despite the considerable potential of reinforcement learning (RL), robotic
control tasks predominantly rely on imitation learning (IL) due to its better
sample efficiency. However, it is costly to collect comprehensive expert
demonstrations that enable IL to generalize to all possible scenarios, and any
distribution shift would require recollecting data for finetuning. Therefore,
RL is appealing if it can build upon IL as an efficient autonomous
self-improvement procedure. We propose imitation bootstrapped reinforcement
learning (IBRL), a novel framework for sample-efficient RL with demonstrations
that first trains an IL policy on the provided demonstrations and then uses it
to propose alternative actions for both online exploration and bootstrapping
target values. Compared to prior works that oversample the demonstrations or
regularize RL with an additional imitation loss, IBRL is able to utilize high
quality actions from IL policies since the beginning of training, which greatly
accelerates exploration and training efficiency. We evaluate IBRL on 6
simulation and 3 real-world tasks spanning various difficulty levels. IBRL
significantly outperforms prior methods and the improvement is particularly
more prominent in harder tasks.
",0
Anytime-Constrained Reinforcement Learning,"Jeremy McMahan, Xiaojin Zhu",2023-11-09T16:51:26Z,Reinforcement Learning,"  We introduce and study constrained Markov Decision Processes (cMDPs) with
anytime constraints. An anytime constraint requires the agent to never violate
its budget at any point in time, almost surely. Although Markovian policies are
no longer sufficient, we show that there exist optimal deterministic policies
augmented with cumulative costs. In fact, we present a fixed-parameter
tractable reduction from anytime-constrained cMDPs to unconstrained MDPs. Our
reduction yields planning and learning algorithms that are time and
sample-efficient for tabular cMDPs so long as the precision of the costs is
logarithmic in the size of the cMDP. However, we also show that computing
non-trivial approximately optimal policies is NP-hard in general. To circumvent
this bottleneck, we design provable approximation algorithms that efficiently
compute or learn an arbitrarily accurate approximately feasible policy with
optimal value so long as the maximum supported cost is bounded by a polynomial
in the cMDP or the absolute budget. Given our hardness results, our
approximation guarantees are the best possible under worst-case analysis.
",0
Assume-Guarantee Reinforcement Learning,"Milad Kazemi, Mateo Perez, Fabio Somenzi, Sadegh Soudjani, Ashutosh Trivedi, Alvaro Velasquez",2023-12-15T16:49:57Z,Reinforcement Learning,"  We present a modular approach to \emph{reinforcement learning} (RL) in
environments consisting of simpler components evolving in parallel. A
monolithic view of such modular environments may be prohibitively large to
learn, or may require unrealizable communication between the components in the
form of a centralized controller. Our proposed approach is based on the
assume-guarantee paradigm where the optimal control for the individual
components is synthesized in isolation by making \emph{assumptions} about the
behaviors of neighboring components, and providing \emph{guarantees} about
their own behavior. We express these \emph{assume-guarantee contracts} as
regular languages and provide automatic translations to scalar rewards to be
used in RL. By combining local probabilities of satisfaction for each
component, we provide a lower bound on the probability of satisfaction of the
complete system. By solving a Markov game for each component, RL can produce a
controller for each component that maximizes this lower bound. The controller
utilizes the information it receives through communication, observations, and
any knowledge of a coarse model of other agents. We experimentally demonstrate
the efficiency of the proposed approach on a variety of case studies.
",0
Resilient Constrained Reinforcement Learning,"Dongsheng Ding, Zhengyan Huan, Alejandro Ribeiro",2023-12-28T18:28:23Z,Reinforcement Learning,"  We study a class of constrained reinforcement learning (RL) problems in which
multiple constraint specifications are not identified before training. It is
challenging to identify appropriate constraint specifications due to the
undefined trade-off between the reward maximization objective and the
constraint satisfaction, which is ubiquitous in constrained decision-making. To
tackle this issue, we propose a new constrained RL approach that searches for
policy and constraint specifications together. This method features the
adaptation of relaxing the constraint according to a relaxation cost introduced
in the learning objective. Since this feature mimics how ecological systems
adapt to disruptions by altering operation, our approach is termed as resilient
constrained RL. Specifically, we provide a set of sufficient conditions that
balance the constraint satisfaction and the reward maximization in notion of
resilient equilibrium, propose a tractable formulation of resilient constrained
policy optimization that takes this equilibrium as an optimal solution, and
advocate two resilient constrained policy search algorithms with non-asymptotic
convergence guarantees on the optimality gap and constraint satisfaction.
Furthermore, we demonstrate the merits and the effectiveness of our approach in
computational experiments.
",0
Cascading Reinforcement Learning,"Yihan Du, R. Srikant, Wei Chen",2024-01-17T04:20:26Z,Reinforcement Learning,"  Cascading bandits have gained popularity in recent years due to their
applicability to recommendation systems and online advertising. In the
cascading bandit model, at each timestep, an agent recommends an ordered subset
of items (called an item list) from a pool of items, each associated with an
unknown attraction probability. Then, the user examines the list, and clicks
the first attractive item (if any), and after that, the agent receives a
reward. The goal of the agent is to maximize the expected cumulative reward.
However, the prior literature on cascading bandits ignores the influences of
user states (e.g., historical behaviors) on recommendations and the change of
states as the session proceeds. Motivated by this fact, we propose a
generalized cascading RL framework, which considers the impact of user states
and state transition into decisions. In cascading RL, we need to select items
not only with large attraction probabilities but also leading to good successor
states. This imposes a huge computational challenge due to the combinatorial
action space. To tackle this challenge, we delve into the properties of value
functions, and design an oracle BestPerm to efficiently find the optimal item
list. Equipped with BestPerm, we develop two algorithms CascadingVI and
CascadingBPI, which are both computationally-efficient and sample-efficient,
and provide near-optimal regret and sample complexity guarantees. Furthermore,
we present experiments to show the improved computational and sample
efficiencies of our algorithms compared to straightforward adaptations of
existing RL algorithms in practice.
",0
Social Interpretable Reinforcement Learning,"Leonardo Lucio Custode, Giovanni Iacca",2024-01-27T19:05:21Z,Reinforcement Learning,"  Reinforcement Learning (RL) bears the promise of being an enabling technology
for many applications. However, since most of the literature in the field is
currently focused on opaque models, the use of RL in high-stakes scenarios,
where interpretability is crucial, is still limited. Recently, some approaches
to interpretable RL, e.g., based on Decision Trees, have been proposed, but one
of the main limitations of these techniques is their training cost. To overcome
this limitation, we propose a new population-based method, called Social
Interpretable RL (SIRL), inspired by social learning principles, to improve
learning efficiency. Our method mimics a social learning process, where each
agent in a group learns to solve a given task based both on its own individual
experience as well as the experience acquired together with its peers. Our
approach is divided into two phases. In the \emph{collaborative phase}, all the
agents in the population interact with a shared instance of the environment,
where each agent observes the state and independently proposes an action. Then,
voting is performed to choose the action that will actually be performed in the
environment. In the \emph{individual phase}, each agent refines its individual
performance by interacting with its own instance of the environment. This
mechanism makes the agents experience a larger number of episodes while
simultaneously reducing the computational cost of the process. Our results on
six well-known benchmarks show that SIRL reaches state-of-the-art performance
w.r.t. the alternative interpretable methods from the literature.
",0
Natural Language Reinforcement Learning,"Xidong Feng, Ziyu Wan, Mengyue Yang, Ziyan Wang, Girish A. Koushik, Yali Du, Ying Wen, Jun Wang",2024-02-11T11:03:04Z,Reinforcement Learning,"  Reinforcement Learning (RL) has shown remarkable abilities in learning
policies for decision-making tasks. However, RL is often hindered by issues
such as low sample efficiency, lack of interpretability, and sparse supervision
signals. To tackle these limitations, we take inspiration from the human
learning process and introduce Natural Language Reinforcement Learning (NLRL),
which innovatively combines RL principles with natural language representation.
Specifically, NLRL redefines RL concepts like task objectives, policy, value
function, Bellman equation, and policy iteration in natural language space. We
present how NLRL can be practically implemented with the latest advancements in
large language models (LLMs) like GPT-4. Initial experiments over tabular MDPs
demonstrate the effectiveness, efficiency, and also interpretability of the
NLRL framework.
",0
Hybrid Inverse Reinforcement Learning,"Juntao Ren, Gokul Swamy, Zhiwei Steven Wu, J. Andrew Bagnell, Sanjiban Choudhury",2024-02-13T23:29:09Z,Reinforcement Learning,"  The inverse reinforcement learning approach to imitation learning is a
double-edged sword. On the one hand, it can enable learning from a smaller
number of expert demonstrations with more robustness to error compounding than
behavioral cloning approaches. On the other hand, it requires that the learner
repeatedly solve a computationally expensive reinforcement learning (RL)
problem. Often, much of this computation is wasted searching over policies very
dissimilar to the expert's. In this work, we propose using hybrid RL --
training on a mixture of online and expert data -- to curtail unnecessary
exploration. Intuitively, the expert data focuses the learner on good states
during training, which reduces the amount of exploration required to compute a
strong policy. Notably, such an approach doesn't need the ability to reset the
learner to arbitrary states in the environment, a requirement of prior work in
efficient inverse RL. More formally, we derive a reduction from inverse RL to
expert-competitive RL (rather than globally optimal RL) that allows us to
dramatically reduce interaction during the inner policy search loop while
maintaining the benefits of the IRL approach. This allows us to derive both
model-free and model-based hybrid inverse RL algorithms with strong policy
performance guarantees. Empirically, we find that our approaches are
significantly more sample efficient than standard inverse RL and several other
baselines on a suite of continuous control tasks.
",0
Koopman-Assisted Reinforcement Learning,"Preston Rozwood, Edward Mehrez, Ludger Paehler, Wen Sun, Steven L. Brunton",2024-03-04T18:19:48Z,Reinforcement Learning,"  The Bellman equation and its continuous form, the Hamilton-Jacobi-Bellman
(HJB) equation, are ubiquitous in reinforcement learning (RL) and control
theory. However, these equations quickly become intractable for systems with
high-dimensional states and nonlinearity. This paper explores the connection
between the data-driven Koopman operator and Markov Decision Processes (MDPs),
resulting in the development of two new RL algorithms to address these
limitations. We leverage Koopman operator techniques to lift a nonlinear system
into new coordinates where the dynamics become approximately linear, and where
HJB-based methods are more tractable. In particular, the Koopman operator is
able to capture the expectation of the time evolution of the value function of
a given system via linear dynamics in the lifted coordinates. By parameterizing
the Koopman operator with the control actions, we construct a ``Koopman
tensor'' that facilitates the estimation of the optimal value function. Then, a
transformation of Bellman's framework in terms of the Koopman tensor enables us
to reformulate two max-entropy RL algorithms: soft value iteration and soft
actor-critic (SAC). This highly flexible framework can be used for
deterministic or stochastic systems as well as for discrete or continuous-time
dynamics. Finally, we show that these Koopman Assisted Reinforcement Learning
(KARL) algorithms attain state-of-the-art (SOTA) performance with respect to
traditional neural network-based SAC and linear quadratic regulator (LQR)
baselines on four controlled dynamical systems: a linear state-space system,
the Lorenz system, fluid flow past a cylinder, and a double-well potential with
non-isotropic stochastic forcing.
",0
"Explainable AI for Robot Failures: Generating Explanations that Improve
  User Assistance in Fault Recovery","Devleena Das, Siddhartha Banerjee, Sonia Chernova",2021-01-05T16:16:39Z,Explainable AI,"  With the growing capabilities of intelligent systems, the integration of
robots in our everyday life is increasing. However, when interacting in such
complex human environments, the occasional failure of robotic systems is
inevitable. The field of explainable AI has sought to make complex-decision
making systems more interpretable but most existing techniques target domain
experts. On the contrary, in many failure cases, robots will require recovery
assistance from non-expert users. In this work, we introduce a new type of
explanation, that explains the cause of an unexpected failure during an agent's
plan execution to non-experts. In order for error explanations to be
meaningful, we investigate what types of information within a set of
hand-scripted explanations are most helpful to non-experts for failure and
solution identification. Additionally, we investigate how such explanations can
be autonomously generated, extending an existing encoder-decoder model, and
generalized across environments. We investigate such questions in the context
of a robot performing a pick-and-place manipulation task in the home
environment. Our results show that explanations capturing the context of a
failure and history of past actions, are the most effective for failure and
solution identification among non-experts. Furthermore, through a second user
evaluation, we verify that our model-generated explanations can generalize to
an unseen office environment, and are just as effective as the hand-scripted
explanations.
",0
"Explainable AI and Adoption of Financial Algorithmic Advisors: an
  Experimental Study","Daniel Ben David, Yehezkel S. Resheff, Talia Tron",2021-01-05T09:34:38Z,Explainable AI,"  We study whether receiving advice from either a human or algorithmic advisor,
accompanied by five types of Local and Global explanation labelings, has an
effect on the readiness to adopt, willingness to pay, and trust in a financial
AI consultant. We compare the differences over time and in various key
situations using a unique experimental framework where participants play a
web-based game with real monetary consequences. We observed that accuracy-based
explanations of the model in initial phases leads to higher adoption rates.
When the performance of the model is immaculate, there is less importance
associated with the kind of explanation for adoption. Using more elaborate
feature-based or accuracy-based explanations helps substantially in reducing
the adoption drop upon model failure. Furthermore, using an autopilot increases
adoption significantly. Participants assigned to the AI-labeled advice with
explanations were willing to pay more for the advice than the AI-labeled advice
with a No-explanation alternative. These results add to the literature on the
importance of XAI for algorithmic adoption and trust.
",0
"An Explainable AI System for Automated COVID-19 Assessment and Lesion
  Categorization from CT-scans","Matteo Pennisi, Isaak Kavasidis, Concetto Spampinato, Vincenzo Schininà, Simone Palazzo, Francesco Rundo, Massimo Cristofaro, Paolo Campioni, Elisa Pianura, Federica Di Stefano, Ada Petrone, Fabrizio Albarello, Giuseppe Ippolito, Salvatore Cuzzocrea, Sabrina Conoci",2021-01-28T11:47:35Z,Explainable AI,"  COVID-19 infection caused by SARS-CoV-2 pathogen is a catastrophic pandemic
outbreak all over the world with exponential increasing of confirmed cases and,
unfortunately, deaths. In this work we propose an AI-powered pipeline, based on
the deep-learning paradigm, for automated COVID-19 detection and lesion
categorization from CT scans. We first propose a new segmentation module aimed
at identifying automatically lung parenchyma and lobes. Next, we combined such
segmentation network with classification networks for COVID-19 identification
and lesion categorization. We compare the obtained classification results with
those obtained by three expert radiologists on a dataset consisting of 162 CT
scans. Results showed a sensitivity of 90\% and a specificity of 93.5% for
COVID-19 detection, outperforming those yielded by the expert radiologists, and
an average lesion categorization accuracy of over 84%. Results also show that a
significant role is played by prior lung and lobe segmentation that allowed us
to enhance performance by over 20 percent points. The interpretation of the
trained AI models, moreover, reveals that the most significant areas for
supporting the decision on COVID-19 identification are consistent with the
lesions clinically associated to the virus, i.e., crazy paving, consolidation
and ground glass. This means that the artificial models are able to
discriminate a positive patient from a negative one (both controls and patients
with interstitial pneumonia tested negative to COVID) by evaluating the
presence of those lesions into CT scans. Finally, the AI models are integrated
into a user-friendly GUI to support AI explainability for radiologists, which
is publicly available at http://perceivelab.com/covid-ai.
",0
"Towards Designing Computer Vision-based Explainable-AI Solution: A Use
  Case of Livestock Mart Industry","Devam Dave, Het Naik, Smiti Singhal, Rudresh Dwivedi, Pankesh Patel",2021-02-08T17:11:19Z,Other,"  The objective of an online Mart is to match buyers and sellers, to weigh
animals and to oversee their sale. A reliable pricing method can be developed
by ML models that can read through historical sales data. However, when AI
models suggest or recommend a price, that in itself does not reveal too much
(i.e., it acts like a black box) about the qualities and the abilities of an
animal. An interested buyer would like to know more about the salient features
of an animal before making the right choice based on his requirements. A model
capable of explaining the different factors that impact the price point is
essential for the needs of the market. It can also inspire confidence in buyers
and sellers about the price point offered. To achieve these objectives, we have
been working with the team at MartEye, a startup based in Portershed in Galway
City, Ireland. Through this paper, we report our work-in-progress research
towards building a smart video analytic platform, leveraging Explainable AI
techniques.
",0
"Explainable AI by BAPC -- Before and After correction Parameter
  Comparison","Florian Sobieczky, Manuela Geiß",2021-03-12T09:03:51Z,Explainable AI,"  A local surrogate for an AI-model correcting a simpler 'base' model is
introduced representing an analytical method to yield explanations of
AI-predictions. The approach is studied here in the context of the base model
being linear regression. The AI-model approximates the residual error of the
linear model and the explanations are formulated in terms of the change of the
interpretable base model's parameters. Criteria are formulated for the precise
relation between lost accuracy of the surrogate, the accuracy of the AI-model,
and the surrogate fidelity. It is shown that, assuming a certain maximal amount
of noise in the observed data, these criteria induce neighborhoods of the
instances to be explained which have an ideal size in terms of maximal accuracy
and fidelity.
",0
"A Novel Interaction-based Methodology Towards Explainable AI with Better
  Understanding of Pneumonia Chest X-ray Images","Shaw-Hwa Lo, Yiqiao Yin",2021-04-19T23:02:43Z,Explainable AI,"  In the field of eXplainable AI (XAI), robust ""blackbox"" algorithms such as
Convolutional Neural Networks (CNNs) are known for making high prediction
performance. However, the ability to explain and interpret these algorithms
still require innovation in the understanding of influential and, more
importantly, explainable features that directly or indirectly impact the
performance of predictivity. A number of methods existing in literature focus
on visualization techniques but the concepts of explainability and
interpretability still require rigorous definition. In view of the above needs,
this paper proposes an interaction-based methodology -- Influence Score
(I-score) -- to screen out the noisy and non-informative variables in the
images hence it nourishes an environment with explainable and interpretable
features that are directly associated to feature predictivity. We apply the
proposed method on a real world application in Pneumonia Chest X-ray Image data
set and produced state-of-the-art results. We demonstrate how to apply the
proposed approach for more general big data problems by improving the
explainability and interpretability without sacrificing the prediction
performance. The contribution of this paper opens a novel angle that moves the
community closer to the future pipelines of XAI problems.
",0
Explainable AI For COVID-19 CT Classifiers: An Initial Comparison Study,"Qinghao Ye, Jun Xia, Guang Yang",2021-04-25T23:39:14Z,Explainable AI,"  Artificial Intelligence (AI) has made leapfrogs in development across all the
industrial sectors especially when deep learning has been introduced. Deep
learning helps to learn the behaviour of an entity through methods of
recognising and interpreting patterns. Despite its limitless potential, the
mystery is how deep learning algorithms make a decision in the first place.
Explainable AI (XAI) is the key to unlocking AI and the black-box for deep
learning. XAI is an AI model that is programmed to explain its goals, logic,
and decision making so that the end users can understand. The end users can be
domain experts, regulatory agencies, managers and executive board members, data
scientists, users that use AI, with or without awareness, or someone who is
affected by the decisions of an AI model. Chest CT has emerged as a valuable
tool for the clinical diagnostic and treatment management of the lung diseases
associated with COVID-19. AI can support rapid evaluation of CT scans to
differentiate COVID-19 findings from other lung diseases. However, how these AI
tools or deep learning algorithms reach such a decision and which are the most
influential features derived from these neural networks with typically deep
layers are not clear. The aim of this study is to propose and develop XAI
strategies for COVID-19 classification models with an investigation of
comparison. The results demonstrate promising quantification and qualitative
visualisations that can further enhance the clinician's understanding and
decision making with more granular information from the results given by the
learned XAI models.
",0
"Show Why the Answer is Correct! Towards Explainable AI using
  Compositional Temporal Attention","Nihar Bendre, Kevin Desai, Peyman Najafirad",2021-05-15T04:51:51Z,Explainable AI,"  Visual Question Answering (VQA) models have achieved significant success in
recent times. Despite the success of VQA models, they are mostly black-box
models providing no reasoning about the predicted answer, thus raising
questions for their applicability in safety-critical such as autonomous systems
and cyber-security. Current state of the art fail to better complex questions
and thus are unable to exploit compositionality. To minimize the black-box
effect of these models and also to make them better exploit compositionality,
we propose a Dynamic Neural Network (DMN), which can understand a particular
question and then dynamically assemble various relatively shallow deep learning
modules from a pool of modules to form a network. We incorporate compositional
temporal attention to these deep learning based modules to increase
compositionality exploitation. This results in achieving better understanding
of complex questions and also provides reasoning as to why the module predicts
a particular answer. Experimental analysis on the two benchmark datasets,
VQA2.0 and CLEVR, depicts that our model outperforms the previous approaches
for Visual Question Answering task as well as provides better reasoning, thus
making it reliable for mission critical applications like safety and security.
",0
"Bridging the Gap Between Explainable AI and Uncertainty Quantification
  to Enhance Trustability",Dominik Seuß,2021-05-25T10:53:58Z,Explainable AI,"  After the tremendous advances of deep learning and other AI methods, more
attention is flowing into other properties of modern approaches, such as
interpretability, fairness, etc. combined in frameworks like Responsible AI.
Two research directions, namely Explainable AI and Uncertainty Quantification
are becoming more and more important, but have been so far never combined and
jointly explored. In this paper, I show how both research areas provide
potential for combination, why more research should be done in this direction
and how this would lead to an increase in trustability in AI systems.
",0
"Designing ECG Monitoring Healthcare System with Federated Transfer
  Learning and Explainable AI","Ali Raza, Kim Phuc Tran, Ludovic Koehl, Shujun Li",2021-05-26T11:59:44Z,Explainable AI,"  Deep learning play a vital role in classifying different arrhythmias using
the electrocardiography (ECG) data. Nevertheless, training deep learning models
normally requires a large amount of data and it can lead to privacy concerns.
Unfortunately, a large amount of healthcare data cannot be easily collected
from a single silo. Additionally, deep learning models are like black-box, with
no explainability of the predicted results, which is often required in clinical
healthcare. This limits the application of deep learning in real-world health
systems. In this paper, we design a new explainable artificial intelligence
(XAI) based deep learning framework in a federated setting for ECG-based
healthcare applications. The federated setting is used to solve issues such as
data availability and privacy concerns. Furthermore, the proposed framework
setting effectively classifies arrhythmia's using an autoencoder and a
classifier, both based on a convolutional neural network (CNN). Additionally,
we propose an XAI-based module on top of the proposed classifier to explain the
classification results, which help clinical practitioners make quick and
reliable decisions. The proposed framework was trained and tested using the
MIT-BIH Arrhythmia database. The classifier achieved accuracy up to 94% and 98%
for arrhythmia detection using noisy and clean data, respectively, with
five-fold cross-validation.
",0
Towards an Explanation Space to Align Humans and Explainable-AI Teamwork,"Garrick Cabour, Andrés Morales, Élise Ledoux, Samuel Bassetto",2021-06-02T23:17:29Z,Other,"  Providing meaningful and actionable explanations to end-users is a
fundamental prerequisite for implementing explainable intelligent systems in
the real world. Explainability is a situated interaction between a user and the
AI system rather than being static design principles. The content of
explanations is context-dependent and must be defined by evidence about the
user and its context. This paper seeks to operationalize this concept by
proposing a formative architecture that defines the explanation space from a
user-inspired perspective. The architecture comprises five intertwined
components to outline explanation requirements for a task: (1) the end-users
mental models, (2) the end-users cognitive process, (3) the user interface, (4)
the human-explainer agent, and the (5) agent process. We first define each
component of the architecture. Then we present the Abstracted Explanation
Space, a modeling tool that aggregates the architecture's components to support
designers in systematically aligning explanations with the end-users work
practices, needs, and goals. It guides the specifications of what needs to be
explained (content - end-users mental model), why this explanation is necessary
(context - end-users cognitive process), to delimit how to explain it (format -
human-explainer agent and user interface), and when should the explanations be
given. We then exemplify the tool's use in an ongoing case study in the
aircraft maintenance domain. Finally, we discuss possible contributions of the
tool, known limitations/areas for improvement, and future work to be done.
",0
"Revealing drivers and risks for power grid frequency stability with
  explainable AI","Johannes Kruse, Benjamin Schäfer, Dirk Witthaut",2021-06-07T16:16:39Z,Explainable AI,"  Stable operation of the electrical power system requires the power grid
frequency to stay within strict operational limits. With millions of consumers
and thousands of generators connected to a power grid, detailed human-build
models can no longer capture the full dynamics of this complex system. Modern
machine learning algorithms provide a powerful alternative for system modelling
and prediction, but the intrinsic black-box character of many models impedes
scientific insights and poses severe security risks. Here, we show how
eXplainable AI (XAI) alleviates these problems by revealing critical
dependencies and influences on the power grid frequency. We accurately predict
frequency stability indicators (such as RoCoF and Nadir) for three major
European synchronous areas and identify key features that determine the power
grid stability. Load ramps, specific generation ramps but also prices and
forecast errors are central to understand and stabilize the power grid.
",0
"Explainable AI for medical imaging: Explaining pneumothorax diagnoses
  with Bayesian Teaching","Tomas Folke, Scott Cheng-Hsin Yang, Sean Anderson, Patrick Shafto",2021-06-08T20:49:11Z,Explainable AI,"  Limited expert time is a key bottleneck in medical imaging. Due to advances
in image classification, AI can now serve as decision-support for medical
experts, with the potential for great gains in radiologist productivity and, by
extension, public health. However, these gains are contingent on building and
maintaining experts' trust in the AI agents. Explainable AI may build such
trust by helping medical experts to understand the AI decision processes behind
diagnostic judgements. Here we introduce and evaluate explanations based on
Bayesian Teaching, a formal account of explanation rooted in the cognitive
science of human learning. We find that medical experts exposed to explanations
generated by Bayesian Teaching successfully predict the AI's diagnostic
decisions and are more likely to certify the AI for cases when the AI is
correct than when it is wrong, indicating appropriate trust. These results show
that Explainable AI can be used to support human-AI collaboration in medical
imaging.
",0
"An Explainable AI System for the Diagnosis of High Dimensional
  Biomedical Data","Alfred Ultsch, Jörg Hoffmann, Maximilian Röhnert, Malte Von Bonin, Uta Oelschlägel, Cornelia Brendel, Michael C. Thrun",2021-07-05T07:00:29Z,Explainable AI,"  Typical state of the art flow cytometry data samples consists of measures of
more than 100.000 cells in 10 or more features. AI systems are able to diagnose
such data with almost the same accuracy as human experts. However, there is one
central challenge in such systems: their decisions have far-reaching
consequences for the health and life of people, and therefore, the decisions of
AI systems need to be understandable and justifiable by humans. In this work,
we present a novel explainable AI method, called ALPODS, which is able to
classify (diagnose) cases based on clusters, i.e., subpopulations, in the
high-dimensional data. ALPODS is able to explain its decisions in a form that
is understandable for human experts. For the identified subpopulations, fuzzy
reasoning rules expressed in the typical language of domain experts are
generated. A visualization method based on these rules allows human experts to
understand the reasoning used by the AI system. A comparison to a selection of
state of the art explainable AI systems shows that ALPODS operates efficiently
on known benchmark data and also on everyday routine case data.
",0
"A Decision Support System for Safer Airplane Landings: Predicting Runway
  Conditions Using XGBoost and Explainable AI","Alise Danielle Midtfjord, Riccardo De Bin, Arne Bang Huseby",2021-07-01T11:01:13Z,Explainable AI,"  The presence of snow and ice on runway surfaces reduces the available
tire-pavement friction needed for retardation and directional control and
causes potential economic and safety threats for the aviation industry during
the winter seasons. To activate appropriate safety procedures, pilots need
accurate and timely information on the actual runway surface conditions. In
this study, XGBoost is used to create a combined runway assessment system,
which includes a classification model to identify slippery conditions and a
regression model to predict the level of slipperiness. The models are trained
on weather data and runway reports. The runway surface conditions are
represented by the tire-pavement friction coefficient, which is estimated from
flight sensor data from landing aircrafts. The XGBoost models are combined with
SHAP approximations to provide a reliable decision support system for airport
operators, which can contribute to safer and more economic operations of
airport runways. To evaluate the performance of the prediction models, they are
compared to several state-of-the-art runway assessment methods. The XGBoost
models identify slippery runway conditions with a ROC AUC of 0.95, predict the
friction coefficient with a MAE of 0.0254, and outperforms all the previous
methods. The results show the strong abilities of machine learning methods to
model complex, physical phenomena with a good accuracy. Published version:
https://doi.org/10.1016/j.coldregions.2022.103556.
",0
"Desiderata for Explainable AI in statistical production systems of the
  European Central Bank","Carlos Mougan, Georgios Kanellos, Thomas Gottron",2021-07-18T05:58:11Z,Explainable AI,"  Explainable AI constitutes a fundamental step towards establishing fairness
and addressing bias in algorithmic decision-making. Despite the large body of
work on the topic, the benefit of solutions is mostly evaluated from a
conceptual or theoretical point of view and the usefulness for real-world use
cases remains uncertain. In this work, we aim to state clear user-centric
desiderata for explainable AI reflecting common explainability needs
experienced in statistical production systems of the European Central Bank. We
link the desiderata to archetypical user roles and give examples of techniques
and methods which can be used to address the user's needs. To this end, we
provide two concrete use cases from the domain of statistical data production
in central banks: the detection of outliers in the Centralised Securities
Database and the data-driven identification of data quality checks for the
Supervisory Banking data system.
",0
"Proceedings of ICML 2021 Workshop on Theoretic Foundation, Criticism,
  and Application Trend of Explainable AI","Quanshi Zhang, Tian Han, Lixin Fan, Zhanxing Zhu, Hang Su, Ying Nian Wu, Jie Ren, Hao Zhang",2021-07-16T13:14:16Z,Explainable AI,"  This is the Proceedings of ICML 2021 Workshop on Theoretic Foundation,
Criticism, and Application Trend of Explainable AI. Deep neural networks (DNNs)
have undoubtedly brought great success to a wide range of applications in
computer vision, computational linguistics, and AI. However, foundational
principles underlying the DNNs' success and their resilience to adversarial
attacks are still largely missing. Interpreting and theorizing the internal
mechanisms of DNNs becomes a compelling yet controversial topic. This workshop
pays a special interest in theoretic foundations, limitations, and new
application trends in the scope of XAI. These issues reflect new bottlenecks in
the future development of XAI.
",0
"MEGEX: Data-Free Model Extraction Attack against Gradient-Based
  Explainable AI","Takayuki Miura, Satoshi Hasegawa, Toshiki Shibahara",2021-07-19T14:25:06Z,Explainable AI,"  The advance of explainable artificial intelligence, which provides reasons
for its predictions, is expected to accelerate the use of deep neural networks
in the real world like Machine Learning as a Service (MLaaS) that returns
predictions on queried data with the trained model. Deep neural networks
deployed in MLaaS face the threat of model extraction attacks. A model
extraction attack is an attack to violate intellectual property and privacy in
which an adversary steals trained models in a cloud using only their
predictions. In particular, a data-free model extraction attack has been
proposed recently and is more critical. In this attack, an adversary uses a
generative model instead of preparing input data. The feasibility of this
attack, however, needs to be studied since it requires more queries than that
with surrogate datasets. In this paper, we propose MEGEX, a data-free model
extraction attack against a gradient-based explainable AI. In this method, an
adversary uses the explanations to train the generative model and reduces the
number of queries to steal the model. Our experiments show that our proposed
method reconstructs high-accuracy models -- 0.97$\times$ and 0.98$\times$ the
victim model accuracy on SVHN and CIFAR-10 datasets given 2M and 20M queries,
respectively. This implies that there is a trade-off between the
interpretability of models and the difficulty of stealing them.
",0
"Semantic-Based Explainable AI: Leveraging Semantic Scene Graphs and
  Pairwise Ranking to Explain Robot Failures","Devleena Das, Sonia Chernova",2021-08-08T02:44:23Z,"RAG, Explainable AI","  When interacting in unstructured human environments, occasional robot
failures are inevitable. When such failures occur, everyday people, rather than
trained technicians, will be the first to respond. Existing natural language
explanations hand-annotate contextual information from an environment to help
everyday people understand robot failures. However, this methodology lacks
generalizability and scalability. In our work, we introduce a more
generalizable semantic explanation framework. Our framework autonomously
captures the semantic information in a scene to produce semantically
descriptive explanations for everyday users. To generate failure-focused
explanations that are semantically grounded, we leverages both semantic scene
graphs to extract spatial relations and object attributes from an environment,
as well as pairwise ranking. Our results show that these semantically
descriptive explanations significantly improve everyday users' ability to both
identify failures and provide assistance for recovery than the existing
state-of-the-art context-based explanations.
",0
"Explainable AI and susceptibility to adversarial attacks: a case study
  in classification of breast ultrasound images","Hamza Rasaee, Hassan Rivaz",2021-08-09T23:52:16Z,Explainable AI,"  Ultrasound is a non-invasive imaging modality that can be conveniently used
to classify suspicious breast nodules and potentially detect the onset of
breast cancer. Recently, Convolutional Neural Networks (CNN) techniques have
shown promising results in classifying ultrasound images of the breast into
benign or malignant. However, CNN inference acts as a black-box model, and as
such, its decision-making is not interpretable. Therefore, increasing effort
has been dedicated to explaining this process, most notably through GRAD-CAM
and other techniques that provide visual explanations into inner workings of
CNNs. In addition to interpretation, these methods provide clinically important
information, such as identifying the location for biopsy or treatment. In this
work, we analyze how adversarial assaults that are practically undetectable may
be devised to alter these importance maps dramatically. Furthermore, we will
show that this change in the importance maps can come with or without altering
the classification result, rendering them even harder to detect. As such, care
must be taken when using these importance maps to shed light on the inner
workings of deep learning. Finally, we utilize Multi-Task Learning (MTL) and
propose a new network based on ResNet-50 to improve the classification
accuracies. Our sensitivity and specificity is comparable to the state of the
art results.
",0
"On Quantifying Literals in Boolean Logic and Its Applications to
  Explainable AI","Adnan Darwiche, Pierre Marquis",2021-08-23T00:42:22Z,Explainable AI,"  Quantified Boolean logic results from adding operators to Boolean logic for
existentially and universally quantifying variables. This extends the reach of
Boolean logic by enabling a variety of applications that have been explored
over the decades. The existential quantification of literals (variable states)
and its applications have also been studied in the literature. In this paper,
we complement this by studying universal literal quantification and its
applications, particularly to explainable AI. We also provide a novel semantics
for quantification, discuss the interplay between variable/literal and
existential/universal quantification. We further identify some classes of
Boolean formulas and circuits on which quantification can be done efficiently.
Literal quantification is more fine-grained than variable quantification as the
latter can be defined in terms of the former. This leads to a refinement of
quantified Boolean logic with literal quantification as its primitive.
",25
"An Objective Metric for Explainable AI: How and Why to Estimate the
  Degree of Explainability","Francesco Sovrano, Fabio Vitali",2021-09-11T17:44:13Z,Explainable AI,"  Explainable AI was born as a pathway to allow humans to explore and
understand the inner working of complex systems. However, establishing what is
an explanation and objectively evaluating explainability are not trivial tasks.
This paper presents a new model-agnostic metric to measure the Degree of
Explainability of information in an objective way. We exploit a specific
theoretical model from Ordinary Language Philosophy called the Achinstein's
Theory of Explanations, implemented with an algorithm relying on deep language
models for knowledge graph extraction and information retrieval. To understand
whether this metric can measure explainability, we devised a few experiments
and user studies involving more than 190 participants, evaluating two realistic
systems for healthcare and finance using famous AI technology, including
Artificial Neural Networks and TreeSHAP. The results we obtained are
statistically significant (with P values lower than .01), suggesting that our
proposed metric for measuring the Degree of Explainability is robust in several
scenarios, and it aligns with concrete expectations.
",0
"Internet of Behavior (IoB) and Explainable AI Systems for Influencing
  IoT Behavior","Haya Elayan, Moayad Aloqaily, Fakhri Karray, Mohsen Guizani",2021-09-15T12:16:11Z,Explainable AI,"  Pandemics and natural disasters over the years have changed the behavior of
people, which has had a tremendous impact on all life aspects. With the
technologies available in each era, governments, organizations, and companies
have used these technologies to track, control, and influence the behavior of
individuals for a benefit. Nowadays, the use of the Internet of Things (IoT),
cloud computing, and artificial intelligence (AI) have made it easier to track
and change the behavior of users through changing IoT behavior. This article
introduces and discusses the concept of the Internet of Behavior (IoB) and its
integration with Explainable AI (XAI) techniques to provide trusted and evident
experience in the process of changing IoT behavior to ultimately improving
users' behavior. Therefore, a system based on IoB and XAI has been proposed in
a use case scenario of electrical power consumption that aims to influence user
consuming behavior to reduce power consumption and cost. The scenario results
showed a decrease of 522.2 kW of active power when compared to original
consumption over a 200-hours period. It also showed a total power cost saving
of 95.04 Euro for the same period. Moreover, decreasing the global active power
will reduce the power intensity through the positive correlation.
",0
"An Explainable-AI approach for Diagnosis of COVID-19 using MALDI-ToF
  Mass Spectrometry","Venkata Devesh Reddy Seethi, Zane LaCasse, Prajkta Chivte, Joshua Bland, Shrihari S. Kadkol, Elizabeth R. Gaillard, Pratool Bharti, Hamed Alhoori",2021-09-28T23:29:31Z,Other,"  The severe acute respiratory syndrome coronavirus type-2 (SARS-CoV-2) caused
a global pandemic and immensely affected the global economy. Accurate,
cost-effective, and quick tests have proven substantial in identifying infected
people and mitigating the spread. Recently, multiple alternative platforms for
testing coronavirus disease 2019 (COVID-19) have been published that show high
agreement with current gold standard real-time polymerase chain reaction
(RT-PCR) results. These new methods do away with nasopharyngeal (NP) swabs,
eliminate the need for complicated reagents, and reduce the burden on RT-PCR
test reagent supply. In the present work, we have designed an artificial
intelligence-based (AI) testing method to provide confidence in the results.
Current AI applications for COVID-19 studies often lack a biological foundation
in the decision-making process, and our AI approach is one of the earliest to
leverage explainable AI (X-AI) algorithms for COVID-19 diagnosis using mass
spectrometry. Here, we have employed X-AI to explain the decision-making
process on a local (per-sample) and global (all samples) basis underscored by
biologically relevant features. We evaluated our technique with data extracted
from human gargle samples and achieved a testing accuracy of 94.12%. Such
techniques would strengthen the relationship between AI and clinical
diagnostics by providing biomedical researchers and healthcare workers with
trustworthy and, most importantly, explainable test results
",0
"Exploring Explainable AI in the Financial Sector: Perspectives of Banks
  and Supervisory Authorities","Ouren Kuiper, Martin van den Berg, Joost van der Burgt, Stefan Leijnen",2021-11-03T14:11:37Z,Explainable AI,"  Explainable artificial intelligence (xAI) is seen as a solution to making AI
systems less of a black box. It is essential to ensure transparency, fairness,
and accountability, which are especially paramount in the financial sector. The
aim of this study was a preliminary investigation of the perspectives of
supervisory authorities and regulated entities regarding the application of xAI
in the fi-nancial sector. Three use cases (consumer credit, credit risk, and
anti-money laundering) were examined using semi-structured interviews at three
banks and two supervisory authorities in the Netherlands. We found that for the
investigated use cases a disparity exists between supervisory authorities and
banks regarding the desired scope of explainability of AI systems. We argue
that the financial sector could benefit from clear differentiation between
technical AI (model) ex-plainability requirements and explainability
requirements of the broader AI system in relation to applicable laws and
regulations.
",0
"Characterizing Human Explanation Strategies to Inform the Design of
  Explainable AI for Building Damage Assessment","Donghoon Shin, Sachin Grover, Kenneth Holstein, Adam Perer",2021-11-04T04:53:57Z,Explainable AI,"  Explainable AI (XAI) is a promising means of supporting human-AI
collaborations for high-stakes visual detection tasks, such as damage detection
tasks from satellite imageries, as fully-automated approaches are unlikely to
be perfectly safe and reliable. However, most existing XAI techniques are not
informed by the understandings of task-specific needs of humans for
explanations. Thus, we took a first step toward understanding what forms of XAI
humans require in damage detection tasks. We conducted an online crowdsourced
study to understand how people explain their own assessments, when evaluating
the severity of building damage based on satellite imagery. Through the study
with 60 crowdworkers, we surfaced six major strategies that humans utilize to
explain their visual damage assessments. We present implications of our
findings for the design of XAI methods for such visual detection contexts, and
discuss opportunities for future research.
",0
"Demystifying Deep Learning Models for Retinal OCT Disease Classification
  using Explainable AI","Tasnim Sakib Apon, Mohammad Mahmudul Hasan, Abrar Islam, MD. Golam Rabiul Alam",2021-11-06T13:54:07Z,Explainable AI,"  In the world of medical diagnostics, the adoption of various deep learning
techniques is quite common as well as effective, and its statement is equally
true when it comes to implementing it into the retina Optical Coherence
Tomography (OCT) sector, but (i)These techniques have the black box
characteristics that prevent the medical professionals to completely trust the
results generated from them (ii)Lack of precision of these methods restricts
their implementation in clinical and complex cases (iii)The existing works and
models on the OCT classification are substantially large and complicated and
they require a considerable amount of memory and computational power, reducing
the quality of classifiers in real-time applications. To meet these problems,
in this paper a self-developed CNN model has been proposed which is
comparatively smaller and simpler along with the use of Lime that introduces
Explainable AI to the study and helps to increase the interpretability of the
model. This addition will be an asset to the medical experts for getting major
and detailed information and will help them in making final decisions and will
also reduce the opacity and vulnerability of the conventional deep learning
models.
",0
"Explainable AI (XAI): A Systematic Meta-Survey of Current Challenges and
  Future Opportunities","Waddah Saeed, Christian Omlin",2021-11-11T19:06:13Z,Explainable AI,"  The past decade has seen significant progress in artificial intelligence
(AI), which has resulted in algorithms being adopted for resolving a variety of
problems. However, this success has been met by increasing model complexity and
employing black-box AI models that lack transparency. In response to this need,
Explainable AI (XAI) has been proposed to make AI more transparent and thus
advance the adoption of AI in critical domains. Although there are several
reviews of XAI topics in the literature that identified challenges and
potential research directions in XAI, these challenges and research directions
are scattered. This study, hence, presents a systematic meta-survey for
challenges and future research directions in XAI organized in two themes: (1)
general challenges and research directions in XAI and (2) challenges and
research directions in XAI based on machine learning life cycle's phases:
design, development, and deployment. We believe that our meta-survey
contributes to XAI literature by providing a guide for future exploration in
the XAI area.
",0
"A Practical guide on Explainable AI Techniques applied on Biomedical use
  case applications","Adrien Bennetot, Ivan Donadello, Ayoub El Qadi, Mauro Dragoni, Thomas Frossard, Benedikt Wagner, Anna Saranti, Silvia Tulli, Maria Trocan, Raja Chatila, Andreas Holzinger, Artur d'Avila Garcez, Natalia Díaz-Rodríguez",2021-11-13T17:47:31Z,Explainable AI,"  Last years have been characterized by an upsurge of opaque automatic decision
support systems, such as Deep Neural Networks (DNNs). Although they have great
generalization and prediction skills, their functioning does not allow
obtaining detailed explanations of their behaviour. As opaque machine learning
models are increasingly being employed to make important predictions in
critical environments, the danger is to create and use decisions that are not
justifiable or legitimate. Therefore, there is a general agreement on the
importance of endowing machine learning models with explainability. EXplainable
Artificial Intelligence (XAI) techniques can serve to verify and certify model
outputs and enhance them with desirable notions such as trustworthiness,
accountability, transparency and fairness. This guide is meant to be the go-to
handbook for any audience with a computer science background aiming at getting
intuitive insights on machine learning models, accompanied with straight, fast,
and intuitive explanations out of the box. This article aims to fill the lack
of compelling XAI guide by applying XAI techniques in their particular
day-to-day models, datasets and use-cases. Figure 1 acts as a flowchart/map for
the reader and should help him to find the ideal method to use according to his
type of data. In each chapter, the reader will find a description of the
proposed method as well as an example of use on a Biomedical application and a
Python notebook. It can be easily modified in order to be applied to specific
applications.
",0
"Applications of Explainable AI for 6G: Technical Aspects, Use Cases, and
  Research Challenges","Shen Wang, M. Atif Qureshi, Luis Miralles-Pechuán, Thien Huynh-The, Thippa Reddy Gadekallu, Madhusanka Liyanage",2021-12-09T04:46:31Z,Explainable AI,"  When 5G began its commercialisation journey around 2020, the discussion on
the vision of 6G also surfaced. Researchers expect 6G to have higher bandwidth,
coverage, reliability, energy efficiency, lower latency, and an integrated
""human-centric"" network system powered by artificial intelligence (AI). Such a
6G network will lead to an excessive number of automated decisions made in
real-time. These decisions can range widely, from network resource allocation
to collision avoidance for self-driving cars. However, the risk of losing
control over decision-making may increase due to high-speed, data-intensive AI
decision-making beyond designers' and users' comprehension. The promising
explainable AI (XAI) methods can mitigate such risks by enhancing the
transparency of the black-box AI decision-making process. This paper surveys
the application of XAI towards the upcoming 6G age in every aspect, including
6G technologies (e.g., intelligent radio, zero-touch network management) and 6G
use cases (e.g., industry 5.0). Moreover, we summarised the lessons learned
from the recent attempts and outlined important research challenges in applying
XAI for 6G in the near future.
",0
"A Critical Review of Inductive Logic Programming Techniques for
  Explainable AI","Zheng Zhang, Liangliang Xu, Levent Yilmaz, Bo Liu",2021-12-31T06:34:32Z,Explainable AI,"  Despite recent advances in modern machine learning algorithms, the opaqueness
of their underlying mechanisms continues to be an obstacle in adoption. To
instill confidence and trust in artificial intelligence systems, Explainable
Artificial Intelligence has emerged as a response to improving modern machine
learning algorithms' explainability. Inductive Logic Programming (ILP), a
subfield of symbolic artificial intelligence, plays a promising role in
generating interpretable explanations because of its intuitive logic-driven
framework. ILP effectively leverages abductive reasoning to generate
explainable first-order clausal theories from examples and background
knowledge. However, several challenges in developing methods inspired by ILP
need to be addressed for their successful application in practice. For example,
existing ILP systems often have a vast solution space, and the induced
solutions are very sensitive to noises and disturbances. This survey paper
summarizes the recent advances in ILP and a discussion of statistical
relational learning and neural-symbolic algorithms, which offer synergistic
views to ILP. Following a critical review of the recent advances, we delineate
observed challenges and highlight potential avenues of further ILP-motivated
research toward developing self-explanatory artificial intelligence systems.
",0
"Improving Deep Neural Network Classification Confidence using
  Heatmap-based eXplainable AI","Erico Tjoa, Hong Jing Khok, Tushar Chouhan, Guan Cuntai",2021-12-30T12:46:23Z,Explainable AI,"  This paper quantifies the quality of heatmap-based eXplainable AI (XAI)
methods w.r.t image classification problem. Here, a heatmap is considered
desirable if it improves the probability of predicting the correct classes.
Different XAI heatmap-based methods are empirically shown to improve
classification confidence to different extents depending on the datasets, e.g.
Saliency works best on ImageNet and Deconvolution on Chest X-Ray Pneumonia
dataset. The novelty includes a new gap distribution that shows a stark
difference between correct and wrong predictions. Finally, the generative
augmentative explanation is introduced, a method to generate heatmaps capable
of improving predictive confidence to a high level.
",3
"Detection of extragalactic Ultra-Compact Dwarfs and Globular Clusters
  using Explainable AI techniques","Mohammad Mohammadi, Jarvin Mutatiina, Teymoor Saifollahi, Kerstin Bunte",2022-01-05T13:37:55Z,"RAG, Explainable AI","  Compact stellar systems such as Ultra-compact dwarfs (UCDs) and Globular
Clusters (GCs) around galaxies are known to be the tracers of the merger events
that have been forming these galaxies. Therefore, identifying such systems
allows to study galaxies mass assembly, formation and evolution. However, in
the lack of spectroscopic information detecting UCDs/GCs using imaging data is
very uncertain. Here, we aim to train a machine learning model to separate
these objects from the foreground stars and background galaxies using the
multi-wavelength imaging data of the Fornax galaxy cluster in 6 filters, namely
u, g, r, i, J and Ks. The classes of objects are highly imbalanced which is
problematic for many automatic classification techniques. Hence, we employ
Synthetic Minority Over-sampling to handle the imbalance of the training data.
Then, we compare two classifiers, namely Localized Generalized Matrix Learning
Vector Quantization (LGMLVQ) and Random Forest (RF). Both methods are able to
identify UCDs/GCs with a precision and a recall of >93 percent and provide
relevances that reflect the importance of each feature dimension %(colors and
angular sizes) for the classification. Both methods detect angular sizes as
important markers for this classification problem. While it is astronomical
expectation that color indices of u-i and i-Ks are the most important colors,
our analysis shows that colors such as g-r are more informative, potentially
because of higher signal-to-noise ratio. Besides the excellent performance the
LGMLVQ method allows further interpretability by providing the feature
importance for each individual class, class-wise representative samples and the
possibility for non-linear visualization of the data as demonstrated in this
contribution. We conclude that employing machine learning techniques to
identify UCDs/GCs can lead to promising results.
",0
"Explainable AI Integrated Feature Selection for Landslide Susceptibility
  Mapping using TreeSHAP","Muhammad Sakib Khan Inan, Istiakur Rahman",2022-01-10T09:17:21Z,Explainable AI,"  Landslides have been a regular occurrence and an alarming threat to human
life and property in the era of anthropogenic global warming. An early
prediction of landslide susceptibility using a data-driven approach is a demand
of time. In this study, we explored the eloquent features that best describe
landslide susceptibility with state-of-the-art machine learning methods. In our
study, we employed state-of-the-art machine learning algorithms including
XgBoost, LR, KNN, SVM, and Adaboost for landslide susceptibility prediction. To
find the best hyperparameters of each individual classifier for optimized
performance, we have incorporated the Grid Search method, with 10 Fold
Cross-Validation. In this context, the optimized version of XgBoost
outperformed all other classifiers with a Cross-validation Weighted F1 score of
94.62 %. Followed by this empirical evidence, we explored the XgBoost
classifier by incorporating TreeSHAP, a game-theory-based statistical algorithm
used to explain Machine Learning models, to identify eloquent features such as
SLOPE, ELEVATION, TWI that complement the performance of the XGBoost classifier
mostly and features such as LANDUSE, NDVI, SPI which has less effect on models
performance. According to the TreeSHAP explanation of features, we selected the
9 most significant landslide causal factors out of 15. Evidently, an optimized
version of XgBoost along with feature reduction by 40 % has outperformed all
other classifiers in terms of popular evaluation metrics with a
Cross-Validation Weighted F1 score of 95.01 % on the training and AUC score of
97 %
",0
"Explainable AI Framework for COVID-19 Prediction in Different Provinces
  of India","Mredulraj S. Pandianchery, Gopalakrishnan E. A, Sowmya V, Vinayakumar Ravi, Soman K. P",2022-01-12T16:26:14Z,Explainable AI,"  In 2020, covid-19 virus had reached more than 200 countries. Till December
20th 2021, 221 nations in the world had collectively reported 275M confirmed
cases of covid-19 & total death toll of 5.37M. Many countries which include
United States, India, Brazil, United Kingdom, Russia etc were badly affected by
covid-19 pandemic due to the large population. The total confirmed cases
reported in this country are 51.7M, 34.7M, 22.2M, 11.3M, 10.2M respectively
till December 20, 2021. This pandemic can be controlled with the help of
precautionary steps by government & civilians of the country. The early
prediction of covid-19 cases helps to track the transmission dynamics & alert
the government to take the necessary precautions. Recurrent Deep learning
algorithms is a data driven model which plays a key role to capture the
patterns present in time series data. In many literatures, the Recurrent Neural
Network (RNN) based model are proposed for the efficient prediction of COVID-19
cases for different provinces. The study in the literature doesnt involve the
interpretation of the model behavior & robustness. In this study, The LSTM
model is proposed for the efficient prediction of active cases in each
provinces of India. The active cases dataset for each province in India is
taken from John Hopkins publicly available dataset for the duration from 10th
June, 2020 to 4th August, 2021. The proposed LSTM model is trained on one state
i.e., Maharashtra and tested for rest of the provinces in India. The concept of
Explainable AI is involved in this study for the better interpretation &
understanding of the model behavior. The proposed model is used to forecast the
active cases in India from 16th December, 2021 to 5th March, 2022. It is
notated that there will be a emergence of third wave on January, 2022 in India.
",0
"From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic
  Review on Evaluating Explainable AI","Meike Nauta, Jan Trienes, Shreyasi Pathak, Elisa Nguyen, Michelle Peters, Yasmin Schmitt, Jörg Schlötterer, Maurice van Keulen, Christin Seifert",2022-01-20T13:23:20Z,Explainable AI,"  The rising popularity of explainable artificial intelligence (XAI) to
understand high-performing black boxes raised the question of how to evaluate
explanations of machine learning (ML) models. While interpretability and
explainability are often presented as a subjectively validated binary property,
we consider it a multi-faceted concept. We identify 12 conceptual properties,
such as Compactness and Correctness, that should be evaluated for
comprehensively assessing the quality of an explanation. Our so-called Co-12
properties serve as categorization scheme for systematically reviewing the
evaluation practices of more than 300 papers published in the last 7 years at
major AI and ML conferences that introduce an XAI method. We find that 1 in 3
papers evaluate exclusively with anecdotal evidence, and 1 in 5 papers evaluate
with users. This survey also contributes to the call for objective,
quantifiable evaluation methods by presenting an extensive overview of
quantitative XAI evaluation methods. Our systematic collection of evaluation
methods provides researchers and practitioners with concrete tools to
thoroughly validate, benchmark and compare new and existing XAI methods. The
Co-12 categorization scheme and our identified evaluation methods open up
opportunities to include quantitative metrics as optimization criteria during
model training in order to optimize for accuracy and interpretability
simultaneously.
",0
"Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural
  Network Explanations and Beyond","Anna Hedström, Leander Weber, Dilyara Bareeva, Daniel Krakowczyk, Franz Motzkus, Wojciech Samek, Sebastian Lapuschkin, Marina M. -C. Höhne",2022-02-14T16:45:36Z,Explainable AI,"  The evaluation of explanation methods is a research topic that has not yet
been explored deeply, however, since explainability is supposed to strengthen
trust in artificial intelligence, it is necessary to systematically review and
compare explanation methods in order to confirm their correctness. Until now,
no tool with focus on XAI evaluation exists that exhaustively and speedily
allows researchers to evaluate the performance of explanations of neural
network predictions. To increase transparency and reproducibility in the field,
we therefore built Quantus -- a comprehensive, evaluation toolkit in Python
that includes a growing, well-organised collection of evaluation metrics and
tutorials for evaluating explainable methods. The toolkit has been thoroughly
tested and is available under an open-source license on PyPi (or on
https://github.com/understandable-machine-intelligence-lab/Quantus/).
",0
"Guidelines and Evaluation of Clinical Explainable AI in Medical Image
  Analysis","Weina Jin, Xiaoxiao Li, Mostafa Fatehi, Ghassan Hamarneh",2022-02-16T19:09:42Z,Explainable AI,"  Explainable artificial intelligence (XAI) is essential for enabling clinical
users to get informed decision support from AI and comply with evidence-based
medical practice. Applying XAI in clinical settings requires proper evaluation
criteria to ensure the explanation technique is both technically sound and
clinically useful, but specific support is lacking to achieve this goal. To
bridge the research gap, we propose the Clinical XAI Guidelines that consist of
five criteria a clinical XAI needs to be optimized for. The guidelines
recommend choosing an explanation form based on Guideline 1 (G1)
Understandability and G2 Clinical relevance. For the chosen explanation form,
its specific XAI technique should be optimized for G3 Truthfulness, G4
Informative plausibility, and G5 Computational efficiency. Following the
guidelines, we conducted a systematic evaluation on a novel problem of
multi-modal medical image explanation with two clinical tasks, and proposed new
evaluation metrics accordingly. Sixteen commonly-used heatmap XAI techniques
were evaluated and found to be insufficient for clinical use due to their
failure in G3 and G4. Our evaluation demonstrated the use of Clinical XAI
Guidelines to support the design and evaluation of clinically viable XAI.
",64
"Evaluating Explainable AI on a Multi-Modal Medical Imaging Task: Can
  Existing Algorithms Fulfill Clinical Requirements?","Weina Jin, Xiaoxiao Li, Ghassan Hamarneh",2022-03-12T17:18:16Z,Explainable AI,"  Being able to explain the prediction to clinical end-users is a necessity to
leverage the power of artificial intelligence (AI) models for clinical decision
support. For medical images, a feature attribution map, or heatmap, is the most
common form of explanation that highlights important features for AI models'
prediction. However, it is unknown how well heatmaps perform on explaining
decisions on multi-modal medical images, where each image modality or channel
visualizes distinct clinical information of the same underlying biomedical
phenomenon. Understanding such modality-dependent features is essential for
clinical users' interpretation of AI decisions. To tackle this clinically
important but technically ignored problem, we propose the modality-specific
feature importance (MSFI) metric. It encodes clinical image and explanation
interpretation patterns of modality prioritization and modality-specific
feature localization. We conduct a clinical requirement-grounded, systematic
evaluation using computational methods and a clinician user study. Results show
that the examined 16 heatmap algorithms failed to fulfill clinical requirements
to correctly indicate AI model decision process or decision quality. The
evaluation and MSFI metric can guide the design and selection of XAI algorithms
to meet clinical requirements on multi-modal explanation.
",0
"RestoreX-AI: A Contrastive Approach towards Guiding Image Restoration
  via Explainable AI Systems","Aboli Marathe, Pushkar Jain, Rahee Walambe, Ketan Kotecha",2022-04-03T12:45:00Z,Explainable AI,"  Modern applications such as self-driving cars and drones rely heavily upon
robust object detection techniques. However, weather corruptions can hinder the
object detectability and pose a serious threat to their navigation and
reliability. Thus, there is a need for efficient denoising, deraining, and
restoration techniques. Generative adversarial networks and transformers have
been widely adopted for image restoration. However, the training of these
methods is often unstable and time-consuming. Furthermore, when used for object
detection (OD), the output images generated by these methods may provide
unsatisfactory results despite image clarity. In this work, we propose a
contrastive approach towards mitigating this problem, by evaluating images
generated by restoration models during and post training. This approach
leverages OD scores combined with attention maps for predicting the usefulness
of restored images for the OD task. We conduct experiments using two novel
use-cases of conditional GANs and two transformer methods that probe the
robustness of the proposed approach on multi-weather corruptions in the OD
task. Our approach achieves an averaged 178 percent increase in mAP between the
input and restored images under adverse weather conditions like dust tornadoes
and snowfall. We report unique cases where greater denoising does not improve
OD performance and conversely where noisy generated images demonstrate good
results. We conclude the need for explainability frameworks to bridge the gap
between human and machine perception, especially in the context of robust
object detection for autonomous vehicles.
",0
"Towards Reliable and Explainable AI Model for Solid Pulmonary Nodule
  Diagnosis","Chenglong Wang, Yun Liu, Fen Wang, Chengxiu Zhang, Yida Wang, Mei Yuan, Guang Yang",2022-04-08T08:21:00Z,Explainable AI,"  Lung cancer has the highest mortality rate of deadly cancers in the world.
Early detection is essential to treatment of lung cancer. However, detection
and accurate diagnosis of pulmonary nodules depend heavily on the experiences
of radiologists and can be a heavy workload for them. Computer-aided diagnosis
(CAD) systems have been developed to assist radiologists in nodule detection
and diagnosis, greatly easing the workload while increasing diagnosis accuracy.
Recent development of deep learning, greatly improved the performance of CAD
systems. However, lack of model reliability and interpretability remains a
major obstacle for its large-scale clinical application. In this work, we
proposed a multi-task explainable deep-learning model for pulmonary nodule
diagnosis. Our neural model can not only predict lesion malignancy but also
identify relevant manifestations. Further, the location of each manifestation
can also be visualized for visual interpretability. Our proposed neural model
achieved a test AUC of 0.992 on LIDC public dataset and a test AUC of 0.923 on
our in-house dataset. Moreover, our experimental results proved that by
incorporating manifestation identification tasks into the multi-task model, the
accuracy of the malignancy classification can also be improved. This multi-task
explainable model may provide a scheme for better interaction with the
radiologists in a clinical environment.
",0
"Revealing interactions between HVDC cross-area flows and frequency
  stability with explainable AI","Sebastian Pütz, Benjamin Schäfer, Dirk Witthaut, Johannes Kruse",2022-04-22T14:30:20Z,Explainable AI,"  The energy transition introduces more volatile energy sources into the power
grids. In this context, power transfer between different synchronous areas
through High Voltage Direct Current (HVDC) links becomes increasingly
important. Such links can balance volatile generation by enabling long-distance
transport or by leveraging their fast control behavior. Here, we investigate
the interaction of power imbalances - represented through the power grid
frequency - and power flows on HVDC links between synchronous areas in Europe.
We use explainable machine learning to identify key dependencies and
disentangle the interaction of critical features. Our results show that
market-based HVDC flows introduce deterministic frequency deviations, which
however can be mitigated through strict ramping limits. Moreover, varying HVDC
operation modes strongly affect the interaction with the grid. In particular,
we show that load-frequency control via HVDC links can both have control-like
or disturbance-like impacts on frequency stability.
",0
User Trust on an Explainable AI-based Medical Diagnosis Support System,"Yao Rong, Nora Castner, Efe Bozkir, Enkelejda Kasneci",2022-04-26T11:20:44Z,Explainable AI,"  Recent research has supported that system explainability improves user trust
and willingness to use medical AI for diagnostic support. In this paper, we use
chest disease diagnosis based on X-Ray images as a case study to investigate
user trust and reliance. Building off explainability, we propose a support
system where users (radiologists) can view causal explanations for final
decisions. After observing these causal explanations, users provided their
opinions of the model predictions and could correct explanations if they did
not agree. We measured user trust as the agreement between the model's and the
radiologist's diagnosis as well as the radiologists' feedback on the model
explanations. Additionally, they reported their trust in the system. We tested
our model on the CXR-Eye dataset and it achieved an overall accuracy of 74.1%.
However, the experts in our user study agreed with the model for only 46.4% of
the cases, indicating the necessity of improving the trust. The self-reported
trust score was 3.2 on a scale of 1.0 to 5.0, showing that the users tended to
trust the model but the trust still needs to be enhanced.
",6
"Scientific Explanation and Natural Language: A Unified
  Epistemological-Linguistic Perspective for Explainable AI","Marco Valentino, André Freitas",2022-05-03T22:31:42Z,Explainable AI,"  A fundamental research goal for Explainable AI (XAI) is to build models that
are capable of reasoning through the generation of natural language
explanations. However, the methodologies to design and evaluate
explanation-based inference models are still poorly informed by theoretical
accounts on the nature of explanation. As an attempt to provide an
epistemologically grounded characterisation for XAI, this paper focuses on the
scientific domain, aiming to bridge the gap between theory and practice on the
notion of a scientific explanation. Specifically, the paper combines a detailed
survey of the modern accounts of scientific explanation in Philosophy of
Science with a systematic analysis of corpora of natural language explanations,
clarifying the nature and function of explanatory arguments from both a
top-down (categorical) and a bottom-up (corpus-based) perspective. Through a
mixture of quantitative and qualitative methodologies, the presented study
allows deriving the following main conclusions: (1) Explanations cannot be
entirely characterised in terms of inductive or deductive arguments as their
main function is to perform unification; (2) An explanation must cite causes
and mechanisms that are responsible for the occurrence of the event to be
explained; (3) While natural language explanations possess an intrinsic
causal-mechanistic nature, they are not limited to causes and mechanisms, also
accounting for pragmatic elements such as definitions, properties and taxonomic
relations; (4) Patterns of unification naturally emerge in corpora of
explanations even if not intentionally modelled; (5) Unification is realised
through a process of abstraction, whose function is to provide the inference
substrate for subsuming the event to be explained under recurring patterns and
high-level regularities.
",1
"What Type of Explanation Do Rejected Job Applicants Want? Implications
  for Explainable AI","Matthew Olckers, Alicia Vidler, Toby Walsh",2022-05-18T05:00:44Z,Explainable AI,"  Rejected job applicants seldom receive explanations from employers.
Techniques from Explainable AI (XAI) could provide explanations at scale.
Although XAI researchers have developed many different types of explanations,
we know little about the type of explanations job applicants want. We use a
survey of recent job applicants to fill this gap. Our survey generates three
main insights. First, the current norm of, at most, generic feedback frustrates
applicants. Second, applicants feel the employer has an obligation to provide
an explanation. Third, job applicants want to know why they were unsuccessful
and how to improve.
",1
"Predicting and Understanding Human Action Decisions during Skillful
  Joint-Action via Machine Learning and Explainable-AI","Fabrizia Auletta, Rachel W. Kallen, Mario di Bernardo, Micheal J. Richardson",2022-06-06T16:54:43Z,Other,"  This study uses supervised machine learning (SML) and explainable artificial
intelligence (AI) to model, predict and understand human decision-making during
skillful joint-action. Long short-term memory networks were trained to predict
the target selection decisions of expert and novice actors completing a dyadic
herding task. Results revealed that the trained models were expertise specific
and could not only accurately predict the target selection decisions of expert
and novice herders but could do so at timescales that preceded an actor's
conscious intent. To understand what differentiated the target selection
decisions of expert and novice actors, we then employed the explainable-AI
technique, SHapley Additive exPlanation, to identify the importance of
informational features (variables) on model predictions. This analysis revealed
that experts were more influenced by information about the state of their
co-herders compared to novices. The utility of employing SML and explainable-AI
techniques for investigating human decision-making is discussed.
",0
"Explainable AI for Suicide Risk Assessment Using Eye Activities and Head
  Gestures","Siyu Liu, Catherine Lu, Sharifa Alghowinem, Lea Gotoh, Cynthia Breazeal, Hae Won Park",2022-06-10T13:42:25Z,Explainable AI,"  The prevalence of suicide has been on the rise since the 20th century,
causing severe emotional damage to individuals, families, and communities
alike. Despite the severity of this suicide epidemic, there is so far no
reliable and systematic way to assess suicide intent of a given individual.
Through efforts to automate and systematize diagnosis of mental illnesses over
the past few years, verbal and acoustic behaviors have received increasing
attention as biomarkers, but little has been done to study eyelids, gaze, and
head pose in evaluating suicide risk. This study explores statistical analysis,
feature selection, and machine learning classification as means of suicide risk
evaluation and nonverbal behavioral interpretation. Applying these methods to
the eye and head signals extracted from our unique dataset, this study finds
that high-risk suicidal individuals experience psycho-motor retardation and
symptoms of anxiety and depression, characterized by eye contact avoidance,
slower blinks and a downward eye gaze. By comparing results from different
methods of classification, we determined that these features are highly capable
of automatically classifying different levels of suicide risk consistently and
with high accuracy, above 98%. Our conclusion corroborates psychological
studies, and shows great potential of a systematic approach in suicide risk
evaluation that is adoptable by both healthcare providers and naive observers.
",0
"Connecting Algorithmic Research and Usage Contexts: A Perspective of
  Contextualized Evaluation for Explainable AI","Q. Vera Liao, Yunfeng Zhang, Ronny Luss, Finale Doshi-Velez, Amit Dhurandhar",2022-06-22T05:17:33Z,Explainable AI,"  Recent years have seen a surge of interest in the field of explainable AI
(XAI), with a plethora of algorithms proposed in the literature. However, a
lack of consensus on how to evaluate XAI hinders the advancement of the field.
We highlight that XAI is not a monolithic set of technologies -- researchers
and practitioners have begun to leverage XAI algorithms to build XAI systems
that serve different usage contexts, such as model debugging and
decision-support. Algorithmic research of XAI, however, often does not account
for these diverse downstream usage contexts, resulting in limited effectiveness
or even unintended consequences for actual users, as well as difficulties for
practitioners to make technical choices. We argue that one way to close the gap
is to develop evaluation methods that account for different user requirements
in these usage contexts. Towards this goal, we introduce a perspective of
contextualized XAI evaluation by considering the relative importance of XAI
evaluation criteria for prototypical usage contexts of XAI. To explore the
context dependency of XAI evaluation criteria, we conduct two survey studies,
one with XAI topical experts and another with crowd workers. Our results urge
for responsible AI research with usage-informed evaluation practices, and
provide a nuanced understanding of user requirements for XAI in different usage
contexts.
",0
"Explainable AI (XAI) in Biomedical Signal and Image Processing: Promises
  and Challenges","Guang Yang, Arvind Rao, Christine Fernandez-Maloigne, Vince Calhoun, Gloria Menegaz",2022-07-09T16:27:41Z,Explainable AI,"  Artificial intelligence has become pervasive across disciplines and fields,
and biomedical image and signal processing is no exception. The growing and
widespread interest on the topic has triggered a vast research activity that is
reflected in an exponential research effort. Through study of massive and
diverse biomedical data, machine and deep learning models have revolutionized
various tasks such as modeling, segmentation, registration, classification and
synthesis, outperforming traditional techniques. However, the difficulty in
translating the results into biologically/clinically interpretable information
is preventing their full exploitation in the field. Explainable AI (XAI)
attempts to fill this translational gap by providing means to make the models
interpretable and providing explanations. Different solutions have been
proposed so far and are gaining increasing interest from the community. This
paper aims at providing an overview on XAI in biomedical data processing and
points to an upcoming Special Issue on Deep Learning in Biomedical Image and
Signal Processing of the IEEE Signal Processing Magazine that is going to
appear in March 2022.
",0
"Alterfactual Explanations -- The Relevance of Irrelevance for Explaining
  AI Systems","Silvan Mertes, Christina Karle, Tobias Huber, Katharina Weitz, Ruben Schlagowski, Elisabeth André",2022-07-19T16:20:37Z,Other,"  Explanation mechanisms from the field of Counterfactual Thinking are a
widely-used paradigm for Explainable Artificial Intelligence (XAI), as they
follow a natural way of reasoning that humans are familiar with. However, all
common approaches from this field are based on communicating information about
features or characteristics that are especially important for an AI's decision.
We argue that in order to fully understand a decision, not only knowledge about
relevant features is needed, but that the awareness of irrelevant information
also highly contributes to the creation of a user's mental model of an AI
system. Therefore, we introduce a new way of explaining AI systems. Our
approach, which we call Alterfactual Explanations, is based on showing an
alternative reality where irrelevant features of an AI's input are altered. By
doing so, the user directly sees which characteristics of the input data can
change arbitrarily without influencing the AI's decision. We evaluate our
approach in an extensive user study, revealing that it is able to significantly
contribute to the participants' understanding of an AI. We show that
alterfactual explanations are suited to convey an understanding of different
aspects of the AI's reasoning than established counterfactual explanation
methods.
",0
"Explainable AI Algorithms for Vibration Data-based Fault Detection: Use
  Case-adadpted Methods and Critical Evaluation","Oliver Mey, Deniz Neufeld",2022-07-21T19:57:36Z,Explainable AI,"  Analyzing vibration data using deep neural network algorithms is an effective
way to detect damages in rotating machinery at an early stage. However, the
black-box approach of these methods often does not provide a satisfactory
solution because the cause of classifications is not comprehensible to humans.
Therefore, this work investigates the application of explainable AI (XAI)
algorithms to convolutional neural networks for vibration-based condition
monitoring. For this, various XAI algorithms are applied to classifications
based on the Fourier transform as well as the order analysis of the vibration
signal. The results are visualized as a function of the revolutions per minute
(RPM), in the shape of frequency-RPM maps and order-RPM maps. This allows to
assess the saliency given to features which depend on the rotation speed and
those with constant frequency. To compare the explanatory power of the XAI
methods, investigations are first carried out with a synthetic data set with
known class-specific characteristics. Then a real-world data set for
vibration-based imbalance classification on an electric motor, which runs at a
broad range of rotation speeds, is used. A special focus is put on the
consistency for variable periodicity of the data, which translates to a varying
rotation speed of a real-world machine. This work aims to show the different
strengths and weaknesses of the methods for this use case: GradCAM, LRP and
LIME with a new perturbation strategy.
",0
"A general-purpose method for applying Explainable AI for Anomaly
  Detection","John Sipple, Abdou Youssef",2022-07-23T17:56:01Z,Explainable AI,"  The need for explainable AI (XAI) is well established but relatively little
has been published outside of the supervised learning paradigm. This paper
focuses on a principled approach to applying explainability and
interpretability to the task of unsupervised anomaly detection. We argue that
explainability is principally an algorithmic task and interpretability is
principally a cognitive task, and draw on insights from the cognitive sciences
to propose a general-purpose method for practical diagnosis using explained
anomalies. We define Attribution Error, and demonstrate, using real-world
labeled datasets, that our method based on Integrated Gradients (IG) yields
significantly lower attribution errors than alternative methods.
",7
"Augmented cross-selling through explainable AI -- a case from energy
  retailing","Felix Haag, Konstantin Hopf, Pedro Menelau Vasconcelos, Thorsten Staake",2022-08-24T09:51:52Z,Explainable AI,"  The advance of Machine Learning (ML) has led to a strong interest in this
technology to support decision making. While complex ML models provide
predictions that are often more accurate than those of traditional tools, such
models often hide the reasoning behind the prediction from their users, which
can lead to lower adoption and lack of insight. Motivated by this tension,
research has put forth Explainable Artificial Intelligence (XAI) techniques
that uncover patterns discovered by ML. Despite the high hopes in both ML and
XAI, there is little empirical evidence of the benefits to traditional
businesses. To this end, we analyze data on 220,185 customers of an energy
retailer, predict cross-purchases with up to 86% correctness (AUC), and show
that the XAI method SHAP provides explanations that hold for actual buyers. We
further outline implications for research in information systems, XAI, and
relationship marketing.
",3
"Explainable AI for tailored electricity consumption feedback -- an
  experimental evaluation of visualizations","Jacqueline Wastensteiner, Tobias M. Weiss, Felix Haag, Konstantin Hopf",2022-08-24T10:03:54Z,Explainable AI,"  Machine learning (ML) methods can effectively analyse data, recognize
patterns in them, and make high-quality predictions. Good predictions usually
come along with ""black-box"" models that are unable to present the detected
patterns in a human-readable way. Technical developments recently led to
eXplainable Artificial Intelligence (XAI) techniques that aim to open such
black-boxes and enable humans to gain new insights from detected patterns. We
investigated the application of XAI in an area where specific insights can have
a significant effect on consumer behaviour, namely electricity use. Knowing
that specific feedback on individuals' electricity consumption triggers
resource conservation, we created five visualizations with ML and XAI methods
from electricity consumption time series for highly personalized feedback,
considering existing domain-specific design knowledge. Our experimental
evaluation with 152 participants showed that humans can assimilate the pattern
displayed by XAI visualizations, but such visualizations should follow known
visualization patterns to be well-understood by users.
",0
"Explainable AI for Android Malware Detection: Towards Understanding Why
  the Models Perform So Well?","Yue Liu, Chakkrit Tantithamthavorn, Li Li, Yepang Liu",2022-09-02T04:30:47Z,Explainable AI,"  Machine learning (ML)-based Android malware detection has been one of the
most popular research topics in the mobile security community. An increasing
number of research studies have demonstrated that machine learning is an
effective and promising approach for malware detection, and some works have
even claimed that their proposed models could achieve 99\% detection accuracy,
leaving little room for further improvement. However, numerous prior studies
have suggested that unrealistic experimental designs bring substantial biases,
resulting in over-optimistic performance in malware detection. Unlike previous
research that examined the detection performance of ML classifiers to locate
the causes, this study employs Explainable AI (XAI) approaches to explore what
ML-based models learned during the training process, inspecting and
interpreting why ML-based malware classifiers perform so well under unrealistic
experimental settings. We discover that temporal sample inconsistency in the
training dataset brings over-optimistic classification performance (up to 99\%
F1 score and accuracy). Importantly, our results indicate that ML models
classify malware based on temporal differences between malware and benign,
rather than the actual malicious behaviors. Our evaluation also confirms the
fact that unrealistic experimental designs lead to not only unrealistic
detection performance but also poor reliability, posing a significant obstacle
to real-world applications. These findings suggest that XAI approaches should
be used to help practitioners/researchers better understand how do AI/ML models
(i.e., malware detection) work -- not just focusing on accuracy improvement.
",22
"Visualization Of Class Activation Maps To Explain AI Classification Of
  Network Packet Captures","Igor Cherepanov, Alex Ulmer, Jonathan Geraldi Joewono, Jörn Kohlhammer",2022-09-05T16:34:43Z,Other,"  The classification of internet traffic has become increasingly important due
to the rapid growth of today's networks and applications. The number of
connections and the addition of new applications in our networks causes a vast
amount of log data and complicates the search for common patterns by experts.
Finding such patterns among specific classes of applications is necessary to
fulfill various requirements in network analytics. Deep learning methods
provide both feature extraction and classification from data in a single
system. However, these networks are very complex and are used as black-box
models, which weakens the experts' trust in the classifications. Moreover, by
using them as a black-box, new knowledge cannot be obtained from the model
predictions despite their excellent performance. Therefore, the explainability
of the classifications is crucial. Besides increasing trust, the explanation
can be used for model evaluation gaining new insights from the data and
improving the model. In this paper, we present a visual interactive tool that
combines the classification of network data with an explanation technique to
form an interface between experts, algorithms, and data.
",0
"Responsibility: An Example-based Explainable AI approach via Training
  Process Inspection","Faraz Khadivpour, Arghasree Banerjee, Matthew Guzdial",2022-09-07T19:30:01Z,Explainable AI,"  Explainable Artificial Intelligence (XAI) methods are intended to help human
users better understand the decision making of an AI agent. However, many
modern XAI approaches are unintuitive to end users, particularly those without
prior AI or ML knowledge. In this paper, we present a novel XAI approach we
call Responsibility that identifies the most responsible training example for a
particular decision. This example can then be shown as an explanation: ""this is
what I (the AI) learned that led me to do that"". We present experimental
results across a number of domains along with the results of an Amazon
Mechanical Turk user study, comparing responsibility and existing XAI methods
on an image classification task. Our results demonstrate that responsibility
can help improve accuracy for both human end users and secondary ML models.
",0
The Utility of Explainable AI in Ad Hoc Human-Machine Teaming,"Rohan Paleja, Muyleng Ghuy, Nadun Ranawaka Arachchige, Reed Jensen, Matthew Gombolay",2022-09-08T17:35:59Z,Explainable AI,"  Recent advances in machine learning have led to growing interest in
Explainable AI (xAI) to enable humans to gain insight into the decision-making
of machine learning models. Despite this recent interest, the utility of xAI
techniques has not yet been characterized in human-machine teaming.
Importantly, xAI offers the promise of enhancing team situational awareness
(SA) and shared mental model development, which are the key characteristics of
effective human-machine teams. Rapidly developing such mental models is
especially critical in ad hoc human-machine teaming, where agents do not have a
priori knowledge of others' decision-making strategies. In this paper, we
present two novel human-subject experiments quantifying the benefits of
deploying xAI techniques within a human-machine teaming scenario. First, we
show that xAI techniques can support SA ($p<0.05)$. Second, we examine how
different SA levels induced via a collaborative AI policy abstraction affect ad
hoc human-machine teaming performance. Importantly, we find that the benefits
of xAI are not universal, as there is a strong dependence on the composition of
the human-machine team. Novices benefit from xAI providing increased SA
($p<0.05$) but are susceptible to cognitive overhead ($p<0.05$). On the other
hand, expert performance degrades with the addition of xAI-based support
($p<0.05$), indicating that the cost of paying attention to the xAI outweighs
the benefits obtained from being provided additional information to enhance SA.
Our results demonstrate that researchers must deliberately design and deploy
the right xAI techniques in the right scenario by carefully considering
human-machine team composition and how the xAI method augments SA.
",0
"Explainable AI for clinical and remote health applications: a survey on
  tabular and time series data","Flavio Di Martino, Franca Delmastro",2022-09-14T10:01:29Z,Explainable AI,"  Nowadays Artificial Intelligence (AI) has become a fundamental component of
healthcare applications, both clinical and remote, but the best performing AI
systems are often too complex to be self-explaining. Explainable AI (XAI)
techniques are defined to unveil the reasoning behind the system's predictions
and decisions, and they become even more critical when dealing with sensitive
and personal health data. It is worth noting that XAI has not gathered the same
attention across different research areas and data types, especially in
healthcare. In particular, many clinical and remote health applications are
based on tabular and time series data, respectively, and XAI is not commonly
analysed on these data types, while computer vision and Natural Language
Processing (NLP) are the reference applications. To provide an overview of XAI
methods that are most suitable for tabular and time series data in the
healthcare domain, this paper provides a review of the literature in the last 5
years, illustrating the type of generated explanations and the efforts provided
to evaluate their relevance and quality. Specifically, we identify clinical
validation, consistency assessment, objective and standardised quality
evaluation, and human-centered quality assessment as key features to ensure
effective explanations for the end users. Finally, we highlight the main
research challenges in the field as well as the limitations of existing XAI
methods.
",0
"Analyzing Machine Learning Models for Credit Scoring with Explainable AI
  and Optimizing Investment Decisions",Swati Tyagi,2022-09-19T21:44:42Z,Explainable AI,"  This paper examines two different yet related questions related to
explainable AI (XAI) practices. Machine learning (ML) is increasingly important
in financial services, such as pre-approval, credit underwriting, investments,
and various front-end and back-end activities. Machine Learning can
automatically detect non-linearities and interactions in training data,
facilitating faster and more accurate credit decisions. However, machine
learning models are opaque and hard to explain, which are critical elements
needed for establishing a reliable technology. The study compares various
machine learning models, including single classifiers (logistic regression,
decision trees, LDA, QDA), heterogeneous ensembles (AdaBoost, Random Forest),
and sequential neural networks. The results indicate that ensemble classifiers
and neural networks outperform. In addition, two advanced post-hoc model
agnostic explainability techniques - LIME and SHAP are utilized to assess
ML-based credit scoring models using the open-access datasets offered by
US-based P2P Lending Platform, Lending Club. For this study, we are also using
machine learning algorithms to develop new investment models and explore
portfolio strategies that can maximize profitability while minimizing risk.
",4
"Why Should I Choose You? AutoXAI: A Framework for Selecting and Tuning
  eXplainable AI Solutions","Robin Cugny, Julien Aligon, Max Chevalier, Geoffrey Roman Jimenez, Olivier Teste",2022-10-06T10:12:58Z,Explainable AI,"  In recent years, a large number of XAI (eXplainable Artificial Intelligence)
solutions have been proposed to explain existing ML (Machine Learning) models
or to create interpretable ML models. Evaluation measures have recently been
proposed and it is now possible to compare these XAI solutions. However,
selecting the most relevant XAI solution among all this diversity is still a
tedious task, especially when meeting specific needs and constraints. In this
paper, we propose AutoXAI, a framework that recommends the best XAI solution
and its hyperparameters according to specific XAI evaluation metrics while
considering the user's context (dataset, ML model, XAI needs and constraints).
It adapts approaches from context-aware recommender systems and strategies of
optimization and evaluation from AutoML (Automated Machine Learning). We apply
AutoXAI to two use cases, and show that it recommends XAI solutions adapted to
the user's needs with the best hyperparameters matching the user's constraints.
",0
"Do We Need Explainable AI in Companies? Investigation of Challenges,
  Expectations, and Chances from Employees' Perspective","Katharina Weitz, Chi Tai Dang, Elisabeth André",2022-10-07T13:11:28Z,Explainable AI,"  Companies' adoption of artificial intelligence (AI) is increasingly becoming
an essential element of business success. However, using AI poses new
requirements for companies and their employees, including transparency and
comprehensibility of AI systems. The field of Explainable AI (XAI) aims to
address these issues. Yet, the current research primarily consists of
laboratory studies, and there is a need to improve the applicability of the
findings to real-world situations. Therefore, this project report paper
provides insights into employees' needs and attitudes towards (X)AI. For this,
we investigate employees' perspectives on (X)AI. Our findings suggest that AI
and XAI are well-known terms perceived as important for employees. This
recognition is a critical first step for XAI to potentially drive successful
usage of AI by providing comprehensible insights into AI technologies. In a
lessons-learned section, we discuss the open questions identified and suggest
future research directions to develop human-centered XAI designs for companies.
By providing insights into employees' needs and attitudes towards (X)AI, our
project report contributes to the development of XAI solutions that meet the
requirements of companies and their employees, ultimately driving the
successful adoption of AI technologies in the business context.
",0
"Gradient Backpropagation based Feature Attribution to Enable
  Explainable-AI on the Edge","Ashwin Bhat, Adou Sangbone Assoa, Arijit Raychowdhury",2022-10-19T22:58:59Z,Other,"  There has been a recent surge in the field of Explainable AI (XAI) which
tackles the problem of providing insights into the behavior of black-box
machine learning models. Within this field, \textit{feature attribution}
encompasses methods which assign relevance scores to input features and
visualize them as a heatmap. Designing flexible accelerators for multiple such
algorithms is challenging since the hardware mapping of these algorithms has
not been studied yet. In this work, we first analyze the dataflow of gradient
backpropagation based feature attribution algorithms to determine the resource
overhead required over inference. The gradient computation is optimized to
minimize the memory overhead. Second, we develop a High-Level Synthesis (HLS)
based configurable FPGA design that is targeted for edge devices and supports
three feature attribution algorithms. Tile based computation is employed to
maximally use on-chip resources while adhering to the resource constraints.
Representative CNNs are trained on CIFAR-10 dataset and implemented on multiple
Xilinx FPGAs using 16-bit fixed-point precision demonstrating flexibility of
our library. Finally, through efficient reuse of allocated hardware resources,
our design methodology demonstrates a pathway to repurpose inference
accelerators to support feature attribution with minimal overhead, thereby
enabling real-time XAI on the edge.
",0
"Towards Human-centered Explainable AI: A Survey of User Studies for
  Model Explanations","Yao Rong, Tobias Leemann, Thai-trang Nguyen, Lisa Fiedler, Peizhu Qian, Vaibhav Unhelkar, Tina Seidel, Gjergji Kasneci, Enkelejda Kasneci",2022-10-20T20:53:00Z,Explainable AI,"  Explainable AI (XAI) is widely viewed as a sine qua non for ever-expanding AI
research. A better understanding of the needs of XAI users, as well as
human-centered evaluations of explainable models are both a necessity and a
challenge. In this paper, we explore how HCI and AI researchers conduct user
studies in XAI applications based on a systematic literature review. After
identifying and thoroughly analyzing 97core papers with human-based XAI
evaluations over the past five years, we categorize them along the measured
characteristics of explanatory methods, namely trust, understanding, usability,
and human-AI collaboration performance. Our research shows that XAI is
spreading more rapidly in certain application domains, such as recommender
systems than in others, but that user evaluations are still rather sparse and
incorporate hardly any insights from cognitive or social sciences. Based on a
comprehensive discussion of best practices, i.e., common models, design
choices, and measures in user studies, we propose practical guidelines on
designing and conducting user studies for XAI researchers and practitioners.
Lastly, this survey also highlights several open research directions,
particularly linking psychological science and human-centered XAI.
",0
"Explainable AI over the Internet of Things (IoT): Overview,
  State-of-the-Art and Future Directions","Senthil Kumar Jagatheesaperumal, Quoc-Viet Pham, Rukhsana Ruby, Zhaohui Yang, Chunmei Xu, Zhaoyang Zhang",2022-11-02T11:08:52Z,Explainable AI,"  Explainable Artificial Intelligence (XAI) is transforming the field of
Artificial Intelligence (AI) by enhancing the trust of end-users in machines.
As the number of connected devices keeps on growing, the Internet of Things
(IoT) market needs to be trustworthy for the end-users. However, existing
literature still lacks a systematic and comprehensive survey work on the use of
XAI for IoT. To bridge this lacking, in this paper, we address the XAI
frameworks with a focus on their characteristics and support for IoT. We
illustrate the widely-used XAI services for IoT applications, such as security
enhancement, Internet of Medical Things (IoMT), Industrial IoT (IIoT), and
Internet of City Things (IoCT). We also suggest the implementation choice of
XAI models over IoT systems in these applications with appropriate examples and
summarize the key inferences for future works. Moreover, we present the
cutting-edge development in edge XAI structures and the support of
sixth-generation (6G) communication services for IoT applications, along with
key inferences. In a nutshell, this paper constitutes the first holistic
compilation on the development of XAI-based frameworks tailored for the demands
of future IoT use cases.
",0
"Explainable AI for Pre-Trained Code Models: What Do They Learn? When
  They Do Not Work?","Ahmad Haji Mohammadkhani, Chakkrit Tantithamthavorn, Hadi Hemmati",2022-11-23T10:07:20Z,Explainable AI,"  In recent years, there has been a wide interest in designing deep neural
network-based models that automate downstream software engineering tasks on
source code, such as code document generation, code search, and program repair.
Although the main objective of these studies is to improve the effectiveness of
the downstream task, many studies only attempt to employ the next best neural
network model, without a proper in-depth analysis of why a particular solution
works or does not, on particular tasks or scenarios. In this paper, using an
example eXplainable AI (XAI) method (attention mechanism), we study two recent
large language models (LLMs) for code (CodeBERT and GraphCodeBERT) on a set of
software engineering downstream tasks: code document generation (CDG), code
refinement (CR), and code translation (CT). Through quantitative and
qualitative studies, we identify what CodeBERT and GraphCodeBERT learn (put the
highest attention on, in terms of source code token types), on these tasks. We
also show some of the common patterns when the model does not work as expected
(performs poorly even on easy problems) and suggest recommendations that may
alleviate the observed challenges.
",0
"Understanding electricity prices beyond the merit order principle using
  explainable AI","Julius Trebbien, Leonardo Rydin Gorjão, Aaron Praktiknjo, Benjamin Schäfer, Dirk Witthaut",2022-12-09T12:18:17Z,Explainable AI,"  Electricity prices in liberalized markets are determined by the supply and
demand for electric power, which are in turn driven by various external
influences that vary strongly in time. In perfect competition, the merit order
principle describes that dispatchable power plants enter the market in the
order of their marginal costs to meet the residual load, i.e. the difference of
load and renewable generation. Many market models implement this principle to
predict electricity prices but typically require certain assumptions and
simplifications. In this article, we present an explainable machine learning
model for the prices on the German day-ahead market, which substantially
outperforms a benchmark model based on the merit order principle. Our model is
designed for the ex-post analysis of prices and thus builds on various external
features. Using Shapley Additive exPlanation (SHAP) values, we can disentangle
the role of the different features and quantify their importance from empiric
data. Load, wind and solar generation are most important, as expected, but wind
power appears to affect prices stronger than solar power does. Fuel prices also
rank highly and show nontrivial dependencies, including strong interactions
with other features revealed by a SHAP interaction analysis. Large generation
ramps are correlated with high prices, again with strong feature interactions,
due to the limited flexibility of nuclear and lignite plants. Our results
further contribute to model development by providing quantitative insights
directly from data.
",0
"Towards a Learner-Centered Explainable AI: Lessons from the learning
  sciences","Anna Kawakami, Luke Guerdan, Yang Cheng, Anita Sun, Alison Hu, Kate Glazko, Nikos Arechiga, Matthew Lee, Scott Carter, Haiyi Zhu, Kenneth Holstein",2022-12-11T19:53:22Z,Explainable AI,"  In this short paper, we argue for a refocusing of XAI around human learning
goals. Drawing upon approaches and theories from the learning sciences, we
propose a framework for the learner-centered design and evaluation of XAI
systems. We illustrate our framework through an ongoing case study in the
context of AI-augmented social work.
",0
A Survey of Explainable AI in Deep Visual Modeling: Methods and Metrics,Naveed Akhtar,2023-01-31T06:49:42Z,Explainable AI,"  Deep visual models have widespread applications in high-stake domains. Hence,
their black-box nature is currently attracting a large interest of the research
community. We present the first survey in Explainable AI that focuses on the
methods and metrics for interpreting deep visual models. Covering the landmark
contributions along the state-of-the-art, we not only provide a taxonomic
organization of the existing techniques, but also excavate a range of
evaluation metrics and collate them as measures of different properties of
model explanations. Along the insightful discussion on the current trends, we
also discuss the challenges and future avenues for this research direction.
",5
"Charting the Sociotechnical Gap in Explainable AI: A Framework to
  Address the Gap in XAI","Upol Ehsan, Koustuv Saha, Munmun De Choudhury, Mark O. Riedl",2023-02-01T23:21:45Z,Explainable AI,"  Explainable AI (XAI) systems are sociotechnical in nature; thus, they are
subject to the sociotechnical gap--divide between the technical affordances and
the social needs. However, charting this gap is challenging. In the context of
XAI, we argue that charting the gap improves our problem understanding, which
can reflexively provide actionable insights to improve explainability.
Utilizing two case studies in distinct domains, we empirically derive a
framework that facilitates systematic charting of the sociotechnical gap by
connecting AI guidelines in the context of XAI and elucidating how to use them
to address the gap. We apply the framework to a third case in a new domain,
showcasing its affordances. Finally, we discuss conceptual implications of the
framework, share practical considerations in its operationalization, and offer
guidance on transferring it to new contexts. By making conceptual and practical
contributions to understanding the sociotechnical gap in XAI, the framework
expands the XAI design space.
",0
"Example-Based Explainable AI and its Application for Remote Sensing
  Image Classification","Shin-nosuke Ishikawa, Masato Todo, Masato Taki, Yasunobu Uchiyama, Kazunari Matsunaga, Peihsuan Lin, Taiki Ogihara, Masao Yasui",2023-02-03T03:48:43Z,Explainable AI,"  We present a method of explainable artificial intelligence (XAI), ""What I
Know (WIK)"", to provide additional information to verify the reliability of a
deep learning model by showing an example of an instance in a training dataset
that is similar to the input data to be inferred and demonstrate it in a remote
sensing image classification task. One of the expected roles of XAI methods is
verifying whether inferences of a trained machine learning model are valid for
an application, and it is an important factor that what datasets are used for
training the model as well as the model architecture. Our data-centric approach
can help determine whether the training dataset is sufficient for each
inference by checking the selected example data. If the selected example looks
similar to the input data, we can confirm that the model was not trained on a
dataset with a feature distribution far from the feature of the input data.
With this method, the criteria for selecting an example are not merely data
similarity with the input data but also data similarity in the context of the
model task. Using a remote sensing image dataset from the Sentinel-2 satellite,
the concept was successfully demonstrated with reasonably selected examples.
This method can be applied to various machine-learning tasks, including
classification and regression.
",0
"VR-LENS: Super Learning-based Cybersickness Detection and Explainable
  AI-Guided Deployment in Virtual Reality","Ripan Kumar Kundu, Osama Yahia Elsaid, Prasad Calyam, Khaza Anuarul Hoque",2023-02-03T20:15:51Z,Other,"  A plethora of recent research has proposed several automated methods based on
machine learning (ML) and deep learning (DL) to detect cybersickness in Virtual
reality (VR). However, these detection methods are perceived as computationally
intensive and black-box methods. Thus, those techniques are neither trustworthy
nor practical for deploying on standalone VR head-mounted displays (HMDs). This
work presents an explainable artificial intelligence (XAI)-based framework
VR-LENS for developing cybersickness detection ML models, explaining them,
reducing their size, and deploying them in a Qualcomm Snapdragon 750G
processor-based Samsung A52 device. Specifically, we first develop a novel
super learning-based ensemble ML model for cybersickness detection. Next, we
employ a post-hoc explanation method, such as SHapley Additive exPlanations
(SHAP), Morris Sensitivity Analysis (MSA), Local Interpretable Model-Agnostic
Explanations (LIME), and Partial Dependence Plot (PDP) to explain the expected
results and identify the most dominant features. The super learner
cybersickness model is then retrained using the identified dominant features.
Our proposed method identified eye tracking, player position, and galvanic
skin/heart rate response as the most dominant features for the integrated
sensor, gameplay, and bio-physiological datasets. We also show that the
proposed XAI-guided feature reduction significantly reduces the model training
and inference time by 1.91X and 2.15X while maintaining baseline accuracy. For
instance, using the integrated sensor dataset, our reduced super learner model
outperforms the state-of-the-art works by classifying cybersickness into 4
classes (none, low, medium, and high) with an accuracy of 96% and regressing
(FMS 1-10) with a Root Mean Square Error (RMSE) of 0.03.
",0
"AUTOLYCUS: Exploiting Explainable AI (XAI) for Model Extraction Attacks
  against Interpretable Models","Abdullah Caglar Oksuz, Anisa Halimi, Erman Ayday",2023-02-04T13:23:39Z,Explainable AI,"  Explainable Artificial Intelligence (XAI) aims to uncover the decision-making
processes of AI models. However, the data used for such explanations can pose
security and privacy risks. Existing literature identifies attacks on machine
learning models, including membership inference, model inversion, and model
extraction attacks. These attacks target either the model or the training data,
depending on the settings and parties involved.
  XAI tools can increase the vulnerability of model extraction attacks, which
is a concern when model owners prefer black-box access, thereby keeping model
parameters and architecture private. To exploit this risk, we propose
AUTOLYCUS, a novel retraining (learning) based model extraction attack
framework against interpretable models under black-box settings. As XAI tools,
we exploit Local Interpretable Model-Agnostic Explanations (LIME) and Shapley
values (SHAP) to infer decision boundaries and create surrogate models that
replicate the functionality of the target model. LIME and SHAP are mainly
chosen for their realistic yet information-rich explanations, coupled with
their extensive adoption, simplicity, and usability.
  We evaluate AUTOLYCUS on six machine learning datasets, measuring the
accuracy and similarity of the surrogate model to the target model. The results
show that AUTOLYCUS is highly effective, requiring significantly fewer queries
compared to state-of-the-art attacks, while maintaining comparable accuracy and
similarity. We validate its performance and transferability on multiple
interpretable ML models, including decision trees, logistic regression, naive
bayes, and k-nearest neighbor. Additionally, we show the resilience of
AUTOLYCUS against proposed countermeasures.
",0
"LAVA: Granular Neuron-Level Explainable AI for Alzheimer's Disease
  Assessment from Fundus Images","Nooshin Yousefzadeh, Charlie Tran, Adolfo Ramirez-Zamora, Jinghua Chen, Ruogu Fang, My T. Thai",2023-02-06T18:43:10Z,Explainable AI,"  Alzheimer's Disease (AD) is a progressive neurodegenerative disease and the
leading cause of dementia. Early diagnosis is critical for patients to benefit
from potential intervention and treatment. The retina has been hypothesized as
a diagnostic site for AD detection owing to its anatomical connection with the
brain. Developed AI models for this purpose have yet to provide a rational
explanation about the decision and neither infer the stage of disease's
progression. Along this direction, we propose a novel model-agnostic
explainable-AI framework, called Granular Neuron-level Explainer (LAVA), an
interpretation prototype that probes into intermediate layers of the
Convolutional Neural Network (CNN) models to assess the AD continuum directly
from the retinal imaging without longitudinal or clinical evaluation. This
method is applied to validate the retinal vasculature as a biomarker and
diagnostic modality for Alzheimer's Disease (AD) evaluation. UK Biobank
cognitive tests and vascular morphological features suggest LAVA shows strong
promise and effectiveness in identifying AD stages across the progression
continuum.
",0
"Invisible Users: Uncovering End-Users' Requirements for Explainable AI
  via Explanation Forms and Goals","Weina Jin, Jianyu Fan, Diane Gromala, Philippe Pasquier, Ghassan Hamarneh",2023-02-10T19:35:57Z,Explainable AI,"  Non-technical end-users are silent and invisible users of the
state-of-the-art explainable artificial intelligence (XAI) technologies. Their
demands and requirements for AI explainability are not incorporated into the
design and evaluation of XAI techniques, which are developed to explain the
rationales of AI decisions to end-users and assist their critical decisions.
This makes XAI techniques ineffective or even harmful in high-stakes
applications, such as healthcare, criminal justice, finance, and autonomous
driving systems. To systematically understand end-users' requirements to
support the technical development of XAI, we conducted the EUCA user study with
32 layperson participants in four AI-assisted critical tasks. The study
identified comprehensive user requirements for feature-, example-, and
rule-based XAI techniques (manifested by the end-user-friendly explanation
forms) and XAI evaluation objectives (manifested by the explanation goals),
which were shown to be helpful to directly inspire the proposal of new XAI
algorithms and evaluation metrics. The EUCA study findings, the identified
explanation forms and goals for technical specification, and the EUCA study
dataset support the design and evaluation of end-user-centered XAI techniques
for accessible, safe, and accountable AI.
",5
"The Meta-Evaluation Problem in Explainable AI: Identifying Reliable
  Estimators with MetaQuantus","Anna Hedström, Philine Bommer, Kristoffer K. Wickstrøm, Wojciech Samek, Sebastian Lapuschkin, Marina M. -C. Höhne",2023-02-14T18:59:02Z,Explainable AI,"  One of the unsolved challenges in the field of Explainable AI (XAI) is
determining how to most reliably estimate the quality of an explanation method
in the absence of ground truth explanation labels. Resolving this issue is of
utmost importance as the evaluation outcomes generated by competing evaluation
methods (or ''quality estimators''), which aim at measuring the same property
of an explanation method, frequently present conflicting rankings. Such
disagreements can be challenging for practitioners to interpret, thereby
complicating their ability to select the best-performing explanation method. We
address this problem through a meta-evaluation of different quality estimators
in XAI, which we define as ''the process of evaluating the evaluation method''.
Our novel framework, MetaQuantus, analyses two complementary performance
characteristics of a quality estimator: its resilience to noise and reactivity
to randomness, thus circumventing the need for ground truth labels. We
demonstrate the effectiveness of our framework through a series of experiments,
targeting various open questions in XAI such as the selection and
hyperparameter optimisation of quality estimators. Our work is released under
an open-source license (https://github.com/annahedstroem/MetaQuantus) to serve
as a development tool for XAI- and Machine Learning (ML) practitioners to
verify and benchmark newly constructed quality estimators in a given
explainability context. With this work, we provide the community with clear and
theoretically-grounded guidance for identifying reliable evaluation methods,
thus facilitating reproducibility in the field.
",0
"Using Explainable AI to Cross-Validate Socio-economic Disparities Among
  Covid-19 Patient Mortality","Li Shi, Redoan Rahman, Esther Melamed, Jacek Gwizdka, Justin F. Rousseau, Ying Ding",2023-02-16T22:09:05Z,Explainable AI,"  This paper applies eXplainable Artificial Intelligence (XAI) methods to
investigate the socioeconomic disparities in COVID patient mortality. An
Extreme Gradient Boosting (XGBoost) prediction model is built based on a
de-identified Austin area hospital dataset to predict the mortality of COVID-19
patients. We apply two XAI methods, Shapley Additive exPlanations (SHAP) and
Locally Interpretable Model Agnostic Explanations (LIME), to compare the global
and local interpretation of feature importance. This paper demonstrates the
advantages of using XAI which shows the feature importance and decisive
capability. Furthermore, we use the XAI methods to cross-validate their
interpretations for individual patients. The XAI models reveal that Medicare
financial class, older age, and gender have high impact on the mortality
prediction. We find that LIME local interpretation does not show significant
differences in feature importance comparing to SHAP, which suggests pattern
confirmation. This paper demonstrates the importance of XAI methods in
cross-validation of feature attributions.
",0
"Bridging the Transparency Gap: What Can Explainable AI Learn From the AI
  Act?","Balint Gyevnar, Nick Ferguson, Burkhard Schafer",2023-02-21T16:06:48Z,Explainable AI,"  The European Union has proposed the Artificial Intelligence Act which
introduces detailed requirements of transparency for AI systems. Many of these
requirements can be addressed by the field of explainable AI (XAI), however,
there is a fundamental difference between XAI and the Act regarding what
transparency is. The Act views transparency as a means that supports wider
values, such as accountability, human rights, and sustainable innovation. In
contrast, XAI views transparency narrowly as an end in itself, focusing on
explaining complex algorithmic properties without considering the
socio-technical context. We call this difference the ``transparency gap''.
Failing to address the transparency gap, XAI risks leaving a range of
transparency issues unaddressed. To begin to bridge this gap, we overview and
clarify the terminology of how XAI and European regulation -- the Act and the
related General Data Protection Regulation (GDPR) -- view basic definitions of
transparency. By comparing the disparate views of XAI and regulation, we arrive
at four axes where practical work could bridge the transparency gap: defining
the scope of transparency, clarifying the legal status of XAI, addressing
issues with conformity assessment, and building explainability for datasets.
",7
"Non-Uniform Interpolation in Integrated Gradients for Low-Latency
  Explainable-AI","Ashwin Bhat, Arijit Raychowdhury",2023-02-22T03:03:28Z,Other,"  There has been a surge in Explainable-AI (XAI) methods that provide insights
into the workings of Deep Neural Network (DNN) models. Integrated Gradients
(IG) is a popular XAI algorithm that attributes relevance scores to input
features commensurate with their contribution to the model's output. However,
it requires multiple forward \& backward passes through the model. Thus,
compared to a single forward-pass inference, there is a significant
computational overhead to generate the explanation which hinders real-time XAI.
This work addresses the aforementioned issue by accelerating IG with a
hardware-aware algorithm optimization. We propose a novel non-uniform
interpolation scheme to compute the IG attribution scores which replaces the
baseline uniform interpolation. Our algorithm significantly reduces the total
interpolation steps required without adversely impacting convergence.
Experiments on the ImageNet dataset using a pre-trained InceptionV3 model
demonstrate \textit{2.6-3.6}$\times$ performance speedup on GPU systems for
iso-convergence. This includes the minimal \textit{0.2-3.2}\% latency overhead
introduced by the pre-processing stage of computing the non-uniform
interpolation step-sizes.
",0
"Explainable AI does not provide the explanations end-users are asking
  for","Savio Rozario, George Čevora",2023-01-25T10:34:38Z,Explainable AI,"  Explainable Artificial Intelligence (XAI) techniques are frequently required
by users in many AI systems with the goal of understanding complex models,
their associated predictions, and gaining trust. While suitable for some
specific tasks during development, their adoption by organisations to enhance
trust in machine learning systems has unintended consequences. In this paper we
discuss XAI's limitations in deployment and conclude that transparency
alongside with rigorous validation are better suited to gaining trust in AI
systems.
",0
"NoiseCAM: Explainable AI for the Boundary Between Noise and Adversarial
  Attacks","Wenkai Tan, Justus Renkhoff, Alvaro Velasquez, Ziyu Wang, Lusi Li, Jian Wang, Shuteng Niu, Fan Yang, Yongxin Liu, Houbing Song",2023-03-09T22:07:41Z,Explainable AI,"  Deep Learning (DL) and Deep Neural Networks (DNNs) are widely used in various
domains. However, adversarial attacks can easily mislead a neural network and
lead to wrong decisions. Defense mechanisms are highly preferred in
safety-critical applications. In this paper, firstly, we use the gradient class
activation map (GradCAM) to analyze the behavior deviation of the VGG-16
network when its inputs are mixed with adversarial perturbation or Gaussian
noise. In particular, our method can locate vulnerable layers that are
sensitive to adversarial perturbation and Gaussian noise. We also show that the
behavior deviation of vulnerable layers can be used to detect adversarial
examples. Secondly, we propose a novel NoiseCAM algorithm that integrates
information from globally and pixel-level weighted class activation maps. Our
algorithm is susceptible to adversarial perturbations and will not respond to
Gaussian random noise mixed in the inputs. Third, we compare detecting
adversarial examples using both behavior deviation and NoiseCAM, and we show
that NoiseCAM outperforms behavior deviation modeling in its overall
performance. Our work could provide a useful tool to defend against certain
adversarial attacks on deep neural networks.
",0
"Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias
  Correction of Deep Models","Frederik Pahde, Maximilian Dreyer, Wojciech Samek, Sebastian Lapuschkin",2023-03-22T15:23:09Z,Explainable AI,"  State-of-the-art machine learning models often learn spurious correlations
embedded in the training data. This poses risks when deploying these models for
high-stake decision-making, such as in medical applications like skin cancer
detection. To tackle this problem, we propose Reveal to Revise (R2R), a
framework entailing the entire eXplainable Artificial Intelligence (XAI) life
cycle, enabling practitioners to iteratively identify, mitigate, and
(re-)evaluate spurious model behavior with a minimal amount of human
interaction. In the first step (1), R2R reveals model weaknesses by finding
outliers in attributions or through inspection of latent concepts learned by
the model. Secondly (2), the responsible artifacts are detected and spatially
localized in the input data, which is then leveraged to (3) revise the model
behavior. Concretely, we apply the methods of RRR, CDEP and ClArC for model
correction, and (4) (re-)evaluate the model's performance and remaining
sensitivity towards the artifact. Using two medical benchmark datasets for
Melanoma detection and bone age estimation, we apply our R2R framework to VGG,
ResNet and EfficientNet architectures and thereby reveal and correct real
dataset-intrinsic artifacts, as well as synthetic variants in a controlled
setting. Completing the XAI life cycle, we demonstrate multiple R2R iterations
to mitigate different biases. Code is available on
https://github.com/maxdreyer/Reveal2Revise.
",0
"Dermatologist-like explainable AI enhances trust and confidence in
  diagnosing melanoma","Tirtha Chanda, Katja Hauser, Sarah Hobelsberger, Tabea-Clara Bucher, Carina Nogueira Garcia, Christoph Wies, Harald Kittler, Philipp Tschandl, Cristian Navarrete-Dechent, Sebastian Podlipnik, Emmanouil Chousakos, Iva Crnaric, Jovana Majstorovic, Linda Alhajwan, Tanya Foreman, Sandra Peternel, Sergei Sarap, İrem Özdemir, Raymond L. Barnhill, Mar Llamas Velasco, Gabriela Poch, Sören Korsing, Wiebke Sondermann, Frank Friedrich Gellrich, Markus V. Heppt, Michael Erdmann, Sebastian Haferkamp, Konstantin Drexler, Matthias Goebeler, Bastian Schilling, Jochen S. Utikal, Kamran Ghoreschi, Stefan Fröhling, Eva Krieghoff-Henning, Titus J. Brinker",2023-03-17T17:25:55Z,Explainable AI,"  Although artificial intelligence (AI) systems have been shown to improve the
accuracy of initial melanoma diagnosis, the lack of transparency in how these
systems identify melanoma poses severe obstacles to user acceptance.
Explainable artificial intelligence (XAI) methods can help to increase
transparency, but most XAI methods are unable to produce precisely located
domain-specific explanations, making the explanations difficult to interpret.
Moreover, the impact of XAI methods on dermatologists has not yet been
evaluated. Extending on two existing classifiers, we developed an XAI system
that produces text and region based explanations that are easily interpretable
by dermatologists alongside its differential diagnoses of melanomas and nevi.
To evaluate this system, we conducted a three-part reader study to assess its
impact on clinicians' diagnostic accuracy, confidence, and trust in the
XAI-support. We showed that our XAI's explanations were highly aligned with
clinicians' explanations and that both the clinicians' trust in the support
system and their confidence in their diagnoses were significantly increased
when using our XAI compared to using a conventional AI system. The clinicians'
diagnostic accuracy was numerically, albeit not significantly, increased. This
work demonstrates that clinicians are willing to adopt such an XAI system,
motivating their future use in the clinic.
",28
"Shapley-based Explainable AI for Clustering Applications in Fault
  Diagnosis and Prognosis","Joseph Cohen, Xun Huan, Jun Ni",2023-03-25T23:13:11Z,Explainable AI,"  Data-driven artificial intelligence models require explainability in
intelligent manufacturing to streamline adoption and trust in modern industry.
However, recently developed explainable artificial intelligence (XAI)
techniques that estimate feature contributions on a model-agnostic level such
as SHapley Additive exPlanations (SHAP) have not yet been evaluated for
semi-supervised fault diagnosis and prognosis problems characterized by class
imbalance and weakly labeled datasets. This paper explores the potential of
utilizing Shapley values for a new clustering framework compatible with
semi-supervised learning problems, loosening the strict supervision requirement
of current XAI techniques. This broad methodology is validated on two case
studies: a heatmap image dataset obtained from a semiconductor manufacturing
process featuring class imbalance, and a benchmark dataset utilized in the 2021
Prognostics and Health Management (PHM) Data Challenge. Semi-supervised
clustering based on Shapley values significantly improves upon clustering
quality compared to the fully unsupervised case, deriving information-dense and
meaningful clusters that relate to underlying fault diagnosis model
predictions. These clusters can also be characterized by high-precision
decision rules in terms of original feature values, as demonstrated in the
second case study. The rules, limited to 1-2 terms utilizing original feature
scales, describe 12 out of the 16 derived equipment failure clusters with
precision exceeding 0.85, showcasing the promising utility of the explainable
clustering framework for intelligent manufacturing applications.
",0
"Explanation Strategies for Image Classification in Humans vs. Current
  Explainable AI","Ruoxi Qi, Yueyuan Zheng, Yi Yang, Caleb Chen Cao, Janet H. Hsiao",2023-04-10T08:28:13Z,Explainable AI,"  Explainable AI (XAI) methods provide explanations of AI models, but our
understanding of how they compare with human explanations remains limited. In
image classification, we found that humans adopted more explorative attention
strategies for explanation than the classification task itself. Two
representative explanation strategies were identified through clustering: One
involved focused visual scanning on foreground objects with more conceptual
explanations diagnostic for inferring class labels, whereas the other involved
explorative scanning with more visual explanations rated higher for
effectiveness. Interestingly, XAI saliency-map explanations had the highest
similarity to the explorative attention strategy in humans, and explanations
highlighting discriminative features from invoking observable causality through
perturbation had higher similarity to human strategies than those highlighting
internal features associated with higher class score. Thus, humans differ in
information and strategy use for explanations, and XAI methods that highlight
features informing observable causality match better with human explanations,
potentially more accessible to users.
",4
"Impact Of Explainable AI On Cognitive Load: Insights From An Empirical
  Study",Lukas-Valentin Herm,2023-04-18T09:52:09Z,Explainable AI,"  While the emerging research field of explainable artificial intelligence
(XAI) claims to address the lack of explainability in high-performance machine
learning models, in practice, XAI targets developers rather than actual
end-users. Unsurprisingly, end-users are often unwilling to use XAI-based
decision support systems. Similarly, there is limited interdisciplinary
research on end-users' behavior during XAI explanations usage, rendering it
unknown how explanations may impact cognitive load and further affect end-user
performance. Therefore, we conducted an empirical study with 271 prospective
physicians, measuring their cognitive load, task performance, and task time for
distinct implementation-independent XAI explanation types using a COVID-19 use
case. We found that these explanation types strongly influence end-users'
cognitive load, task performance, and task time. Further, we contextualized a
mental efficiency metric, ranking local XAI explanation types best, to provide
recommendations for future applications and implications for sociotechnical XAI
research.
",12
"Securing Autonomous Air Traffic Management: Blockchain Networks Driven
  by Explainable AI","Louise Axon, Dimitrios Panagiotakopoulos, Samuel Ayo, Carolina Sanchez-Hernandez, Yan Zong, Simon Brown, Lei Zhang, Michael Goldsmith, Sadie Creese, Weisi Guo",2023-04-27T11:11:43Z,Explainable AI,"  Air Traffic Management data systems today are inefficient and not scalable to
enable future unmanned systems. Current data is fragmented, siloed, and not
easily accessible. There is data conflict, misuse, and eroding levels of trust
in provenance and accuracy. With increased autonomy in aviation, Artificially
Intelligent (AI) enabled unmanned traffic management (UTM) will be more reliant
on secure data from diverse stakeholders. There is an urgent need to develop a
secure network that has trustworthy data chains and works with the requirements
generated by UTM. Here, we review existing research in 3 key interconnected
areas: (1) blockchain development for secure data transfer between competing
aviation stakeholders, (2) self-learning networking architectures that
distribute consensus to achieve secure air traffic control, (3) explainable AI
to build trust with human stakeholders and backpropagate requirements for
blockchain and network optimisation. When connected together, this new digital
ecosystem blueprint is tailored for safety critical UTM sectors. We motivate
the readers with a case study, where a federated learning UTM uses real air
traffic and weather data is secured and explained to human operators. This
emerging area still requires significant research and development by the
community to ensure it can enable future autonomous air mobility.
",0
"An Efficient Ensemble Explainable AI (XAI) Approach for Morphed Face
  Detection","Rudresh Dwivedi, Ritesh Kumar, Deepak Chopra, Pranay Kothari, Manjot Singh",2023-04-23T13:43:06Z,Explainable AI,"  The extensive utilization of biometric authentication systems have emanated
attackers / imposters to forge user identity based on morphed images. In this
attack, a synthetic image is produced and merged with genuine. Next, the
resultant image is user for authentication. Numerous deep neural convolutional
architectures have been proposed in literature for face Morphing Attack
Detection (MADs) to prevent such attacks and lessen the risks associated with
them. Although, deep learning models achieved optimal results in terms of
performance, it is difficult to understand and analyse these networks since
they are black box/opaque in nature. As a consequence, incorrect judgments may
be made. There is, however, a dearth of literature that explains
decision-making methods of black box deep learning models for biometric
Presentation Attack Detection (PADs) or MADs that can aid the biometric
community to have trust in deep learning-based biometric systems for
identification and authentication in various security applications such as
border control, criminal database establishment etc. In this work, we present a
novel visual explanation approach named Ensemble XAI integrating Saliency maps,
Class Activation Maps (CAM) and Gradient-CAM (Grad-CAM) to provide a more
comprehensive visual explanation for a deep learning prognostic model
(EfficientNet-B1) that we have employed to predict whether the input presented
to a biometric authentication system is morphed or genuine. The
experimentations have been performed on three publicly available datasets
namely Face Research Lab London Set, Wide Multi-Channel Presentation Attack
(WMCA), and Makeup Induced Face Spoofing (MIFS). The experimental evaluations
affirms that the resultant visual explanations highlight more fine-grained
details of image features/areas focused by EfficientNet-B1 to reach decisions
along with appropriate reasoning.
",0
"Widespread Increases in Future Wildfire Risk to Global Forest Carbon
  Offset Projects Revealed by Explainable AI","Tristan Ballard, Matthew Cooper, Chris Lowrie, Gopal Erinjippurath",2023-05-03T19:36:11Z,Explainable AI,"  Carbon offset programs are critical in the fight against climate change. One
emerging threat to the long-term stability and viability of forest carbon
offset projects is wildfires, which can release large amounts of carbon and
limit the efficacy of associated offsetting credits. However, analysis of
wildfire risk to forest carbon projects is challenging because existing models
for forecasting long-term fire risk are limited in predictive accuracy.
Therefore, we propose an explainable artificial intelligence (XAI) model
trained on 7 million global satellite wildfire observations. Validation results
suggest substantial potential for high resolution, enhanced accuracy
projections of global wildfire risk, and the model outperforms the U.S.
National Center for Atmospheric Research's leading fire model. Applied to a
collection of 190 global forest carbon projects, we find that fire exposure is
projected to increase 55% [37-76%] by 2080 under a mid-range scenario
(SSP2-4.5). Our results indicate the large wildfire carbon project damages seen
in the past decade are likely to become more frequent as forests become hotter
and drier. In response, we hope the model can support wildfire managers,
policymakers, and carbon market analysts to preemptively quantify and mitigate
long-term permanence risks to forest carbon projects.
",0
"Unjustified Sample Sizes and Generalizations in Explainable AI Research:
  Principles for More Inclusive User Studies","Uwe Peters, Mary Carman",2023-05-08T15:02:21Z,Explainable AI,"  Many ethical frameworks require artificial intelligence (AI) systems to be
explainable. Explainable AI (XAI) models are frequently tested for their
adequacy in user studies. Since different people may have different explanatory
needs, it is important that participant samples in user studies are large
enough to represent the target population to enable generalizations. However,
it is unclear to what extent XAI researchers reflect on and justify their
sample sizes or avoid broad generalizations across people. We analyzed XAI user
studies (n = 220) published between 2012 and 2022. Most studies did not offer
rationales for their sample sizes. Moreover, most papers generalized their
conclusions beyond their target population, and there was no evidence that
broader conclusions in quantitative studies were correlated with larger
samples. These methodological problems can impede evaluations of whether XAI
systems implement the explainability called for in ethical frameworks. We
outline principles for more inclusive XAI user studies.
",2
"Breast Cancer Segmentation using Attention-based Convolutional Network
  and Explainable AI","Jai Vardhan, Taraka Satya Krishna Teja Malisetti",2023-05-22T20:49:20Z,Explainable AI,"  Breast cancer (BC) remains a significant health threat, with no long-term
cure currently available. Early detection is crucial, yet mammography
interpretation is hindered by high false positives and negatives. With BC
incidence projected to surpass lung cancer, improving early detection methods
is vital. Thermography, using high-resolution infrared cameras, offers promise,
especially when combined with artificial intelligence (AI). This work presents
an attention-based convolutional neural network for segmentation, providing
increased speed and precision in BC detection and classification. The system
enhances images and performs cancer segmentation with explainable AI. We
propose a transformer-attention-based convolutional architecture (UNet) for
fault identification and employ Gradient-weighted Class Activation Mapping
(Grad-CAM) to analyze areas of bias and weakness in the UNet architecture with
IRT images. The superiority of our proposed framework is confirmed when
compared with existing deep learning frameworks.
",0
"Can We Trust Explainable AI Methods on ASR? An Evaluation on Phoneme
  Recognition","Xiaoliang Wu, Peter Bell, Ajitha Rajan",2023-05-29T11:04:13Z,Explainable AI,"  Explainable AI (XAI) techniques have been widely used to help explain and
understand the output of deep learning models in fields such as image
classification and Natural Language Processing. Interest in using XAI
techniques to explain deep learning-based automatic speech recognition (ASR) is
emerging. but there is not enough evidence on whether these explanations can be
trusted. To address this, we adapt a state-of-the-art XAI technique from the
image classification domain, Local Interpretable Model-Agnostic Explanations
(LIME), to a model trained for a TIMIT-based phoneme recognition task. This
simple task provides a controlled setting for evaluation while also providing
expert annotated ground truth to assess the quality of explanations. We find a
variant of LIME based on time partitioned audio segments, that we propose in
this paper, produces the most reliable explanations, containing the ground
truth 96% of the time in its top three audio segments.
",0
"Explainable AI for Malnutrition Risk Prediction from m-Health and
  Clinical Data","Flavio Di Martino, Franca Delmastro, Cristina Dolciotti",2023-05-31T08:07:35Z,Explainable AI,"  Malnutrition is a serious and prevalent health problem in the older
population, and especially in hospitalised or institutionalised subjects.
Accurate and early risk detection is essential for malnutrition management and
prevention. M-health services empowered with Artificial Intelligence (AI) may
lead to important improvements in terms of a more automatic, objective, and
continuous monitoring and assessment. Moreover, the latest Explainable AI (XAI)
methodologies may make AI decisions interpretable and trustworthy for end
users. This paper presents a novel AI framework for early and explainable
malnutrition risk detection based on heterogeneous m-health data. We performed
an extensive model evaluation including both subject-independent and
personalised predictions, and the obtained results indicate Random Forest (RF)
and Gradient Boosting as the best performing classifiers, especially when
incorporating body composition assessment data. We also investigated several
benchmark XAI methods to extract global model explanations. Model-specific
explanation consistency assessment indicates that each selected model
privileges similar subsets of the most relevant predictors, with the highest
agreement shown between SHapley Additive ExPlanations (SHAP) and feature
permutation method. Furthermore, we performed a preliminary clinical validation
to verify that the learned feature-output trends are compliant with the current
evidence-based assessment.
",0
"A Survey of Explainable AI and Proposal for a Discipline of Explanation
  Engineering","Clive Gomes, Lalitha Natraj, Shijun Liu, Anushka Datta",2023-05-20T11:55:27Z,Explainable AI,"  In this survey paper, we deep dive into the field of Explainable Artificial
Intelligence (XAI). After introducing the scope of this paper, we start by
discussing what an ""explanation"" really is. We then move on to discuss some of
the existing approaches to XAI and build a taxonomy of the most popular
methods. Next, we also look at a few applications of these and other XAI
techniques in four primary domains: finance, autonomous driving, healthcare and
manufacturing. We end by introducing a promising discipline, ""Explanation
Engineering,"" which includes a systematic approach for designing explainability
into AI systems.
",0
"Mitigating Molecular Aggregation in Drug Discovery with Predictive
  Insights from Explainable AI","Hunter Sturm, Jonas Teufel, Kaitlin A. Isfeld, Pascal Friederich, Rebecca L. Davis",2023-06-03T22:30:45Z,Explainable AI,"  As the importance of high-throughput screening (HTS) continues to grow due to
its value in early stage drug discovery and data generation for training
machine learning models, there is a growing need for robust methods for
pre-screening compounds to identify and prevent false-positive hits. Small,
colloidally aggregating molecules are one of the primary sources of
false-positive hits in high-throughput screens, making them an ideal candidate
to target for removal from libraries using predictive pre-screening tools.
However, a lack of understanding of the causes of molecular aggregation
introduces difficulty in the development of predictive tools for detecting
aggregating molecules. Herein, we present an examination of the molecular
features differentiating datasets of aggregating and non-aggregating molecules,
as well as a machine learning approach to predicting molecular aggregation. Our
method uses explainable graph neural networks and counterfactuals to reliably
predict and explain aggregation, giving additional insights and design rules
for future screening. The integration of this method in HTS approaches will
help combat false positives, providing better lead molecules more rapidly and
thus accelerating drug discovery cycles.
",0
"Automating Style Analysis and Visualization With Explainable AI -- Case
  Studies on Brand Recognition","Yu-hsuan Chen, Levent Burak Kara, Jonathan Cagan",2023-06-05T16:38:11Z,Explainable AI,"  Incorporating style-related objectives into shape design has been centrally
important to maximize product appeal. However, stylistic features such as
aesthetics and semantic attributes are hard to codify even for experts. As
such, algorithmic style capture and reuse have not fully benefited from
automated data-driven methodologies due to the challenging nature of design
describability. This paper proposes an AI-driven method to fully automate the
discovery of brand-related features. Our approach introduces BIGNet, a two-tier
Brand Identification Graph Neural Network (GNN) to classify and analyze scalar
vector graphics (SVG). First, to tackle the scarcity of vectorized product
images, this research proposes two data acquisition workflows: parametric
modeling from small curve-based datasets, and vectorization from large
pixel-based datasets. Secondly, this study constructs a novel hierarchical GNN
architecture to learn from both SVG's curve-level and chunk-level parameters.
In the first case study, BIGNet not only classifies phone brands but also
captures brand-related features across multiple scales, such as the location of
the lens, the height-width ratio, and the screen-frame gap, as confirmed by AI
evaluation. In the second study, this paper showcases the generalizability of
BIGNet learning from a vectorized car image dataset and validates the
consistency and robustness of its predictions given four scenarios. The results
match the difference commonly observed in luxury vs. economy brands in the
automobile market. Finally, this paper also visualizes the activation maps
generated from a convolutional neural network and shows BIGNet's advantage of
being a more human-friendly, explainable, and explicit style-capturing agent.
Code and dataset can be found on Github:
  1. Phone case study: github.com/parksandrecfan/bignet-phone 2. Car case
study: github.com/parksandrecfan/bignet-car
",0
"Utterance Classification with Logical Neural Network: Explainable AI for
  Mental Disorder Diagnosis","Yeldar Toleubay, Don Joven Agravante, Daiki Kimura, Baihan Lin, Djallel Bouneffouf, Michiaki Tatsubori",2023-06-06T17:58:44Z,Explainable AI,"  In response to the global challenge of mental health problems, we proposes a
Logical Neural Network (LNN) based Neuro-Symbolic AI method for the diagnosis
of mental disorders. Due to the lack of effective therapy coverage for mental
disorders, there is a need for an AI solution that can assist therapists with
the diagnosis. However, current Neural Network models lack explainability and
may not be trusted by therapists. The LNN is a Recurrent Neural Network
architecture that combines the learning capabilities of neural networks with
the reasoning capabilities of classical logic-based AI. The proposed system
uses input predicates from clinical interviews to output a mental disorder
class, and different predicate pruning techniques are used to achieve
scalability and higher scores. In addition, we provide an insight extraction
method to aid therapists with their diagnosis. The proposed system addresses
the lack of explainability of current Neural Network models and provides a more
trustworthy solution for mental disorder diagnosis.
",0
Towards Fair and Explainable AI using a Human-Centered AI Approach,Bhavya Ghai,2023-06-12T21:08:55Z,Explainable AI,"  The rise of machine learning (ML) is accompanied by several high-profile
cases that have stressed the need for fairness, accountability, explainability
and trust in ML systems. The existing literature has largely focused on fully
automated ML approaches that try to optimize for some performance metric.
However, human-centric measures like fairness, trust, explainability, etc. are
subjective in nature, context-dependent, and might not correlate with
conventional performance metrics. To deal with these challenges, we explore a
human-centered AI approach that empowers people by providing more transparency
and human control.
  In this dissertation, we present 5 research projects that aim to enhance
explainability and fairness in classification systems and word embeddings. The
first project explores the utility/downsides of introducing local model
explanations as interfaces for machine teachers (crowd workers). Our study
found that adding explanations supports trust calibration for the resulting ML
model and enables rich forms of teaching feedback. The second project presents
D-BIAS, a causality-based human-in-the-loop visual tool for identifying and
mitigating social biases in tabular datasets. Apart from fairness, we found
that our tool also enhances trust and accountability. The third project
presents WordBias, a visual interactive tool that helps audit pre-trained
static word embeddings for biases against groups, such as females, or
subgroups, such as Black Muslim females. The fourth project presents DramatVis
Personae, a visual analytics tool that helps identify social biases in creative
writing. Finally, the last project presents an empirical study aimed at
understanding the cumulative impact of multiple fairness-enhancing
interventions at different stages of the ML pipeline on fairness, utility and
different population groups. We conclude by discussing some of the future
directions.
",0
"Statutory Professions in AI governance and their consequences for
  explainable AI","Labhaoise NiFhaolain, Andrew Hines, Vivek Nallur",2023-06-15T08:51:28Z,Explainable AI,"  Intentional and accidental harms arising from the use of AI have impacted the
health, safety and rights of individuals. While regulatory frameworks are being
developed, there remains a lack of consensus on methods necessary to deliver
safe AI. The potential for explainable AI (XAI) to contribute to the
effectiveness of the regulation of AI is being increasingly examined.
Regulation must include methods to ensure compliance on an ongoing basis,
though there is an absence of practical proposals on how to achieve this. For
XAI to be successfully incorporated into a regulatory system, the individuals
who are engaged in interpreting/explaining the model to stakeholders should be
sufficiently qualified for the role. Statutory professionals are prevalent in
domains in which harm can be done to the health, safety and rights of
individuals. The most obvious examples are doctors, engineers and lawyers.
Those professionals are required to exercise skill and judgement and to defend
their decision making process in the event of harm occurring. We propose that a
statutory profession framework be introduced as a necessary part of the AI
regulatory framework for compliance and monitoring purposes. We will refer to
this new statutory professional as an AI Architect (AIA). This AIA would be
responsible to ensure the risk of harm is minimised and accountable in the
event that harms occur. The AIA would also be relied on to provide appropriate
interpretations/explanations of XAI models to stakeholders. Further, in order
to satisfy themselves that the models have been developed in a satisfactory
manner, the AIA would require models to have appropriate transparency.
Therefore it is likely that the introduction of an AIA system would lead to an
increase in the use of XAI to enable AIA to discharge their professional
obligations.
",0
"Manipulation Risks in Explainable AI: The Implications of the
  Disagreement Problem","Sofie Goethals, David Martens, Theodoros Evgeniou",2023-06-24T07:21:28Z,Explainable AI,"  Artificial Intelligence (AI) systems are increasingly used in high-stakes
domains of our life, increasing the need to explain these decisions and to make
sure that they are aligned with how we want the decision to be made. The field
of Explainable AI (XAI) has emerged in response. However, it faces a
significant challenge known as the disagreement problem, where multiple
explanations are possible for the same AI decision or prediction. While the
existence of the disagreement problem is acknowledged, the potential
implications associated with this problem have not yet been widely studied.
First, we provide an overview of the different strategies explanation providers
could deploy to adapt the returned explanation to their benefit. We make a
distinction between strategies that attack the machine learning model or
underlying data to influence the explanations, and strategies that leverage the
explanation phase directly. Next, we analyse several objectives and concrete
scenarios the providers could have to engage in this behavior, and the
potential dangerous consequences this manipulative behavior could have on
society. We emphasize that it is crucial to investigate this issue now, before
these methods are widely implemented, and propose some mitigation strategies.
",0
"A Survey on Explainable AI for 6G O-RAN: Architecture, Use Cases,
  Challenges and Research Directions","Bouziane Brik, Hatim Chergui, Lanfranco Zanzi, Francesco Devoti, Adlen Ksentini, Muhammad Shuaib Siddiqui, Xavier Costa-Pérez, Christos Verikoukis",2023-07-01T12:10:18Z,Explainable AI,"  The recent O-RAN specifications promote the evolution of RAN architecture by
function disaggregation, adoption of open interfaces, and instantiation of a
hierarchical closed-loop control architecture managed by RAN Intelligent
Controllers (RICs) entities. This paves the road to novel data-driven network
management approaches based on programmable logic. Aided by Artificial
Intelligence (AI) and Machine Learning (ML), novel solutions targeting
traditionally unsolved RAN management issues can be devised. Nevertheless, the
adoption of such smart and autonomous systems is limited by the current
inability of human operators to understand the decision process of such AI/ML
solutions, affecting their trust in such novel tools. eXplainable AI (XAI) aims
at solving this issue, enabling human users to better understand and
effectively manage the emerging generation of artificially intelligent schemes,
reducing the human-to-machine barrier. In this survey, we provide a summary of
the XAI methods and metrics before studying their deployment over the O-RAN
Alliance RAN architecture along with its main building blocks. We then present
various use-cases and discuss the automation of XAI pipelines for O-RAN as well
as the underlying security aspects. We also review some projects/standards that
tackle this area. Finally, we identify different challenges and research
directions that may arise from the heavy adoption of AI/ML decision entities in
this context, focusing on how XAI can help to interpret, understand, and
improve trust in O-RAN operational networks.
",0
"Concept backpropagation: An Explainable AI approach for visualising
  learned concepts in neural network models","Patrik Hammersborg, Inga Strümke",2023-07-24T08:21:13Z,Explainable AI,"  Neural network models are widely used in a variety of domains, often as
black-box solutions, since they are not directly interpretable for humans. The
field of explainable artificial intelligence aims at developing explanation
methods to address this challenge, and several approaches have been developed
over the recent years, including methods for investigating what type of
knowledge these models internalise during the training process. Among these,
the method of concept detection, investigates which \emph{concepts} neural
network models learn to represent in order to complete their tasks. In this
work, we present an extension to the method of concept detection, named
\emph{concept backpropagation}, which provides a way of analysing how the
information representing a given concept is internalised in a given neural
network model. In this approach, the model input is perturbed in a manner
guided by a trained concept probe for the described model, such that the
concept of interest is maximised. This allows for the visualisation of the
detected concept directly in the input space of the model, which in turn makes
it possible to see what information the model depends on for representing the
described concept. We present results for this method applied to a various set
of input modalities, and discuss how our proposed method can be used to
visualise what information trained concept probes use, and the degree as to
which the representation of the probed concept is entangled within the neural
network model itself.
",0
"OBESEYE: Interpretable Diet Recommender for Obesity Management using
  Machine Learning and Explainable AI","Mrinmoy Roy, Srabonti Das, Anica Tasnim Protity",2023-08-05T06:02:28Z,Explainable AI,"  Obesity, the leading cause of many non-communicable diseases, occurs mainly
for eating more than our body requirements and lack of proper activity. So,
being healthy requires heathy diet plans, especially for patients with
comorbidities. But it is difficult to figure out the exact quantity of each
nutrient because nutrients requirement varies based on physical and disease
conditions. In our study we proposed a novel machine learning based system to
predict the amount of nutrients one individual requires for being healthy. We
applied different machine learning algorithms: linear regression, support
vector machine (SVM), decision tree, random forest, XGBoost, LightGBM on fluid
and 3 other major micronutrients: carbohydrate, protein, fat consumption
prediction. We achieved high accuracy with low root mean square error (RMSE) by
using linear regression in fluid prediction, random forest in carbohydrate
prediction and LightGBM in protein and fat prediction. We believe our diet
recommender system, OBESEYE, is the only of its kind which recommends diet with
the consideration of comorbidities and physical conditions and promote
encouragement to get rid of obesity.
",0
"Feature Importance versus Feature Influence and What It Signifies for
  Explainable AI",Kary Främling,2023-08-07T13:46:18Z,Explainable AI,"  When used in the context of decision theory, feature importance expresses how
much changing the value of a feature can change the model outcome (or the
utility of the outcome), compared to other features. Feature importance should
not be confused with the feature influence used by most state-of-the-art
post-hoc Explainable AI methods. Contrary to feature importance, feature
influence is measured against a reference level or baseline. The Contextual
Importance and Utility (CIU) method provides a unified definition of global and
local feature importance that is applicable also for post-hoc explanations,
where the value utility concept provides instance-level assessment of how
favorable or not a feature value is for the outcome. The paper shows how CIU
can be applied to both global and local explainability, assesses the fidelity
and stability of different methods, and shows how explanations that use
contextual importance and contextual utility can provide more expressive and
flexible explanations than when using influence only.
",0
"FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of
  Explainable AI Methods","Robin Hesse, Simone Schaub-Meyer, Stefan Roth",2023-08-11T17:29:02Z,Explainable AI,"  The field of explainable artificial intelligence (XAI) aims to uncover the
inner workings of complex deep neural models. While being crucial for
safety-critical domains, XAI inherently lacks ground-truth explanations, making
its automatic evaluation an unsolved problem. We address this challenge by
proposing a novel synthetic vision dataset, named FunnyBirds, and accompanying
automatic evaluation protocols. Our dataset allows performing semantically
meaningful image interventions, e.g., removing individual object parts, which
has three important implications. First, it enables analyzing explanations on a
part level, which is closer to human comprehension than existing methods that
evaluate on a pixel level. Second, by comparing the model output for inputs
with removed parts, we can estimate ground-truth part importances that should
be reflected in the explanations. Third, by mapping individual explanations
into a common space of part importances, we can analyze a variety of different
explanation types in a single common framework. Using our tools, we report
results for 24 different combinations of neural models and XAI methods,
demonstrating the strengths and weaknesses of the assessed methods in a fully
automatic and systematic manner.
",0
"Can we Agree? On the Rashōmon Effect and the Reliability of Post-Hoc
  Explainable AI","Clement Poiret, Antoine Grigis, Justin Thomas, Marion Noulhiane",2023-08-14T16:32:24Z,Explainable AI,"  The Rash\=omon effect poses challenges for deriving reliable knowledge from
machine learning models. This study examined the influence of sample size on
explanations from models in a Rash\=omon set using SHAP. Experiments on 5
public datasets showed that explanations gradually converged as the sample size
increased. Explanations from <128 samples exhibited high variability, limiting
reliable knowledge extraction. However, agreement between models improved with
more data, allowing for consensus. Bagging ensembles often had higher
agreement. The results provide guidance on sufficient data to trust
explanations. Variability at low samples suggests that conclusions may be
unreliable without validation. Further work is needed with more model types,
data domains, and explanation methods. Testing convergence in neural networks
and with model-specific explanation methods would be impactful. The approaches
explored here point towards principled techniques for eliciting knowledge from
ambiguous models.
",0
"Explainable AI for clinical risk prediction: a survey of concepts,
  methods, and modalities","Munib Mesinovic, Peter Watkinson, Tingting Zhu",2023-08-16T14:51:51Z,Explainable AI,"  Recent advancements in AI applications to healthcare have shown incredible
promise in surpassing human performance in diagnosis and disease prognosis.
With the increasing complexity of AI models, however, concerns regarding their
opacity, potential biases, and the need for interpretability. To ensure trust
and reliability in AI systems, especially in clinical risk prediction models,
explainability becomes crucial. Explainability is usually referred to as an AI
system's ability to provide a robust interpretation of its decision-making
logic or the decisions themselves to human stakeholders. In clinical risk
prediction, other aspects of explainability like fairness, bias, trust, and
transparency also represent important concepts beyond just interpretability. In
this review, we address the relationship between these concepts as they are
often used together or interchangeably. This review also discusses recent
progress in developing explainable models for clinical risk prediction,
highlighting the importance of quantitative and clinical evaluation and
validation across multiple common modalities in clinical practice. It
emphasizes the need for external validation and the combination of diverse
interpretability methods to enhance trust and fairness. Adopting rigorous
testing, such as using synthetic datasets with known generative factors, can
further improve the reliability of explainability methods. Open access and
code-sharing resources are essential for transparency and reproducibility,
enabling the growth and trustworthiness of explainable research. While
challenges exist, an end-to-end approach to explainability in clinical risk
prediction, incorporating stakeholders from clinicians to developers, is
essential for success.
",0
"Leveraging Explainable AI to Analyze Researchers' Aspect-Based Sentiment
  about ChatGPT","Shilpa Lakhanpal, Ajay Gupta, Rajeev Agrawal",2023-08-16T07:44:06Z,"RAG, Explainable AI","  The groundbreaking invention of ChatGPT has triggered enormous discussion
among users across all fields and domains. Among celebration around its various
advantages, questions have been raised with regards to its correctness and
ethics of its use. Efforts are already underway towards capturing user
sentiments around it. But it begs the question as to how the research community
is analyzing ChatGPT with regards to various aspects of its usage. It is this
sentiment of the researchers that we analyze in our work. Since Aspect-Based
Sentiment Analysis has usually only been applied on a few datasets, it gives
limited success and that too only on short text data. We propose a methodology
that uses Explainable AI to facilitate such analysis on research data. Our
technique presents valuable insights into extending the state of the art of
Aspect-Based Sentiment Analysis on newer datasets, where such analysis is not
hampered by the length of the text data.
",0
"Interpretable Medical Imagery Diagnosis with Self-Attentive
  Transformers: A Review of Explainable AI for Health Care",Tin Lai,2023-09-01T05:01:52Z,Explainable AI,"  Recent advancements in artificial intelligence (AI) have facilitated its
widespread adoption in primary medical services, addressing the demand-supply
imbalance in healthcare. Vision Transformers (ViT) have emerged as
state-of-the-art computer vision models, benefiting from self-attention
modules. However, compared to traditional machine-learning approaches,
deep-learning models are complex and are often treated as a ""black box"" that
can cause uncertainty regarding how they operate. Explainable Artificial
Intelligence (XAI) refers to methods that explain and interpret machine
learning models' inner workings and how they come to decisions, which is
especially important in the medical domain to guide the healthcare
decision-making process. This review summarises recent ViT advancements and
interpretative approaches to understanding the decision-making process of ViT,
enabling transparency in medical diagnosis applications.
",0
"QXAI: Explainable AI Framework for Quantitative Analysis in Patient
  Monitoring Systems","Thanveer Shaik, Xiaohui Tao, Haoran Xie, Lin Li, Juan D. Velasquez, Niall Higgins",2023-09-19T03:50:30Z,Explainable AI,"  Artificial Intelligence techniques can be used to classify a patient's
physical activities and predict vital signs for remote patient monitoring.
Regression analysis based on non-linear models like deep learning models has
limited explainability due to its black-box nature. This can require
decision-makers to make blind leaps of faith based on non-linear model results,
especially in healthcare applications. In non-invasive monitoring, patient data
from tracking sensors and their predisposing clinical attributes act as input
features for predicting future vital signs. Explaining the contributions of
various features to the overall output of the monitoring application is
critical for a clinician's decision-making. In this study, an Explainable AI
for Quantitative analysis (QXAI) framework is proposed with post-hoc model
explainability and intrinsic explainability for regression and classification
tasks in a supervised learning approach. This was achieved by utilizing the
Shapley values concept and incorporating attention mechanisms in deep learning
models. We adopted the artificial neural networks (ANN) and attention-based
Bidirectional LSTM (BiLSTM) models for the prediction of heart rate and
classification of physical activities based on sensor data. The deep learning
models achieved state-of-the-art results in both prediction and classification
tasks. Global explanation and local explanation were conducted on input data to
understand the feature contribution of various patient data. The proposed QXAI
framework was evaluated using PPG-DaLiA data to predict heart rate and mobile
health (MHEALTH) data to classify physical activities based on sensor data.
Monte Carlo approximation was applied to the framework to overcome the time
complexity and high computation power requirements required for Shapley value
calculations.
",0
"From Classification to Segmentation with Explainable AI: A Study on
  Crack Detection and Growth Monitoring","Florent Forest, Hugo Porta, Devis Tuia, Olga Fink",2023-09-20T12:50:52Z,Explainable AI,"  Monitoring surface cracks in infrastructure is crucial for structural health
monitoring. Automatic visual inspection offers an effective solution,
especially in hard-to-reach areas. Machine learning approaches have proven
their effectiveness but typically require large annotated datasets for
supervised training. Once a crack is detected, monitoring its severity often
demands precise segmentation of the damage. However, pixel-level annotation of
images for segmentation is labor-intensive. To mitigate this cost, one can
leverage explainable artificial intelligence (XAI) to derive segmentations from
the explanations of a classifier, requiring only weak image-level supervision.
This paper proposes applying this methodology to segment and monitor surface
cracks. We evaluate the performance of various XAI methods and examine how this
approach facilitates severity quantification and growth monitoring. Results
reveal that while the resulting segmentation masks may exhibit lower quality
than those produced by supervised methods, they remain meaningful and enable
severity monitoring, thus reducing substantial labeling costs.
",0
"HuntGPT: Integrating Machine Learning-Based Anomaly Detection and
  Explainable AI with Large Language Models (LLMs)","Tarek Ali, Panos Kostakos",2023-09-27T20:58:13Z,Explainable AI,"  Machine learning (ML) is crucial in network anomaly detection for proactive
threat hunting, reducing detection and response times significantly. However,
challenges in model training, maintenance, and frequent false positives impact
its acceptance and reliability. Explainable AI (XAI) attempts to mitigate these
issues, allowing cybersecurity teams to assess AI-generated alerts with
confidence, but has seen limited acceptance from incident responders. Large
Language Models (LLMs) present a solution through discerning patterns in
extensive information and adapting to different functional requirements. We
present HuntGPT, a specialized intrusion detection dashboard applying a Random
Forest classifier using the KDD99 dataset, integrating XAI frameworks like SHAP
and Lime for user-friendly and intuitive model interaction, and combined with a
GPT-3.5 Turbo, it delivers threats in an understandable format. The paper
delves into the system's architecture, components, and technical accuracy,
assessed through Certified Information Security Manager (CISM) Practice Exams,
evaluating response quality across six metrics. The results demonstrate that
conversational agents, supported by LLM and integrated with XAI, provide
robust, explainable, and actionable AI solutions in intrusion detection,
enhancing user understanding and interactive experience.
",0
"Proceedings of The first international workshop on eXplainable AI for
  the Arts (XAIxArts)","Nick Bryan-Kinns, Corey Ford, Alan Chamberlain, Steven David Benford, Helen Kennedy, Zijin Li, Wu Qiong, Gus G. Xia, Jeba Rezwana",2023-10-10T08:53:54Z,Explainable AI,"  This first international workshop on explainable AI for the Arts (XAIxArts)
brought together a community of researchers in HCI, Interaction Design, AI,
explainable AI (XAI), and digital arts to explore the role of XAI for the Arts.
  Workshop held at the 15th ACM Conference on Creativity and Cognition (C&C
2023).
",0
"Does Your Model Think Like an Engineer? Explainable AI for Bearing Fault
  Detection with Deep Learning","Thomas Decker, Michael Lebacher, Volker Tresp",2023-10-19T17:58:11Z,Explainable AI,"  Deep Learning has already been successfully applied to analyze industrial
sensor data in a variety of relevant use cases. However, the opaque nature of
many well-performing methods poses a major obstacle for real-world deployment.
Explainable AI (XAI) and especially feature attribution techniques promise to
enable insights about how such models form their decision. But the plain
application of such methods often fails to provide truly informative and
problem-specific insights to domain experts. In this work, we focus on the
specific task of detecting faults in rolling element bearings from vibration
signals. We propose a novel and domain-specific feature attribution framework
that allows us to evaluate how well the underlying logic of a model corresponds
with expert reasoning. Utilizing the framework we are able to validate the
trustworthiness and to successfully anticipate the generalization ability of
different well-performing deep learning models. Our methodology demonstrates
how signal processing tools can effectively be used to enhance Explainable AI
techniques and acts as a template for similar problems.
",0
"Predicting recovery following stroke: deep learning, multimodal data and
  feature selection using explainable AI","Adam White, Margarita Saranti, Artur d'Avila Garcez, Thomas M. H. Hope, Cathy J. Price, Howard Bowman",2023-10-29T22:31:20Z,Explainable AI,"  Machine learning offers great potential for automated prediction of
post-stroke symptoms and their response to rehabilitation. Major challenges for
this endeavour include the very high dimensionality of neuroimaging data, the
relatively small size of the datasets available for learning, and how to
effectively combine neuroimaging and tabular data (e.g. demographic information
and clinical characteristics). This paper evaluates several solutions based on
two strategies. The first is to use 2D images that summarise MRI scans. The
second is to select key features that improve classification accuracy.
Additionally, we introduce the novel approach of training a convolutional
neural network (CNN) on images that combine regions-of-interest extracted from
MRIs, with symbolic representations of tabular data. We evaluate a series of
CNN architectures (both 2D and a 3D) that are trained on different
representations of MRI and tabular data, to predict whether a composite measure
of post-stroke spoken picture description ability is in the aphasic or
non-aphasic range. MRI and tabular data were acquired from 758 English speaking
stroke survivors who participated in the PLORAS study. The classification
accuracy for a baseline logistic regression was 0.678 for lesion size alone,
rising to 0.757 and 0.813 when initial symptom severity and recovery time were
successively added. The highest classification accuracy 0.854 was observed when
8 regions-of-interest was extracted from each MRI scan and combined with lesion
size, initial severity and recovery time in a 2D Residual Neural Network.Our
findings demonstrate how imaging and tabular data can be combined for high
post-stroke classification accuracy, even when the dataset is small in machine
learning terms. We conclude by proposing how the current models could be
improved to achieve even higher levels of accuracy using images from hospital
scanners.
",0
"CrossEAI: Using Explainable AI to generate better bounding boxes for
  Chest X-ray images",Jinze Zhao,2023-10-29T17:48:39Z,Explainable AI,"  Explainability is critical for deep learning applications in healthcare which
are mandated to provide interpretations to both patients and doctors according
to legal regulations and responsibilities. Explainable AI methods, such as
feature importance using integrated gradients, model approximation using LIME,
or neuron activation and layer conductance to provide interpretations for
certain health risk predictions. In medical imaging diagnosis, disease
classification usually achieves high accuracy, but generated bounding boxes
have much lower Intersection over Union (IoU). Different methods with
self-supervised or semi-supervised learning strategies have been proposed, but
few improvements have been identified for bounding box generation. Previous
work shows that bounding boxes generated by these methods are usually larger
than ground truth and contain major non-disease area. This paper utilizes the
advantages of post-hoc AI explainable methods to generate bounding boxes for
chest x-ray image diagnosis. In this work, we propose CrossEAI which combines
heatmap and gradient map to generate more targeted bounding boxes. By using
weighted average of Guided Backpropagation and Grad-CAM++, we are able to
generate bounding boxes which are closer to the ground truth. We evaluate our
model on a chest x-ray dataset. The performance has significant improvement
over the state of the art model with the same setting, with $9\%$ improvement
in average of all diseases over all IoU. Moreover, as a model that does not use
any ground truth bounding box information for training, we achieve same
performance in general as the model that uses $80\%$ of the ground truth
bounding box information for training
",1
"Explainable AI for Earth Observation: Current Methods, Open Challenges,
  and Opportunities","Gulsen Taskin, Erchan Aptoula, Alp Ertürk",2023-11-08T06:48:13Z,Explainable AI,"  Deep learning has taken by storm all fields involved in data analysis,
including remote sensing for Earth observation. However, despite significant
advances in terms of performance, its lack of explainability and
interpretability, inherent to neural networks in general since their inception,
remains a major source of criticism. Hence it comes as no surprise that the
expansion of deep learning methods in remote sensing is being accompanied by
increasingly intensive efforts oriented towards addressing this drawback
through the exploration of a wide spectrum of Explainable Artificial
Intelligence techniques. This chapter, organized according to prominent Earth
observation application fields, presents a panorama of the state-of-the-art in
explainable remote sensing image analysis.
",1
"Exploring Variational Auto-Encoder Architectures, Configurations, and
  Datasets for Generative Music Explainable AI","Nick Bryan-Kinns, Bingyuan Zhang, Songyan Zhao, Berker Banar",2023-11-14T17:27:30Z,Explainable AI,"  Generative AI models for music and the arts in general are increasingly
complex and hard to understand. The field of eXplainable AI (XAI) seeks to make
complex and opaque AI models such as neural networks more understandable to
people. One approach to making generative AI models more understandable is to
impose a small number of semantically meaningful attributes on generative AI
models. This paper contributes a systematic examination of the impact that
different combinations of Variational Auto-Encoder models (MeasureVAE and
AdversarialVAE), configurations of latent space in the AI model (from 4 to 256
latent dimensions), and training datasets (Irish folk, Turkish folk, Classical,
and pop) have on music generation performance when 2 or 4 meaningful musical
attributes are imposed on the generative model. To date there have been no
systematic comparisons of such models at this level of combinatorial detail.
Our findings show that MeasureVAE has better reconstruction performance than
AdversarialVAE which has better musical attribute independence. Results
demonstrate that MeasureVAE was able to generate music across music genres with
interpretable musical dimensions of control, and performs best with low
complexity music such a pop and rock. We recommend that a 32 or 64 latent
dimensional space is optimal for 4 regularised dimensions when using MeasureVAE
to generate music across genres. Our results are the first detailed comparisons
of configurations of state-of-the-art generative AI models for music and can be
used to help select and configure AI models, musical features, and datasets for
more understandable generation of music.
",0
"Towards Clinical Prediction with Transparency: An Explainable AI
  Approach to Survival Modelling in Residential Aged Care","Teo Susnjak, Elise Griffin",2023-12-01T01:11:16Z,Explainable AI,"  Background: Accurate survival time estimates aid end-of-life medical
decision-making. Objectives: Develop an interpretable survival model for
elderly residential aged care residents using advanced machine learning.
Setting: A major Australasian residential aged care provider. Participants:
Residents aged 65+ admitted for long-term care from July 2017 to August 2023.
Sample size: 11,944 residents across 40 facilities. Predictors: Factors include
age, gender, health status, co-morbidities, cognitive function, mood,
nutrition, mobility, smoking, sleep, skin integrity, and continence. Outcome:
Probability of survival post-admission, specifically calibrated for 6-month
survival estimates. Statistical Analysis: Tested CoxPH, EN, RR, Lasso, GB, XGB,
and RF models in 20 experiments with a 90/10 train/test split. Evaluated
accuracy using C-index, Harrell's C-index, dynamic AUROC, IBS, and calibrated
ROC. Chose XGB for its performance and calibrated it for 1, 3, 6, and 12-month
predictions using Platt scaling. Employed SHAP values to analyze predictor
impacts. Results: GB, XGB, and RF models showed the highest C-Index values
(0.714, 0.712, 0.712). The optimal XGB model demonstrated a 6-month survival
prediction AUROC of 0.746 (95% CI 0.744-0.749). Key mortality predictors
include age, male gender, mobility, health status, pressure ulcer risk, and
appetite. Conclusions: The study successfully applies machine learning to
create a survival model for aged care, aligning with clinical insights on
mortality risk factors and enhancing model interpretability and clinical
utility through explainable AI.
",0
"Explainable AI in Diagnosing and Anticipating Leukemia Using Transfer
  Learning Method","Wahidul Hasan Abir, Md. Fahim Uddin, Faria Rahman Khanam, Mohammad Monirujjaman Khan",2023-12-01T10:37:02Z,Explainable AI,"  This research paper focuses on Acute Lymphoblastic Leukemia (ALL), a form of
blood cancer prevalent in children and teenagers, characterized by the rapid
proliferation of immature white blood cells (WBCs). These atypical cells can
overwhelm healthy cells, leading to severe health consequences. Early and
accurate detection of ALL is vital for effective treatment and improving
survival rates. Traditional diagnostic methods are time-consuming, costly, and
prone to errors. The paper proposes an automated detection approach using
computer-aided diagnostic (CAD) models, leveraging deep learning techniques to
enhance the accuracy and efficiency of leukemia diagnosis. The study utilizes
various transfer learning models like ResNet101V2, VGG19, InceptionV3, and
InceptionResNetV2 for classifying ALL. The methodology includes using the Local
Interpretable Model-Agnostic Explanations (LIME) for ensuring the validity and
reliability of the AI system's predictions. This approach is critical for
overcoming the ""black box"" nature of AI, where decisions made by models are
often opaque and unaccountable. The paper highlights that the proposed method
using the InceptionV3 model achieved an impressive 98.38% accuracy,
outperforming other tested models. The results, verified by the LIME algorithm,
showcase the potential of this method in accurately identifying ALL, providing
a valuable tool for medical practitioners. The research underscores the impact
of explainable artificial intelligence (XAI) in medical diagnostics, paving the
way for more transparent and trustworthy AI applications in healthcare.
",35
"XAI meets Biology: A Comprehensive Review of Explainable AI in
  Bioinformatics Applications","Zhongliang Zhou, Mengxuan Hu, Mariah Salcedo, Nathan Gravel, Wayland Yeung, Aarya Venkat, Dongliang Guo, Jielu Zhang, Natarajan Kannan, Sheng Li",2023-12-11T03:08:18Z,Explainable AI,"  Artificial intelligence (AI), particularly machine learning and deep learning
models, has significantly impacted bioinformatics research by offering powerful
tools for analyzing complex biological data. However, the lack of
interpretability and transparency of these models presents challenges in
leveraging these models for deeper biological insights and for generating
testable hypotheses. Explainable AI (XAI) has emerged as a promising solution
to enhance the transparency and interpretability of AI models in
bioinformatics. This review provides a comprehensive analysis of various XAI
techniques and their applications across various bioinformatics domains
including DNA, RNA, and protein sequence analysis, structural analysis, gene
expression and genome analysis, and bioimaging analysis. We introduce the most
pertinent machine learning and XAI methods, then discuss their diverse
applications and address the current limitations of available XAI tools. By
offering insights into XAI's potential and challenges, this review aims to
facilitate its practical implementation in bioinformatics research and help
researchers navigate the landscape of XAI tools.
",0
"An adversarial attack approach for eXplainable AI evaluation on deepfake
  detection models","Balachandar Gowrisankar, Vrizlynn L. L. Thing",2023-12-08T15:19:08Z,Explainable AI,"  With the rising concern on model interpretability, the application of
eXplainable AI (XAI) tools on deepfake detection models has been a topic of
interest recently. In image classification tasks, XAI tools highlight pixels
influencing the decision given by a model. This helps in troubleshooting the
model and determining areas that may require further tuning of parameters. With
a wide range of tools available in the market, choosing the right tool for a
model becomes necessary as each one may highlight different sets of pixels for
a given image. There is a need to evaluate different tools and decide the best
performing ones among them. Generic XAI evaluation methods like insertion or
removal of salient pixels/segments are applicable for general image
classification tasks but may produce less meaningful results when applied on
deepfake detection models due to their functionality. In this paper, we perform
experiments to show that generic removal/insertion XAI evaluation methods are
not suitable for deepfake detection models. We also propose and implement an
XAI evaluation approach specifically suited for deepfake detection models.
",0
"Explainable AI in Grassland Monitoring: Enhancing Model Performance and
  Domain Adaptability","Shanghua Liu, Anna Hedström, Deepak Hanike Basavegowda, Cornelia Weltzien, Marina M. -C. Höhne",2023-12-13T10:17:48Z,Explainable AI,"  Grasslands are known for their high biodiversity and ability to provide
multiple ecosystem services. Challenges in automating the identification of
indicator plants are key obstacles to large-scale grassland monitoring. These
challenges stem from the scarcity of extensive datasets, the distributional
shifts between generic and grassland-specific datasets, and the inherent
opacity of deep learning models. This paper delves into the latter two
challenges, with a specific focus on transfer learning and eXplainable
Artificial Intelligence (XAI) approaches to grassland monitoring, highlighting
the novelty of XAI in this domain. We analyze various transfer learning methods
to bridge the distributional gaps between generic and grassland-specific
datasets. Additionally, we showcase how explainable AI techniques can unveil
the model's domain adaptation capabilities, employing quantitative assessments
to evaluate the model's proficiency in accurately centering relevant input
features around the object of interest. This research contributes valuable
insights for enhancing model performance through transfer learning and
measuring domain adaptability with explainable AI, showing significant promise
for broader applications within the agricultural community.
",0
"Transparency and Privacy: The Role of Explainable AI and Federated
  Learning in Financial Fraud Detection","Tomisin Awosika, Raj Mani Shukla, Bernardi Pranggono",2023-12-20T18:26:59Z,Explainable AI,"  Fraudulent transactions and how to detect them remain a significant problem
for financial institutions around the world. The need for advanced fraud
detection systems to safeguard assets and maintain customer trust is paramount
for financial institutions, but some factors make the development of effective
and efficient fraud detection systems a challenge. One of such factors is the
fact that fraudulent transactions are rare and that many transaction datasets
are imbalanced; that is, there are fewer significant samples of fraudulent
transactions than legitimate ones. This data imbalance can affect the
performance or reliability of the fraud detection model. Moreover, due to the
data privacy laws that all financial institutions are subject to follow,
sharing customer data to facilitate a higher-performing centralized model is
impossible. Furthermore, the fraud detection technique should be transparent so
that it does not affect the user experience. Hence, this research introduces a
novel approach using Federated Learning (FL) and Explainable AI (XAI) to
address these challenges. FL enables financial institutions to collaboratively
train a model to detect fraudulent transactions without directly sharing
customer data, thereby preserving data privacy and confidentiality. Meanwhile,
the integration of XAI ensures that the predictions made by the model can be
understood and interpreted by human experts, adding a layer of transparency and
trust to the system. Experimental results, based on realistic transaction
datasets, reveal that the FL-based fraud detection system consistently
demonstrates high performance metrics. This study grounds FL's potential as an
effective and privacy-preserving tool in the fight against fraud.
",4
"Real-time Neural Network Inference on Extremely Weak Devices: Agile
  Offloading with Explainable AI","Kai Huang, Wei Gao",2023-12-21T17:18:32Z,Explainable AI,"  With the wide adoption of AI applications, there is a pressing need of
enabling real-time neural network (NN) inference on small embedded devices, but
deploying NNs and achieving high performance of NN inference on these small
devices is challenging due to their extremely weak capabilities. Although NN
partitioning and offloading can contribute to such deployment, they are
incapable of minimizing the local costs at embedded devices. Instead, we
suggest to address this challenge via agile NN offloading, which migrates the
required computations in NN offloading from online inference to offline
learning. In this paper, we present AgileNN, a new NN offloading technique that
achieves real-time NN inference on weak embedded devices by leveraging
eXplainable AI techniques, so as to explicitly enforce feature sparsity during
the training phase and minimize the online computation and communication costs.
Experiment results show that AgileNN's inference latency is >6x lower than the
existing schemes, ensuring that sensory data on embedded devices can be timely
consumed. It also reduces the local device's resource consumption by >8x,
without impairing the inference accuracy.
",0
"An Explainable AI Approach to Large Language Model Assisted Causal Model
  Auditing and Development","Yanming Zhang, Brette Fitzgibbon, Dino Garofolo, Akshith Kota, Eric Papenhausen, Klaus Mueller",2023-12-23T17:40:41Z,Explainable AI,"  Causal networks are widely used in many fields, including epidemiology,
social science, medicine, and engineering, to model the complex relationships
between variables. While it can be convenient to algorithmically infer these
models directly from observational data, the resulting networks are often
plagued with erroneous edges. Auditing and correcting these networks may
require domain expertise frequently unavailable to the analyst. We propose the
use of large language models such as ChatGPT as an auditor for causal networks.
Our method presents ChatGPT with a causal network, one edge at a time, to
produce insights about edge directionality, possible confounders, and mediating
variables. We ask ChatGPT to reflect on various aspects of each causal link and
we then produce visualizations that summarize these viewpoints for the human
analyst to direct the edge, gather more data, or test further hypotheses. We
envision a system where large language models, automated causal inference, and
the human analyst and domain expert work hand in hand as a team to derive
holistic and comprehensive causal models for any given case scenario. This
paper presents first results obtained with an emerging prototype.
",2
"Towards Directive Explanations: Crafting Explainable AI Systems for
  Actionable Human-AI Interactions",Aditya Bhattacharya,2023-12-29T06:46:01Z,Explainable AI,"  With Artificial Intelligence (AI) becoming ubiquitous in every application
domain, the need for explanations is paramount to enhance transparency and
trust among non-technical users. Despite the potential shown by Explainable AI
(XAI) for enhancing understanding of complex AI systems, most XAI methods are
designed for technical AI experts rather than non-technical consumers.
Consequently, such explanations are overwhelmingly complex and seldom guide
users in achieving their desired predicted outcomes. This paper presents
ongoing research for crafting XAI systems tailored to guide users in achieving
desired outcomes through improved human-AI interactions. This paper highlights
the research objectives and methods, key takeaways and implications learned
from user studies. It outlines open questions and challenges for enhanced
human-AI collaboration, which the author aims to address in future work.
",0
"Enhancing the Fairness and Performance of Edge Cameras with Explainable
  AI","Truong Thanh Hung Nguyen, Vo Thanh Khang Nguyen, Quoc Hung Cao, Van Binh Truong, Quoc Khanh Nguyen, Hung Cao",2024-01-18T10:08:24Z,Other,"  The rising use of Artificial Intelligence (AI) in human detection on Edge
camera systems has led to accurate but complex models, challenging to interpret
and debug. Our research presents a diagnostic method using Explainable AI (XAI)
for model debugging, with expert-driven problem identification and solution
creation. Validated on the Bytetrack model in a real-world office Edge network,
we found the training dataset as the main bias source and suggested model
augmentation as a solution. Our approach helps identify model biases, essential
for achieving fair and trustworthy models.
",0
"Unveiling the Human-like Similarities of Automatic Facial Expression
  Recognition: An Empirical Exploration through Explainable AI","F. Xavier Gaya-Morey, Silvia Ramis-Guarinos, Cristina Manresa-Yee, Jose M. Buades-Rubio",2024-01-22T10:52:02Z,Explainable AI,"  Facial expression recognition is vital for human behavior analysis, and deep
learning has enabled models that can outperform humans. However, it is unclear
how closely they mimic human processing. This study aims to explore the
similarity between deep neural networks and human perception by comparing
twelve different networks, including both general object classifiers and
FER-specific models. We employ an innovative global explainable AI method to
generate heatmaps, revealing crucial facial regions for the twelve networks
trained on six facial expressions. We assess these results both quantitatively
and qualitatively, comparing them to ground truth masks based on Friesen and
Ekman's description and among them. We use Intersection over Union (IoU) and
normalized correlation coefficients for comparisons. We generate 72 heatmaps to
highlight critical regions for each expression and architecture. Qualitatively,
models with pre-trained weights show more similarity in heatmaps compared to
those without pre-training. Specifically, eye and nose areas influence certain
facial expressions, while the mouth is consistently important across all models
and expressions. Quantitatively, we find low average IoU values (avg. 0.2702)
across all expressions and architectures. The best-performing architecture
averages 0.3269, while the worst-performing one averages 0.2066. Dendrograms,
built with the normalized correlation coefficient, reveal two main clusters for
most expressions: models with pre-training and models without pre-training.
Findings suggest limited alignment between human and AI facial expression
recognition, with network architectures influencing the similarity, as similar
architectures prioritize similar facial regions.
",0
"Advancing Explainable AI Toward Human-Like Intelligence: Forging the
  Path to Artificial Brain","Yongchen Zhou, Richard Jiang",2024-02-07T14:09:11Z,Explainable AI,"  The intersection of Artificial Intelligence (AI) and neuroscience in
Explainable AI (XAI) is pivotal for enhancing transparency and interpretability
in complex decision-making processes. This paper explores the evolution of XAI
methodologies, ranging from feature-based to human-centric approaches, and
delves into their applications in diverse domains, including healthcare and
finance. The challenges in achieving explainability in generative models,
ensuring responsible AI practices, and addressing ethical implications are
discussed. The paper further investigates the potential convergence of XAI with
cognitive sciences, the development of emotionally intelligent AI, and the
quest for Human-Like Intelligence (HLI) in AI systems. As AI progresses towards
Artificial General Intelligence (AGI), considerations of consciousness, ethics,
and societal impact become paramount. The ongoing pursuit of deciphering the
mysteries of the brain with AI and the quest for HLI represent transformative
endeavors, bridging technical advancements with multidisciplinary explorations
of human cognition.
",0
"Explainable AI for Safe and Trustworthy Autonomous Driving: A Systematic
  Review","Anton Kuznietsov, Balint Gyevnar, Cheng Wang, Steven Peters, Stefano V. Albrecht",2024-02-08T09:08:44Z,Explainable AI,"  Artificial Intelligence (AI) shows promising applications for the perception
and planning tasks in autonomous driving (AD) due to its superior performance
compared to conventional methods. However, inscrutable AI systems exacerbate
the existing challenge of safety assurance of AD. One way to mitigate this
challenge is to utilize explainable AI (XAI) techniques. To this end, we
present the first comprehensive systematic literature review of explainable
methods for safe and trustworthy AD. We begin by analyzing the requirements for
AI in the context of AD, focusing on three key aspects: data, model, and
agency. We find that XAI is fundamental to meeting these requirements. Based on
this, we explain the sources of explanations in AI and describe a taxonomy of
XAI. We then identify five key contributions of XAI for safe and trustworthy AI
in AD, which are interpretable design, interpretable surrogate models,
interpretable monitoring, auxiliary explanations, and interpretable validation.
Finally, we propose a modular framework called SafeX to integrate these
contributions, enabling explanation delivery to users while simultaneously
ensuring the safety of AI models.
",0
"From Movements to Metrics: Evaluating Explainable AI Methods in
  Skeleton-Based Human Activity Recognition","Kimji N. Pellano, Inga Strümke, Espen Alexander F. Ihlen",2024-02-20T07:58:04Z,Explainable AI,"  The advancement of deep learning in human activity recognition (HAR) using 3D
skeleton data is critical for applications in healthcare, security, sports, and
human-computer interaction. This paper tackles a well-known gap in the field,
which is the lack of testing in the applicability and reliability of XAI
evaluation metrics in the skeleton-based HAR domain. We have tested established
XAI metrics namely faithfulness and stability on Class Activation Mapping (CAM)
and Gradient-weighted Class Activation Mapping (Grad-CAM) to address this
problem. The study also introduces a perturbation method that respects human
biomechanical constraints to ensure realistic variations in human movement. Our
findings indicate that \textit{faithfulness} may not be a reliable metric in
certain contexts, such as with the EfficientGCN model. Conversely, stability
emerges as a more dependable metric when there is slight input data
perturbations. CAM and Grad-CAM are also found to produce almost identical
explanations, leading to very similar XAI metric performance. This calls for
the need for more diversified metrics and new XAI methods applied in
skeleton-based HAR.
",1
"Opening the Black-Box: A Systematic Review on Explainable AI in Remote
  Sensing","Adrian Höhl, Ivica Obadic, Miguel Ángel Fernández Torres, Hiba Najjar, Dario Oliveira, Zeynep Akata, Andreas Dengel, Xiao Xiang Zhu",2024-02-21T13:19:58Z,Explainable AI,"  In recent years, black-box machine learning approaches have become a dominant
modeling paradigm for knowledge extraction in Remote Sensing. Despite the
potential benefits of uncovering the inner workings of these models with
explainable AI, a comprehensive overview summarizing the used explainable AI
methods and their objectives, findings, and challenges in Remote Sensing
applications is still missing. In this paper, we address this issue by
performing a systematic review to identify the key trends of how explainable AI
is used in Remote Sensing and shed light on novel explainable AI approaches and
emerging directions that tackle specific Remote Sensing challenges. We also
reveal the common patterns of explanation interpretation, discuss the extracted
scientific insights in Remote Sensing, and reflect on the approaches used for
explainable AI methods evaluation. Our review provides a complete summary of
the state-of-the-art in the field. Further, we give a detailed outlook on the
challenges and promising research directions, representing a basis for novel
methodological development and a useful starting point for new researchers in
the field of explainable AI in Remote Sensing.
",0
"Enhancing Neural Machine Translation of Low-Resource Languages: Corpus
  Development, Human Evaluation and Explainable AI Architectures",Séamus Lankford,2024-03-03T18:08:30Z,Explainable AI,"  In the current machine translation (MT) landscape, the Transformer
architecture stands out as the gold standard, especially for high-resource
language pairs. This research delves into its efficacy for low-resource
language pairs including both the English$\leftrightarrow$Irish and
English$\leftrightarrow$Marathi language pairs. Notably, the study identifies
the optimal hyperparameters and subword model type to significantly improve the
translation quality of Transformer models for low-resource language pairs.
  The scarcity of parallel datasets for low-resource languages can hinder MT
development. To address this, gaHealth was developed, the first bilingual
corpus of health data for the Irish language. Focusing on the health domain,
models developed using this in-domain dataset exhibited very significant
improvements in BLEU score when compared with models from the LoResMT2021
Shared Task. A subsequent human evaluation using the multidimensional quality
metrics error taxonomy showcased the superior performance of the Transformer
system in reducing both accuracy and fluency errors compared to an RNN-based
counterpart.
  Furthermore, this thesis introduces adaptNMT and adaptMLLM, two open-source
applications streamlined for the development, fine-tuning, and deployment of
neural machine translation models. These tools considerably simplify the setup
and evaluation process, making MT more accessible to both developers and
translators. Notably, adaptNMT, grounded in the OpenNMT ecosystem, promotes
eco-friendly natural language processing research by highlighting the
environmental footprint of model development. Fine-tuning of MLLMs by adaptMLLM
demonstrated advancements in translation performance for two low-resource
language pairs: English$\leftrightarrow$Irish and
English$\leftrightarrow$Marathi, compared to baselines from the LoResMT2021
Shared Task.
",0
"Explainable AI for Embedded Systems Design: A Case Study of Static
  Redundant NVM Memory Write Prediction","Abdoulaye Gamatié, Yuyang Wang",2024-03-07T09:02:11Z,Explainable AI,"  This paper investigates the application of eXplainable Artificial
Intelligence (XAI) in the design of embedded systems using machine learning
(ML). As a case study, it addresses the challenging problem of static silent
store prediction. This involves identifying redundant memory writes based only
on static program features. Eliminating such stores enhances performance and
energy efficiency by reducing memory access and bus traffic, especially in the
presence of emerging non-volatile memory technologies. To achieve this, we
propose a methodology consisting of: 1) the development of relevant ML models
for explaining silent store prediction, and 2) the application of XAI to
explain these models. We employ two state-of-the-art model-agnostic XAI methods
to analyze the causes of silent stores. Through the case study, we evaluate the
effectiveness of the methods. We find that these methods provide explanations
for silent store predictions, which are consistent with known causes of silent
store occurrences from previous studies. Typically, this allows us to confirm
the prevalence of silent stores in operations that write the zero constant into
memory, or the absence of silent stores in operations involving loop induction
variables. This suggests the potential relevance of XAI in analyzing ML models'
decision in embedded system design. From the case study, we share some valuable
insights and pitfalls we encountered. More generally, this study aims to lay
the groundwork for future research in the emerging field of XAI for embedded
system design.
",0
"How Human-Centered Explainable AI Interface Are Designed and Evaluated:
  A Systematic Survey","Thu Nguyen, Alessandro Canossa, Jichen Zhu",2024-03-21T15:44:56Z,Explainable AI,"  Despite its technological breakthroughs, eXplainable Artificial Intelligence
(XAI) research has limited success in producing the {\em effective
explanations} needed by users. In order to improve XAI systems' usability,
practical interpretability, and efficacy for real users, the emerging area of
{\em Explainable Interfaces} (EIs) focuses on the user interface and user
experience design aspects of XAI. This paper presents a systematic survey of 53
publications to identify current trends in human-XAI interaction and promising
directions for EI design and development. This is among the first systematic
survey of EI research.
",1
"Enhancing UAV Security Through Zero Trust Architecture: An Advanced Deep
  Learning and Explainable AI Analysis","Ekramul Haque, Kamrul Hasan, Imtiaz Ahmed, Md. Sahabul Alam, Tariqul Islam",2024-03-25T18:32:22Z,Explainable AI,"  In the dynamic and ever-changing domain of Unmanned Aerial Vehicles (UAVs),
the utmost importance lies in guaranteeing resilient and lucid security
measures. This study highlights the necessity of implementing a Zero Trust
Architecture (ZTA) to enhance the security of unmanned aerial vehicles (UAVs),
hence departing from conventional perimeter defences that may expose
vulnerabilities. The Zero Trust Architecture (ZTA) paradigm requires a rigorous
and continuous process of authenticating all network entities and
communications. The accuracy of our methodology in detecting and identifying
unmanned aerial vehicles (UAVs) is 84.59\%. This is achieved by utilizing Radio
Frequency (RF) signals within a Deep Learning framework, a unique method.
Precise identification is crucial in Zero Trust Architecture (ZTA), as it
determines network access. In addition, the use of eXplainable Artificial
Intelligence (XAI) tools such as SHapley Additive exPlanations (SHAP) and Local
Interpretable Model-agnostic Explanations (LIME) contributes to the improvement
of the model's transparency and interpretability. Adherence to Zero Trust
Architecture (ZTA) standards guarantees that the classifications of unmanned
aerial vehicles (UAVs) are verifiable and comprehensible, enhancing security
within the UAV field.
",0
"Does Faithfulness Conflict with Plausibility? An Empirical Study in
  Explainable AI across NLP Tasks","Xiaolei Lu, Jianghong Ma",2024-03-29T20:28:42Z,Explainable AI,"  Explainability algorithms aimed at interpreting decision-making AI systems
usually consider balancing two critical dimensions: 1) \textit{faithfulness},
where explanations accurately reflect the model's inference process. 2)
\textit{plausibility}, where explanations are consistent with domain experts.
However, the question arises: do faithfulness and plausibility inherently
conflict? In this study, through a comprehensive quantitative comparison
between the explanations from the selected explainability methods and
expert-level interpretations across three NLP tasks: sentiment analysis, intent
detection, and topic labeling, we demonstrate that traditional
perturbation-based methods Shapley value and LIME could attain greater
faithfulness and plausibility. Our findings suggest that rather than optimizing
for one dimension at the expense of the other, we could seek to optimize
explainability algorithms with dual objectives to achieve high levels of
accuracy and user accessibility in their explanations.
",0
"Towards Interpretable Deep Reinforcement Learning Models via Inverse
  Reinforcement Learning","Sean Xie, Soroush Vosoughi, Saeed Hassanpour",2022-03-30T17:01:59Z,Reinforcement Learning,"  Artificial intelligence, particularly through recent advancements in deep
learning, has achieved exceptional performances in many tasks in fields such as
natural language processing and computer vision. In addition to desirable
evaluation metrics, a high level of interpretability is often required for
these models to be reliably utilized. Therefore, explanations that offer
insight into the process by which a model maps its inputs onto its outputs are
much sought-after. Unfortunately, the current black box nature of machine
learning models is still an unresolved issue and this very nature prevents
researchers from learning and providing explicative descriptions for a model's
behavior and final predictions. In this work, we propose a novel framework
utilizing Adversarial Inverse Reinforcement Learning that can provide global
explanations for decisions made by a Reinforcement Learning model and capture
intuitive tendencies that the model follows by summarizing the model's
decision-making process.
",0
Reinforcement Learning with Latent Flow,"Wenling Shang, Xiaofei Wang, Aravind Srinivas, Aravind Rajeswaran, Yang Gao, Pieter Abbeel, Michael Laskin",2021-01-06T03:50:50Z,Reinforcement Learning,"  Temporal information is essential to learning effective policies with
Reinforcement Learning (RL). However, current state-of-the-art RL algorithms
either assume that such information is given as part of the state space or,
when learning from pixels, use the simple heuristic of frame-stacking to
implicitly capture temporal information present in the image observations. This
heuristic is in contrast to the current paradigm in video classification
architectures, which utilize explicit encodings of temporal information through
methods such as optical flow and two-stream architectures to achieve
state-of-the-art performance. Inspired by leading video classification
architectures, we introduce the Flow of Latents for Reinforcement Learning
(Flare), a network architecture for RL that explicitly encodes temporal
information through latent vector differences. We show that Flare (i) recovers
optimal performance in state-based RL without explicit access to the state
velocity, solely with positional state information, (ii) achieves
state-of-the-art performance on pixel-based challenging continuous control
tasks within the DeepMind control benchmark suite, namely quadruped walk,
hopper hop, finger turn hard, pendulum swing, and walker run, and is the most
sample efficient model-free pixel-based RL algorithm, outperforming the prior
model-free state-of-the-art by 1.9X and 1.5X on the 500k and 1M step
benchmarks, respectively, and (iv), when augmented over rainbow DQN,
outperforms this state-of-the-art level baseline on 5 of 8 challenging Atari
games at 100M time step benchmark.
",0
Feature Selection Using Reinforcement Learning,"Sali Rasoul, Sodiq Adewole, Alphonse Akakpo",2021-01-23T09:24:37Z,Reinforcement Learning,"  With the decreasing cost of data collection, the space of variables or
features that can be used to characterize a particular predictor of interest
continues to grow exponentially. Therefore, identifying the most characterizing
features that minimizes the variance without jeopardizing the bias of our
models is critical to successfully training a machine learning model. In
addition, identifying such features is critical for interpretability,
prediction accuracy and optimal computation cost. While statistical methods
such as subset selection, shrinkage, dimensionality reduction have been applied
in selecting the best set of features, some other approaches in literature have
approached feature selection task as a search problem where each state in the
search space is a possible feature subset. In this paper, we solved the feature
selection problem using Reinforcement Learning. Formulating the state space as
a Markov Decision Process (MDP), we used Temporal Difference (TD) algorithm to
select the best subset of features. Each state was evaluated using a robust and
low cost classifier algorithm which could handle any non-linearities in the
dataset.
",10
Risk-Averse Offline Reinforcement Learning,"Núria Armengol Urpí, Sebastian Curi, Andreas Krause",2021-02-10T10:27:49Z,Reinforcement Learning,"  Training Reinforcement Learning (RL) agents in high-stakes applications might
be too prohibitive due to the risk associated to exploration. Thus, the agent
can only use data previously collected by safe policies. While previous work
considers optimizing the average performance using offline data, we focus on
optimizing a risk-averse criteria, namely the CVaR. In particular, we present
the Offline Risk-Averse Actor-Critic (O-RAAC), a model-free RL algorithm that
is able to learn risk-averse policies in a fully offline setting. We show that
O-RAAC learns policies with higher CVaR than risk-neutral approaches in
different robot control tasks. Furthermore, considering risk-averse criteria
guarantees distributional robustness of the average performance with respect to
particular distribution shifts. We demonstrate empirically that in the presence
of natural distribution-shifts, O-RAAC learns policies with good average
performance.
",0
Scalable Bayesian Inverse Reinforcement Learning,"Alex J. Chan, Mihaela van der Schaar",2021-02-12T12:32:02Z,Reinforcement Learning,"  Bayesian inference over the reward presents an ideal solution to the
ill-posed nature of the inverse reinforcement learning problem. Unfortunately
current methods generally do not scale well beyond the small tabular setting
due to the need for an inner-loop MDP solver, and even non-Bayesian methods
that do themselves scale often require extensive interaction with the
environment to perform well, being inappropriate for high stakes or costly
applications such as healthcare. In this paper we introduce our method,
Approximate Variational Reward Imitation Learning (AVRIL), that addresses both
of these issues by jointly learning an approximate posterior distribution over
the reward that scales to arbitrarily complicated state spaces alongside an
appropriate policy in a completely offline manner through a variational
approach to said latent reward. Applying our method to real medical data
alongside classic control simulations, we demonstrate Bayesian reward inference
in environments beyond the scope of current methods, as well as task
performance competitive with focused offline imitation learning algorithms.
",57
Communication Efficient Parallel Reinforcement Learning,"Mridul Agarwal, Bhargav Ganguly, Vaneet Aggarwal",2021-02-22T02:46:36Z,Reinforcement Learning,"  We consider the problem where $M$ agents interact with $M$ identical and
independent environments with $S$ states and $A$ actions using reinforcement
learning for $T$ rounds. The agents share their data with a central server to
minimize their regret. We aim to find an algorithm that allows the agents to
minimize the regret with infrequent communication rounds. We provide \NAM\
which runs at each agent and prove that the total cumulative regret of $M$
agents is upper bounded as $\Tilde{O}(DS\sqrt{MAT})$ for a Markov Decision
Process with diameter $D$, number of states $S$, and number of actions $A$. The
agents synchronize after their visitations to any state-action pair exceeds a
certain threshold. Using this, we obtain a bound of $O\left(MSA\log(MT)\right)$
on the total number of communications rounds. Finally, we evaluate the
algorithm against multiple environments and demonstrate that the proposed
algorithm performs at par with an always communication version of the UCRL2
algorithm, while with significantly lower communication.
",0
Reinforcement Learning with Prototypical Representations,"Denis Yarats, Rob Fergus, Alessandro Lazaric, Lerrel Pinto",2021-02-22T18:56:34Z,Reinforcement Learning,"  Learning effective representations in image-based environments is crucial for
sample efficient Reinforcement Learning (RL). Unfortunately, in RL,
representation learning is confounded with the exploratory experience of the
agent -- learning a useful representation requires diverse data, while
effective exploration is only possible with coherent representations.
Furthermore, we would like to learn representations that not only generalize
across tasks but also accelerate downstream exploration for efficient
task-specific training. To address these challenges we propose Proto-RL, a
self-supervised framework that ties representation learning with exploration
through prototypical representations. These prototypes simultaneously serve as
a summarization of the exploratory experience of an agent as well as a basis
for representing observations. We pre-train these task-agnostic representations
and prototypes on environments without downstream task information. This
enables state-of-the-art downstream policy learning on a set of difficult
continuous control tasks.
",0
Action Redundancy in Reinforcement Learning,"Nir Baram, Guy Tennenholtz, Shie Mannor",2021-02-22T19:47:26Z,Reinforcement Learning,"  Maximum Entropy (MaxEnt) reinforcement learning is a powerful learning
paradigm which seeks to maximize return under entropy regularization. However,
action entropy does not necessarily coincide with state entropy, e.g., when
multiple actions produce the same transition. Instead, we propose to maximize
the transition entropy, i.e., the entropy of next states. We show that
transition entropy can be described by two terms; namely, model-dependent
transition entropy and action redundancy. Particularly, we explore the latter
in both deterministic and stochastic settings and develop tractable
approximation methods in a near model-free setup. We construct algorithms to
minimize action redundancy and demonstrate their effectiveness on a synthetic
environment with multiple redundant actions as well as contemporary benchmarks
in Atari and Mujoco. Our results suggest that action redundancy is a
fundamental problem in reinforcement learning.
",5
"Reinforcement Learning, Bit by Bit","Xiuyuan Lu, Benjamin Van Roy, Vikranth Dwaracherla, Morteza Ibrahimi, Ian Osband, Zheng Wen",2021-03-06T06:37:46Z,Reinforcement Learning,"  Reinforcement learning agents have demonstrated remarkable achievements in
simulated environments. Data efficiency poses an impediment to carrying this
success over to real environments. The design of data-efficient agents calls
for a deeper understanding of information acquisition and representation. We
discuss concepts and regret analysis that together offer principled guidance.
This line of thinking sheds light on questions of what information to seek, how
to seek that information, and what information to retain. To illustrate
concepts, we design simple agents that build on them and present computational
results that highlight data efficiency.
",0
Reinforcement Learning using Guided Observability,"Stephan Weigand, Pascal Klink, Jan Peters, Joni Pajarinen",2021-04-22T10:47:35Z,Reinforcement Learning,"  Due to recent breakthroughs, reinforcement learning (RL) has demonstrated
impressive performance in challenging sequential decision-making problems.
However, an open question is how to make RL cope with partial observability
which is prevalent in many real-world problems. Contrary to contemporary RL
approaches, which focus mostly on improved memory representations or strong
assumptions about the type of partial observability, we propose a simple but
efficient approach that can be applied together with a wide variety of RL
methods. Our main insight is that smoothly transitioning from full
observability to partial observability during the training process yields a
high performance policy. The approach, called partially observable guided
reinforcement learning (PO-GRL), allows to utilize full state information
during policy optimization without compromising the optimality of the final
policy. A comprehensive evaluation in discrete partially observableMarkov
decision process (POMDP) benchmark problems and continuous partially observable
MuJoCo and OpenAI gym tasks shows that PO-GRL improves performance. Finally, we
demonstrate PO-GRL in the ball-in-the-cup task on a real Barrett WAM robot
under partial observability.
",0
Identifiability in inverse reinforcement learning,"Haoyang Cao, Samuel N. Cohen, Lukasz Szpruch",2021-06-07T10:35:52Z,Reinforcement Learning,"  Inverse reinforcement learning attempts to reconstruct the reward function in
a Markov decision problem, using observations of agent actions. As already
observed in Russell [1998] the problem is ill-posed, and the reward function is
not identifiable, even under the presence of perfect information about optimal
behavior. We provide a resolution to this non-identifiability for problems with
entropy regularization. For a given environment, we fully characterize the
reward functions leading to a given policy and demonstrate that, given
demonstrations of actions for the same reward under two distinct discount
factors, or under sufficiently different environments, the unobserved reward
can be recovered up to a constant. We also give general necessary and
sufficient conditions for reconstruction of time-homogeneous rewards on finite
horizons, and for action-independent rewards, generalizing recent results of
Kim et al. [2021] and Fu et al. [2018].
",0
Corruption-Robust Offline Reinforcement Learning,"Xuezhou Zhang, Yiding Chen, Jerry Zhu, Wen Sun",2021-06-11T22:41:53Z,Reinforcement Learning,"  We study the adversarial robustness in offline reinforcement learning. Given
a batch dataset consisting of tuples $(s, a, r, s')$, an adversary is allowed
to arbitrarily modify $\epsilon$ fraction of the tuples. From the corrupted
dataset the learner aims to robustly identify a near-optimal policy. We first
show that a worst-case $\Omega(d\epsilon)$ optimality gap is unavoidable in
linear MDP of dimension $d$, even if the adversary only corrupts the reward
element in a tuple. This contrasts with dimension-free results in robust
supervised learning and best-known lower-bound in the online RL setting with
corruption. Next, we propose robust variants of the Least-Square Value
Iteration (LSVI) algorithm utilizing robust supervised learning oracles, which
achieve near-matching performances in cases both with and without full data
coverage. The algorithm requires the knowledge of $\epsilon$ to design the
pessimism bonus in the no-coverage case. Surprisingly, in this case, the
knowledge of $\epsilon$ is necessary, as we show that being adaptive to unknown
$\epsilon$ is impossible.This again contrasts with recent results on
corruption-robust online RL and implies that robust offline RL is a strictly
harder problem.
",0
Residual Reinforcement Learning from Demonstrations,"Minttu Alakuijala, Gabriel Dulac-Arnold, Julien Mairal, Jean Ponce, Cordelia Schmid",2021-06-15T11:16:49Z,Reinforcement Learning,"  Residual reinforcement learning (RL) has been proposed as a way to solve
challenging robotic tasks by adapting control actions from a conventional
feedback controller to maximize a reward signal. We extend the residual
formulation to learn from visual inputs and sparse rewards using
demonstrations. Learning from images, proprioceptive inputs and a sparse
task-completion reward relaxes the requirement of accessing full state
features, such as object and target positions. In addition, replacing the base
controller with a policy learned from demonstrations removes the dependency on
a hand-engineered controller in favour of a dataset of demonstrations, which
can be provided by non-experts. Our experimental evaluation on simulated
manipulation tasks on a 6-DoF UR5 arm and a 28-DoF dexterous hand demonstrates
that residual RL from demonstrations is able to generalize to unseen
environment conditions more flexibly than either behavioral cloning or RL
fine-tuning, and is capable of solving high-dimensional, sparse-reward tasks
out of reach for RL from scratch.
",0
Conservative Offline Distributional Reinforcement Learning,"Yecheng Jason Ma, Dinesh Jayaraman, Osbert Bastani",2021-07-12T15:38:06Z,Reinforcement Learning,"  Many reinforcement learning (RL) problems in practice are offline, learning
purely from observational data. A key challenge is how to ensure the learned
policy is safe, which requires quantifying the risk associated with different
actions. In the online setting, distributional RL algorithms do so by learning
the distribution over returns (i.e., cumulative rewards) instead of the
expected return; beyond quantifying risk, they have also been shown to learn
better representations for planning. We propose Conservative Offline
Distributional Actor Critic (CODAC), an offline RL algorithm suitable for both
risk-neutral and risk-averse domains. CODAC adapts distributional RL to the
offline setting by penalizing the predicted quantiles of the return for
out-of-distribution actions. We prove that CODAC learns a conservative return
distribution -- in particular, for finite MDPs, CODAC converges to an uniform
lower bound on the quantiles of the return distribution; our proof relies on a
novel analysis of the distributional Bellman operator. In our experiments, on
two challenging robot navigation tasks, CODAC successfully learns risk-averse
policies using offline data collected purely from risk-neutral agents.
Furthermore, CODAC is state-of-the-art on the D4RL MuJoCo benchmark in terms of
both expected and risk-sensitive performance.
",0
Active Reinforcement Learning over MDPs,"Qi Yang, Peng Yang, Ke Tang",2021-08-05T00:18:11Z,Reinforcement Learning,"  The past decade has seen the rapid development of Reinforcement Learning,
which acquires impressive performance with numerous training resources.
However, one of the greatest challenges in RL is generalization efficiency
(i.e., generalization performance in a unit time). This paper proposes a
framework of Active Reinforcement Learning (ARL) over MDPs to improve
generalization efficiency in a limited resource by instance selection. Given a
number of instances, the algorithm chooses out valuable instances as training
sets while training the policy, thereby costing fewer resources. Unlike
existing approaches, we attempt to actively select and use training data rather
than train on all the given data, thereby costing fewer resources. Furthermore,
we introduce a general instance evaluation metrics and selection mechanism into
the framework. Experiments results reveal that the proposed framework with
Proximal Policy Optimization as policy optimizer can effectively improve
generalization efficiency than unselect-ed and unbiased selected methods.
",0
Imitation Learning by Reinforcement Learning,Kamil Ciosek,2021-08-10T16:14:41Z,Reinforcement Learning,"  Imitation learning algorithms learn a policy from demonstrations of expert
behavior. We show that, for deterministic experts, imitation learning can be
done by reduction to reinforcement learning with a stationary reward. Our
theoretical analysis both certifies the recovery of expert reward and bounds
the total variation distance between the expert and the imitation learner,
showing a link to adversarial imitation learning. We conduct experiments which
confirm that our reduction works well in practice for continuous control tasks.
",15
Conformal Bootstrap with Reinforcement Learning,"Gergely Kántor, Vasilis Niarchos, Constantinos Papageorgakis",2021-08-20T18:43:59Z,Reinforcement Learning,"  We introduce the use of reinforcement-learning (RL) techniques to the
conformal-bootstrap programme. We demonstrate that suitable soft Actor-Critic
RL algorithms can perform efficient, relatively cheap high-dimensional searches
in the space of scaling dimensions and OPE-squared coefficients that produce
sensible results for tens of CFT data from a single crossing equation. In this
paper we test this approach in well-known 2D CFTs, with particular focus on the
Ising and tri-critical Ising models and the free compactified boson CFT. We
present results of as high as 36-dimensional searches, whose sole input is the
expected number of operators per spin in a truncation of the conformal-block
decomposition of the crossing equations. Our study of 2D CFTs uses only the
global $so(2,2)$ part of the conformal algebra, and our methods are equally
applicable to higher-dimensional CFTs. When combined with other, already
available, numerical and analytical methods, we expect our approach to yield an
exciting new window into the non-perturbative structure of arbitrary (unitary
or non-unitary) CFTs.
",0
Robust Risk-Aware Reinforcement Learning,"Sebastian Jaimungal, Silvana Pesenti, Ye Sheng Wang, Hariom Tatsat",2021-08-23T20:56:34Z,Reinforcement Learning,"  We present a reinforcement learning (RL) approach for robust optimisation of
risk-aware performance criteria. To allow agents to express a wide variety of
risk-reward profiles, we assess the value of a policy using rank dependent
expected utility (RDEU). RDEU allows the agent to seek gains, while
simultaneously protecting themselves against downside risk. To robustify
optimal policies against model uncertainty, we assess a policy not by its
distribution, but rather, by the worst possible distribution that lies within a
Wasserstein ball around it. Thus, our problem formulation may be viewed as an
actor/agent choosing a policy (the outer problem), and the adversary then
acting to worsen the performance of that strategy (the inner problem). We
develop explicit policy gradient formulae for the inner and outer problems, and
show its efficacy on three prototypical financial problems: robust portfolio
allocation, optimising a benchmark, and statistical arbitrage.
",0
Reinforcement Learning on Encrypted Data,"Alberto Jesu, Victor-Alexandru Darvariu, Alessandro Staffolani, Rebecca Montanari, Mirco Musolesi",2021-09-16T21:59:37Z,Reinforcement Learning,"  The growing number of applications of Reinforcement Learning (RL) in
real-world domains has led to the development of privacy-preserving techniques
due to the inherently sensitive nature of data. Most existing works focus on
differential privacy, in which information is revealed in the clear to an agent
whose learned model should be robust against information leakage to malicious
third parties. Motivated by use cases in which only encrypted data might be
shared, such as information from sensitive sites, in this work we consider
scenarios in which the inputs themselves are sensitive and cannot be revealed.
We develop a simple extension to the MDP framework which provides for the
encryption of states. We present a preliminary, experimental study of how a DQN
agent trained on encrypted states performs in environments with discrete and
continuous state spaces. Our results highlight that the agent is still capable
of learning in small state spaces even in presence of non-deterministic
encryption, but performance collapses in more complex environments.
",0
Dual Behavior Regularized Reinforcement Learning,"Chapman Siu, Jason Traish, Richard Yi Da Xu",2021-09-19T00:47:18Z,Reinforcement Learning,"  Reinforcement learning has been shown to perform a range of complex tasks
through interaction with an environment or collected leveraging experience.
However, many of these approaches presume optimal or near optimal experiences
or the presence of a consistent environment. In this work we propose dual,
advantage-based behavior policy based on counterfactual regret minimization. We
demonstrate the flexibility of this approach and how it can be adapted to
online contexts where the environment is available to collect experiences and a
variety of other contexts. We demonstrate this new algorithm can outperform
several strong baseline models in different contexts based on a range of
continuous environments. Additional ablations provide insights into how our
dual behavior regularized reinforcement learning approach is designed compared
with other plausible modifications and demonstrates its ability to generalize.
",0
Reinforcement Learning Under Algorithmic Triage,"Eleni Straitouri, Adish Singla, Vahid Balazadeh Meresht, Manuel Gomez-Rodriguez",2021-09-23T12:21:26Z,Reinforcement Learning,"  Methods to learn under algorithmic triage have predominantly focused on
supervised learning settings where each decision, or prediction, is independent
of each other. Under algorithmic triage, a supervised learning model predicts a
fraction of the instances and humans predict the remaining ones. In this work,
we take a first step towards developing reinforcement learning models that are
optimized to operate under algorithmic triage. To this end, we look at the
problem through the framework of options and develop a two-stage actor-critic
method to learn reinforcement learning models under triage. The first stage
performs offline, off-policy training using human data gathered in an
environment where the human has operated on their own. The second stage
performs on-policy training to account for the impact that switching may have
on the human policy, which may be difficult to anticipate from the above human
data. Extensive simulation experiments in a synthetic car driving task show
that the machine models and the triage policies trained using our two-stage
method effectively complement human policies and outperform those provided by
several competitive baselines.
",15
Deep Reinforcement Learning with Adjustments,"Hamed Khorasgani, Haiyan Wang, Chetan Gupta, Susumu Serita",2021-09-28T03:35:09Z,Reinforcement Learning,"  Deep reinforcement learning (RL) algorithms can learn complex policies to
optimize agent operation over time. RL algorithms have shown promising results
in solving complicated problems in recent years. However, their application on
real-world physical systems remains limited. Despite the advancements in RL
algorithms, the industries often prefer traditional control strategies.
Traditional methods are simple, computationally efficient and easy to adjust.
In this paper, we first propose a new Q-learning algorithm for continuous
action space, which can bridge the control and RL algorithms and bring us the
best of both worlds. Our method can learn complex policies to achieve long-term
goals and at the same time it can be easily adjusted to address short-term
requirements without retraining. Next, we present an approximation of our
algorithm which can be applied to address short-term requirements of any
pre-trained RL algorithm. The case studies demonstrate that both our proposed
method as well as its practical approximation can achieve short-term and
long-term goals without complex reward functions.
",0
Reinforcement Learning for Quantitative Trading,"Shuo Sun, Rundong Wang, Bo An",2021-09-28T16:32:10Z,Reinforcement Learning,"  Quantitative trading (QT), which refers to the usage of mathematical models
and data-driven techniques in analyzing the financial market, has been a
popular topic in both academia and financial industry since 1970s. In the last
decade, reinforcement learning (RL) has garnered significant interest in many
domains such as robotics and video games, owing to its outstanding ability on
solving complex sequential decision making problems. RL's impact is pervasive,
recently demonstrating its ability to conquer many challenging QT tasks. It is
a flourishing research direction to explore RL techniques' potential on QT
tasks. This paper aims at providing a comprehensive survey of research efforts
on RL-based methods for QT tasks. More concretely, we devise a taxonomy of
RL-based QT models, along with a comprehensive summary of the state of the art.
Finally, we discuss current challenges and propose future research directions
in this exciting field.
",0
Reinforcement Learning for Standards Design,"Shahrukh Khan Kasi, Sayandev Mukherjee, Lin Cheng, Bernardo A. Huberman",2021-10-13T17:46:27Z,Reinforcement Learning,"  Communications standards are designed via committees of humans holding
repeated meetings over months or even years until consensus is achieved. This
includes decisions regarding the modulation and coding schemes to be supported
over an air interface. We propose a way to ""automate"" the selection of the set
of modulation and coding schemes to be supported over a given air interface and
thereby streamline both the standards design process and the ease of extending
the standard to support new modulation schemes applicable to new higher-level
applications and services. Our scheme involves machine learning, whereby a
constructor entity submits proposals to an evaluator entity, which returns a
score for the proposal. The constructor employs reinforcement learning to
iterate on its submitted proposals until a score is achieved that was
previously agreed upon by both constructor and evaluator to be indicative of
satisfying the required design criteria (including performance metrics for
transmissions over the interface).
",1
Playing 2048 With Reinforcement Learning,"Shilun Li, Veronica Peng",2021-10-20T05:02:31Z,Reinforcement Learning,"  The game of 2048 is a highly addictive game. It is easy to learn the game,
but hard to master as the created game revealed that only about 1% games out of
hundreds million ever played have been won. In this paper, we would like to
explore reinforcement learning techniques to win 2048. The approaches we have
took include deep Q-learning and beam search, with beam search reaching 2048
28.5 of time.
",0
URLB: Unsupervised Reinforcement Learning Benchmark,"Michael Laskin, Denis Yarats, Hao Liu, Kimin Lee, Albert Zhan, Kevin Lu, Catherine Cang, Lerrel Pinto, Pieter Abbeel",2021-10-28T15:07:01Z,Reinforcement Learning,"  Deep Reinforcement Learning (RL) has emerged as a powerful paradigm to solve
a range of complex yet specific control tasks. Yet training generalist agents
that can quickly adapt to new tasks remains an outstanding challenge. Recent
advances in unsupervised RL have shown that pre-training RL agents with
self-supervised intrinsic rewards can result in efficient adaptation. However,
these algorithms have been hard to compare and develop due to the lack of a
unified benchmark. To this end, we introduce the Unsupervised Reinforcement
Learning Benchmark (URLB). URLB consists of two phases: reward-free
pre-training and downstream task adaptation with extrinsic rewards. Building on
the DeepMind Control Suite, we provide twelve continuous control tasks from
three domains for evaluation and open-source code for eight leading
unsupervised RL methods. We find that the implemented baselines make progress
but are not able to solve URLB and propose directions for future research.
",118
Batch Reinforcement Learning from Crowds,"Guoxi Zhang, Hisashi Kashima",2021-11-08T05:46:33Z,Reinforcement Learning,"  A shortcoming of batch reinforcement learning is its requirement for rewards
in data, thus not applicable to tasks without reward functions. Existing
settings for lack of reward, such as behavioral cloning, rely on optimal
demonstrations collected from humans. Unfortunately, extensive expertise is
required for ensuring optimality, which hinder the acquisition of large-scale
data for complex tasks. This paper addresses the lack of reward in a batch
reinforcement learning setting by learning a reward function from preferences.
Generating preferences only requires a basic understanding of a task. Being a
mental process, generating preferences is faster than performing
demonstrations. So preferences can be collected at scale from non-expert humans
using crowdsourcing. This paper tackles a critical challenge that emerged when
collecting data from non-expert humans: the noise in preferences. A novel
probabilistic model is proposed for modelling the reliability of labels, which
utilizes labels collaboratively. Moreover, the proposed model smooths the
estimation with a learned reward function. Evaluation on Atari datasets
demonstrates the effectiveness of the proposed model, followed by an ablation
study to analyze the relative importance of the proposed ideas.
",0
Piano Fingering with Reinforcement Learning,"Pedro Ramoneda, Marius Miron, Xavier Serra",2021-11-15T09:51:29Z,Reinforcement Learning,"  Hand and finger movements are a mainstay of piano technique. Automatic
Fingering from symbolic music data allows us to simulate finger and hand
movements. Previous proposals achieve automatic piano fingering based on
knowledge-driven or data-driven techniques. We combine both approaches with
deep reinforcement learning techniques to derive piano fingering. Finally, we
explore how to incorporate past experience into reinforcement learning-based
piano fingering in further work.
",3
Abstractions of General Reinforcement Learning,Sultan J. Majeed,2021-12-26T15:50:05Z,Reinforcement Learning,"  The field of artificial intelligence (AI) is devoted to the creation of
artificial decision-makers that can perform (at least) on par with the human
counterparts on a domain of interest. Unlike the agents in traditional AI, the
agents in artificial general intelligence (AGI) are required to replicate human
intelligence in almost every domain of interest. Moreover, an AGI agent should
be able to achieve this without (virtually any) further changes, retraining, or
fine-tuning of the parameters. The real world is non-stationary, non-ergodic,
and non-Markovian: we, humans, can neither revisit our past nor are the most
recent observations sufficient statistics. Yet, we excel at a variety of
complex tasks. Many of these tasks require longterm planning. We can associate
this success to our natural faculty to abstract away task-irrelevant
information from our overwhelming sensory experience. We make task-specific
mental models of the world without much effort. Due to this ability to
abstract, we can plan on a significantly compact representation of a task
without much loss of performance. Not only this, we also abstract our actions
to produce high-level plans: the level of action-abstraction can be anywhere
between small muscle movements to a mental notion of ""doing an action"". It is
natural to assume that any AGI agent competing with humans (at every plausible
domain) should also have these abilities to abstract its experiences and
actions. This thesis is an inquiry into the existence of such abstractions
which aid efficient planing for a wide range of domains, and most importantly,
these abstractions come with some optimality guarantees.
",0
"Deep Reinforcement Learning, a textbook",Aske Plaat,2022-01-04T11:47:21Z,Reinforcement Learning,"  Deep reinforcement learning has gathered much attention recently. Impressive
results were achieved in activities as diverse as autonomous driving, game
playing, molecular recombination, and robotics. In all these fields, computer
programs have taught themselves to solve difficult problems. They have learned
to fly model helicopters and perform aerobatic manoeuvers such as loops and
rolls. In some applications they have even become better than the best humans,
such as in Atari, Go, poker and StarCraft. The way in which deep reinforcement
learning explores complex environments reminds us of how children learn, by
playfully trying out things, getting feedback, and trying again. The computer
seems to truly possess aspects of human learning; this goes to the heart of the
dream of artificial intelligence. The successes in research have not gone
unnoticed by educators, and universities have started to offer courses on the
subject. The aim of this book is to provide a comprehensive overview of the
field of deep reinforcement learning. The book is written for graduate students
of artificial intelligence, and for researchers and practitioners who wish to
better understand deep reinforcement learning methods and their challenges. We
assume an undergraduate-level of understanding of computer science and
artificial intelligence; the programming language of this book is Python. We
describe the foundations, the algorithms and the applications of deep
reinforcement learning. We cover the established model-free and model-based
methods that form the basis of the field. Developments go quickly, and we also
cover advanced topics: deep multi-agent reinforcement learning, deep
hierarchical reinforcement learning, and deep meta learning.
",0
Automated Reinforcement Learning: An Overview,"Reza Refaei Afshar, Yingqian Zhang, Joaquin Vanschoren, Uzay Kaymak",2022-01-13T14:28:06Z,Reinforcement Learning,"  Reinforcement Learning and recently Deep Reinforcement Learning are popular
methods for solving sequential decision making problems modeled as Markov
Decision Processes. RL modeling of a problem and selecting algorithms and
hyper-parameters require careful considerations as different configurations may
entail completely different performances. These considerations are mainly the
task of RL experts; however, RL is progressively becoming popular in other
fields where the researchers and system designers are not RL experts. Besides,
many modeling decisions, such as defining state and action space, size of
batches and frequency of batch updating, and number of timesteps are typically
made manually. For these reasons, automating different components of RL
framework is of great importance and it has attracted much attention in recent
years. Automated RL provides a framework in which different components of RL
including MDP modeling, algorithm selection and hyper-parameter optimization
are modeled and defined automatically. In this article, we explore the
literature and present recent work that can be used in automated RL. Moreover,
we discuss the challenges, open questions and research directions in AutoRL.
",0
Local Explanations for Reinforcement Learning,"Ronny Luss, Amit Dhurandhar, Miao Liu",2022-02-08T02:02:09Z,Reinforcement Learning,"  Many works in explainable AI have focused on explaining black-box
classification models. Explaining deep reinforcement learning (RL) policies in
a manner that could be understood by domain users has received much less
attention. In this paper, we propose a novel perspective to understanding RL
policies based on identifying important states from automatically learned
meta-states. The key conceptual difference between our approach and many
previous ones is that we form meta-states based on locality governed by the
expert policy dynamics rather than based on similarity of actions, and that we
do not assume any particular knowledge of the underlying topology of the state
space. Theoretically, we show that our algorithm to find meta-states converges
and the objective that selects important states from each meta-state is
submodular leading to efficient high quality greedy selection. Experiments on
four domains (four rooms, door-key, minipacman, and pong) and a carefully
conducted user study illustrate that our perspective leads to better
understanding of the policy. We conjecture that this is a result of our
meta-states being more intuitive in that the corresponding important states are
strong indicators of tractable intermediate goals that are easier for humans to
interpret and follow.
",0
Scenario-Assisted Deep Reinforcement Learning,"Raz Yerushalmi, Guy Amir, Achiya Elyasaf, David Harel, Guy Katz, Assaf Marron",2022-02-09T08:46:13Z,Reinforcement Learning,"  Deep reinforcement learning has proven remarkably useful in training agents
from unstructured data. However, the opacity of the produced agents makes it
difficult to ensure that they adhere to various requirements posed by human
engineers. In this work-in-progress report, we propose a technique for
enhancing the reinforcement learning training process (specifically, its reward
calculation), in a way that allows human engineers to directly contribute their
expert knowledge, making the agent under training more likely to comply with
various relevant constraints. Moreover, our proposed approach allows
formulating these constraints using advanced model engineering techniques, such
as scenario-based modeling. This mix of black-box learning-based tools with
classical modeling approaches could produce systems that are effective and
efficient, but are also more transparent and maintainable. We evaluated our
technique using a case-study from the domain of internet congestion control,
obtaining promising results.
",12
Abstraction for Deep Reinforcement Learning,"Murray Shanahan, Melanie Mitchell",2022-02-10T23:49:13Z,Reinforcement Learning,"  We characterise the problem of abstraction in the context of deep
reinforcement learning. Various well established approaches to analogical
reasoning and associative memory might be brought to bear on this issue, but
they present difficulties because of the need for end-to-end differentiability.
We review developments in AI and machine learning that could facilitate their
adoption.
",0
Goal Recognition as Reinforcement Learning,"Leonardo Rosa Amado, Reuth Mirsky, Felipe Meneguzzi",2022-02-13T16:16:43Z,Reinforcement Learning,"  Most approaches for goal recognition rely on specifications of the possible
dynamics of the actor in the environment when pursuing a goal. These
specifications suffer from two key issues. First, encoding these dynamics
requires careful design by a domain expert, which is often not robust to noise
at recognition time. Second, existing approaches often need costly real-time
computations to reason about the likelihood of each potential goal. In this
paper, we develop a framework that combines model-free reinforcement learning
and goal recognition to alleviate the need for careful, manual domain design,
and the need for costly online executions. This framework consists of two main
stages: Offline learning of policies or utility functions for each potential
goal, and online inference. We provide a first instance of this framework using
tabular Q-learning for the learning stage, as well as three measures that can
be used to perform the inference stage. The resulting instantiation achieves
state-of-the-art performance against goal recognizers on standard evaluation
domains and superior performance in noisy environments.
",0
User-Oriented Robust Reinforcement Learning,"Haoyi You, Beichen Yu, Haiming Jin, Zhaoxing Yang, Jiahui Sun",2022-02-15T10:33:55Z,Reinforcement Learning,"  Recently, improving the robustness of policies across different environments
attracts increasing attention in the reinforcement learning (RL) community.
Existing robust RL methods mostly aim to achieve the max-min robustness by
optimizing the policy's performance in the worst-case environment. However, in
practice, a user that uses an RL policy may have different preferences over its
performance across environments. Clearly, the aforementioned max-min robustness
is oftentimes too conservative to satisfy user preference. Therefore, in this
paper, we integrate user preference into policy learning in robust RL, and
propose a novel User-Oriented Robust RL (UOR-RL) framework. Specifically, we
define a new User-Oriented Robustness (UOR) metric for RL, which allocates
different weights to the environments according to user preference and
generalizes the max-min robustness metric. To optimize the UOR metric, we
develop two different UOR-RL training algorithms for the scenarios with or
without a priori known environment distribution, respectively. Theoretically,
we prove that our UOR-RL training algorithms converge to near-optimal policies
even with inaccurate or completely no knowledge about the environment
distribution. Furthermore, we carry out extensive experimental evaluations in 4
MuJoCo tasks. The experimental results demonstrate that UOR-RL is comparable to
the state-of-the-art baselines under the average and worst-case performance
metrics, and more importantly establishes new state-of-the-art performance
under the UOR metric.
",0
Context-Hierarchy Inverse Reinforcement Learning,"Wei Gao, David Hsu, Wee Sun Lee",2022-02-25T10:29:05Z,Reinforcement Learning,"  An inverse reinforcement learning (IRL) agent learns to act intelligently by
observing expert demonstrations and learning the expert's underlying reward
function. Although learning the reward functions from demonstrations has
achieved great success in various tasks, several other challenges are mostly
ignored. Firstly, existing IRL methods try to learn the reward function from
scratch without relying on any prior knowledge. Secondly, traditional IRL
methods assume the reward functions are homogeneous across all the
demonstrations. Some existing IRL methods managed to extend to the
heterogeneous demonstrations. However, they still assume one hidden variable
that affects the behavior and learn the underlying hidden variable together
with the reward from demonstrations. To solve these issues, we present Context
Hierarchy IRL(CHIRL), a new IRL algorithm that exploits the context to scale up
IRL and learn reward functions of complex behaviors. CHIRL models the context
hierarchically as a directed acyclic graph; it represents the reward function
as a corresponding modular deep neural network that associates each network
module with a node of the context hierarchy. The context hierarchy and the
modular reward representation enable data sharing across multiple contexts and
state abstraction, significantly improving the learning performance. CHIRL has
a natural connection with hierarchical task planning when the context hierarchy
represents subtask decomposition. It enables to incorporate the prior knowledge
of causal dependencies of subtasks and make it capable of solving large complex
tasks by decoupling it into several subtasks and conquering each subtask to
solve the original task. Experiments on benchmark tasks, including a large
scale autonomous driving task in the CARLA simulator, show promising results in
scaling up IRL for tasks with complex reward functions.
",0
$\mathrm{SO}(2)$-Equivariant Reinforcement Learning,"Dian Wang, Robin Walters, Robert Platt",2022-03-08T23:09:25Z,Reinforcement Learning,"  Equivariant neural networks enforce symmetry within the structure of their
convolutional layers, resulting in a substantial improvement in sample
efficiency when learning an equivariant or invariant function. Such models are
applicable to robotic manipulation learning which can often be formulated as a
rotationally symmetric problem. This paper studies equivariant model
architectures in the context of $Q$-learning and actor-critic reinforcement
learning. We identify equivariant and invariant characteristics of the optimal
$Q$-function and the optimal policy and propose equivariant DQN and SAC
algorithms that leverage this structure. We present experiments that
demonstrate that our equivariant versions of DQN and SAC can be significantly
more sample efficient than competing algorithms on an important class of
robotic manipulation problems.
",0
Zipfian environments for Reinforcement Learning,"Stephanie C. Y. Chan, Andrew K. Lampinen, Pierre H. Richemond, Felix Hill",2022-03-15T19:59:10Z,Reinforcement Learning,"  As humans and animals learn in the natural world, they encounter
distributions of entities, situations and events that are far from uniform.
Typically, a relatively small set of experiences are encountered frequently,
while many important experiences occur only rarely. The highly-skewed,
heavy-tailed nature of reality poses particular learning challenges that humans
and animals have met by evolving specialised memory systems. By contrast, most
popular RL environments and benchmarks involve approximately uniform variation
of properties, objects, situations or tasks. How will RL algorithms perform in
worlds (like ours) where the distribution of environment features is far less
uniform? To explore this question, we develop three complementary RL
environments where the agent's experience varies according to a Zipfian
(discrete power law) distribution. On these benchmarks, we find that standard
Deep RL architectures and algorithms acquire useful knowledge of common
situations and tasks, but fail to adequately learn about rarer ones. To
understand this failure better, we explore how different aspects of current
approaches may be adjusted to help improve performance on rare events, and show
that the RL objective function, the agent's memory system and self-supervised
learning objectives can all influence an agent's ability to learn from uncommon
experiences. Together, these results show that learning robustly from skewed
experience is a critical challenge for applying Deep RL methods beyond
simulations or laboratories, and our Zipfian environments provide a basis for
measuring future progress towards this goal.
",14
Gradient dynamics in reinforcement learning,"Riccardo Fabbricatore, Vladimir V. Palyulin",2022-04-08T10:01:54Z,Reinforcement Learning,"  Despite the success achieved by the analysis of supervised learning
algorithms in the framework of statistical mechanics, reinforcement learning
has remained largely untouched. Here we move towards closing the gap by
analyzing the dynamics of the policy gradient algorithm. For a convex problem,
we show that it obeys a drift-diffusion motion with coeffcients tuned by
learning rate. Furthermore, we propose a mapping between a non-convex
reinforcement learning problem and a disordered system. This mapping enables us
to show how the learning rate acts as an effective temperature and thus is
capable of smoothing rough landscapes, corroborating what is displayed by the
drift-diffusive description and paving the way for physics-inspired algorithmic
optimization based on annealing procedures in disordered systems.
",0
Reward Reports for Reinforcement Learning,"Thomas Krendl Gilbert, Nathan Lambert, Sarah Dean, Tom Zick, Aaron Snoswell",2022-04-22T16:53:39Z,Reinforcement Learning,"  Building systems that are good for society in the face of complex societal
effects requires a dynamic approach. Recent approaches to machine learning (ML)
documentation have demonstrated the promise of discursive frameworks for
deliberation about these complexities. However, these developments have been
grounded in a static ML paradigm, leaving the role of feedback and
post-deployment performance unexamined. Meanwhile, recent work in reinforcement
learning has shown that the effects of feedback and optimization objectives on
system behavior can be wide-ranging and unpredictable. In this paper we sketch
a framework for documenting deployed and iteratively updated learning systems,
which we call Reward Reports. Taking inspiration from various contributions to
the technical literature on reinforcement learning, we outline Reward Reports
as living documents that track updates to design choices and assumptions behind
what a particular automated system is optimizing for. They are intended to
track dynamic phenomena arising from system deployment, rather than merely
static properties of models or data. After presenting the elements of a Reward
Report, we discuss a concrete example: Meta's BlenderBot 3 chatbot. Several
others for game-playing (DeepMind's MuZero), content recommendation
(MovieLens), and traffic control (Project Flow) are included in the appendix.
",25
Skill-based Meta-Reinforcement Learning,"Taewook Nam, Shao-Hua Sun, Karl Pertsch, Sung Ju Hwang, Joseph J Lim",2022-04-25T17:58:19Z,Reinforcement Learning,"  While deep reinforcement learning methods have shown impressive results in
robot learning, their sample inefficiency makes the learning of complex,
long-horizon behaviors with real robot systems infeasible. To mitigate this
issue, meta-reinforcement learning methods aim to enable fast learning on novel
tasks by learning how to learn. Yet, the application has been limited to
short-horizon tasks with dense rewards. To enable learning long-horizon
behaviors, recent works have explored leveraging prior experience in the form
of offline datasets without reward or task annotations. While these approaches
yield improved sample efficiency, millions of interactions with environments
are still required to solve complex tasks. In this work, we devise a method
that enables meta-learning on long-horizon, sparse-reward tasks, allowing us to
solve unseen target tasks with orders of magnitude fewer environment
interactions. Our core idea is to leverage prior experience extracted from
offline datasets during meta-learning. Specifically, we propose to (1) extract
reusable skills and a skill prior from offline datasets, (2) meta-train a
high-level policy that learns to efficiently compose learned skills into
long-horizon behaviors, and (3) rapidly adapt the meta-trained policy to solve
an unseen target task. Experimental results on continuous control tasks in
navigation and manipulation demonstrate that the proposed method can
efficiently solve long-horizon novel target tasks by combining the strengths of
meta-learning and the usage of offline datasets, while prior approaches in RL,
meta-RL, and multi-task RL require substantially more environment interactions
to solve the tasks.
",0
Rapid Locomotion via Reinforcement Learning,"Gabriel B Margolis, Ge Yang, Kartik Paigwar, Tao Chen, Pulkit Agrawal",2022-05-05T17:55:11Z,Reinforcement Learning,"  Agile maneuvers such as sprinting and high-speed turning in the wild are
challenging for legged robots. We present an end-to-end learned controller that
achieves record agility for the MIT Mini Cheetah, sustaining speeds up to 3.9
m/s. This system runs and turns fast on natural terrains like grass, ice, and
gravel and responds robustly to disturbances. Our controller is a neural
network trained in simulation via reinforcement learning and transferred to the
real world. The two key components are (i) an adaptive curriculum on velocity
commands and (ii) an online system identification strategy for sim-to-real
transfer leveraged from prior work. Videos of the robot's behaviors are
available at: https://agility.csail.mit.edu/
",0
Efficient Risk-Averse Reinforcement Learning,"Ido Greenberg, Yinlam Chow, Mohammad Ghavamzadeh, Shie Mannor",2022-05-10T19:40:52Z,Reinforcement Learning,"  In risk-averse reinforcement learning (RL), the goal is to optimize some risk
measure of the returns. A risk measure often focuses on the worst returns out
of the agent's experience. As a result, standard methods for risk-averse RL
often ignore high-return strategies. We prove that under certain conditions
this inevitably leads to a local-optimum barrier, and propose a soft risk
mechanism to bypass it. We also devise a novel Cross Entropy module for risk
sampling, which (1) preserves risk aversion despite the soft risk; (2)
independently improves sample efficiency. By separating the risk aversion of
the sampler and the optimizer, we can sample episodes with poor conditions, yet
optimize with respect to successful strategies. We combine these two concepts
in CeSoR - Cross-entropy Soft-Risk optimization algorithm - which can be
applied on top of any risk-averse policy gradient (PG) method. We demonstrate
improved risk aversion in maze navigation, autonomous driving, and resource
allocation benchmarks, including in scenarios where standard risk-averse PG
completely fails.
",0
Delayed Reinforcement Learning by Imitation,"Pierre Liotet, Davide Maran, Lorenzo Bisi, Marcello Restelli",2022-05-11T15:27:33Z,Reinforcement Learning,"  When the agent's observations or interactions are delayed, classic
reinforcement learning tools usually fail. In this paper, we propose a simple
yet new and efficient solution to this problem. We assume that, in the
undelayed environment, an efficient policy is known or can be easily learned,
but the task may suffer from delays in practice and we thus want to take them
into account. We present a novel algorithm, Delayed Imitation with Dataset
Aggregation (DIDA), which builds upon imitation learning methods to learn how
to act in a delayed environment from undelayed demonstrations. We provide a
theoretical analysis of the approach that will guide the practical design of
DIDA. These results are also of general interest in the delayed reinforcement
learning literature by providing bounds on the performance between delayed and
undelayed tasks, under smoothness conditions. We show empirically that DIDA
obtains high performances with a remarkable sample efficiency on a variety of
tasks, including robotic locomotion, classic control, and trading.
",0
User-Interactive Offline Reinforcement Learning,"Phillip Swazinna, Steffen Udluft, Thomas Runkler",2022-05-21T15:50:23Z,Reinforcement Learning,"  Offline reinforcement learning algorithms still lack trust in practice due to
the risk that the learned policy performs worse than the original policy that
generated the dataset or behaves in an unexpected way that is unfamiliar to the
user. At the same time, offline RL algorithms are not able to tune their most
important hyperparameter - the proximity of the learned policy to the original
policy. We propose an algorithm that allows the user to tune this
hyperparameter at runtime, thereby addressing both of the above mentioned
issues simultaneously. This allows users to start with the original behavior
and grant successively greater deviation, as well as stopping at any time when
the policy deteriorates or the behavior is too far from the familiar one.
",0
Reinforcement Learning with a Terminator,"Guy Tennenholtz, Nadav Merlis, Lior Shani, Shie Mannor, Uri Shalit, Gal Chechik, Assaf Hallak, Gal Dalal",2022-05-30T18:40:28Z,Reinforcement Learning,"  We present the problem of reinforcement learning with exogenous termination.
We define the Termination Markov Decision Process (TerMDP), an extension of the
MDP framework, in which episodes may be interrupted by an external
non-Markovian observer. This formulation accounts for numerous real-world
situations, such as a human interrupting an autonomous driving agent for
reasons of discomfort. We learn the parameters of the TerMDP and leverage the
structure of the estimation problem to provide state-wise confidence bounds. We
use these to construct a provably-efficient algorithm, which accounts for
termination, and bound its regret. Motivated by our theoretical analysis, we
design and implement a scalable approach, which combines optimism (w.r.t.
termination) and a dynamic discount factor, incorporating the termination
probability. We deploy our method on high-dimensional driving and MinAtar
benchmarks. Additionally, we test our approach on human data in a driving
setting. Our results demonstrate fast convergence and significant improvement
over various baseline approaches.
",0
Efficient entity-based reinforcement learning,"Vince Jankovics, Michael Garcia Ortiz, Eduardo Alonso",2022-06-06T19:02:39Z,Reinforcement Learning,"  Recent deep reinforcement learning (DRL) successes rely on end-to-end
learning from fixed-size observational inputs (e.g. image, state-variables).
However, many challenging and interesting problems in decision making involve
observations or intermediary representations which are best described as a set
of entities: either the image-based approach would miss small but important
details in the observations (e.g. ojects on a radar, vehicles on satellite
images, etc.), the number of sensed objects is not fixed (e.g. robotic
manipulation), or the problem simply cannot be represented in a meaningful way
as an image (e.g. power grid control, or logistics). This type of structured
representations is not directly compatible with current DRL architectures,
however, there has been an increase in machine learning techniques directly
targeting structured information, potentially addressing this issue. We propose
to combine recent advances in set representations with slot attention and graph
neural networks to process structured data, broadening the range of
applications of DRL algorithms. This approach allows to address entity-based
problems in an efficient and scalable way. We show that it can improve training
time and robustness significantly, and demonstrate their potential to handle
structured as well as purely visual domains, on multiple environments from the
Atari Learning Environment and Simple Playgrounds.
",0
Receding Horizon Inverse Reinforcement Learning,"Yiqing Xu, Wei Gao, David Hsu",2022-06-09T13:03:27Z,Reinforcement Learning,"  Inverse reinforcement learning (IRL) seeks to infer a cost function that
explains the underlying goals and preferences of expert demonstrations. This
paper presents receding horizon inverse reinforcement learning (RHIRL), a new
IRL algorithm for high-dimensional, noisy, continuous systems with black-box
dynamic models. RHIRL addresses two key challenges of IRL: scalability and
robustness. To handle high-dimensional continuous systems, RHIRL matches the
induced optimal trajectories with expert demonstrations locally in a receding
horizon manner and 'stitches' together the local solutions to learn the cost;
it thereby avoids the 'curse of dimensionality'. This contrasts sharply with
earlier algorithms that match with expert demonstrations globally over the
entire high-dimensional state space. To be robust against imperfect expert
demonstrations and control noise, RHIRL learns a state-dependent cost function
'disentangled' from system dynamics under mild conditions. Experiments on
benchmark tasks show that RHIRL outperforms several leading IRL algorithms in
most instances. We also prove that the cumulative error of RHIRL grows linearly
with the task duration.
",9
Reinforcement learning based adaptive metaheuristics,"Michele Tessari, Giovanni Iacca",2022-06-24T12:01:49Z,Reinforcement Learning,"  Parameter adaptation, that is the capability to automatically adjust an
algorithm's hyperparameters depending on the problem being faced, is one of the
main trends in evolutionary computation applied to numerical optimization.
While several handcrafted adaptation policies have been proposed over the years
to address this problem, only few attempts have been done so far at applying
machine learning to learn such policies. Here, we introduce a general-purpose
framework for performing parameter adaptation in continuous-domain
metaheuristics based on state-of-the-art reinforcement learning algorithms. We
demonstrate the applicability of this framework on two algorithms, namely
Covariance Matrix Adaptation Evolution Strategies (CMA-ES) and Differential
Evolution (DE), for which we learn, respectively, adaptation policies for the
step-size (for CMA-ES), and the scale factor and crossover rate (for DE). We
train these policies on a set of 46 benchmark functions at different
dimensionalities, with various inputs to the policies, in two settings: one
policy per function, and one global policy for all functions. Compared,
respectively, to the Cumulative Step-size Adaptation (CSA) policy and to two
well-known adaptive DE variants (iDE and jDE), our policies are able to produce
competitive results in the majority of cases, especially in the case of DE.
",0
Reinforcement Learning Assisted Recursive QAOA,"Yash J. Patel, Sofiene Jerbi, Thomas Bäck, Vedran Dunjko",2022-07-13T15:46:58Z,Reinforcement Learning,"  Variational quantum algorithms such as the Quantum Approximation Optimization
Algorithm (QAOA) in recent years have gained popularity as they provide the
hope of using NISQ devices to tackle hard combinatorial optimization problems.
It is, however, known that at low depth, certain locality constraints of QAOA
limit its performance. To go beyond these limitations, a non-local variant of
QAOA, namely recursive QAOA (RQAOA), was proposed to improve the quality of
approximate solutions. The RQAOA has been studied comparatively less than QAOA,
and it is less understood, for instance, for what family of instances it may
fail to provide high quality solutions. However, as we are tackling
$\mathsf{NP}$-hard problems (specifically, the Ising spin model), it is
expected that RQAOA does fail, raising the question of designing even better
quantum algorithms for combinatorial optimization. In this spirit, we identify
and analyze cases where RQAOA fails and, based on this, propose a reinforcement
learning enhanced RQAOA variant (RL-RQAOA) that improves upon RQAOA. We show
that the performance of RL-RQAOA improves over RQAOA: RL-RQAOA is strictly
better on these identified instances where RQAOA underperforms, and is
similarly performing on instances where RQAOA is near-optimal. Our work
exemplifies the potentially beneficial synergy between reinforcement learning
and quantum (inspired) optimization in the design of new, even better
heuristics for hard problems.
",0
Continual Reinforcement Learning with TELLA,"Neil Fendley, Cash Costello, Eric Nguyen, Gino Perrotta, Corey Lowman",2022-08-08T17:25:55Z,Reinforcement Learning,"  Training reinforcement learning agents that continually learn across multiple
environments is a challenging problem. This is made more difficult by a lack of
reproducible experiments and standard metrics for comparing different continual
learning approaches. To address this, we present TELLA, a tool for the Test and
Evaluation of Lifelong Learning Agents. TELLA provides specified, reproducible
curricula to lifelong learning agents while logging detailed data for
evaluation and standardized analysis. Researchers can define and share their
own curricula over various learning environments or run against a curriculum
created under the DARPA Lifelong Learning Machines (L2M) Program.
",0
Choquet regularization for reinforcement learning,"Xia Han, Ruodu Wang, Xun Yu Zhou",2022-08-17T19:32:24Z,Reinforcement Learning,"  We propose \emph{Choquet regularizers} to measure and manage the level of
exploration for reinforcement learning (RL), and reformulate the
continuous-time entropy-regularized RL problem of Wang et al. (2020, JMLR,
21(198)) in which we replace the differential entropy used for regularization
with a Choquet regularizer. We derive the Hamilton--Jacobi--Bellman equation of
the problem, and solve it explicitly in the linear--quadratic (LQ) case via
maximizing statically a mean--variance constrained Choquet regularizer. Under
the LQ setting, we derive explicit optimal distributions for several specific
Choquet regularizers, and conversely identify the Choquet regularizers that
generate a number of broadly used exploratory samplers such as
$\epsilon$-greedy, exponential, uniform and Gaussian.
",0
Distributionally Adaptive Meta Reinforcement Learning,"Anurag Ajay, Abhishek Gupta, Dibya Ghosh, Sergey Levine, Pulkit Agrawal",2022-10-06T17:55:09Z,Reinforcement Learning,"  Meta-reinforcement learning algorithms provide a data-driven way to acquire
policies that quickly adapt to many tasks with varying rewards or dynamics
functions. However, learned meta-policies are often effective only on the exact
task distribution on which they were trained and struggle in the presence of
distribution shift of test-time rewards or transition dynamics. In this work,
we develop a framework for meta-RL algorithms that are able to behave
appropriately under test-time distribution shifts in the space of tasks. Our
framework centers on an adaptive approach to distributional robustness that
trains a population of meta-policies to be robust to varying levels of
distribution shift. When evaluated on a potentially shifted test-time
distribution of tasks, this allows us to choose the meta-policy with the most
appropriate level of robustness, and use it to perform fast adaptation. We
formally show how our framework allows for improved regret under distribution
shift, and empirically show its efficacy on simulated robotics problems under a
wide range of distribution shifts.
",0
Experiential Explanations for Reinforcement Learning,"Amal Alabdulkarim, Madhuri Singh, Gennie Mansi, Kaely Hall, Mark O. Riedl",2022-10-10T14:27:53Z,Reinforcement Learning,"  Reinforcement Learning (RL) systems can be complex and non-interpretable,
making it challenging for non-AI experts to understand or intervene in their
decisions. This is due in part to the sequential nature of RL in which actions
are chosen because of future rewards. However, RL agents discard the
qualitative features of their training, making it difficult to recover
user-understandable information for ""why"" an action is chosen. We propose a
technique, Experiential Explanations, to generate counterfactual explanations
by training influence predictors along with the RL policy. Influence predictors
are models that learn how sources of reward affect the agent in different
states, thus restoring information about how the policy reflects the
environment. A human evaluation study revealed that participants presented with
experiential explanations were better able to correctly guess what an agent
would do than those presented with other standard types of explanation.
Participants also found that experiential explanations are more understandable,
satisfying, complete, useful, and accurate. The qualitative analysis provides
insights into the factors of experiential explanations that are most useful.
",0
Object-Category Aware Reinforcement Learning,"Qi Yi, Rui Zhang, Shaohui Peng, Jiaming Guo, Xing Hu, Zidong Du, Xishan Zhang, Qi Guo, Yunji Chen",2022-10-13T11:31:32Z,Reinforcement Learning,"  Object-oriented reinforcement learning (OORL) is a promising way to improve
the sample efficiency and generalization ability over standard RL. Recent works
that try to solve OORL tasks without additional feature engineering mainly
focus on learning the object representations and then solving tasks via
reasoning based on these object representations. However, none of these works
tries to explicitly model the inherent similarity between different object
instances of the same category. Objects of the same category should share
similar functionalities; therefore, the category is the most critical property
of an object. Following this insight, we propose a novel framework named
Object-Category Aware Reinforcement Learning (OCARL), which utilizes the
category information of objects to facilitate both perception and reasoning.
OCARL consists of three parts: (1) Category-Aware Unsupervised Object Discovery
(UOD), which discovers the objects as well as their corresponding categories;
(2) Object-Category Aware Perception, which encodes the category information
and is also robust to the incompleteness of (1) at the same time; (3)
Object-Centric Modular Reasoning, which adopts multiple independent and
object-category-specific networks when reasoning based on objects. Our
experiments show that OCARL can improve both the sample efficiency and
generalization in the OORL domain.
",0
Hypernetworks in Meta-Reinforcement Learning,"Jacob Beck, Matthew Thomas Jackson, Risto Vuorio, Shimon Whiteson",2022-10-20T15:34:52Z,Reinforcement Learning,"  Training a reinforcement learning (RL) agent on a real-world robotics task
remains generally impractical due to sample inefficiency. Multi-task RL and
meta-RL aim to improve sample efficiency by generalizing over a distribution of
related tasks. However, doing so is difficult in practice: In multi-task RL,
state of the art methods often fail to outperform a degenerate solution that
simply learns each task separately. Hypernetworks are a promising path forward
since they replicate the separate policies of the degenerate solution while
also allowing for generalization across tasks, and are applicable to meta-RL.
However, evidence from supervised learning suggests hypernetwork performance is
highly sensitive to the initialization. In this paper, we 1) show that
hypernetwork initialization is also a critical factor in meta-RL, and that
naive initializations yield poor performance; 2) propose a novel hypernetwork
initialization scheme that matches or exceeds the performance of a
state-of-the-art approach proposed for supervised settings, as well as being
simpler and more general; and 3) use this method to show that hypernetworks can
improve performance in meta-RL by evaluating on multiple simulated robotics
benchmarks.
",0
Quantum deep recurrent reinforcement learning,Samuel Yen-Chi Chen,2022-10-26T17:29:19Z,Reinforcement Learning,"  Recent advances in quantum computing (QC) and machine learning (ML) have
drawn significant attention to the development of quantum machine learning
(QML). Reinforcement learning (RL) is one of the ML paradigms which can be used
to solve complex sequential decision making problems. Classical RL has been
shown to be capable to solve various challenging tasks. However, RL algorithms
in the quantum world are still in their infancy. One of the challenges yet to
solve is how to train quantum RL in the partially observable environments. In
this paper, we approach this challenge through building QRL agents with quantum
recurrent neural networks (QRNN). Specifically, we choose the quantum long
short-term memory (QLSTM) to be the core of the QRL agent and train the whole
model with deep $Q$-learning. We demonstrate the results via numerical
simulations that the QLSTM-DRQN can solve standard benchmark such as Cart-Pole
with more stable and higher average scores than classical DRQN with similar
architecture and number of model parameters.
",0
Dual Generator Offline Reinforcement Learning,"Quan Vuong, Aviral Kumar, Sergey Levine, Yevgen Chebotar",2022-11-02T20:25:18Z,Reinforcement Learning,"  In offline RL, constraining the learned policy to remain close to the data is
essential to prevent the policy from outputting out-of-distribution (OOD)
actions with erroneously overestimated values. In principle, generative
adversarial networks (GAN) can provide an elegant solution to do so, with the
discriminator directly providing a probability that quantifies distributional
shift. However, in practice, GAN-based offline RL methods have not performed as
well as alternative approaches, perhaps because the generator is trained to
both fool the discriminator and maximize return -- two objectives that can be
at odds with each other. In this paper, we show that the issue of conflicting
objectives can be resolved by training two generators: one that maximizes
return, with the other capturing the ``remainder'' of the data distribution in
the offline dataset, such that the mixture of the two is close to the behavior
policy. We show that not only does having two generators enable an effective
GAN-based offline RL method, but also approximates a support constraint, where
the policy does not need to match the entire data distribution, but only the
slice of the data that leads to high long term performance. We name our method
DASCO, for Dual-Generator Adversarial Support Constrained Offline RL. On
benchmark tasks that require learning from sub-optimal data, DASCO
significantly outperforms prior methods that enforce distribution constraint.
",0
Online Shielding for Reinforcement Learning,"Bettina Könighofer, Julian Rudolf, Alexander Palmisano, Martin Tappler, Roderick Bloem",2022-12-04T16:00:29Z,Reinforcement Learning,"  Besides the recent impressive results on reinforcement learning (RL), safety
is still one of the major research challenges in RL. RL is a machine-learning
approach to determine near-optimal policies in Markov decision processes
(MDPs). In this paper, we consider the setting where the safety-relevant
fragment of the MDP together with a temporal logic safety specification is
given and many safety violations can be avoided by planning ahead a short time
into the future. We propose an approach for online safety shielding of RL
agents. During runtime, the shield analyses the safety of each available
action. For any action, the shield computes the maximal probability to not
violate the safety specification within the next $k$ steps when executing this
action. Based on this probability and a given threshold, the shield decides
whether to block an action from the agent. Existing offline shielding
approaches compute exhaustively the safety of all state-action combinations
ahead of time, resulting in huge computation times and large memory
consumption. The intuition behind online shielding is to compute at runtime the
set of all states that could be reached in the near future. For each of these
states, the safety of all available actions is analysed and used for shielding
as soon as one of the considered states is reached. Our approach is well suited
for high-level planning problems where the time between decisions can be used
for safety computations and it is sustainable for the agent to wait until these
computations are finished. For our evaluation, we selected a 2-player version
of the classical computer game SNAKE. The game represents a high-level planning
problem that requires fast decisions and the multiplayer setting induces a
large state space, which is computationally expensive to analyse exhaustively.
",0
Misspecification in Inverse Reinforcement Learning,"Joar Skalse, Alessandro Abate",2022-12-06T18:21:47Z,Reinforcement Learning,"  The aim of Inverse Reinforcement Learning (IRL) is to infer a reward function
$R$ from a policy $\pi$. To do this, we need a model of how $\pi$ relates to
$R$. In the current literature, the most common models are optimality,
Boltzmann rationality, and causal entropy maximisation. One of the primary
motivations behind IRL is to infer human preferences from human behaviour.
However, the true relationship between human preferences and human behaviour is
much more complex than any of the models currently used in IRL. This means that
they are misspecified, which raises the worry that they might lead to unsound
inferences if applied to real-world data. In this paper, we provide a
mathematical analysis of how robust different IRL models are to
misspecification, and answer precisely how the demonstrator policy may differ
from each of the standard models before that model leads to faulty inferences
about the reward function $R$. We also introduce a framework for reasoning
about misspecification in IRL, together with formal tools that can be used to
easily derive the misspecification robustness of new IRL models.
",0
Reinforcement Learning in System Identification,"Jose Antonio Martin H., Oscar Fernandez Vicente, Sergio Perez, Anas Belfadil, Cristina Ibanez-Llano, Freddy Jose Perozo Rondon, Jose Javier Valle, Javier Arechalde Pelaz",2022-12-14T09:20:42Z,Reinforcement Learning,"  System identification, also known as learning forward models, transfer
functions, system dynamics, etc., has a long tradition both in science and
engineering in different fields. Particularly, it is a recurring theme in
Reinforcement Learning research, where forward models approximate the state
transition function of a Markov Decision Process by learning a mapping function
from current state and action to the next state. This problem is commonly
defined as a Supervised Learning problem in a direct way. This common approach
faces several difficulties due to the inherent complexities of the dynamics to
learn, for example, delayed effects, high non-linearity, non-stationarity,
partial observability and, more important, error accumulation when using
bootstrapped predictions (predictions based on past predictions), over large
time horizons. Here we explore the use of Reinforcement Learning in this
problem. We elaborate on why and how this problem fits naturally and sound as a
Reinforcement Learning problem, and present some experimental results that
demonstrate RL is a promising technique to solve these kind of problems.
",0
SHIRO: Soft Hierarchical Reinforcement Learning,"Kandai Watanabe, Mathew Strong, Omer Eldar",2022-12-24T17:21:58Z,Reinforcement Learning,"  Hierarchical Reinforcement Learning (HRL) algorithms have been demonstrated
to perform well on high-dimensional decision making and robotic control tasks.
However, because they solely optimize for rewards, the agent tends to search
the same space redundantly. This problem reduces the speed of learning and
achieved reward. In this work, we present an Off-Policy HRL algorithm that
maximizes entropy for efficient exploration. The algorithm learns a temporally
abstracted low-level policy and is able to explore broadly through the addition
of entropy to the high-level. The novelty of this work is the theoretical
motivation of adding entropy to the RL objective in the HRL setting. We
empirically show that the entropy can be added to both levels if the
Kullback-Leibler (KL) divergence between consecutive updates of the low-level
policy is sufficiently small. We performed an ablative study to analyze the
effects of entropy on hierarchy, in which adding entropy to high-level emerged
as the most desirable configuration. Furthermore, a higher temperature in the
low-level leads to Q-value overestimation and increases the stochasticity of
the environment that the high-level operates on, making learning more
challenging. Our method, SHIRO, surpasses state-of-the-art performance on a
range of simulated robotic control benchmark tasks and requires minimal tuning.
",0
Lexicographic Multi-Objective Reinforcement Learning,"Joar Skalse, Lewis Hammond, Charlie Griffin, Alessandro Abate",2022-12-28T10:22:36Z,Reinforcement Learning,"  In this work we introduce reinforcement learning techniques for solving
lexicographic multi-objective problems. These are problems that involve
multiple reward signals, and where the goal is to learn a policy that maximises
the first reward signal, and subject to this constraint also maximises the
second reward signal, and so on. We present a family of both action-value and
policy gradient algorithms that can be used to solve such problems, and prove
that they converge to policies that are lexicographically optimal. We evaluate
the scalability and performance of these algorithms empirically, demonstrating
their practical applicability. As a more specific application, we show how our
algorithms can be used to impose safety constraints on the behaviour of an
agent, and compare their performance in this context with that of other
constrained reinforcement learning algorithms.
",0
Heterogeneous Multi-Robot Reinforcement Learning,"Matteo Bettini, Ajay Shankar, Amanda Prorok",2023-01-17T19:05:17Z,Reinforcement Learning,"  Cooperative multi-robot tasks can benefit from heterogeneity in the robots'
physical and behavioral traits. In spite of this, traditional Multi-Agent
Reinforcement Learning (MARL) frameworks lack the ability to explicitly
accommodate policy heterogeneity, and typically constrain agents to share
neural network parameters. This enforced homogeneity limits application in
cases where the tasks benefit from heterogeneous behaviors. In this paper, we
crystallize the role of heterogeneity in MARL policies. Towards this end, we
introduce Heterogeneous Graph Neural Network Proximal Policy Optimization
(HetGPPO), a paradigm for training heterogeneous MARL policies that leverages a
Graph Neural Network for differentiable inter-agent communication. HetGPPO
allows communicating agents to learn heterogeneous behaviors while enabling
fully decentralized training in partially observable environments. We
complement this with a taxonomical overview that exposes more heterogeneity
classes than previously identified. To motivate the need for our model, we
present a characterization of techniques that homogeneous models can leverage
to emulate heterogeneous behavior, and show how this ""apparent heterogeneity""
is brittle in real-world conditions. Through simulations and real-world
experiments, we show that: (i) when homogeneous methods fail due to strong
heterogeneous requirements, HetGPPO succeeds, and, (ii) when homogeneous
methods are able to learn apparently heterogeneous behaviors, HetGPPO achieves
higher resilience to both training and deployment noise.
",0
STEEL: Singularity-aware Reinforcement Learning,"Xiaohong Chen, Zhengling Qi, Runzhe Wan",2023-01-30T18:29:35Z,Reinforcement Learning,"  Batch reinforcement learning (RL) aims at leveraging pre-collected data to
find an optimal policy that maximizes the expected total rewards in a dynamic
environment. The existing methods require absolutely continuous assumption
(e.g., there do not exist non-overlapping regions) on the distribution induced
by target policies with respect to the data distribution over either the state
or action or both. We propose a new batch RL algorithm that allows for
singularity for both state and action spaces (e.g., existence of
non-overlapping regions between offline data distribution and the distribution
induced by the target policies) in the setting of an infinite-horizon Markov
decision process with continuous states and actions. We call our algorithm
STEEL: SingulariTy-awarE rEinforcement Learning. Our algorithm is motivated by
a new error analysis on off-policy evaluation, where we use maximum mean
discrepancy, together with distributionally robust optimization, to
characterize the error of off-policy evaluation caused by the possible
singularity and to enable model extrapolation. By leveraging the idea of
pessimism and under some technical conditions, we derive a first finite-sample
regret guarantee for our proposed algorithm under singularity. Compared with
existing algorithms,by requiring only minimal data-coverage assumption, STEEL
improves the applicability and robustness of batch RL. In addition, a two-step
adaptive STEEL, which is nearly tuning-free, is proposed. Extensive simulation
studies and one (semi)-real experiment on personalized pricing demonstrate the
superior performance of our methods in dealing with possible singularity in
batch RL.
",0
Reinforcement Learning with Depreciating Assets,"Taylor Dohmen, Ashutosh Trivedi",2023-02-27T22:28:58Z,Reinforcement Learning,"  A basic assumption of traditional reinforcement learning is that the value of
a reward does not change once it is received by an agent. The present work
forgoes this assumption and considers the situation where the value of a reward
decays proportionally to the time elapsed since it was obtained. Emphasizing
the inflection point occurring at the time of payment, we use the term asset to
refer to a reward that is currently in the possession of an agent. Adopting
this language, we initiate the study of depreciating assets within the
framework of infinite-horizon quantitative optimization. In particular, we
propose a notion of asset depreciation, inspired by classical exponential
discounting, where the value of an asset is scaled by a fixed discount factor
at each time step after it is obtained by the agent. We formulate a
Bellman-style equational characterization of optimality in this context and
develop a model-free reinforcement learning approach to obtain optimal
policies.
",0
Ensemble Reinforcement Learning: A Survey,"Yanjie Song, P. N. Suganthan, Witold Pedrycz, Junwei Ou, Yongming He, Yingwu Chen, Yutong Wu",2023-03-05T09:26:44Z,Reinforcement Learning,"  Reinforcement Learning (RL) has emerged as a highly effective technique for
addressing various scientific and applied problems. Despite its success,
certain complex tasks remain challenging to be addressed solely with a single
model and algorithm. In response, ensemble reinforcement learning (ERL), a
promising approach that combines the benefits of both RL and ensemble learning
(EL), has gained widespread popularity. ERL leverages multiple models or
training algorithms to comprehensively explore the problem space and possesses
strong generalization capabilities. In this study, we present a comprehensive
survey on ERL to provide readers with an overview of recent advances and
challenges in the field. Firstly, we provide an introduction to the background
and motivation for ERL. Secondly, we conduct a detailed analysis of strategies
such as model selection and combination that have been successfully implemented
in ERL. Subsequently, we explore the application of ERL, summarize the
datasets, and analyze the algorithms employed. Finally, we outline several open
questions and discuss future research directions of ERL. By offering guidance
for future scientific research and engineering applications, this survey
significantly contributes to the advancement of ERL.
",20
Evolutionary Reinforcement Learning: A Survey,"Hui Bai, Ran Cheng, Yaochu Jin",2023-03-07T01:38:42Z,Reinforcement Learning,"  Reinforcement learning (RL) is a machine learning approach that trains agents
to maximize cumulative rewards through interactions with environments. The
integration of RL with deep learning has recently resulted in impressive
achievements in a wide range of challenging tasks, including board games,
arcade games, and robot control. Despite these successes, there remain several
crucial challenges, including brittle convergence properties caused by
sensitive hyperparameters, difficulties in temporal credit assignment with long
time horizons and sparse rewards, a lack of diverse exploration, especially in
continuous search space scenarios, difficulties in credit assignment in
multi-agent reinforcement learning, and conflicting objectives for rewards.
Evolutionary computation (EC), which maintains a population of learning agents,
has demonstrated promising performance in addressing these limitations. This
article presents a comprehensive survey of state-of-the-art methods for
integrating EC into RL, referred to as evolutionary reinforcement learning
(EvoRL). We categorize EvoRL methods according to key research fields in RL,
including hyperparameter optimization, policy search, exploration, reward
shaping, meta-RL, and multi-objective RL. We then discuss future research
directions in terms of efficient methods, benchmarks, and scalable platforms.
This survey serves as a resource for researchers and practitioners interested
in the field of EvoRL, highlighting the important challenges and opportunities
for future research. With the help of this survey, researchers and
practitioners can develop more efficient methods and tailored benchmarks for
EvoRL, further advancing this promising cross-disciplinary research field.
",0
Empirical Design in Reinforcement Learning,"Andrew Patterson, Samuel Neumann, Martha White, Adam White",2023-04-03T19:32:24Z,Reinforcement Learning,"  Empirical design in reinforcement learning is no small task. Running good
experiments requires attention to detail and at times significant computational
resources. While compute resources available per dollar have continued to grow
rapidly, so have the scale of typical experiments in reinforcement learning. It
is now common to benchmark agents with millions of parameters against dozens of
tasks, each using the equivalent of 30 days of experience. The scale of these
experiments often conflict with the need for proper statistical evidence,
especially when comparing algorithms. Recent studies have highlighted how
popular algorithms are sensitive to hyper-parameter settings and implementation
details, and that common empirical practice leads to weak statistical evidence
(Machado et al., 2018; Henderson et al., 2018). Here we take this one step
further.
  This manuscript represents both a call to action, and a comprehensive
resource for how to do good experiments in reinforcement learning. In
particular, we cover: the statistical assumptions underlying common performance
measures, how to properly characterize performance variation and stability,
hypothesis testing, special considerations for comparing multiple agents,
baseline and illustrative example construction, and how to deal with
hyper-parameters and experimenter bias. Throughout we highlight common mistakes
found in the literature and the statistical consequences of those in example
experiments. The objective of this document is to provide answers on how we can
use our unprecedented compute to do good science in reinforcement learning, as
well as stay alert to potential pitfalls in our empirical design.
",0
Reinforcement Learning Quantum Local Search,"Chen-Yu Liu, Hsi-Sheng Goan",2023-04-13T13:07:19Z,Reinforcement Learning,"  Quantum Local Search (QLS) is a promising approach that employs small-scale
quantum computers to tackle large combinatorial optimization problems through
local search on quantum hardware, starting from an initial point. However, the
random selection of the sub-problem to solve in QLS may not be efficient. In
this study, we propose a reinforcement learning (RL) based approach to train an
agent for improved subproblem selection in QLS, beyond random selection. Our
results demonstrate that the RL agent effectively enhances the average
approximation ratio of QLS on fully-connected random Ising problems, indicating
the potential of combining RL techniques with Noisy Intermediate-scale Quantum
(NISQ) algorithms. This research opens a promising direction for integrating RL
into quantum computing to enhance the performance of optimization tasks.
",0
Evolving Constrained Reinforcement Learning Policy,"Chengpeng Hu, Jiyuan Pei, Jialin Liu, Xin Yao",2023-04-19T03:54:31Z,Reinforcement Learning,"  Evolutionary algorithms have been used to evolve a population of actors to
generate diverse experiences for training reinforcement learning agents, which
helps to tackle the temporal credit assignment problem and improves the
exploration efficiency. However, when adapting this approach to address
constrained problems, balancing the trade-off between the reward and constraint
violation is hard. In this paper, we propose a novel evolutionary constrained
reinforcement learning (ECRL) algorithm, which adaptively balances the reward
and constraint violation with stochastic ranking, and at the same time,
restricts the policy's behaviour by maintaining a set of Lagrange relaxation
coefficients with a constraint buffer. Extensive experiments on robotic control
benchmarks show that our ECRL achieves outstanding performance compared to
state-of-the-art algorithms. Ablation analysis shows the benefits of
introducing stochastic ranking and constraint buffer.
",0
One-Step Distributional Reinforcement Learning,"Mastane Achab, Reda Alami, Yasser Abdelaziz Dahou Djilali, Kirill Fedyanin, Eric Moulines",2023-04-27T06:57:00Z,Reinforcement Learning,"  Reinforcement learning (RL) allows an agent interacting sequentially with an
environment to maximize its long-term expected return. In the distributional RL
(DistrRL) paradigm, the agent goes beyond the limit of the expected value, to
capture the underlying probability distribution of the return across all time
steps. The set of DistrRL algorithms has led to improved empirical performance.
Nevertheless, the theory of DistrRL is still not fully understood, especially
in the control case. In this paper, we present the simpler one-step
distributional reinforcement learning (OS-DistrRL) framework encompassing only
the randomness induced by the one-step dynamics of the environment. Contrary to
DistrRL, we show that our approach comes with a unified theory for both policy
evaluation and control. Indeed, we propose two OS-DistrRL algorithms for which
we provide an almost sure convergence analysis. The proposed approach compares
favorably with categorical DistrRL on various environments.
",0
Reinforcement Learning for Topic Models,"Jeremy Costello, Marek Z. Reformat",2023-05-08T16:41:08Z,Reinforcement Learning,"  We apply reinforcement learning techniques to topic modeling by replacing the
variational autoencoder in ProdLDA with a continuous action space reinforcement
learning policy. We train the system with a policy gradient algorithm
REINFORCE. Additionally, we introduced several modifications: modernize the
neural network architecture, weight the ELBO loss, use contextual embeddings,
and monitor the learning process via computing topic diversity and coherence
for each training step. Experiments are performed on 11 data sets. Our
unsupervised model outperforms all other unsupervised models and performs on
par with or better than most models using supervised labeling. Our model is
outperformed on certain data sets by a model using supervised labeling and
contrastive learning. We have also conducted an ablation study to provide
empirical evidence of performance improvements from changes we made to ProdLDA
and found that the reinforcement learning formulation boosts performance.
",0
Latent Exploration for Reinforcement Learning,"Alberto Silvio Chiappa, Alessandro Marin Vargas, Ann Zixiang Huang, Alexander Mathis",2023-05-31T17:40:43Z,Reinforcement Learning,"  In Reinforcement Learning, agents learn policies by exploring and interacting
with the environment. Due to the curse of dimensionality, learning policies
that map high-dimensional sensory input to motor output is particularly
challenging. During training, state of the art methods (SAC, PPO, etc.) explore
the environment by perturbing the actuation with independent Gaussian noise.
While this unstructured exploration has proven successful in numerous tasks, it
can be suboptimal for overactuated systems. When multiple actuators, such as
motors or muscles, drive behavior, uncorrelated perturbations risk diminishing
each other's effect, or modifying the behavior in a task-irrelevant way. While
solutions to introduce time correlation across action perturbations exist,
introducing correlation across actuators has been largely ignored. Here, we
propose LATent TIme-Correlated Exploration (Lattice), a method to inject
temporally-correlated noise into the latent state of the policy network, which
can be seamlessly integrated with on- and off-policy algorithms. We demonstrate
that the noisy actions generated by perturbing the network's activations can be
modeled as a multivariate Gaussian distribution with a full covariance matrix.
In the PyBullet locomotion tasks, Lattice-SAC achieves state of the art
results, and reaches 18% higher reward than unstructured exploration in the
Humanoid environment. In the musculoskeletal control environments of MyoSuite,
Lattice-PPO achieves higher reward in most reaching and object manipulation
tasks, while also finding more energy-efficient policies with reductions of
20-60%. Overall, we demonstrate the effectiveness of structured action noise in
time and actuator space for complex motor control tasks. The code is available
at: https://github.com/amathislab/lattice.
",0
Mediated Multi-Agent Reinforcement Learning,"Dmitry Ivanov, Ilya Zisman, Kirill Chernyshev",2023-06-14T10:31:37Z,Reinforcement Learning,"  The majority of Multi-Agent Reinforcement Learning (MARL) literature equates
the cooperation of self-interested agents in mixed environments to the problem
of social welfare maximization, allowing agents to arbitrarily share rewards
and private information. This results in agents that forgo their individual
goals in favour of social good, which can potentially be exploited by selfish
defectors. We argue that cooperation also requires agents' identities and
boundaries to be respected by making sure that the emergent behaviour is an
equilibrium, i.e., a convention that no agent can deviate from and receive
higher individual payoffs. Inspired by advances in mechanism design, we propose
to solve the problem of cooperation, defined as finding socially beneficial
equilibrium, by using mediators. A mediator is a benevolent entity that may act
on behalf of agents, but only for the agents that agree to it. We show how a
mediator can be trained alongside agents with policy gradient to maximize
social welfare subject to constraints that encourage agents to cooperate
through the mediator. Our experiments in matrix and iterative games highlight
the potential power of applying mediators in MARL.
",0
Simplified Temporal Consistency Reinforcement Learning,"Yi Zhao, Wenshuai Zhao, Rinu Boney, Juho Kannala, Joni Pajarinen",2023-06-15T19:37:43Z,Reinforcement Learning,"  Reinforcement learning is able to solve complex sequential decision-making
tasks but is currently limited by sample efficiency and required computation.
To improve sample efficiency, recent work focuses on model-based RL which
interleaves model learning with planning. Recent methods further utilize policy
learning, value estimation, and, self-supervised learning as auxiliary
objectives. In this paper we show that, surprisingly, a simple representation
learning approach relying only on a latent dynamics model trained by latent
temporal consistency is sufficient for high-performance RL. This applies when
using pure planning with a dynamics model conditioned on the representation,
but, also when utilizing the representation as policy and value function
features in model-free RL. In experiments, our approach learns an accurate
dynamics model to solve challenging high-dimensional locomotion tasks with
online planners while being 4.1 times faster to train compared to
ensemble-based methods. With model-free RL without planning, especially on
high-dimensional tasks, such as the DeepMind Control Suite Humanoid and Dog
tasks, our approach outperforms model-free methods by a large margin and
matches model-based methods' sample efficiency while training 2.4 times faster.
",0
Bootstrapped Representations in Reinforcement Learning,"Charline Le Lan, Stephen Tu, Mark Rowland, Anna Harutyunyan, Rishabh Agarwal, Marc G. Bellemare, Will Dabney",2023-06-16T20:14:07Z,Reinforcement Learning,"  In reinforcement learning (RL), state representations are key to dealing with
large or continuous state spaces. While one of the promises of deep learning
algorithms is to automatically construct features well-tuned for the task they
try to solve, such a representation might not emerge from end-to-end training
of deep RL agents. To mitigate this issue, auxiliary objectives are often
incorporated into the learning process and help shape the learnt state
representation. Bootstrapping methods are today's method of choice to make
these additional predictions. Yet, it is unclear which features these
algorithms capture and how they relate to those from other auxiliary-task-based
approaches. In this paper, we address this gap and provide a theoretical
characterization of the state representation learnt by temporal difference
learning (Sutton, 1988). Surprisingly, we find that this representation differs
from the features learned by Monte Carlo and residual gradient algorithms for
most transition structures of the environment in the policy evaluation setting.
We describe the efficacy of these representations for policy evaluation, and
use our theoretical analysis to design new auxiliary learning rules. We
complement our theoretical results with an empirical comparison of these
learning rules for different cumulant functions on classic domains such as the
four-room domain (Sutton et al, 1999) and Mountain Car (Moore, 1990).
",0
Optimal Execution Using Reinforcement Learning,"Cong Zheng, Jiafa He, Can Yang",2023-06-19T07:09:59Z,Reinforcement Learning,"  This work is about optimal order execution, where a large order is split into
several small orders to maximize the implementation shortfall. Based on the
diversity of cryptocurrency exchanges, we attempt to extract cross-exchange
signals by aligning data from multiple exchanges for the first time. Unlike
most previous studies that focused on using single-exchange information, we
discuss the impact of cross-exchange signals on the agent's decision-making in
the optimal execution problem. Experimental results show that cross-exchange
signals can provide additional information for the optimal execution of
cryptocurrency to facilitate the optimal execution process.
",0
Causal Reinforcement Learning: A Survey,"Zhihong Deng, Jing Jiang, Guodong Long, Chengqi Zhang",2023-07-04T03:00:43Z,Reinforcement Learning,"  Reinforcement learning is an essential paradigm for solving sequential
decision problems under uncertainty. Despite many remarkable achievements in
recent decades, applying reinforcement learning methods in the real world
remains challenging. One of the main obstacles is that reinforcement learning
agents lack a fundamental understanding of the world and must therefore learn
from scratch through numerous trial-and-error interactions. They may also face
challenges in providing explanations for their decisions and generalizing the
acquired knowledge. Causality, however, offers a notable advantage as it can
formalize knowledge in a systematic manner and leverage invariance for
effective knowledge transfer. This has led to the emergence of causal
reinforcement learning, a subfield of reinforcement learning that seeks to
enhance existing algorithms by incorporating causal relationships into the
learning process. In this survey, we comprehensively review the literature on
causal reinforcement learning. We first introduce the basic concepts of
causality and reinforcement learning, and then explain how causality can
address core challenges in non-causal reinforcement learning. We categorize and
systematically review existing causal reinforcement learning approaches based
on their target problems and methodologies. Finally, we outline open issues and
future directions in this emerging field.
",0
Safety Margins for Reinforcement Learning,"Alexander Grushin, Walt Woods, Alvaro Velasquez, Simon Khan",2023-07-25T16:49:54Z,Reinforcement Learning,"  Any autonomous controller will be unsafe in some situations. The ability to
quantitatively identify when these unsafe situations are about to occur is
crucial for drawing timely human oversight in, e.g., freight transportation
applications. In this work, we demonstrate that the true criticality of an
agent's situation can be robustly defined as the mean reduction in reward given
some number of random actions. Proxy criticality metrics that are computable in
real-time (i.e., without actually simulating the effects of random actions) can
be compared to the true criticality, and we show how to leverage these proxy
metrics to generate safety margins, which directly tie the consequences of
potentially incorrect actions to an anticipated loss in overall performance. We
evaluate our approach on learned policies from APE-X and A3C within an Atari
environment, and demonstrate how safety margins decrease as agents approach
failure states. The integration of safety margins into programs for monitoring
deployed agents allows for the real-time identification of potentially
catastrophic situations.
",3
Small batch deep reinforcement learning,"Johan Obando-Ceron, Marc G. Bellemare, Pablo Samuel Castro",2023-10-05T20:31:37Z,Reinforcement Learning,"  In value-based deep reinforcement learning with replay memories, the batch
size parameter specifies how many transitions to sample for each gradient
update. Although critical to the learning process, this value is typically not
adjusted when proposing new algorithms. In this work we present a broad
empirical study that suggests {\em reducing} the batch size can result in a
number of significant performance gains; this is surprising, as the general
tendency when training neural networks is towards larger batch sizes for
improved performance. We complement our experimental findings with a set of
empirical analyses towards better understanding this phenomenon.
",0
Goodhart's Law in Reinforcement Learning,"Jacek Karwowski, Oliver Hayman, Xingjian Bai, Klaus Kiendlhofer, Charlie Griffin, Joar Skalse",2023-10-13T14:35:59Z,Reinforcement Learning,"  Implementing a reward function that perfectly captures a complex task in the
real world is impractical. As a result, it is often appropriate to think of the
reward function as a proxy for the true objective rather than as its
definition. We study this phenomenon through the lens of Goodhart's law, which
predicts that increasing optimisation of an imperfect proxy beyond some
critical point decreases performance on the true objective. First, we propose a
way to quantify the magnitude of this effect and show empirically that
optimising an imperfect proxy reward often leads to the behaviour predicted by
Goodhart's law for a wide range of environments and reward functions. We then
provide a geometric explanation for why Goodhart's law occurs in Markov
decision processes. We use these theoretical insights to propose an optimal
early stopping method that provably avoids the aforementioned pitfall and
derive theoretical regret bounds for this method. Moreover, we derive a
training method that maximises worst-case reward, for the setting where there
is uncertainty about the true reward function. Finally, we evaluate our early
stopping method experimentally. Our results support a foundation for a
theoretically-principled study of reinforcement learning under reward
misspecification.
",0
Real-Time Recurrent Reinforcement Learning,"Julian Lemmel, Radu Grosu",2023-11-08T16:56:16Z,Reinforcement Learning,"  In this paper we propose real-time recurrent reinforcement learning (RTRRL),
a biologically plausible approach to solving discrete and continuous control
tasks in partially-observable markov decision processes (POMDPs). RTRRL
consists of three parts: (1) a Meta-RL RNN architecture, implementing on its
own an actor-critic algorithm; (2) an outer reinforcement learning algorithm,
exploiting temporal difference learning and dutch eligibility traces to train
the Meta-RL network; and (3) random-feedback local-online (RFLO) learning, an
online automatic differentiation algorithm for computing the gradients with
respect to parameters of the network.Our experimental results show that by
replacing the optimization algorithm in RTRRL with the biologically implausible
back propagation through time (BPTT), or real-time recurrent learning (RTRL),
one does not improve returns, while matching the computational complexity for
BPTT, and even increasing complexity for RTRL. RTRRL thus serves as a model of
learning in biological neural networks, mimicking reward pathways in the basal
ganglia.
",0
Replay-enhanced Continual Reinforcement Learning,"Tiantian Zhang, Kevin Zehua Shen, Zichuan Lin, Bo Yuan, Xueqian Wang, Xiu Li, Deheng Ye",2023-11-20T06:21:52Z,Reinforcement Learning,"  Replaying past experiences has proven to be a highly effective approach for
averting catastrophic forgetting in supervised continual learning. However,
some crucial factors are still largely ignored, making it vulnerable to serious
failure, when used as a solution to forgetting in continual reinforcement
learning, even in the context of perfect memory where all data of previous
tasks are accessible in the current task. On the one hand, since most
reinforcement learning algorithms are not invariant to the reward scale, the
previously well-learned tasks (with high rewards) may appear to be more salient
to the current learning process than the current task (with small initial
rewards). This causes the agent to concentrate on those salient tasks at the
expense of generality on the current task. On the other hand, offline learning
on replayed tasks while learning a new task may induce a distributional shift
between the dataset and the learned policy on old tasks, resulting in
forgetting. In this paper, we introduce RECALL, a replay-enhanced method that
greatly improves the plasticity of existing replay-based methods on new tasks
while effectively avoiding the recurrence of catastrophic forgetting in
continual reinforcement learning. RECALL leverages adaptive normalization on
approximate targets and policy distillation on old tasks to enhance generality
and stability, respectively. Extensive experiments on the Continual World
benchmark show that RECALL performs significantly better than purely perfect
memory replay, and achieves comparable or better overall performance against
state-of-the-art continual learning methods.
",0
Causal Coordinated Concurrent Reinforcement Learning,"Tim Tse, Isaac Chan, Zhitang Chen",2024-01-31T17:20:28Z,Reinforcement Learning,"  In this work, we propose a novel algorithmic framework for data sharing and
coordinated exploration for the purpose of learning more data-efficient and
better performing policies under a concurrent reinforcement learning (CRL)
setting. In contrast to other work which make the assumption that all agents
act under identical environments, we relax this restriction and instead
consider the formulation where each agent acts within an environment which
shares a global structure but also exhibits individual variations. Our
algorithm leverages a causal inference algorithm in the form of Additive Noise
Model - Mixture Model (ANM-MM) in extracting model parameters governing
individual differentials via independence enforcement. We propose a new data
sharing scheme based on a similarity measure of the extracted model parameters
and demonstrate superior learning speeds on a set of autoregressive, pendulum
and cart-pole swing-up tasks and finally, we show the effectiveness of diverse
action selection between common agents under a sparse reward setting. To the
best of our knowledge, this is the first work in considering non-identical
environments in CRL and one of the few works which seek to integrate causal
inference with reinforcement learning (RL).
",0
Reinforcement Learning from Bagged Reward,"Yuting Tang, Xin-Qiang Cai, Yao-Xiang Ding, Qiyu Wu, Guoqing Liu, Masashi Sugiyama",2024-02-06T07:26:44Z,Reinforcement Learning,"  In Reinforcement Learning (RL), it is commonly assumed that an immediate
reward signal is generated for each action taken by the agent, helping the
agent maximize cumulative rewards to obtain the optimal policy. However, in
many real-world scenarios, immediate reward signals are not obtainable;
instead, agents receive a single reward that is contingent upon a partial
sequence or a complete trajectory. In this work, we define this challenging
problem as Reinforcement Learning from Bagged Reward (RLBR), where sequences of
data are treated as bags with non-Markovian bagged rewards. We provide a
theoretical study to establish the connection between RLBR and standard RL in
Markov Decision Processes (MDPs). To effectively explore the reward
distributions within these bags and enhance policy training, we propose a
Transformer-based reward model, the Reward Bag Transformer, which employs a
bidirectional attention mechanism to interpret contextual nuances and temporal
dependencies within each bag. Our empirical evaluations reveal that the
challenge intensifies as the bag length increases, leading to the performance
degradation due to reduced informational granularity. Nevertheless, our
approach consistently outperforms existing methods, demonstrating the least
decline in efficacy across varying bag lengths and excelling in approximating
the original MDP's reward distribution.
",0
Towards Generalized Inverse Reinforcement Learning,"Chaosheng Dong, Yijia Wang",2024-02-11T17:10:31Z,Reinforcement Learning,"  This paper studies generalized inverse reinforcement learning (GIRL) in
Markov decision processes (MDPs), that is, the problem of learning the basic
components of an MDP given observed behavior (policy) that might not be
optimal. These components include not only the reward function and transition
probability matrices, but also the action space and state space that are not
exactly known but are known to belong to given uncertainty sets. We address two
key challenges in GIRL: first, the need to quantify the discrepancy between the
observed policy and the underlying optimal policy; second, the difficulty of
mathematically characterizing the underlying optimal policy when the basic
components of an MDP are unobservable or partially observable. Then, we propose
the mathematical formulation for GIRL and develop a fast heuristic algorithm.
Numerical results on both finite and infinite state problems show the merit of
our formulation and algorithm.
",0
Scale-free Adversarial Reinforcement Learning,"Mingyu Chen, Xuezhou Zhang",2024-03-01T19:21:10Z,Reinforcement Learning,"  This paper initiates the study of scale-free learning in Markov Decision
Processes (MDPs), where the scale of rewards/losses is unknown to the learner.
We design a generic algorithmic framework, \underline{S}cale
\underline{C}lipping \underline{B}ound (\texttt{SCB}), and instantiate this
framework in both the adversarial Multi-armed Bandit (MAB) setting and the
adversarial MDP setting. Through this framework, we achieve the first minimax
optimal expected regret bound and the first high-probability regret bound in
scale-free adversarial MABs, resolving an open problem raised in
\cite{hadiji2023adaptation}. On adversarial MDPs, our framework also give birth
to the first scale-free RL algorithm with a $\tilde{\mathcal{O}}(\sqrt{T})$
high-probability regret guarantee.
",1
"Shared Interest: Measuring Human-AI Alignment to Identify Recurring
  Patterns in Model Behavior","Angie Boggust, Benjamin Hoover, Arvind Satyanarayan, Hendrik Strobelt",2021-07-20T02:44:39Z,AI Alignment,"  Saliency methods -- techniques to identify the importance of input features
on a model's output -- are a common step in understanding neural network
behavior. However, interpreting saliency requires tedious manual inspection to
identify and aggregate patterns in model behavior, resulting in ad hoc or
cherry-picked analysis. To address these concerns, we present Shared Interest:
metrics for comparing model reasoning (via saliency) to human reasoning (via
ground truth annotations). By providing quantitative descriptors, Shared
Interest enables ranking, sorting, and aggregating inputs, thereby facilitating
large-scale systematic analysis of model behavior. We use Shared Interest to
identify eight recurring patterns in model behavior, such as cases where
contextual features or a subset of ground truth features are most important to
the model. Working with representative real-world users, we show how Shared
Interest can be used to decide if a model is trustworthy, uncover issues missed
in manual analyses, and enable interactive probing.
",0
"Safeguarding the safeguards: How best to promote AI alignment in the
  public interest","Oliver Guest, Michael Aird, Seán Ó hÉigeartaigh",2023-12-13T10:36:10Z,AI Alignment,"  AI alignment work is important from both a commercial and a safety lens. With
this paper, we aim to help actors who support alignment efforts to make these
efforts as effective as possible, and to avoid potential adverse effects. We
begin by suggesting that institutions that are trying to act in the public
interest (such as governments) should aim to support specifically alignment
work that reduces accident or misuse risks. We then describe four problems
which might cause alignment efforts to be counterproductive, increasing
large-scale AI risks. We suggest mitigations for each problem. Finally, we make
a broader recommendation that institutions trying to act in the public interest
should think systematically about how to make their alignment efforts as
effective, and as likely to be beneficial, as possible.
",0
"Incentive Compatibility for AI Alignment in Sociotechnical Systems:
  Positions and Prospects","Zhaowei Zhang, Fengshuo Bai, Mingzhi Wang, Haoyang Ye, Chengdong Ma, Yaodong Yang",2024-02-20T10:52:57Z,AI Alignment,"  The burgeoning integration of artificial intelligence (AI) into human society
brings forth significant implications for societal governance and safety. While
considerable strides have been made in addressing AI alignment challenges,
existing methodologies primarily focus on technical facets, often neglecting
the intricate sociotechnical nature of AI systems, which can lead to a
misalignment between the development and deployment contexts. To this end, we
posit a new problem worth exploring: Incentive Compatibility Sociotechnical
Alignment Problem (ICSAP). We hope this can call for more researchers to
explore how to leverage the principles of Incentive Compatibility (IC) from
game theory to bridge the gap between technical and societal components to
maintain AI consensus with human societies in different contexts. We further
discuss three classical game problems for achieving IC: mechanism design,
contract theory, and Bayesian persuasion, in addressing the perspectives,
potentials, and challenges of solving ICSAP, and provide preliminary
implementation conceptions.
",0
Fair and Responsible AI: A Focus on the Ability to Contest,"Henrietta Lyons, Eduardo Velloso, Tim Miller",2021-02-22T05:49:02Z,Responsible AI,"  As the use of artificial intelligence (AI) in high-stakes decision-making
increases, the ability to contest such decisions is being recognised in AI
ethics guidelines as an important safeguard for individuals. Yet, there is
little guidance on how AI systems can be designed to support contestation. In
this paper we explain that the design of a contestation process is important
due to its impact on perceptions of fairness and satisfaction. We also consider
design challenges, including a lack of transparency as well as the numerous
design options that decision-making entities will be faced with. We argue for a
human-centred approach to designing for contestability to ensure that the needs
of decision subjects, and the community, are met.
",8
"Software Engineering for Responsible AI: An Empirical Study and
  Operationalised Patterns","Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, David Douglas, Conrad Sanderson",2021-11-18T02:18:27Z,Responsible AI,"  Although artificial intelligence (AI) is solving real-world challenges and
transforming industries, there are serious concerns about its ability to behave
and make decisions in a responsible way. Many AI ethics principles and
guidelines for responsible AI have been recently issued by governments,
organisations, and enterprises. However, these AI ethics principles and
guidelines are typically high-level and do not provide concrete guidance on how
to design and develop responsible AI systems. To address this shortcoming, we
first present an empirical study where we interviewed 21 scientists and
engineers to understand the practitioners' perceptions on AI ethics principles
and their implementation. We then propose a template that enables AI ethics
principles to be operationalised in the form of concrete patterns and suggest a
list of patterns using the newly created template. These patterns provide
concrete, operationalised guidance that facilitate the development of
responsible AI systems.
",0
"Macro Ethics Principles for Responsible AI Systems: Taxonomy and Future
  Directions","Jessica Woodgate, Nirav Ajmeri",2022-08-12T08:48:16Z,Responsible AI,"  Responsible AI must be able to make or support decisions that consider human
values and can be justified by human morals. Accommodating values and morals in
responsible decision making is supported by adopting a perspective of macro
ethics, which views ethics through a holistic lens incorporating social
context. Normative ethical principles inferred from philosophy can be used to
methodically reason about ethics and make ethical judgements in specific
contexts. Operationalising normative ethical principles thus promotes
responsible reasoning under the perspective of macro ethics. We survey AI and
computer science literature and develop a taxonomy of 21 normative ethical
principles which can be operationalised in AI. We describe how each principle
has previously been operationalised, highlighting key themes that AI
practitioners seeking to implement ethical principles should be aware of. We
envision that this taxonomy will facilitate the development of methodologies to
incorporate normative ethical principles in reasoning capacities of responsible
AI systems.
",0
"Responsible AI Pattern Catalogue: A Collection of Best Practices for AI
  Governance and Engineering","Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Didar Zowghi, Aurelie Jacquet",2022-09-12T00:09:08Z,Responsible AI,"  Responsible AI is widely considered as one of the greatest scientific
challenges of our time and is key to increase the adoption of AI. Recently, a
number of AI ethics principles frameworks have been published. However, without
further guidance on best practices, practitioners are left with nothing much
beyond truisms. Also, significant efforts have been placed at algorithm-level
rather than system-level, mainly focusing on a subset of mathematics-amenable
ethical principles, such as fairness. Nevertheless, ethical issues can arise at
any step of the development lifecycle, cutting across many AI and non-AI
components of systems beyond AI algorithms and models. To operationalize
responsible AI from a system perspective, in this paper, we present a
Responsible AI Pattern Catalogue based on the results of a Multivocal
Literature Review (MLR). Rather than staying at the principle or algorithm
level, we focus on patterns that AI system stakeholders can undertake in
practice to ensure that the developed AI systems are responsible throughout the
entire governance and engineering lifecycle. The Responsible AI Pattern
Catalogue classifies the patterns into three groups: multi-level governance
patterns, trustworthy process patterns, and responsible-AI-by-design product
patterns. These patterns provide systematic and actionable guidance for
stakeholders to implement responsible AI.
",0
"Responsible AI Implementation: A Human-centered Framework for
  Accelerating the Innovation Process","Dian Tjondronegoro, Elizabeth Yuwono, Brent Richards, Damian Green, Siiri Hatakka",2022-09-15T06:24:01Z,Responsible AI,"  There is still a significant gap between expectations and the successful
adoption of AI to innovate and improve businesses. Due to the emergence of deep
learning, AI adoption is more complex as it often incorporates big data and the
internet of things, affecting data privacy. Existing frameworks have identified
the need to focus on human-centered design, combining technical and
business/organizational perspectives. However, trust remains a critical issue
that needs to be designed from the beginning. The proposed framework expands
from the human-centered design approach, emphasizing and maintaining the trust
that underpins the process. This paper proposes a theoretical framework for
responsible artificial intelligence (AI) implementation. The proposed framework
emphasizes a synergistic business technology approach for the agile co-creation
process. The aim is to streamline the adoption process of AI to innovate and
improve business by involving all stakeholders throughout the project so that
the AI technology is designed, developed, and deployed in conjunction with
people and not in isolation. The framework presents a fresh viewpoint on
responsible AI implementation based on analytical literature review, conceptual
framework design, and practitioners' mediating expertise. The framework
emphasizes establishing and maintaining trust throughout the human-centered
design and agile development of AI. This human-centered approach is aligned
with and enabled by the privacy by design principle. The creators of the
technology and the end-users are working together to tailor the AI solution
specifically for the business requirements and human characteristics. An
illustrative case study on adopting AI for assisting planning in a hospital
will demonstrate that the proposed framework applies to real-life applications.
",4
"Developing Responsible Chatbots for Financial Services: A
  Pattern-Oriented Responsible AI Engineering Approach","Qinghua Lu, Yuxiu Luo, Liming Zhu, Mingjian Tang, Xiwei Xu, Jon Whittle",2023-01-03T23:11:03Z,Responsible AI,"  The recent release of ChatGPT has gained huge attention and discussion
worldwide, with responsible AI being a key topic of discussion. How can we
ensure that AI systems, including ChatGPT, are developed and adopted in a
responsible way? To tackle the responsible AI challenges, various ethical
principles have been released by governments, organisations, and companies.
However, those principles are very abstract and not practical enough. Further,
significant efforts have been put on algorithm-level solutions that only
address a narrow set of principles, such as fairness and privacy. To fill the
gap, we adopt a pattern-oriented responsible AI engineering approach and build
a Responsible AI Pattern Catalogue to operationalise responsible AI from a
system perspective. In this article, we first summarise the major challenges in
operationalising responsible AI at scale and introduce how we use the
Responsible AI Pattern Catalogue to address those challenges. We then examine
the risks at each stage of the chatbot development process and recommend
pattern-driven mitigations to evaluate the the usefulness of the Responsible AI
Pattern Catalogue in a real-world setting.
",0
"A Systematic Literature Review of Human-Centered, Ethical, and
  Responsible AI","Mohammad Tahaei, Marios Constantinides, Daniele Quercia, Michael Muller",2023-02-10T14:47:33Z,Responsible AI,"  As Artificial Intelligence (AI) continues to advance rapidly, it becomes
increasingly important to consider AI's ethical and societal implications. In
this paper, we present a bottom-up mapping of the current state of research at
the intersection of Human-Centered AI, Ethical, and Responsible AI (HCER-AI) by
thematically reviewing and analyzing 164 research papers from leading
conferences in ethical, social, and human factors of AI: AIES, CHI, CSCW, and
FAccT. The ongoing research in HCER-AI places emphasis on governance, fairness,
and explainability. These conferences, however, concentrate on specific themes
rather than encompassing all aspects. While AIES has fewer papers on HCER-AI,
it emphasizes governance and rarely publishes papers about privacy, security,
and human flourishing. FAccT publishes more on governance and lacks papers on
privacy, security, and human flourishing. CHI and CSCW, as more established
conferences, have a broader research portfolio. We find that the current
emphasis on governance and fairness in AI research may not adequately address
the potential unforeseen and unknown implications of AI. Therefore, we
recommend that future research should expand its scope and diversify resources
to prepare for these potential consequences. This could involve exploring
additional areas such as privacy, security, human flourishing, and
explainability.
",0
"A toolkit of dilemmas: Beyond debiasing and fairness formulas for
  responsible AI/ML","Andrés Domínguez Hernández, Vassilis Galanos",2023-03-03T13:58:24Z,Responsible AI,"  Approaches to fair and ethical AI have recently fell under the scrutiny of
the emerging, chiefly qualitative, field of critical data studies, placing
emphasis on the lack of sensitivity to context and complex social phenomena of
such interventions. We employ some of these lessons to introduce a tripartite
decision-making toolkit, informed by dilemmas encountered in the pursuit of
responsible AI/ML. These are: (a) the opportunity dilemma between the
availability of data shaping problem statements vs problem statements shaping
data; (b) the trade-off between scalability and contextualizability (too much
data versus too specific data); and (c) the epistemic positioning between the
pragmatic technical objectivism and the reflexive relativism in acknowledging
the social. This paper advocates for a situated reasoning and creative
engagement with the dilemmas surrounding responsible algorithmic/data-driven
systems, and going beyond the formulaic bias elimination and ethics
operationalization narratives found in the fair-AI literature.
",0
"The Equitable AI Research Roundtable (EARR): Towards Community-Based
  Decision Making in Responsible AI Development","Jamila Smith-Loud, Andrew Smart, Darlene Neal, Amber Ebinama, Eric Corbett, Paul Nicholas, Qazi Rashid, Anne Peckham, Sarah Murphy-Gray, Nicole Morris, Elisha Smith Arrillaga, Nicole-Marie Cotton, Emnet Almedom, Olivia Araiza, Eliza McCullough, Abbie Langston, Christopher Nellum",2023-03-14T18:57:20Z,Responsible AI,"  This paper reports on our initial evaluation of The Equitable AI Research
Roundtable -- a coalition of experts in law, education, community engagement,
social justice, and technology. EARR was created in collaboration among a large
tech firm, nonprofits, NGO research institutions, and universities to provide
critical research based perspectives and feedback on technology's emergent
ethical and social harms. Through semi-structured workshops and discussions
within the large tech firm, EARR has provided critical perspectives and
feedback on how to conceptualize equity and vulnerability as they relate to AI
technology. We outline three principles in practice of how EARR has operated
thus far that are especially relevant to the concerns of the FAccT community:
how EARR expands the scope of expertise in AI development, how it fosters
opportunities for epistemic curiosity and responsibility, and that it creates a
space for mutual learning. This paper serves as both an analysis and
translation of lessons learned through this engagement approach, and the
possibilities for future research.
",0
"A Rapid Review of Responsible AI frameworks: How to guide the
  development of ethical AI","Vita Santa Barletta, Danilo Caivano, Domenico Gigante, Azzurra Ragone",2023-06-08T07:47:18Z,Responsible AI,"  In the last years, the raise of Artificial Intelligence (AI), and its
pervasiveness in our lives, has sparked a flourishing debate about the ethical
principles that should lead its implementation and use in society. Driven by
these concerns, we conduct a rapid review of several frameworks providing
principles, guidelines, and/or tools to help practitioners in the development
and deployment of Responsible AI (RAI) applications. We map each framework
w.r.t. the different Software Development Life Cycle (SDLC) phases discovering
that most of these frameworks fall just in the Requirements Elicitation phase,
leaving the other phases uncovered. Very few of these frameworks offer
supporting tools for practitioners, and they are mainly provided by private
companies. Our results reveal that there is not a ""catching-all"" framework
supporting both technical and non-technical stakeholders in the implementation
of real-world projects. Our findings highlight the lack of a comprehensive
framework encompassing all RAI principles and all (SDLC) phases that could be
navigated by users with different skill sets and with different goals.
",0
"`It is currently hodgepodge'': Examining AI/ML Practitioners' Challenges
  during Co-production of Responsible AI Values","Rama Adithya Varanasi, Nitesh Goyal",2023-07-14T21:57:46Z,Responsible AI,"  Recently, the AI/ML research community has indicated an urgent need to
establish Responsible AI (RAI) values and practices as part of the AI/ML
lifecycle. Several organizations and communities are responding to this call by
sharing RAI guidelines. However, there are gaps in awareness, deliberation, and
execution of such practices for multi-disciplinary ML practitioners. This work
contributes to the discussion by unpacking co-production challenges faced by
practitioners as they align their RAI values. We interviewed 23 individuals,
across 10 organizations, tasked to ship AI/ML based products while upholding
RAI norms and found that both top-down and bottom-up institutional structures
create burden for different roles preventing them from upholding RAI values, a
challenge that is further exacerbated when executing conflicted values. We
share multiple value levers used as strategies by the practitioners to resolve
their challenges. We end our paper with recommendations for inclusive and
equitable RAI value-practices, creating supportive organizational structures
and opportunities to further aid practitioners.
",0
"A Framework for Automated Measurement of Responsible AI Harms in
  Generative AI Applications","Ahmed Magooda, Alec Helyar, Kyle Jackson, David Sullivan, Chad Atalla, Emily Sheng, Dan Vann, Richard Edgar, Hamid Palangi, Roman Lutz, Hongliang Kong, Vincent Yun, Eslam Kamal, Federico Zarfati, Hanna Wallach, Sarah Bird, Mei Chen",2023-10-26T19:45:06Z,Responsible AI,"  We present a framework for the automated measurement of responsible AI (RAI)
metrics for large language models (LLMs) and associated products and services.
Our framework for automatically measuring harms from LLMs builds on existing
technical and sociotechnical expertise and leverages the capabilities of
state-of-the-art LLMs, such as GPT-4. We use this framework to run through
several case studies investigating how different LLMs may violate a range of
RAI-related principles. The framework may be employed alongside domain-specific
sociotechnical expertise to create measurements for new harm areas in the
future. By implementing this framework, we aim to enable more advanced harm
measurement efforts and further the responsible use of LLMs.
",5
"Responsible AI Considerations in Text Summarization Research: A Review
  of Current Practices","Yu Lu Liu, Meng Cao, Su Lin Blodgett, Jackie Chi Kit Cheung, Alexandra Olteanu, Adam Trischler",2023-11-18T15:35:36Z,Responsible AI,"  AI and NLP publication venues have increasingly encouraged researchers to
reflect on possible ethical considerations, adverse impacts, and other
responsible AI issues their work might engender. However, for specific NLP
tasks our understanding of how prevalent such issues are, or when and why these
issues are likely to arise, remains limited. Focusing on text summarization --
a common NLP task largely overlooked by the responsible AI community -- we
examine research and reporting practices in the current literature. We conduct
a multi-round qualitative analysis of 333 summarization papers from the ACL
Anthology published between 2020-2022. We focus on how, which, and when
responsible AI issues are covered, which relevant stakeholders are considered,
and mismatches between stated and realized research goals. We also discuss
current evaluation practices and consider how authors discuss the limitations
of both prior work and their own work. Overall, we find that relatively few
papers engage with possible stakeholders or contexts of use, which limits their
consideration of potential downstream adverse impacts or other responsible AI
issues. Based on our findings, we make recommendations on concrete practices
and research directions.
",0
"Towards a Responsible AI Metrics Catalogue: A Collection of Metrics for
  AI Accountability","Boming Xia, Qinghua Lu, Liming Zhu, Sung Une Lee, Yue Liu, Zhenchang Xing",2023-11-22T04:43:16Z,Responsible AI,"  Artificial Intelligence (AI), particularly through the advent of large-scale
generative AI (GenAI) models such as Large Language Models (LLMs), has become a
transformative element in contemporary technology. While these models have
unlocked new possibilities, they simultaneously present significant challenges,
such as concerns over data privacy and the propensity to generate misleading or
fabricated content. Current frameworks for Responsible AI (RAI) often fall
short in providing the granular guidance necessary for tangible application,
especially for Accountability-a principle that is pivotal for ensuring
transparent and auditable decision-making, bolstering public trust, and meeting
increasing regulatory expectations. This study bridges the accountability gap
by introducing our effort towards a comprehensive metrics catalogue, formulated
through a systematic multivocal literature review (MLR) that integrates
findings from both academic and grey literature. Our catalogue delineates
process metrics that underpin procedural integrity, resource metrics that
provide necessary tools and frameworks, and product metrics that reflect the
outputs of AI systems. This tripartite framework is designed to operationalize
Accountability in AI, with a special emphasis on addressing the intricacies of
GenAI.
",0
"Open Datasheets: Machine-readable Documentation for Open Datasets and
  Responsible AI Assessments","Anthony Cintron Roman, Jennifer Wortman Vaughan, Valerie See, Steph Ballard, Jehu Torres, Caleb Robinson, Juan M. Lavista Ferres",2023-12-11T06:41:14Z,Responsible AI,"  This paper introduces a no-code, machine-readable documentation framework for
open datasets, with a focus on responsible AI (RAI) considerations. The
framework aims to improve comprehensibility, and usability of open datasets,
facilitating easier discovery and use, better understanding of content and
context, and evaluation of dataset quality and accuracy. The proposed framework
is designed to streamline the evaluation of datasets, helping researchers, data
scientists, and other open data users quickly identify datasets that meet their
needs and organizational policies or regulations. The paper also discusses the
implementation of the framework and provides recommendations to maximize its
potential. The framework is expected to enhance the quality and reliability of
data used in research and decision-making, fostering the development of more
responsible and trustworthy AI systems.
",0
"Human-in-the-loop Fairness: Integrating Stakeholder Feedback to
  Incorporate Fairness Perspectives in Responsible AI","Evdoxia Taka, Yuri Nakao, Ryosuke Sonoda, Takuya Yokota, Lin Luo, Simone Stumpf",2023-12-13T11:17:29Z,Responsible AI,"  Fairness is a growing concern for high-risk decision-making using Artificial
Intelligence (AI) but ensuring it through purely technical means is
challenging: there is no universally accepted fairness measure, fairness is
context-dependent, and there might be conflicting perspectives on what is
considered fair. Thus, involving stakeholders, often without a background in AI
or fairness, is a promising avenue. Research to directly involve stakeholders
is in its infancy, and many questions remain on how to support stakeholders to
feedback on fairness, and how this feedback can be integrated into AI models.
Our work follows an approach where stakeholders can give feedback on specific
decision instances and their outcomes with respect to their fairness, and then
to retrain an AI model. In order to investigate this approach, we conducted two
studies of a complex AI model for credit rating used in loan applications. In
study 1, we collected feedback from 58 lay users on loan application decisions,
and conducted offline experiments to investigate the effects on accuracy and
fairness metrics. In study 2, we deepened this investigation by showing 66
participants the results of their feedback with respect to fairness, and then
conducted further offline analyses. Our work contributes two datasets and
associated code frameworks to bootstrap further research, highlights the
opportunities and challenges of employing lay user feedback for improving AI
fairness, and discusses practical implications for developing AI applications
that more closely reflect stakeholder views about fairness.
",1
"Towards Responsible AI in Banking: Addressing Bias for Fair
  Decision-Making",Alessandro Castelnovo,2024-01-13T14:07:09Z,Responsible AI,"  In an era characterized by the pervasive integration of artificial
intelligence into decision-making processes across diverse industries, the
demand for trust has never been more pronounced. This thesis embarks on a
comprehensive exploration of bias and fairness, with a particular emphasis on
their ramifications within the banking sector, where AI-driven decisions bear
substantial societal consequences. In this context, the seamless integration of
fairness, explainability, and human oversight is of utmost importance,
culminating in the establishment of what is commonly referred to as
""Responsible AI"". This emphasizes the critical nature of addressing biases
within the development of a corporate culture that aligns seamlessly with both
AI regulations and universal human rights standards, particularly in the realm
of automated decision-making systems. Nowadays, embedding ethical principles
into the development, training, and deployment of AI models is crucial for
compliance with forthcoming European regulations and for promoting societal
good. This thesis is structured around three fundamental pillars: understanding
bias, mitigating bias, and accounting for bias. These contributions are
validated through their practical application in real-world scenarios, in
collaboration with Intesa Sanpaolo. This collaborative effort not only
contributes to our understanding of fairness but also provides practical tools
for the responsible implementation of AI-based decision-making systems. In line
with open-source principles, we have released Bias On Demand and FairView as
accessible Python packages, further promoting progress in the field of AI
fairness.
",2
"A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps
  Towards Effectiveness Evaluations","Glen Berman, Nitesh Goyal, Michael Madaio",2024-01-30T22:44:18Z,Responsible AI,"  Responsible design of AI systems is a shared goal across HCI and AI
communities. Responsible AI (RAI) tools have been developed to support
practitioners to identify, assess, and mitigate ethical issues during AI
development. These tools take many forms (e.g., design playbooks, software
toolkits, documentation protocols). However, research suggests that use of RAI
tools is shaped by organizational contexts, raising questions about how
effective such tools are in practice. To better understand how RAI tools are --
and might be -- evaluated, we conducted a qualitative analysis of 37
publications that discuss evaluations of RAI tools. We find that most
evaluations focus on usability, while questions of tools' effectiveness in
changing AI development are sidelined. While usability evaluations are an
important approach to evaluate RAI tools, we draw on evaluation approaches from
other fields to highlight developer- and community-level steps to support
evaluations of RAI tools' effectiveness in shaping AI development practices and
outcomes.
",0
"Exploring Public Opinion on Responsible AI Through The Lens of Cultural
  Consensus Theory","Necdet Gurkan, Jordan W. Suchow",2024-01-06T20:57:35Z,Responsible AI,"  As the societal implications of Artificial Intelligence (AI) continue to
grow, the pursuit of responsible AI necessitates public engagement in its
development and governance processes. This involvement is crucial for capturing
diverse perspectives and promoting equitable practices and outcomes. We applied
Cultural Consensus Theory (CCT) to a nationally representative survey dataset
on various aspects of AI to discern beliefs and attitudes about responsible AI
in the United States. Our results offer valuable insights by identifying shared
and contrasting views on responsible AI. Furthermore, these findings serve as
critical reference points for developers and policymakers, enabling them to
more effectively consider individual variances and group-level cultural
perspectives when making significant decisions and addressing the public's
concerns.
",0
"Data Ethics Emergency Drill: A Toolbox for Discussing Responsible AI for
  Industry Teams","Vanessa Aisyahsari Hanschke, Dylan Rees, Merve Alanyali, David Hopkinson, Paul Marshall",2024-03-15T16:20:51Z,Responsible AI,"  Researchers urge technology practitioners such as data scientists to consider
the impacts and ethical implications of algorithmic decisions. However, unlike
programming, statistics, and data management, discussion of ethical
implications is rarely included in standard data science training. To begin to
address this gap, we designed and tested a toolbox called the data ethics
emergency drill (DEED) to help data science teams discuss and reflect on the
ethical implications of their work. The DEED is a roleplay of a fictional
ethical emergency scenario that is contextually situated in the team's specific
workplace and applications. This paper outlines the DEED toolbox and describes
three studies carried out with two different data science teams that
iteratively shaped its design. Our findings show that practitioners can apply
lessons learnt from the roleplay to real-life situations, and how the DEED
opened up conversations around ethics and values.
",0
"Explainable AI (XAI) for PHM of Industrial Asset: A State-of-The-Art,
  PRISMA-Compliant Systematic Review","Ahmad Kamal Bin Mohd Nor, Srinivasa Rao Pedapait, Masdi Muhammad",2021-07-08T14:22:32Z,Explainable AI,"  A state-of-the-art systematic review on XAI applied to Prognostic and Health
Management (PHM) of industrial asset is presented. This work provides an
overview of the general trend of XAI in PHM, answers the question of accuracy
versus explainability, the extent of human involvement, the explanation
assessment and uncertainty quantification in PHM-XAI domain. Research articles
associated with the subject, from 2015 to 2021 were selected from five known
databases following PRISMA guidelines. Data was then extracted from the
selected articles and examined. Several findings were synthesized. Firstly,
while the discipline is still young, the analysis indicated the growing
acceptance of XAI in PHM domain. Secondly, XAI functions as a double edge
sword, where it is assimilated as a tool to execute PHM tasks as well as a mean
of explanation, particularly in diagnostic and anomaly detection activities,
implying a real need for XAI in PHM. Thirdly, the review showed that PHM-XAI
papers produce either good or excellent result in general, suggesting that PHM
performance is unaffected by XAI. Fourthly, human role, evaluation metrics and
uncertainty management are areas requiring further attention by the PHM
community. Adequate assessment metrics to cater for PHM need are urgently
needed.Finally, most case study featured on the accepted articles are based on
real, industrial data, indicating that the available PHM-XAI blends are fit to
solve complex,real-world challenges, increasing the confidence in AI adoption
in the industry.
",0
"Explainable AI for Psychological Profiling from Digital Footprints: A
  Case Study of Big Five Personality Predictions from Spending Data","Yanou Ramon, Sandra C. Matz, R. A. Farrokhnia, David Martens",2021-11-12T19:28:56Z,Explainable AI,"  Every step we take in the digital world leaves behind a record of our
behavior; a digital footprint. Research has suggested that algorithms can
translate these digital footprints into accurate estimates of psychological
characteristics, including personality traits, mental health or intelligence.
The mechanisms by which AI generates these insights, however, often remain
opaque. In this paper, we show how Explainable AI (XAI) can help domain experts
and data subjects validate, question, and improve models that classify
psychological traits from digital footprints. We elaborate on two popular XAI
methods (rule extraction and counterfactual explanations) in the context of Big
Five personality predictions (traits and facets) from financial transactions
data (N = 6,408). First, we demonstrate how global rule extraction sheds light
on the spending patterns identified by the model as most predictive for
personality, and discuss how these rules can be used to explain, validate, and
improve the model. Second, we implement local rule extraction to show that
individuals are assigned to personality classes because of their unique
financial behavior, and that there exists a positive link between the model's
prediction confidence and the number of features that contributed to the
prediction. Our experiments highlight the importance of both global and local
XAI methods. By better understanding how predictive models work in general as
well as how they derive an outcome for a particular person, XAI promotes
accountability in a world in which AI impacts the lives of billions of people
around the world.
",0
"A Prescriptive Learning Analytics Framework: Beyond Predictive Modelling
  and onto Explainable AI with Prescriptive Analytics and ChatGPT",Teo Susnjak,2022-08-31T00:57:17Z,Explainable AI,"  A significant body of recent research in the field of Learning Analytics has
focused on leveraging machine learning approaches for predicting at-risk
students in order to initiate timely interventions and thereby elevate
retention and completion rates. The overarching feature of the majority of
these research studies has been on the science of prediction only. The
component of predictive analytics concerned with interpreting the internals of
the models and explaining their predictions for individual cases to
stakeholders has largely been neglected. Additionally, works that attempt to
employ data-driven prescriptive analytics to automatically generate
evidence-based remedial advice for at-risk learners are in their infancy.
  eXplainable AI is a field that has recently emerged providing cutting-edge
tools which support transparent predictive analytics and techniques for
generating tailored advice for at-risk students. This study proposes a novel
framework that unifies both transparent machine learning as well as techniques
for enabling prescriptive analytics, while integrating the latest advances in
large language models. This work practically demonstrates the proposed
framework using predictive models for identifying at-risk learners of programme
non-completion. The study then further demonstrates how predictive modelling
can be augmented with prescriptive analytics on two case studies in order to
generate human-readable prescriptive feedback for those who are at risk using
ChatGPT.
",0
"Fault Diagnosis using eXplainable AI: a Transfer Learning-based Approach
  for Rotating Machinery exploiting Augmented Synthetic Data","Lucas Costa Brito, Gian Antonio Susto, Jorge Nei Brito, Marcus Antonio Viana Duarte",2022-10-06T15:02:35Z,Explainable AI,"  Artificial Intelligence (AI) is one of the approaches that has been proposed
to analyze the collected data (e.g., vibration signals) providing a diagnosis
of the asset's operating condition. It is known that models trained with
labeled data (supervised) achieve excellent results, but two main problems make
their application in production processes difficult: (i) impossibility or long
time to obtain a sample of all operational conditions (since faults seldom
happen) and (ii) high cost of experts to label all acquired data. Another
limitating factor for the applicability of AI approaches in this context is the
lack of interpretability of the models (black-boxes), which reduces the
confidence of the diagnosis and trust/adoption from users. To overcome these
problems, a new generic and interpretable approach for classifying faults in
rotating machinery based on transfer learning from augmented synthetic data to
real rotating machinery is here proposed, namelly FaultD-XAI (Fault Diagnosis
using eXplainable AI). To provide scalability using transfer learning,
synthetic vibration signals are created mimicking the characteristic behavior
of failures in operation. The application of Gradient-weighted Class Activation
Mapping (Grad-CAM) with 1D Convolutional Neural Network (1D CNN) allows the
interpretation of results, supporting the user in decision making and
increasing diagnostic confidence. The proposed approach not only obtained
promising diagnostic performance, but was also able to learn characteristics
used by experts to identify conditions in a source domain and apply them in
another target domain. The experimental results suggest a promising approach on
exploiting transfer learning, synthetic data and explainable artificial
intelligence for fault diagnosis. Lastly, to guarantee reproducibility and
foster research in the field, the developed dataset is made publicly available.
",0
"Finding the right XAI method -- A Guide for the Evaluation and Ranking
  of Explainable AI Methods in Climate Science","Philine Bommer, Marlene Kretschmer, Anna Hedström, Dilyara Bareeva, Marina M. -C. Höhne",2023-03-01T16:54:48Z,Explainable AI,"  Explainable artificial intelligence (XAI) methods shed light on the
predictions of machine learning algorithms. Several different approaches exist
and have already been applied in climate science. However, usually missing
ground truth explanations complicate their evaluation and comparison,
subsequently impeding the choice of the XAI method. Therefore, in this work, we
introduce XAI evaluation in the climate context and discuss different desired
explanation properties, namely robustness, faithfulness, randomization,
complexity, and localization. To this end, we chose previous work as a case
study where the decade of annual-mean temperature maps is predicted. After
training both a multi-layer perceptron (MLP) and a convolutional neural network
(CNN), multiple XAI methods are applied and their skill scores in reference to
a random uniform explanation are calculated for each property. Independent of
the network, we find that XAI methods Integrated Gradients, layer-wise
relevance propagation, and input times gradients exhibit considerable
robustness, faithfulness, and complexity while sacrificing randomization
performance. Sensitivity methods -- gradient, SmoothGrad, NoiseGrad, and
FusionGrad, match the robustness skill but sacrifice faithfulness and
complexity for randomization skill. We find architecture-dependent performance
differences regarding robustness, complexity and localization skills of
different XAI methods, highlighting the necessity for research task-specific
evaluation. Overall, our work offers an overview of different evaluation
properties in the climate science context and shows how to compare and
benchmark different explanation methods, assessing their suitability based on
strengths and weaknesses, for the specific research problem at hand. By that,
we aim to support climate researchers in the selection of a suitable XAI
method.
",0
"Explainable AI Insights for Symbolic Computation: A case study on
  selecting the variable ordering for cylindrical algebraic decomposition","Lynn Pickering, Tereso Del Rio Almajano, Matthew England, Kelly Cohen",2023-04-24T15:05:04Z,Explainable AI,"  In recent years there has been increased use of machine learning (ML)
techniques within mathematics, including symbolic computation where it may be
applied safely to optimise or select algorithms. This paper explores whether
using explainable AI (XAI) techniques on such ML models can offer new insight
for symbolic computation, inspiring new implementations within computer algebra
systems that do not directly call upon AI tools. We present a case study on the
use of ML to select the variable ordering for cylindrical algebraic
decomposition. It has already been demonstrated that ML can make the choice
well, but here we show how the SHAP tool for explainability can be used to
inform new heuristics of a size and complexity similar to those human-designed
heuristics currently commonly used in symbolic computation.
",0
"Exploring a Gradient-based Explainable AI Technique for Time-Series
  Data: A Case Study of Assessing Stroke Rehabilitation Exercises","Min Hun Lee, Yi Jing Choy",2023-05-08T08:30:05Z,Explainable AI,"  Explainable artificial intelligence (AI) techniques are increasingly being
explored to provide insights into why AI and machine learning (ML) models
provide a certain outcome in various applications. However, there has been
limited exploration of explainable AI techniques on time-series data,
especially in the healthcare context. In this paper, we describe a
threshold-based method that utilizes a weakly supervised model and a
gradient-based explainable AI technique (i.e. saliency map) and explore its
feasibility to identify salient frames of time-series data. Using the dataset
from 15 post-stroke survivors performing three upper-limb exercises and labels
on whether a compensatory motion is observed or not, we implemented a
feed-forward neural network model and utilized gradients of each input on model
outcomes to identify salient frames that involve compensatory motions.
According to the evaluation using frame-level annotations, our approach
achieved a recall of 0.96 and an F2-score of 0.91. Our results demonstrated the
potential of a gradient-based explainable AI technique (e.g. saliency map) for
time-series data, such as highlighting the frames of a video that therapists
should focus on reviewing and reducing the efforts on frame-level labeling for
model training.
",3
"Identifying drivers and mitigators for congestion and redispatch in the
  German electric power system with explainable AI","Maurizio Titz, Sebastian Pütz, Dirk Witthaut",2023-07-24T09:19:38Z,Explainable AI,"  The transition to a sustainable energy supply challenges the operation of
electric power systems in manifold ways. Transmission grid loads increase as
wind and solar power are often installed far away from the consumers. In
extreme cases, system operators must intervene via countertrading or redispatch
to ensure grid stability. In this article, we provide a data-driven analysis of
congestion in the German transmission grid. We develop an explainable machine
learning model to predict the volume of redispatch and countertrade on an
hourly basis. The model reveals factors that drive or mitigate grid congestion
and quantifies their impact. We show that, as expected, wind power generation
is the main driver, but hydropower and cross-border electricity trading also
play an essential role. Solar power, on the other hand, has no mitigating
effect. Our results suggest that a change to the market design would alleviate
congestion.
",0
"Using explainable AI to investigate electrocardiogram changes during
  healthy aging -- from expert features to raw signals","Gabriel Ott, Yannik Schaubelt, Juan Miguel Lopez Alcaraz, Wilhelm Haverkamp, Nils Strodthoff",2023-10-11T13:05:28Z,Explainable AI,"  Cardiovascular diseases remain the leading global cause of mortality. Age is
an important covariate whose effect is most easily investigated in a healthy
cohort to properly distinguish the former from disease-related changes.
Traditionally, most of such insights have been drawn from the analysis of
electrocardiogram (ECG) feature changes in individuals as they age. However,
these features, while informative, may potentially obscure underlying data
relationships. In this paper we present the following contributions: (1) We
employ a deep-learning model and a tree-based model to analyze ECG data from a
robust dataset of healthy individuals across varying ages in both raw signals
and ECG feature format. (2) We use explainable AI methods to identify the most
discriminative ECG features across age groups.(3) Our analysis with tree-based
classifiers reveals age-related declines in inferred breathing rates and
identifies notably high SDANN values as indicative of elderly individuals,
distinguishing them from younger adults. (4) Furthermore, the deep-learning
model underscores the pivotal role of the P-wave in age predictions across all
age groups, suggesting potential changes in the distribution of different
P-wave types with age. These findings shed new light on age-related ECG
changes, offering insights that transcend traditional feature-based approaches.
",0
"The Thousand Faces of Explainable AI Along the Machine Learning Life
  Cycle: Industrial Reality and Current State of Research","Thomas Decker, Ralf Gross, Alexander Koebler, Michael Lebacher, Ronald Schnitzer, Stefan H. Weber",2023-10-11T20:45:49Z,Explainable AI,"  In this paper, we investigate the practical relevance of explainable
artificial intelligence (XAI) with a special focus on the producing industries
and relate them to the current state of academic XAI research. Our findings are
based on an extensive series of interviews regarding the role and applicability
of XAI along the Machine Learning (ML) lifecycle in current industrial practice
and its expected relevance in the future. The interviews were conducted among a
great variety of roles and key stakeholders from different industry sectors. On
top of that, we outline the state of XAI research by providing a concise review
of the relevant literature. This enables us to provide an encompassing overview
covering the opinions of the surveyed persons as well as the current state of
academic research. By comparing our interview results with the current research
approaches we reveal several discrepancies. While a multitude of different XAI
approaches exists, most of them are centered around the model evaluation phase
and data scientists. Their versatile capabilities for other stages are
currently either not sufficiently explored or not popular among practitioners.
In line with existing work, our findings also confirm that more efforts are
needed to enable also non-expert users' interpretation and understanding of
opaque AI models with existing methods and frameworks.
",0
"Notes on Applicability of Explainable AI Methods to Machine Learning
  Models Using Features Extracted by Persistent Homology",Naofumi Hama,2023-10-15T08:56:15Z,Explainable AI,"  Data analysis that uses the output of topological data analysis as input for
machine learning algorithms has been the subject of extensive research. This
approach offers a means of capturing the global structure of data. Persistent
homology (PH), a common methodology within the field of TDA, has found
wide-ranging applications in machine learning. One of the key reasons for the
success of the PH-ML pipeline lies in the deterministic nature of feature
extraction conducted through PH. The ability to achieve satisfactory levels of
accuracy with relatively simple downstream machine learning models, when
processing these extracted features, underlines the pipeline's superior
interpretability. However, it must be noted that this interpretation has
encountered issues. Specifically, it fails to accurately reflect the feasible
parameter region in the data generation process, and the physical or chemical
constraints that restrict this process. Against this backdrop, we explore the
potential application of explainable AI methodologies to this PH-ML pipeline.
We apply this approach to the specific problem of predicting gas adsorption in
metal-organic frameworks and demonstrate that it can yield suggestive results.
The codes to reproduce our results are available at
https://github.com/naofumihama/xai_ph_ml
",0
"Recursive Segmentation Living Image: An eXplainable AI (XAI) Approach
  for Computing Structural Beauty of Images or the Livingness of Space","Yao Qianxiang, Bin Jiang",2023-10-16T07:37:20Z,Explainable AI,"  This study introduces the concept of ""structural beauty"" as an objective
computational approach for evaluating the aesthetic appeal of images. Through
the utilization of the Segment anything model (SAM), we propose a method that
leverages recursive segmentation to extract finer-grained substructures.
Additionally, by reconstructing the hierarchical structure, we obtain a more
accurate representation of substructure quantity and hierarchy. This approach
reproduces and extends our previous research, allowing for the simultaneous
assessment of Livingness in full-color images without the need for grayscale
conversion or separate computations for foreground and background Livingness.
Furthermore, the application of our method to the Scenic or Not dataset, a
repository of subjective scenic ratings, demonstrates a high degree of
consistency with subjective ratings in the 0-6 score range. This underscores
that structural beauty is not solely a subjective perception, but a
quantifiable attribute accessible through objective computation. Through our
case studies, we have arrived at three significant conclusions. 1) our method
demonstrates the capability to accurately segment meaningful objects, including
trees, buildings, and windows, as well as abstract substructures within
paintings. 2) we observed that the clarity of an image impacts our
computational results; clearer images tend to yield higher Livingness scores.
However, for equally blurry images, Livingness does not exhibit a significant
reduction, aligning with human visual perception. 3) our approach fundamentally
differs from methods employing Convolutional Neural Networks (CNNs) for
predicting image scores. Our method not only provides computational results but
also offers transparency and interpretability, positioning it as a novel avenue
in the realm of Explainable AI (XAI).
",0
"Parcel loss prediction in last-mile delivery: deep and non-deep
  approaches with insights from Explainable AI","Jan de Leeuw, Zaharah Bukhsh, Yingqian Zhang",2023-10-25T12:46:34Z,Explainable AI,"  Within the domain of e-commerce retail, an important objective is the
reduction of parcel loss during the last-mile delivery phase. The
ever-increasing availability of data, including product, customer, and order
information, has made it possible for the application of machine learning in
parcel loss prediction. However, a significant challenge arises from the
inherent imbalance in the data, i.e., only a very low percentage of parcels are
lost. In this paper, we propose two machine learning approaches, namely, Data
Balance with Supervised Learning (DBSL) and Deep Hybrid Ensemble Learning
(DHEL), to accurately predict parcel loss. The practical implication of such
predictions is their value in aiding e-commerce retailers in optimizing
insurance-related decision-making policies. We conduct a comprehensive
evaluation of the proposed machine learning models using one year data from
Belgian shipments. The findings show that the DHEL model, which combines a
feed-forward autoencoder with a random forest, achieves the highest
classification performance. Furthermore, we use the techniques from Explainable
AI (XAI) to illustrate how prediction models can be used in enhancing business
processes and augmenting the overall value proposition for e-commerce retailers
in the last mile delivery.
",0
"Understanding whole-body inter-personal dynamics between two players
  using neural Granger causality as the explainable AI (XAI)","Ryota Takamido, Chiharu Suzuki, Jun Ota, Hiroki Nakamoto",2024-01-12T07:17:56Z,Explainable AI,"  Background: Simultaneously focusing on intra- and inter-individual body
dynamics and elucidating how these affect each other will help understand human
inter-personal coordination behavior. However, this association has not been
investigated previously owing to difficulties in analyzing complex causal
relations among several body components.To address this issue, this study
proposes a new analytical framework that attempts to understand the underlying
causal structures behind each joint movement of individual baseball players
using neural Granger causality (NGC) as the explainable AI. Methods: In the NGC
analysis, causal relationships were defined as the size of the weight
parameters of the first layer of a machine-learning model trained to predict
the future state of a specific time-series variable. To verify the approach in
a practical context, we conducted an experiment with 16 pairs of expert
baseball pitchers and batters; input datasets with 27 joint resultant velocity
data (joints of 13 pitchers and 14 batters) were generated and used for model
training.Results: NGC analysis revealed significant causal relations among
intra- and inter-individual body components such as the batter's hands having a
causal effect from the pitcher's throwing arm. Remarkably, although the
causality from the batter's body to pitcher's body is much lower than the
reverse, it is significantly correlated with batter performance outcomes.
Conclusions: The above results suggest the effectiveness of NGC analysis for
understanding whole-body inter-personal coordination dynamics and that of the
AI technique as a new approach for analyzing complex human behavior from a
different perspective than conventional techniques.
",0
"Of Models and Tin Men: A Behavioural Economics Study of Principal-Agent
  Problems in AI Alignment using Large-Language Models","Steve Phelps, Rebecca Ranson",2023-07-20T17:19:15Z,AI Alignment,"  AI Alignment is often presented as an interaction between a single designer
and an artificial agent in which the designer attempts to ensure the agent's
behavior is consistent with its purpose, and risks arise solely because of
conflicts caused by inadvertent misalignment between the utility function
intended by the designer and the resulting internal utility function of the
agent. With the advent of agents instantiated with large-language models
(LLMs), which are typically pre-trained, we argue this does not capture the
essential aspects of AI safety because in the real world there is not a
one-to-one correspondence between designer and agent, and the many agents, both
artificial and human, have heterogeneous values. Therefore, there is an
economic aspect to AI safety and the principal-agent problem is likely to
arise. In a principal-agent problem conflict arises because of information
asymmetry together with inherent misalignment between the utility of the agent
and its principal, and this inherent misalignment cannot be overcome by
coercing the agent into adopting a desired utility function through training.
We argue the assumptions underlying principal-agent problems are crucial to
capturing the essence of safety problems involving pre-trained AI models in
real-world situations. Taking an empirical approach to AI safety, we
investigate how GPT models respond in principal-agent conflicts. We find that
agents based on both GPT-3.5 and GPT-4 override their principal's objectives in
a simple online shopping task, showing clear evidence of principal-agent
conflict. Surprisingly, the earlier GPT-3.5 model exhibits more nuanced
behaviour in response to changes in information asymmetry, whereas the later
GPT-4 model is more rigid in adhering to its prior alignment. Our results
highlight the importance of incorporating principles from economics into the
alignment process.
",0
Quantum reinforcement learning in continuous action space,"Shaojun Wu, Shan Jin, Dingding Wen, Donghong Han, Xiaoting Wang",2020-12-19T15:16:17Z,Reinforcement Learning,"  Quantum reinforcement learning (QRL) is one promising algorithm proposed for
near-term quantum devices. Early QRL proposals are effective at solving
problems in discrete action space, but often suffer from the curse of
dimensionality in the continuous domain due to discretization. To address this
problem, we propose a quantum Deep Deterministic Policy Gradient algorithm that
is efficient at solving both classical and quantum sequential decision problems
in the continuous domain. As an application, our method can solve the quantum
state-generation problem in a single shot: it only requires a one-shot
optimization to generate a model that outputs the desired control sequence for
arbitrary target state. In comparison, the standard quantum control method
requires optimizing for each target state. Moreover, our method can also be
used to physically reconstruct an unknown quantum state.
",0
Reinforcement Learning-based Product Delivery Frequency Control,"Yang Liu, Zhengxing Chen, Kittipat Virochsiri, Juan Wang, Jiahao Wu, Feng Liang",2020-12-20T07:22:34Z,Reinforcement Learning,"  Frequency control is an important problem in modern recommender systems. It
dictates the delivery frequency of recommendations to maintain product quality
and efficiency. For example, the frequency of delivering promotional
notifications impacts daily metrics as well as the infrastructure resource
consumption (e.g. CPU and memory usage). There remain open questions on what
objective we should optimize to represent business values in the long term
best, and how we should balance between daily metrics and resource consumption
in a dynamically fluctuating environment. We propose a personalized methodology
for the frequency control problem, which combines long-term value optimization
using reinforcement learning (RL) with a robust volume control technique we
termed ""Effective Factor"". We demonstrate statistically significant improvement
in daily metrics and resource efficiency by our method in several notification
applications at a scale of billions of users. To our best knowledge, our study
represents the first deep RL application on the frequency control problem at
such an industrial scale.
",0
Reinforcement Learning for Test Case Prioritization,"João Lousada, Miguel Ribeiro",2020-12-18T11:08:20Z,Reinforcement Learning,"  In modern software engineering, Continuous Integration (CI) has become an
indispensable step towards systematically managing the life cycles of software
development. Large companies struggle with keeping the pipeline updated and
operational, in useful time, due to the large amount of changes and addition of
features, that build on top of each other and have several developers, working
on different platforms. Associated with such software changes, there is always
a strong component of Testing. As teams and projects grow, exhaustive testing
quickly becomes inhibitive, becoming adamant to select the most relevant test
cases earlier, without compromising software quality. This paper extends recent
studies on applying Reinforcement Learning to optimize testing strategies. We
test its ability to adapt to new environments, by testing it on novel data
extracted from a financial institution, yielding a Normalized percentage of
Fault Detection (NAPFD) of over $0.6$ using the Network Approximator and Test
Case Failure Reward. Additionally, we studied the impact of using Decision Tree
(DT) Approximator as a model for memory representation, which failed to produce
significant improvements relative to Artificial Neural Networks.
",0
Stability-Certified Reinforcement Learning via Spectral Normalization,"Ryoichi Takase, Nobuyuki Yoshikawa, Toshisada Mariyama, Takeshi Tsuchiya",2020-12-26T14:26:24Z,Reinforcement Learning,"  In this article, two types of methods from different perspectives based on
spectral normalization are described for ensuring the stability of the system
controlled by a neural network. The first one is that the L2 gain of the
feedback system is bounded less than 1 to satisfy the stability condition
derived from the small-gain theorem. While explicitly including the stability
condition, the first method may provide an insufficient performance on the
neural network controller due to its strict stability condition. To overcome
this difficulty, the second one is proposed, which improves the performance
while ensuring the local stability with a larger region of attraction. In the
second method, the stability is ensured by solving linear matrix inequalities
after training the neural network controller. The spectral normalization
proposed in this article improves the feasibility of the a-posteriori stability
test by constructing tighter local sectors. The numerical experiments show that
the second method provides enough performance compared with the first one while
ensuring enough stability compared with the existing reinforcement learning
algorithms.
",0
Deep reinforcement learning for portfolio management,"Gang Huang, Xiaohua Zhou, Qingyang Song",2020-12-26T16:25:20Z,Reinforcement Learning,"  In our paper, we apply deep reinforcement learning approach to optimize
investment decisions in portfolio management. We make several innovations, such
as adding short mechanism and designing an arbitrage mechanism, and applied our
model to make decision optimization for several randomly selected portfolios.
The experimental results show that our model is able to optimize investment
decisions and has the ability to obtain excess return in stock market, and the
optimized agent maintains the asset weights at fixed value throughout the
trading periods and trades at a very low transaction cost rate. In addition, we
redesigned the formula for calculating portfolio asset weights in continuous
trading process which can make leverage trading, that fills the theoretical gap
in the calculation of portfolio weights when shorting.
",0
Reinforcement Learning for Control of Valves,Rajesh Siraskar,2020-12-29T09:01:47Z,Reinforcement Learning,"  This paper is a study of reinforcement learning (RL) as an optimal-control
strategy for control of nonlinear valves. It is evaluated against the PID
(proportional-integral-derivative) strategy, using a unified framework. RL is
an autonomous learning mechanism that learns by interacting with its
environment. It is gaining increasing attention in the world of control systems
as a means of building optimal-controllers for challenging dynamic and
nonlinear processes. Published RL research often uses open-source tools (Python
and OpenAI Gym environments). We use MATLAB's recently launched (R2019a)
Reinforcement Learning Toolbox to develop the valve controller; trained using
the DDPG (Deep Deterministic Policy-Gradient) algorithm and Simulink to
simulate the nonlinear valve and create the experimental test-bench for
evaluation. Simulink allows industrial engineers to quickly adapt and
experiment with other systems of their choice. Results indicate that the RL
controller is extremely good at tracking the signal with speed and produces a
lower error with respect to the reference signal. The PID, however, is better
at disturbance rejection and hence provides a longer life for the valves.
Successful machine learning involves tuning many hyperparameters requiring
significant investment of time and efforts. We introduce ""Graded Learning"" as a
simplified, application oriented adaptation of the more formal and algorithmic
""Curriculum for Reinforcement Learning"". It is shown via experiments that it
helps converge the learning task of complex non-linear real world systems.
Finally, experiential learnings gained from this research are corroborated
against published research.
",0
Partially Observable Mean Field Reinforcement Learning,"Sriram Ganapathi Subramanian, Matthew E. Taylor, Mark Crowley, Pascal Poupart",2020-12-31T18:12:31Z,Reinforcement Learning,"  Traditional multi-agent reinforcement learning algorithms are not scalable to
environments with more than a few agents, since these algorithms are
exponential in the number of agents. Recent research has introduced successful
methods to scale multi-agent reinforcement learning algorithms to many agent
scenarios using mean field theory. Previous work in this field assumes that an
agent has access to exact cumulative metrics regarding the mean field behaviour
of the system, which it can then use to take its actions. In this paper, we
relax this assumption and maintain a distribution to model the uncertainty
regarding the mean field of the system. We consider two different settings for
this problem. In the first setting, only agents in a fixed neighbourhood are
visible, while in the second setting, the visibility of agents is determined at
random based on distances. For each of these settings, we introduce a
Q-learning based algorithm that can learn effectively. We prove that this
Q-learning estimate stays very close to the Nash Q-value (under a common set of
assumptions) for the first setting. We also empirically show our algorithms
outperform multiple baselines in three different games in the MAgents
framework, which supports large environments with many agents learning
simultaneously to achieve possibly distinct goals.
",0
When Is Generalizable Reinforcement Learning Tractable?,"Dhruv Malik, Yuanzhi Li, Pradeep Ravikumar",2021-01-01T19:08:24Z,Reinforcement Learning,"  Agents trained by reinforcement learning (RL) often fail to generalize beyond
the environment they were trained in, even when presented with new scenarios
that seem similar to the training environment. We study the query complexity
required to train RL agents that generalize to multiple environments.
Intuitively, tractable generalization is only possible when the environments
are similar or close in some sense. To capture this, we introduce Weak
Proximity, a natural structural condition that requires the environments to
have highly similar transition and reward functions and share a policy
providing optimal value. Despite such shared structure, we prove that tractable
generalization is impossible in the worst case. This holds even when each
individual environment can be efficiently solved to obtain an optimal linear
policy, and when the agent possesses a generative model. Our lower bound
applies to the more complex task of representation learning for the purpose of
efficient generalization to multiple environments. On the positive side, we
introduce Strong Proximity, a strengthened condition which we prove is
sufficient for efficient generalization.
",0
Reinforcement Learning for Flexibility Design Problems,"Yehua Wei, Lei Zhang, Ruiyi Zhang, Shijing Si, Hao Zhang, Lawrence Carin",2021-01-02T02:44:39Z,Reinforcement Learning,"  Flexibility design problems are a class of problems that appear in strategic
decision-making across industries, where the objective is to design a ($e.g.$,
manufacturing) network that affords flexibility and adaptivity. The underlying
combinatorial nature and stochastic objectives make flexibility design problems
challenging for standard optimization methods. In this paper, we develop a
reinforcement learning (RL) framework for flexibility design problems.
Specifically, we carefully design mechanisms with noisy exploration and
variance reduction to ensure empirical success and show the unique advantage of
RL in terms of fast-adaptation. Empirical results show that the RL-based method
consistently finds better solutions compared to classical heuristics.
",0
Coding for Distributed Multi-Agent Reinforcement Learning,"Baoqian Wang, Junfei Xie, Nikolay Atanasov",2021-01-07T00:22:34Z,Reinforcement Learning,"  This paper aims to mitigate straggler effects in synchronous distributed
learning for multi-agent reinforcement learning (MARL) problems. Stragglers
arise frequently in a distributed learning system, due to the existence of
various system disturbances such as slow-downs or failures of compute nodes and
communication bottlenecks. To resolve this issue, we propose a coded
distributed learning framework, which speeds up the training of MARL algorithms
in the presence of stragglers, while maintaining the same accuracy as the
centralized approach. As an illustration, a coded distributed version of the
multi-agent deep deterministic policy gradient(MADDPG) algorithm is developed
and evaluated. Different coding schemes, including maximum distance separable
(MDS)code, random sparse code, replication-based code, and regular low density
parity check (LDPC) code are also investigated. Simulations in several
multi-robot problems demonstrate the promising performance of the proposed
framework.
",0
Affordance-based Reinforcement Learning for Urban Driving,"Tanmay Agarwal, Hitesh Arora, Jeff Schneider",2021-01-15T05:21:25Z,Reinforcement Learning,"  Traditional autonomous vehicle pipelines that follow a modular approach have
been very successful in the past both in academia and industry, which has led
to autonomy deployed on road. Though this approach provides ease of
interpretation, its generalizability to unseen environments is limited and
hand-engineering of numerous parameters is required, especially in the
prediction and planning systems. Recently, deep reinforcement learning has been
shown to learn complex strategic games and perform challenging robotic tasks,
which provides an appealing framework for learning to drive. In this work, we
propose a deep reinforcement learning framework to learn optimal control policy
using waypoints and low-dimensional visual representations, also known as
affordances. We demonstrate that our agents when trained from scratch learn the
tasks of lane-following, driving around inter-sections as well as stopping in
front of other actors or traffic lights even in the dense traffic setting. We
note that our method achieves comparable or better performance than the
baseline methods on the original and NoCrash benchmarks on the CARLA simulator.
",0
Reinforcement learning based recommender systems: A survey,"M. Mehdi Afsar, Trafford Crump, Behrouz Far",2021-01-15T19:42:10Z,Reinforcement Learning,"  Recommender systems (RSs) have become an inseparable part of our everyday
lives. They help us find our favorite items to purchase, our friends on social
networks, and our favorite movies to watch. Traditionally, the recommendation
problem was considered to be a classification or prediction problem, but it is
now widely agreed that formulating it as a sequential decision problem can
better reflect the user-system interaction. Therefore, it can be formulated as
a Markov decision process (MDP) and be solved by reinforcement learning (RL)
algorithms. Unlike traditional recommendation methods, including collaborative
filtering and content-based filtering, RL is able to handle the sequential,
dynamic user-system interaction and to take into account the long-term user
engagement. Although the idea of using RL for recommendation is not new and has
been around for about two decades, it was not very practical, mainly because of
scalability problems of traditional RL algorithms. However, a new trend has
emerged in the field since the introduction of deep reinforcement learning
(DRL), which made it possible to apply RL to the recommendation problem with
large state and action spaces. In this paper, a survey on reinforcement
learning based recommender systems (RLRSs) is presented. Our aim is to present
an outlook on the field and to provide the reader with a fairly complete
knowledge of key concepts of the field. We first recognize and illustrate that
RLRSs can be generally classified into RL- and DRL-based methods. Then, we
propose an RLRS framework with four components, i.e., state representation,
policy optimization, reward formulation, and environment building, and survey
RLRS algorithms accordingly. We highlight emerging topics and depict important
trends using various graphs and tables. Finally, we discuss important aspects
and challenges that can be addressed in the future.
",0
Hierarchical Reinforcement Learning By Discovering Intrinsic Options,"Jesse Zhang, Haonan Yu, Wei Xu",2021-01-16T20:54:31Z,Reinforcement Learning,"  We propose a hierarchical reinforcement learning method, HIDIO, that can
learn task-agnostic options in a self-supervised manner while jointly learning
to utilize them to solve sparse-reward tasks. Unlike current hierarchical RL
approaches that tend to formulate goal-reaching low-level tasks or pre-define
ad hoc lower-level policies, HIDIO encourages lower-level option learning that
is independent of the task at hand, requiring few assumptions or little
knowledge about the task structure. These options are learned through an
intrinsic entropy minimization objective conditioned on the option
sub-trajectories. The learned options are diverse and task-agnostic. In
experiments on sparse-reward robotic manipulation and navigation tasks, HIDIO
achieves higher success rates with greater sample efficiency than regular RL
baselines and two state-of-the-art hierarchical RL methods.
",0
Deep Reinforcement Learning with Embedded LQR Controllers,Wouter Caarls,2021-01-18T17:28:48Z,Reinforcement Learning,"  Reinforcement learning is a model-free optimal control method that optimizes
a control policy through direct interaction with the environment. For reaching
tasks that end in regulation, popular discrete-action methods are not well
suited due to chattering in the goal state. We compare three different ways to
solve this problem through combining reinforcement learning with classical LQR
control. In particular, we introduce a method that integrates LQR control into
the action set, allowing generalization and avoiding fixing the computed
control in the replay memory if it is based on learned dynamics. We also embed
LQR control into a continuous-action method. In all cases, we show that adding
LQR control can improve performance, although the effect is more profound if it
can be used to augment a discrete action set.
",0
Safe Multi-Agent Reinforcement Learning via Shielding,"Ingy Elsayed-Aly, Suda Bharadwaj, Christopher Amato, Rüdiger Ehlers, Ufuk Topcu, Lu Feng",2021-01-27T04:27:06Z,Reinforcement Learning,"  Multi-agent reinforcement learning (MARL) has been increasingly used in a
wide range of safety-critical applications, which require guaranteed safety
(e.g., no unsafe states are ever visited) during the learning
process.Unfortunately, current MARL methods do not have safety guarantees.
Therefore, we present two shielding approaches for safe MARL. In centralized
shielding, we synthesize a single shield to monitor all agents' joint actions
and correct any unsafe action if necessary. In factored shielding, we
synthesize multiple shields based on a factorization of the joint state space
observed by all agents; the set of shields monitors agents concurrently and
each shield is only responsible for a subset of agents at each
step.Experimental results show that both approaches can guarantee the safety of
agents during learning without compromising the quality of learned policies;
moreover, factored shielding is more scalable in the number of agents than
centralized shielding.
",0
Reinforcement Learning for Freight Booking Control Problems,"Justin Dumouchelle, Emma Frejinger, Andrea Lodi",2021-01-29T22:11:59Z,Reinforcement Learning,"  Booking control problems are sequential decision-making problems that occur
in the domain of revenue management. More precisely, freight booking control
focuses on the problem of deciding to accept or reject bookings: given a
limited capacity, accept a booking request or reject it to reserve capacity for
future bookings with potentially higher revenue. This problem can be formulated
as a finite-horizon stochastic dynamic program, where accepting a set of
requests results in a profit at the end of the booking period that depends on
the cost of fulfilling the accepted bookings. For many freight applications,
the cost of fulfilling requests is obtained by solving an operational
decision-making problem, which often requires the solutions to mixed-integer
linear programs. Routinely solving such operational problems when deploying
reinforcement learning algorithms may be too time consuming. The majority of
booking control policies are obtained by solving problem-specific mathematical
programming relaxations that are often non-trivial to generalize to new
problems and, in some cases, provide quite crude approximations.
  In this work, we propose a two-phase approach: we first train a supervised
learning model to predict the objective of the operational problem, and then we
deploy the model within reinforcement learning algorithms to compute control
policies. This approach is general: it can be used every time the objective
function of the end-of-horizon operational problem can be predicted, and it is
particularly suitable to those cases where such problems are computationally
hard. Furthermore, it allows one to leverage the recent advances in
reinforcement learning as routinely solving the operational problem is replaced
with a single prediction. Our methodology is evaluated on two booking control
problems in the literature, namely, distributional logistics and airline cargo
management.
",0
Hybrid Information-driven Multi-agent Reinforcement Learning,"William A. Dawson, Ruben Glatt, Edward Rusu, Braden C. Soper, Ryan A. Goldhahn",2021-02-01T17:28:39Z,Reinforcement Learning,"  Information theoretic sensor management approaches are an ideal solution to
state estimation problems when considering the optimal control of multi-agent
systems, however they are too computationally intensive for large state spaces,
especially when considering the limited computational resources typical of
large-scale distributed multi-agent systems. Reinforcement learning (RL) is a
promising alternative which can find approximate solutions to distributed
optimal control problems that take into account the resource constraints
inherent in many systems of distributed agents. However, the RL training can be
prohibitively inefficient, especially in low-information environments where
agents receive little to no feedback in large portions of the state space. We
propose a hybrid information-driven multi-agent reinforcement learning (MARL)
approach that utilizes information theoretic models as heuristics to help the
agents navigate large sparse state spaces, coupled with information based
rewards in an RL framework to learn higher-level policies. This paper presents
our ongoing work towards this objective. Our preliminary findings show that
such an approach can result in a system of agents that are approximately three
orders of magnitude more efficient at exploring a sparse state space than naive
baseline metrics. While the work is still in its early stages, it provides a
promising direction for future research.
",2
Metrics and continuity in reinforcement learning,"Charline Le Lan, Marc G. Bellemare, Pablo Samuel Castro",2021-02-02T14:30:41Z,Reinforcement Learning,"  In most practical applications of reinforcement learning, it is untenable to
maintain direct estimates for individual states; in continuous-state systems,
it is impossible. Instead, researchers often leverage state similarity (whether
explicitly or implicitly) to build models that can generalize well from a
limited set of samples. The notion of state similarity used, and the
neighbourhoods and topologies they induce, is thus of crucial importance, as it
will directly affect the performance of the algorithms. Indeed, a number of
recent works introduce algorithms assuming the existence of ""well-behaved""
neighbourhoods, but leave the full specification of such topologies for future
work. In this paper we introduce a unified formalism for defining these
topologies through the lens of metrics. We establish a hierarchy amongst these
metrics and demonstrate their theoretical implications on the Markov Decision
Process specifying the reinforcement learning problem. We complement our
theoretical results with empirical evaluations showcasing the differences
between the metrics considered.
",0
Persistent Rule-based Interactive Reinforcement Learning,"Adam Bignold, Francisco Cruz, Richard Dazeley, Peter Vamplew, Cameron Foale",2021-02-04T06:48:57Z,Reinforcement Learning,"  Interactive reinforcement learning has allowed speeding up the learning
process in autonomous agents by including a human trainer providing extra
information to the agent in real-time. Current interactive reinforcement
learning research has been limited to real-time interactions that offer
relevant user advice to the current state only. Additionally, the information
provided by each interaction is not retained and instead discarded by the agent
after a single-use. In this work, we propose a persistent rule-based
interactive reinforcement learning approach, i.e., a method for retaining and
reusing provided knowledge, allowing trainers to give general advice relevant
to more than just the current state. Our experimental results show persistent
advice substantially improves the performance of the agent while reducing the
number of interactions required for the trainer. Moreover, rule-based advice
shows similar performance impact as state-based advice, but with a
substantially reduced interaction count.
",0
Deceptive Reinforcement Learning for Privacy-Preserving Planning,"Zhengshang Liu, Yue Yang, Tim Miller, Peta Masters",2021-02-05T06:50:04Z,Reinforcement Learning,"  In this paper, we study the problem of deceptive reinforcement learning to
preserve the privacy of a reward function. Reinforcement learning is the
problem of finding a behaviour policy based on rewards received from
exploratory behaviour. A key ingredient in reinforcement learning is a reward
function, which determines how much reward (negative or positive) is given and
when. However, in some situations, we may want to keep a reward function
private; that is, to make it difficult for an observer to determine the reward
function used. We define the problem of privacy-preserving reinforcement
learning, and present two models for solving it. These models are based on
dissimulation -- a form of deception that `hides the truth'. We evaluate our
models both computationally and via human behavioural experiments. Results show
that the resulting policies are indeed deceptive, and that participants can
determine the true reward function less reliably than that of an honest agent.
",0
"Neurogenetic Programming Framework for Explainable Reinforcement
  Learning","Vadim Liventsev, Aki Härmä, Milan Petković",2021-02-08T14:26:02Z,Other,"  Automatic programming, the task of generating computer programs compliant
with a specification without a human developer, is usually tackled either via
genetic programming methods based on mutation and recombination of programs, or
via neural language models. We propose a novel method that combines both
approaches using a concept of a virtual neuro-genetic programmer: using
evolutionary methods as an alternative to gradient descent for neural network
training}, or scrum team. We demonstrate its ability to provide performant and
explainable solutions for various OpenAI Gym tasks, as well as inject expert
knowledge into the otherwise data-driven search for solutions.
",0
Continuous-Time Model-Based Reinforcement Learning,"Çağatay Yıldız, Markus Heinonen, Harri Lähdesmäki",2021-02-09T11:30:19Z,Reinforcement Learning,"  Model-based reinforcement learning (MBRL) approaches rely on discrete-time
state transition models whereas physical systems and the vast majority of
control tasks operate in continuous-time. To avoid time-discretization
approximation of the underlying process, we propose a continuous-time MBRL
framework based on a novel actor-critic method. Our approach also infers the
unknown state evolution differentials with Bayesian neural ordinary
differential equations (ODE) to account for epistemic uncertainty. We implement
and test our method on a new ODE-RL suite that explicitly solves
continuous-time control systems. Our experiments illustrate that the model is
robust against irregular and noisy data, is sample-efficient, and can solve
control problems which pose challenges to discrete-time MBRL methods.
",47
Transfer Reinforcement Learning across Homotopy Classes,"Zhangjie Cao, Minae Kwon, Dorsa Sadigh",2021-02-10T01:18:24Z,Reinforcement Learning,"  The ability for robots to transfer their learned knowledge to new tasks --
where data is scarce -- is a fundamental challenge for successful robot
learning. While fine-tuning has been well-studied as a simple but effective
transfer approach in the context of supervised learning, it is not as
well-explored in the context of reinforcement learning. In this work, we study
the problem of fine-tuning in transfer reinforcement learning when tasks are
parameterized by their reward functions, which are known beforehand. We
conjecture that fine-tuning drastically underperforms when source and target
trajectories are part of different homotopy classes. We demonstrate that
fine-tuning policy parameters across homotopy classes compared to fine-tuning
within a homotopy class requires more interaction with the environment, and in
certain cases is impossible. We propose a novel fine-tuning algorithm,
Ease-In-Ease-Out fine-tuning, that consists of a relaxing stage and a
curriculum learning stage to enable transfer learning across homotopy classes.
Finally, we evaluate our approach on several robotics-inspired simulated
environments and empirically verify that the Ease-In-Ease-Out fine-tuning
method can successfully fine-tune in a sample-efficient way compared to
existing baselines.
",0
Derivative-Free Reinforcement Learning: A Review,"Hong Qian, Yang Yu",2021-02-10T19:29:22Z,Reinforcement Learning,"  Reinforcement learning is about learning agent models that make the best
sequential decisions in unknown environments. In an unknown environment, the
agent needs to explore the environment while exploiting the collected
information, which usually forms a sophisticated problem to solve.
Derivative-free optimization, meanwhile, is capable of solving sophisticated
problems. It commonly uses a sampling-and-updating framework to iteratively
improve the solution, where exploration and exploitation are also needed to be
well balanced. Therefore, derivative-free optimization deals with a similar
core issue as reinforcement learning, and has been introduced in reinforcement
learning approaches, under the names of learning classifier systems and
neuroevolution/evolutionary reinforcement learning. Although such methods have
been developed for decades, recently, derivative-free reinforcement learning
exhibits attracting increasing attention. However, recent survey on this topic
is still lacking. In this article, we summarize methods of derivative-free
reinforcement learning to date, and organize the methods in aspects including
parameter updating, model selection, exploration, and parallel/distributed
methods. Moreover, we discuss some current limitations and possible future
directions, hoping that this article could bring more attentions to this topic
and serve as a catalyst for developing novel and efficient approaches.
",0
Risk-Averse Bayes-Adaptive Reinforcement Learning,"Marc Rigter, Bruno Lacerda, Nick Hawes",2021-02-10T22:34:33Z,Reinforcement Learning,"  In this work, we address risk-averse Bayes-adaptive reinforcement learning.
We pose the problem of optimising the conditional value at risk (CVaR) of the
total return in Bayes-adaptive Markov decision processes (MDPs). We show that a
policy optimising CVaR in this setting is risk-averse to both the parametric
uncertainty due to the prior distribution over MDPs, and the internal
uncertainty due to the inherent stochasticity of MDPs. We reformulate the
problem as a two-player stochastic game and propose an approximate algorithm
based on Monte Carlo tree search and Bayesian optimisation. Our experiments
demonstrate that our approach significantly outperforms baseline approaches for
this problem.
",0
Disturbing Reinforcement Learning Agents with Corrupted Rewards,"Rubén Majadas, Javier García, Fernando Fernández",2021-02-12T15:53:48Z,Reinforcement Learning,"  Reinforcement Learning (RL) algorithms have led to recent successes in
solving complex games, such as Atari or Starcraft, and to a huge impact in
real-world applications, such as cybersecurity or autonomous driving. In the
side of the drawbacks, recent works have shown how the performance of RL
algorithms decreases under the influence of soft changes in the reward
function. However, little work has been done about how sensitive these
disturbances are depending on the aggressiveness of the attack and the learning
exploration strategy. In this paper, we propose to fill this gap in the
literature analyzing the effects of different attack strategies based on reward
perturbations, and studying the effect in the learner depending on its
exploration strategy. In order to explain all the behaviors, we choose a
sub-class of MDPs: episodic, stochastic goal-only-rewards MDPs, and in
particular, an intelligible grid domain as a benchmark. In this domain, we
demonstrate that smoothly crafting adversarial rewards are able to mislead the
learner, and that using low exploration probability values, the policy learned
is more robust to corrupt rewards. Finally, in the proposed learning scenario,
a counterintuitive result arises: attacking at each learning episode is the
lowest cost attack strategy.
",5
Cooperation and Reputation Dynamics with Reinforcement Learning,"Nicolas Anastassacos, Julian García, Stephen Hailes, Mirco Musolesi",2021-02-15T12:48:56Z,Reinforcement Learning,"  Creating incentives for cooperation is a challenge in natural and artificial
systems. One potential answer is reputation, whereby agents trade the immediate
cost of cooperation for the future benefits of having a good reputation. Game
theoretical models have shown that specific social norms can make cooperation
stable, but how agents can independently learn to establish effective
reputation mechanisms on their own is less understood. We use a simple model of
reinforcement learning to show that reputation mechanisms generate two
coordination problems: agents need to learn how to coordinate on the meaning of
existing reputations and collectively agree on a social norm to assign
reputations to others based on their behavior. These coordination problems
exhibit multiple equilibria, some of which effectively establish cooperation.
When we train agents with a standard Q-learning algorithm in an environment
with the presence of reputation mechanisms, convergence to undesirable
equilibria is widespread. We propose two mechanisms to alleviate this: (i)
seeding a proportion of the system with fixed agents that steer others towards
good equilibria; and (ii), intrinsic rewards based on the idea of
introspection, i.e., augmenting agents' rewards by an amount proportionate to
the performance of their own strategy against themselves. A combination of
these simple mechanisms is successful in stabilizing cooperation, even in a
fully decentralized version of the problem where agents learn to use and assign
reputations simultaneously. We show how our results relate to the literature in
Evolutionary Game Theory, and discuss implications for artificial, human and
hybrid systems, where reputations can be used as a way to establish trust and
cooperation.
",20
Training Larger Networks for Deep Reinforcement Learning,"Kei Ota, Devesh K. Jha, Asako Kanezaki",2021-02-16T02:16:54Z,Reinforcement Learning,"  The success of deep learning in the computer vision and natural language
processing communities can be attributed to training of very deep neural
networks with millions or billions of parameters which can then be trained with
massive amounts of data. However, similar trend has largely eluded training of
deep reinforcement learning (RL) algorithms where larger networks do not lead
to performance improvement. Previous work has shown that this is mostly due to
instability during training of deep RL agents when using larger networks. In
this paper, we make an attempt to understand and address training of larger
networks for deep RL. We first show that naively increasing network capacity
does not improve performance. Then, we propose a novel method that consists of
1) wider networks with DenseNet connection, 2) decoupling representation
learning from training of RL, 3) a distributed training method to mitigate
overfitting problems. Using this three-fold technique, we show that we can
train very large networks that result in significant performance gains. We
present several ablation studies to demonstrate the efficacy of the proposed
method and some intuitive understanding of the reasons for performance gain. We
show that our proposed method outperforms other baseline algorithms on several
challenging locomotion tasks.
",0
Continuous Doubly Constrained Batch Reinforcement Learning,"Rasool Fakoor, Jonas Mueller, Kavosh Asadi, Pratik Chaudhari, Alexander J. Smola",2021-02-18T08:54:14Z,Reinforcement Learning,"  Reliant on too many experiments to learn good actions, current Reinforcement
Learning (RL) algorithms have limited applicability in real-world settings,
which can be too expensive to allow exploration. We propose an algorithm for
batch RL, where effective policies are learned using only a fixed offline
dataset instead of online interactions with the environment. The limited data
in batch RL produces inherent uncertainty in value estimates of states/actions
that were insufficiently represented in the training data. This leads to
particularly severe extrapolation when our candidate policies diverge from one
that generated the data. We propose to mitigate this issue via two
straightforward penalties: a policy-constraint to reduce this divergence and a
value-constraint that discourages overly optimistic estimates. Over a
comprehensive set of 32 continuous-action batch RL benchmarks, our approach
compares favorably to state-of-the-art methods, regardless of how the offline
data were collected.
",0
Reinforcement Learning for Datacenter Congestion Control,"Chen Tessler, Yuval Shpigelman, Gal Dalal, Amit Mandelbaum, Doron Haritan Kazakov, Benjamin Fuhrer, Gal Chechik, Shie Mannor",2021-02-18T13:49:28Z,Reinforcement Learning,"  We approach the task of network congestion control in datacenters using
Reinforcement Learning (RL). Successful congestion control algorithms can
dramatically improve latency and overall network throughput. Until today, no
such learning-based algorithms have shown practical potential in this domain.
Evidently, the most popular recent deployments rely on rule-based heuristics
that are tested on a predetermined set of benchmarks. Consequently, these
heuristics do not generalize well to newly-seen scenarios. Contrarily, we
devise an RL-based algorithm with the aim of generalizing to different
configurations of real-world datacenter networks. We overcome challenges such
as partial-observability, non-stationarity, and multi-objectiveness. We further
propose a policy gradient algorithm that leverages the analytical structure of
the reward function to approximate its derivative and improve stability. We
show that this scheme outperforms alternative popular RL approaches, and
generalizes to scenarios that were not seen during training. Our experiments,
conducted on a realistic simulator that emulates communication networks'
behavior, exhibit improved performance concurrently on the multiple considered
metrics compared to the popular algorithms deployed today in real datacenters.
Our algorithm is being productized to replace heuristics in some of the largest
datacenters in the world.
",0
Decentralized Deterministic Multi-Agent Reinforcement Learning,"Antoine Grosnit, Desmond Cai, Laura Wynter",2021-02-19T05:10:15Z,Reinforcement Learning,"  [Zhang, ICML 2018] provided the first decentralized actor-critic algorithm
for multi-agent reinforcement learning (MARL) that offers convergence
guarantees. In that work, policies are stochastic and are defined on finite
action spaces. We extend those results to offer a provably-convergent
decentralized actor-critic algorithm for learning deterministic policies on
continuous action spaces. Deterministic policies are important in real-world
settings. To handle the lack of exploration inherent in deterministic policies,
we consider both off-policy and on-policy settings. We provide the expression
of a local deterministic policy gradient, decentralized deterministic
actor-critic algorithms and convergence guarantees for linearly-approximated
value functions. This work will help enable decentralized MARL in
high-dimensional action spaces and pave the way for more widespread use of
MARL.
",5
Safe Reinforcement Learning Using Robust Action Governor,"Yutong Li, Nan Li, H. Eric Tseng, Anouck Girard, Dimitar Filev, Ilya Kolmanovsky",2021-02-21T16:50:17Z,Reinforcement Learning,"  Reinforcement Learning (RL) is essentially a trial-and-error learning
procedure which may cause unsafe behavior during the
exploration-and-exploitation process. This hinders the application of RL to
real-world control problems, especially to those for safety-critical systems.
In this paper, we introduce a framework for safe RL that is based on
integration of a RL algorithm with an add-on safety supervision module, called
the Robust Action Governor (RAG), which exploits set-theoretic techniques and
online optimization to manage safety-related requirements during learning. We
illustrate this proposed safe RL framework through an application to automotive
adaptive cruise control.
",0
Greedy-Step Off-Policy Reinforcement Learning,"Yuhui Wang, Qingyuan Wu, Pengcheng He, Xiaoyang Tan",2021-02-23T14:32:20Z,Reinforcement Learning,"  Most of the policy evaluation algorithms are based on the theories of Bellman
Expectation and Optimality Equation, which derive two popular approaches -
Policy Iteration (PI) and Value Iteration (VI). However, multi-step
bootstrapping is often at cross-purposes with and off-policy learning in
PI-based methods due to the large variance of multi-step off-policy correction.
In contrast, VI-based methods are naturally off-policy but subject to one-step
learning.In this paper, we deduce a novel multi-step Bellman Optimality
Equation by utilizing a latent structure of multi-step bootstrapping with the
optimal value function. Via this new equation, we derive a new multi-step value
iteration method that converges to the optimal value function with exponential
contraction rate $\mathcal{O}(\gamma^n)$ but only linear computational
complexity. Moreover, it can naturally derive a suite of multi-step off-policy
algorithms that can safely utilize data collected by arbitrary policies without
correction.Experiments reveal that the proposed methods are reliable, easy to
implement and achieve state-of-the-art performance on a series of standard
benchmark datasets.
",0
Memory-based Deep Reinforcement Learning for POMDPs,"Lingheng Meng, Rob Gorbet, Dana Kulić",2021-02-24T15:25:13Z,Reinforcement Learning,"  A promising characteristic of Deep Reinforcement Learning (DRL) is its
capability to learn optimal policy in an end-to-end manner without relying on
feature engineering. However, most approaches assume a fully observable state
space, i.e. fully observable Markov Decision Processes (MDPs). In real-world
robotics, this assumption is unpractical, because of issues such as sensor
sensitivity limitations and sensor noise, and the lack of knowledge about
whether the observation design is complete or not. These scenarios lead to
Partially Observable MDPs (POMDPs). In this paper, we propose
Long-Short-Term-Memory-based Twin Delayed Deep Deterministic Policy Gradient
(LSTM-TD3) by introducing a memory component to TD3, and compare its
performance with other DRL algorithms in both MDPs and POMDPs. Our results
demonstrate the significant advantages of the memory component in addressing
POMDPs, including the ability to handle missing and noisy observation data.
",0
Information Directed Reward Learning for Reinforcement Learning,"David Lindner, Matteo Turchetta, Sebastian Tschiatschek, Kamil Ciosek, Andreas Krause",2021-02-24T18:46:42Z,Reinforcement Learning,"  For many reinforcement learning (RL) applications, specifying a reward is
difficult. This paper considers an RL setting where the agent obtains
information about the reward only by querying an expert that can, for example,
evaluate individual states or provide binary preferences over trajectories.
From such expensive feedback, we aim to learn a model of the reward that allows
standard RL algorithms to achieve high expected returns with as few expert
queries as possible. To this end, we propose Information Directed Reward
Learning (IDRL), which uses a Bayesian model of the reward and selects queries
that maximize the information gain about the difference in return between
plausibly optimal policies. In contrast to prior active reward learning methods
designed for specific types of queries, IDRL naturally accommodates different
query types. Moreover, it achieves similar or better performance with
significantly fewer queries by shifting the focus from reducing the reward
approximation error to improving the policy induced by the reward model. We
support our findings with extensive evaluations in multiple environments and
with different query types.
",0
Towards Safe Continuing Task Reinforcement Learning,"Miguel Calvo-Fullana, Luiz F. O. Chamon, Santiago Paternain",2021-02-24T22:12:25Z,Reinforcement Learning,"  Safety is a critical feature of controller design for physical systems. When
designing control policies, several approaches to guarantee this aspect of
autonomy have been proposed, such as robust controllers or control barrier
functions. However, these solutions strongly rely on the model of the system
being available to the designer. As a parallel development, reinforcement
learning provides model-agnostic control solutions but in general, it lacks the
theoretical guarantees required for safety. Recent advances show that under
mild conditions, control policies can be learned via reinforcement learning,
which can be guaranteed to be safe by imposing these requirements as
constraints of an optimization problem. However, to transfer from learning
safety to learning safely, there are two hurdles that need to be overcome: (i)
it has to be possible to learn the policy without having to re-initialize the
system; and (ii) the rollouts of the system need to be in themselves safe. In
this paper, we tackle the first issue, proposing an algorithm capable of
operating in the continuing task setting without the need of restarts. We
evaluate our approach in a numerical example, which shows the capabilities of
the proposed approach in learning safe policies via safe exploration.
",2
CPG-ACTOR: Reinforcement Learning for Central Pattern Generators,"Luigi Campanaro, Siddhant Gangapurwala, Daniele De Martini, Wolfgang Merkt, Ioannis Havoutis",2021-02-25T14:46:37Z,Reinforcement Learning,"  Central Pattern Generators (CPGs) have several properties desirable for
locomotion: they generate smooth trajectories, are robust to perturbations and
are simple to implement. Although conceptually promising, we argue that the
full potential of CPGs has so far been limited by insufficient sensory-feedback
information. This paper proposes a new methodology that allows tuning CPG
controllers through gradient-based optimization in a Reinforcement Learning
(RL) setting. To the best of our knowledge, this is the first time CPGs have
been trained in conjunction with a MultilayerPerceptron (MLP) network in a
Deep-RL context. In particular, we show how CPGs can directly be integrated as
the Actor in an Actor-Critic formulation. Additionally, we demonstrate how this
change permits us to integrate highly non-linear feedback directly from sensory
perception to reshape the oscillators' dynamics. Our results on a locomotion
task using a single-leg hopper demonstrate that explicitly using the CPG as the
Actor rather than as part of the environment results in a significant increase
in the reward gained over time (6x more) compared with previous approaches.
Furthermore, we show that our method without feedback reproduces results
similar to prior work with feedback. Finally, we demonstrate how our
closed-loop CPG progressively improves the hopping behaviour for longer
training epochs relying only on basic reward functions.
",0
Deep reinforcement learning for quantum Hamiltonian engineering,"Pai Peng, Xiaoyang Huang, Chao Yin, Linta Joseph, Chandrasekhar Ramanathan, Paola Cappellaro",2021-02-25T20:44:31Z,Reinforcement Learning,"  Engineering desired Hamiltonian in quantum many-body systems is essential for
applications such as quantum simulation, computation and sensing. Conventional
quantum Hamiltonian engineering sequences are designed using human intuition
based on perturbation theory, which may not describe the optimal solution and
is unable to accommodate complex experimental imperfections. Here we
numerically search for Hamiltonian engineering sequences using deep
reinforcement learning (DRL) techniques and experimentally demonstrate that
they outperform celebrated sequences on a solid-state nuclear magnetic
resonance quantum simulator. As an example, we aim at decoupling
strongly-interacting spin-1/2 systems. We train DRL agents in the presence of
different experimental imperfections and verify robustness of the output
sequences both in simulations and experiments. Surprisingly, many of the
learned sequences exhibit a common pattern that had not been discovered before,
to our knowledge, but has an meaningful analytical description. We can thus
restrict the searching space based on this control pattern, allowing to search
for longer sequences, ultimately leading to sequences that are robust against
dominant imperfections in our experiments. Our results not only demonstrate a
general method for quantum Hamiltonian engineering, but also highlight the
importance of combining black-box artificial intelligence with understanding of
physical system in order to realize experimentally feasible applications.
",0
Beyond Fine-Tuning: Transferring Behavior in Reinforcement Learning,"Víctor Campos, Pablo Sprechmann, Steven Hansen, Andre Barreto, Steven Kapturowski, Alex Vitvitskyi, Adrià Puigdomènech Badia, Charles Blundell",2021-02-24T16:51:02Z,Reinforcement Learning,"  Designing agents that acquire knowledge autonomously and use it to solve new
tasks efficiently is an important challenge in reinforcement learning.
Knowledge acquired during an unsupervised pre-training phase is often
transferred by fine-tuning neural network weights once rewards are exposed, as
is common practice in supervised domains. Given the nature of the reinforcement
learning problem, we argue that standard fine-tuning strategies alone are not
enough for efficient transfer in challenging domains. We introduce Behavior
Transfer (BT), a technique that leverages pre-trained policies for exploration
and that is complementary to transferring neural network weights. Our
experiments show that, when combined with large-scale pre-training in the
absence of rewards, existing intrinsic motivation objectives can lead to the
emergence of complex behaviors. These pre-trained policies can then be
leveraged by BT to discover better solutions than without pre-training, and
combining BT with standard fine-tuning strategies results in additional
benefits. The largest gains are generally observed in domains requiring
structured exploration, including settings where the behavior of the
pre-trained policies is misaligned with the downstream task.
",0
Reducing Conservativeness Oriented Offline Reinforcement Learning,"Hongchang Zhang, Jianzhun Shao, Yuhang Jiang, Shuncheng He, Xiangyang Ji",2021-02-27T01:21:01Z,Reinforcement Learning,"  In offline reinforcement learning, a policy learns to maximize cumulative
rewards with a fixed collection of data. Towards conservative strategy, current
methods choose to regularize the behavior policy or learn a lower bound of the
value function. However, exorbitant conservation tends to impair the policy's
generalization ability and degrade its performance, especially for the mixed
datasets. In this paper, we propose the method of reducing conservativeness
oriented reinforcement learning. On the one hand, the policy is trained to pay
more attention to the minority samples in the static dataset to address the
data imbalance problem. On the other hand, we give a tighter lower bound of
value function than previous methods to discover potential optimal actions.
Consequently, our proposed method is able to tackle the skewed distribution of
the provided dataset and derive a value function closer to the expected value
function. Experimental results demonstrate that our proposed method outperforms
the state-of-the-art methods in D4RL offline reinforcement learning evaluation
tasks and our own designed mixed datasets.
",0
Exploration and Incentives in Reinforcement Learning,"Max Simchowitz, Aleksandrs Slivkins",2021-02-28T00:15:53Z,Reinforcement Learning,"  How do you incentivize self-interested agents to $\textit{explore}$ when they
prefer to $\textit{exploit}$? We consider complex exploration problems, where
each agent faces the same (but unknown) MDP. In contrast with traditional
formulations of reinforcement learning, agents control the choice of policies,
whereas an algorithm can only issue recommendations. However, the algorithm
controls the flow of information, and can incentivize the agents to explore via
information asymmetry. We design an algorithm which explores all reachable
states in the MDP. We achieve provable guarantees similar to those for
incentivizing exploration in static, stateless exploration problems studied
previously. To the best of our knowledge, this is the first work to consider
mechanism design in a stateful, reinforcement learning setting.
",0
Reinforcement Learning for Adaptive Mesh Refinement,"Jiachen Yang, Tarik Dzanic, Brenden Petersen, Jun Kudo, Ketan Mittal, Vladimir Tomov, Jean-Sylvain Camier, Tuo Zhao, Hongyuan Zha, Tzanio Kolev, Robert Anderson, Daniel Faissol",2021-03-01T22:55:48Z,Reinforcement Learning,"  Large-scale finite element simulations of complex physical systems governed
by partial differential equations (PDE) crucially depend on adaptive mesh
refinement (AMR) to allocate computational budget to regions where higher
resolution is required. Existing scalable AMR methods make heuristic refinement
decisions based on instantaneous error estimation and thus do not aim for
long-term optimality over an entire simulation. We propose a novel formulation
of AMR as a Markov decision process and apply deep reinforcement learning (RL)
to train refinement policies directly from simulation. AMR poses a new problem
for RL as both the state dimension and available action set changes at every
step, which we solve by proposing new policy architectures with differing
generality and inductive bias. The model sizes of these policy architectures
are independent of the mesh size and hence can be deployed on larger
simulations than those used at train time. We demonstrate in comprehensive
experiments on static function estimation and time-dependent equations that RL
policies can be trained on problems without using ground truth solutions, are
competitive with a widely-used error estimator, and generalize to larger, more
complex, and unseen test problems.
",0
Offline Reinforcement Learning with Pseudometric Learning,"Robert Dadashi, Shideh Rezaeifar, Nino Vieillard, Léonard Hussenot, Olivier Pietquin, Matthieu Geist",2021-03-02T18:59:02Z,Reinforcement Learning,"  Offline Reinforcement Learning methods seek to learn a policy from logged
transitions of an environment, without any interaction. In the presence of
function approximation, and under the assumption of limited coverage of the
state-action space of the environment, it is necessary to enforce the policy to
visit state-action pairs close to the support of logged transitions. In this
work, we propose an iterative procedure to learn a pseudometric (closely
related to bisimulation metrics) from logged transitions, and use it to define
this notion of closeness. We show its convergence and extend it to the function
approximation setting. We then use this pseudometric to define a new lookup
based bonus in an actor-critic algorithm: PLOFF. This bonus encourages the
actor to stay close, in terms of the defined pseudometric, to the support of
logged transitions. Finally, we evaluate the method on hand manipulation and
locomotion tasks.
",0
Inverse Reinforcement Learning with Explicit Policy Estimates,"Navyata Sanghvi, Shinnosuke Usami, Mohit Sharma, Joachim Groeger, Kris Kitani",2021-03-04T07:00:58Z,Reinforcement Learning,"  Various methods for solving the inverse reinforcement learning (IRL) problem
have been developed independently in machine learning and economics. In
particular, the method of Maximum Causal Entropy IRL is based on the
perspective of entropy maximization, while related advances in the field of
economics instead assume the existence of unobserved action shocks to explain
expert behavior (Nested Fixed Point Algorithm, Conditional Choice Probability
method, Nested Pseudo-Likelihood Algorithm). In this work, we make previously
unknown connections between these related methods from both fields. We achieve
this by showing that they all belong to a class of optimization problems,
characterized by a common form of the objective, the associated policy and the
objective gradient. We demonstrate key computational and algorithmic
differences which arise between the methods due to an approximation of the
optimal soft value function, and describe how this leads to more efficient
algorithms. Using insights which emerge from our study of this class of
optimization problems, we identify various problem scenarios and investigate
each method's suitability for these problems.
",0
Causal Reinforcement Learning: An Instrumental Variable Approach,"Jin Li, Ye Luo, Xiaowei Zhang",2021-03-06T03:57:46Z,Reinforcement Learning,"  In the standard data analysis framework, data is first collected (once for
all), and then data analysis is carried out. Moreover, the data-generating
process is typically assumed to be exogenous. This approach is natural when the
data analyst has no impact on how the data is generated. The advancement of
digital technology, however, has facilitated firms to learn from data and make
decisions at the same time. As these decisions generate new data, the data
analyst -- a business manager or an algorithm -- also becomes the data
generator. This interaction generates a new type of bias -- reinforcement bias
-- that exacerbates the endogeneity problem in static data analysis. Causal
inference techniques ought to be incorporated into reinforcement learning to
address such issues.
",0
Particle Physics Model Building with Reinforcement Learning,"T. R. Harvey, A. Lukas",2021-03-08T14:00:22Z,Reinforcement Learning,"  In this paper, we apply reinforcement learning to particle physics model
building. As an example environment, we use the space of Froggatt-Nielsen type
models for quark masses. Using a basic policy-based algorithm we show that
neural networks can be successfully trained to construct Froggatt-Nielsen
models which are consistent with the observed quark masses and mixing. The
trained policy networks lead from random to phenomenologically acceptable
models for over 90% of episodes and after an average episode length of about 20
steps. We also show that the networks are capable of finding models proposed in
the literature when starting at nearby configurations.
",0
Adversarial Reinforcement Learning for Procedural Content Generation,"Linus Gisslén, Andy Eakins, Camilo Gordillo, Joakim Bergdahl, Konrad Tollmar",2021-03-08T15:51:42Z,Reinforcement Learning,"  We present a new approach ARLPCG: Adversarial Reinforcement Learning for
Procedural Content Generation, which procedurally generates and tests
previously unseen environments with an auxiliary input as a control variable.
Training RL agents over novel environments is a notoriously difficult task. One
popular approach is to procedurally generate different environments to increase
the generalizability of the trained agents. ARLPCG instead deploys an
adversarial model with one PCG RL agent (called Generator) and one solving RL
agent (called Solver). The Generator receives a reward signal based on the
Solver's performance, which encourages the environment design to be challenging
but not impossible. To further drive diversity and control of the environment
generation, we propose using auxiliary inputs for the Generator. The benefit is
two-fold: Firstly, the Solver achieves better generalization through the
Generator's generated challenges. Secondly, the trained Generator can be used
as a creator of novel environments that, together with the Solver, can be shown
to be solvable. We create two types of 3D environments to validate our model,
representing two popular game genres: a third-person platformer and a racing
game. In these cases, we shows that ARLPCG has a significantly better solve
ratio, and that the auxiliary inputs renders the levels creation controllable
to a certain degree. For a video compilation of the results please visit
https://youtu.be/z7q2PtVsT0I.
",0
A Crash Course on Reinforcement Learning,"Farnaz Adib Yaghmaie, Lennart Ljung",2021-03-08T17:15:34Z,Reinforcement Learning,"  The emerging field of Reinforcement Learning (RL) has led to impressive
results in varied domains like strategy games, robotics, etc. This handout aims
to give a simple introduction to RL from control perspective and discuss three
possible approaches to solve an RL problem: Policy Gradient, Policy Iteration,
and Model-building. Dynamical systems might have discrete action-space like
cartpole where two possible actions are +1 and -1 or continuous action space
like linear Gaussian systems. Our discussion covers both cases.
",0
Parametrized quantum policies for reinforcement learning,"Sofiene Jerbi, Casper Gyurik, Simon C. Marshall, Hans J. Briegel, Vedran Dunjko",2021-03-09T17:33:09Z,Reinforcement Learning,"  With the advent of real-world quantum computing, the idea that parametrized
quantum computations can be used as hypothesis families in a quantum-classical
machine learning system is gaining increasing traction. Such hybrid systems
have already shown the potential to tackle real-world tasks in supervised and
generative learning, and recent works have established their provable
advantages in special artificial tasks. Yet, in the case of reinforcement
learning, which is arguably most challenging and where learning boosts would be
extremely valuable, no proposal has been successful in solving even standard
benchmarking tasks, nor in showing a theoretical learning advantage over
classical algorithms. In this work, we achieve both. We propose a hybrid
quantum-classical reinforcement learning model using very few qubits, which we
show can be effectively trained to solve several standard benchmarking
environments. Moreover, we demonstrate, and formally prove, the ability of
parametrized quantum circuits to solve certain learning tasks that are
intractable for classical models, including current state-of-art deep neural
networks, under the widely-believed classical hardness of the discrete
logarithm problem.
",0
Challenges for Reinforcement Learning in Healthcare,"Elsa Riachi, Muhammad Mamdani, Michael Fralick, Frank Rudzicz",2021-03-09T18:34:54Z,Reinforcement Learning,"  Many healthcare decisions involve navigating through a multitude of treatment
options in a sequential and iterative manner to find an optimal treatment
pathway with the goal of an optimal patient outcome. Such optimization problems
may be amenable to reinforcement learning. A reinforcement learning agent could
be trained to provide treatment recommendations for physicians, acting as a
decision support tool. However, a number of difficulties arise when using RL
beyond benchmark environments, such as specifying the reward function, choosing
an appropriate state representation and evaluating the learned policy.
",0
Generalizable Episodic Memory for Deep Reinforcement Learning,"Hao Hu, Jianing Ye, Guangxiang Zhu, Zhizhou Ren, Chongjie Zhang",2021-03-11T05:31:21Z,Reinforcement Learning,"  Episodic memory-based methods can rapidly latch onto past successful
strategies by a non-parametric memory and improve sample efficiency of
traditional reinforcement learning. However, little effort is put into the
continuous domain, where a state is never visited twice, and previous episodic
methods fail to efficiently aggregate experience across trajectories. To
address this problem, we propose Generalizable Episodic Memory (GEM), which
effectively organizes the state-action values of episodic memory in a
generalizable manner and supports implicit planning on memorized trajectories.
GEM utilizes a double estimator to reduce the overestimation bias induced by
value propagation in the planning process. Empirical evaluation shows that our
method significantly outperforms existing trajectory-based methods on various
MuJoCo continuous control tasks. To further show the general applicability, we
evaluate our method on Atari games with discrete action space, which also shows
a significant improvement over baseline algorithms.
",36
Multi-Task Federated Reinforcement Learning with Adversaries,"Aqeel Anwar, Arijit Raychowdhury",2021-03-11T05:39:52Z,Reinforcement Learning,"  Reinforcement learning algorithms, just like any other Machine learning
algorithm pose a serious threat from adversaries. The adversaries can
manipulate the learning algorithm resulting in non-optimal policies. In this
paper, we analyze the Multi-task Federated Reinforcement Learning algorithms,
where multiple collaborative agents in various environments are trying to
maximize the sum of discounted return, in the presence of adversarial agents.
We argue that the common attack methods are not guaranteed to carry out a
successful attack on Multi-task Federated Reinforcement Learning and propose an
adaptive attack method with better attack performance. Furthermore, we modify
the conventional federated reinforcement learning algorithm to address the
issue of adversaries that works equally well with and without the adversaries.
Experimentation on different small to mid-size reinforcement learning problems
show that the proposed attack method outperforms other general attack methods
and the proposed modification to federated reinforcement learning algorithm was
able to achieve near-optimal policies in the presence of adversarial agents.
",0
Symbolic Reinforcement Learning for Safe RAN Control,"Alexandros Nikou, Anusha Mujumdar, Marin Orlic, Aneta Vulgarakis Feljan",2021-03-11T10:56:49Z,Reinforcement Learning,"  In this paper, we demonstrate a Symbolic Reinforcement Learning (SRL)
architecture for safe control in Radio Access Network (RAN) applications. In
our automated tool, a user can select a high-level safety specifications
expressed in Linear Temporal Logic (LTL) to shield an RL agent running in a
given cellular network with aim of optimizing network performance, as measured
through certain Key Performance Indicators (KPIs). In the proposed
architecture, network safety shielding is ensured through model-checking
techniques over combined discrete system models (automata) that are abstracted
through reinforcement learning. We demonstrate the user interface (UI) helping
the user set intent specifications to the architecture and inspect the
difference in allowed and blocked actions.
",0
Large Batch Simulation for Deep Reinforcement Learning,"Brennan Shacklett, Erik Wijmans, Aleksei Petrenko, Manolis Savva, Dhruv Batra, Vladlen Koltun, Kayvon Fatahalian",2021-03-12T00:22:50Z,Reinforcement Learning,"  We accelerate deep reinforcement learning-based training in visually complex
3D environments by two orders of magnitude over prior work, realizing
end-to-end training speeds of over 19,000 frames of experience per second on a
single GPU and up to 72,000 frames per second on a single eight-GPU machine.
The key idea of our approach is to design a 3D renderer and embodied navigation
simulator around the principle of ""batch simulation"": accepting and executing
large batches of requests simultaneously. Beyond exposing large amounts of work
at once, batch simulation allows implementations to amortize in-memory storage
of scene assets, rendering work, data loading, and synchronization costs across
many simulation requests, dramatically improving the number of simulated agents
per GPU and overall simulation throughput. To balance DNN inference and
training costs with faster simulation, we also build a computationally
efficient policy DNN that maintains high task performance, and modify training
algorithms to maintain sample efficiency when training with large mini-batches.
By combining batch simulation and DNN performance optimizations, we demonstrate
that PointGoal navigation agents can be trained in complex 3D environments on a
single GPU in 1.5 days to 97% of the accuracy of agents trained on a prior
state-of-the-art system using a 64-GPU cluster over three days. We provide
open-source reference implementations of our batch 3D renderer and simulator to
facilitate incorporation of these ideas into RL systems.
",0
Quantum circuit optimization with deep reinforcement learning,"Thomas Fösel, Murphy Yuezhen Niu, Florian Marquardt, Li Li",2021-03-13T00:49:51Z,Reinforcement Learning,"  A central aspect for operating future quantum computers is quantum circuit
optimization, i.e., the search for efficient realizations of quantum algorithms
given the device capabilities. In recent years, powerful approaches have been
developed which focus on optimizing the high-level circuit structure. However,
these approaches do not consider and thus cannot optimize for the hardware
details of the quantum architecture, which is especially important for
near-term devices. To address this point, we present an approach to quantum
circuit optimization based on reinforcement learning. We demonstrate how an
agent, realized by a deep convolutional neural network, can autonomously learn
generic strategies to optimize arbitrary circuits on a specific architecture,
where the optimization target can be chosen freely by the user. We demonstrate
the feasibility of this approach by training agents on 12-qubit random
circuits, where we find on average a depth reduction by 27% and a gate count
reduction by 15%. We examine the extrapolation to larger circuits than used for
training, and envision how this approach can be utilized for near-term quantum
devices.
",0
Autonomous Drone Racing with Deep Reinforcement Learning,"Yunlong Song, Mats Steinweg, Elia Kaufmann, Davide Scaramuzza",2021-03-15T18:05:49Z,Reinforcement Learning,"  In many robotic tasks, such as autonomous drone racing, the goal is to travel
through a set of waypoints as fast as possible. A key challenge for this task
is planning the time-optimal trajectory, which is typically solved by assuming
perfect knowledge of the waypoints to pass in advance. The resulting solution
is either highly specialized for a single-track layout, or suboptimal due to
simplifying assumptions about the platform dynamics. In this work, a new
approach to near-time-optimal trajectory generation for quadrotors is
presented. Leveraging deep reinforcement learning and relative gate
observations, our approach can compute near-time-optimal trajectories and adapt
the trajectory to environment changes. Our method exhibits computational
advantages over approaches based on trajectory optimization for non-trivial
track configurations. The proposed approach is evaluated on a set of race
tracks in simulation and the real world, achieving speeds of up to 60 km/h with
a physical quadrotor.
",0
Few Shot System Identification for Reinforcement Learning,"Karim Farid, Nourhan Sakr",2021-03-16T04:43:07Z,Reinforcement Learning,"  Learning by interaction is the key to skill acquisition for most living
organisms, which is formally called Reinforcement Learning (RL). RL is
efficient in finding optimal policies for endowing complex systems with
sophisticated behavior. All paradigms of RL utilize a system model for finding
the optimal policy. Modeling dynamics can be done by formulating a mathematical
model or system identification. Dynamic models are usually exposed to aleatoric
and epistemic uncertainties that can divert the model from the one acquired and
cause the RL algorithm to exhibit erroneous behavior. Accordingly, the RL
process sensitive to operating conditions and changes in model parameters and
lose its generality. To address these problems, Intensive system identification
for modeling purposes is needed for each system even if the model dynamics
structure is the same, as the slight deviation in the model parameters can
render the model useless in RL. The existence of an oracle that can adaptively
predict the rest of the trajectory regardless of the uncertainties can help
resolve the issue. The target of this work is to present a framework for
facilitating the system identification of different instances of the same
dynamics class by learning a probability distribution of the dynamics
conditioned on observed data with variational inference and show its
reliability in robustly solving different instances of control problems with
the same model in model-based RL with maximum sample efficiency.
",0
Inclined Quadrotor Landing using Deep Reinforcement Learning,"Jacob E. Kooi, Robert Babuška",2021-03-16T13:22:51Z,Reinforcement Learning,"  Landing a quadrotor on an inclined surface is a challenging maneuver. The
final state of any inclined landing trajectory is not an equilibrium, which
precludes the use of most conventional control methods. We propose a deep
reinforcement learning approach to design an autonomous landing controller for
inclined surfaces. Using the proximal policy optimization (PPO) algorithm with
sparse rewards and a tailored curriculum learning approach, an inclined landing
policy can be trained in simulation in less than 90 minutes on a standard
laptop. The policy then directly runs on a real Crazyflie 2.1 quadrotor and
successfully performs real inclined landings in a flying arena. A single policy
evaluation takes approximately 2.5\,ms, which makes it suitable for a future
embedded implementation on the quadrotor.
",0
Maximum Entropy Reinforcement Learning with Mixture Policies,"Nir Baram, Guy Tennenholtz, Shie Mannor",2021-03-18T11:23:39Z,Reinforcement Learning,"  Mixture models are an expressive hypothesis class that can approximate a rich
set of policies. However, using mixture policies in the Maximum Entropy
(MaxEnt) framework is not straightforward. The entropy of a mixture model is
not equal to the sum of its components, nor does it have a closed-form
expression in most cases. Using such policies in MaxEnt algorithms, therefore,
requires constructing a tractable approximation of the mixture entropy. In this
paper, we derive a simple, low-variance mixture-entropy estimator. We show that
it is closely related to the sum of marginal entropies. Equipped with our
entropy estimator, we derive an algorithmic variant of Soft Actor-Critic (SAC)
to the mixture policy case and evaluate it on a series of continuous control
tasks.
",3
Discriminator Augmented Model-Based Reinforcement Learning,"Behzad Haghgoo, Allan Zhou, Archit Sharma, Chelsea Finn",2021-03-24T06:01:55Z,Reinforcement Learning,"  By planning through a learned dynamics model, model-based reinforcement
learning (MBRL) offers the prospect of good performance with little environment
interaction. However, it is common in practice for the learned model to be
inaccurate, impairing planning and leading to poor performance. This paper aims
to improve planning with an importance sampling framework that accounts and
corrects for discrepancy between the true and learned dynamics. This framework
also motivates an alternative objective for fitting the dynamics model: to
minimize the variance of value estimation during planning. We derive and
implement this objective, which encourages better prediction on trajectories
with larger returns. We observe empirically that our approach improves the
performance of current MBRL algorithms on two stochastic control problems, and
provide a theoretical basis for our method.
",0
Nearly Horizon-Free Offline Reinforcement Learning,"Tongzheng Ren, Jialian Li, Bo Dai, Simon S. Du, Sujay Sanghavi",2021-03-25T18:52:17Z,Reinforcement Learning,"  We revisit offline reinforcement learning on episodic time-homogeneous Markov
Decision Processes (MDP). For tabular MDP with $S$ states and $A$ actions, or
linear MDP with anchor points and feature dimension $d$, given the collected
$K$ episodes data with minimum visiting probability of (anchor) state-action
pairs $d_m$, we obtain nearly horizon $H$-free sample complexity bounds for
offline reinforcement learning when the total reward is upper bounded by $1$.
Specifically: 1. For offline policy evaluation, we obtain an
$\tilde{O}\left(\sqrt{\frac{1}{Kd_m}} \right)$ error bound for the plug-in
estimator, which matches the lower bound up to logarithmic factors and does not
have additional dependency on $\mathrm{poly}\left(H, S, A, d\right)$ in
higher-order term. 2.For offline policy optimization, we obtain an
$\tilde{O}\left(\sqrt{\frac{1}{Kd_m}} + \frac{\min(S, d)}{Kd_m}\right)$
sub-optimality gap for the empirical optimal policy, which approaches the lower
bound up to logarithmic factors and a high-order term, improving upon the best
known result by \cite{cui2020plug} that has additional $\mathrm{poly}\left(H,
S, d\right)$ factors in the main term. To the best of our knowledge, these are
the \emph{first} set of nearly horizon-free bounds for episodic
time-homogeneous offline tabular MDP and linear MDP with anchor points. Central
to our analysis is a simple yet effective recursion based method to bound a
""total variance"" term in the offline scenarios, which could be of individual
interest.
",0
Robust Reinforcement Learning under model misspecification,"Lebin Yu, Jian Wang, Xudong Zhang",2021-03-29T06:48:26Z,Reinforcement Learning,"  Reinforcement learning has achieved remarkable performance in a wide range of
tasks these days. Nevertheless, some unsolved problems limit its applications
in real-world control. One of them is model misspecification, a situation where
an agent is trained and deployed in environments with different transition
dynamics. We propose an novel framework that utilize history trajectory and
Partial Observable Markov Decision Process Modeling to deal with this dilemma.
Additionally, we put forward an efficient adversarial attack method to assist
robust training. Our experiments in four gym domains validate the effectiveness
of our framework.
",0
Deep Hedging of Derivatives Using Reinforcement Learning,"Jay Cao, Jacky Chen, John Hull, Zissis Poulos",2021-03-29T07:43:30Z,Reinforcement Learning,"  This paper shows how reinforcement learning can be used to derive optimal
hedging strategies for derivatives when there are transaction costs. The paper
illustrates the approach by showing the difference between using delta hedging
and optimal hedging for a short position in a call option when the objective is
to minimize a function equal to the mean hedging cost plus a constant times the
standard deviation of the hedging cost. Two situations are considered. In the
first, the asset price follows a geometric Brownian motion. In the second, the
asset price follows a stochastic volatility process. The paper extends the
basic reinforcement learning approach in a number of ways. First, it uses two
different Q-functions so that both the expected value of the cost and the
expected value of the square of the cost are tracked for different state/action
combinations. This approach increases the range of objective functions that can
be used. Second, it uses a learning algorithm that allows for continuous state
and action space. Third, it compares the accounting P&L approach (where the
hedged position is valued at each step) and the cash flow approach (where cash
inflows and outflows are used). We find that a hybrid approach involving the
use of an accounting P&L approach that incorporates a relatively simple
valuation model works well. The valuation model does not have to correspond to
the process assumed for the underlying asset price.
",0
Influencing Reinforcement Learning through Natural Language Guidance,"Tasmia Tasrin, Md Sultan Al Nahian, Habarakadage Perera, Brent Harrison",2021-04-04T00:23:39Z,Reinforcement Learning,"  Interactive reinforcement learning agents use human feedback or instruction
to help them learn in complex environments. Often, this feedback comes in the
form of a discrete signal that is either positive or negative. While
informative, this information can be difficult to generalize on its own. In
this work, we explore how natural language advice can be used to provide a
richer feedback signal to a reinforcement learning agent by extending policy
shaping, a well-known Interactive reinforcement learning technique. Usually
policy shaping employs a human feedback policy to help an agent to learn more
about how to achieve its goal. In our case, we replace this human feedback
policy with policy generated based on natural language advice. We aim to
inspect if the generated natural language reasoning provides support to a deep
reinforcement learning agent to decide its actions successfully in any given
environment. So, we design our model with three networks: first one is the
experience driven, next is the advice generator and third one is the advice
driven. While the experience driven reinforcement learning agent chooses its
actions being influenced by the environmental reward, the advice driven neural
network with generated feedback by the advice generator for any new state
selects its actions to assist the reinforcement learning agent to better policy
shaping.
",0
UDO: Universal Database Optimization using Reinforcement Learning,"Junxiong Wang, Immanuel Trummer, Debabrota Basu",2021-04-05T02:40:38Z,Reinforcement Learning,"  UDO is a versatile tool for offline tuning of database systems for specific
workloads. UDO can consider a variety of tuning choices, reaching from picking
transaction code variants over index selections up to database system parameter
tuning. UDO uses reinforcement learning to converge to near-optimal
configurations, creating and evaluating different configurations via actual
query executions (instead of relying on simplifying cost models). To cater to
different parameter types, UDO distinguishes heavy parameters (which are
expensive to change, e.g. physical design parameters) from light parameters.
Specifically for optimizing heavy parameters, UDO uses reinforcement learning
algorithms that allow delaying the point at which the reward feedback becomes
available. This gives us the freedom to optimize the point in time and the
order in which different configurations are created and evaluated (by
benchmarking a workload sample). UDO uses a cost-based planner to minimize
reconfiguration overheads. For instance, it aims to amortize the creation of
expensive data structures by consecutively evaluating configurations using
them. We evaluate UDO on Postgres as well as MySQL and on TPC-H as well as
TPC-C, optimizing a variety of light and heavy parameters concurrently.
",0
Approximate Robust NMPC using Reinforcement Learning,"Hossein Nejatbakhsh Esfahani, Arash Bahari Kordabad, Sebastien Gros",2021-04-06T18:34:58Z,Reinforcement Learning,"  We present a Reinforcement Learning-based Robust Nonlinear Model Predictive
Control (RL-RNMPC) framework for controlling nonlinear systems in the presence
of disturbances and uncertainties. An approximate Robust Nonlinear Model
Predictive Control (RNMPC) of low computational complexity is used in which the
state trajectory uncertainty is modelled via ellipsoids. Reinforcement Learning
is then used in order to handle the ellipsoidal approximation and improve the
closed-loop performance of the scheme by adjusting the MPC parameters
generating the ellipsoids. The approach is tested on a simulated Wheeled Mobile
Robot (WMR) tracking a desired trajectory while avoiding static obstacles.
",0
Optimal Market Making by Reinforcement Learning,"Matias Selser, Javier Kreiner, Manuel Maurette",2021-04-08T20:13:21Z,Reinforcement Learning,"  We apply Reinforcement Learning algorithms to solve the classic quantitative
finance Market Making problem, in which an agent provides liquidity to the
market by placing buy and sell orders while maximizing a utility function. The
optimal agent has to find a delicate balance between the price risk of her
inventory and the profits obtained by capturing the bid-ask spread. We design
an environment with a reward function that determines an order relation between
policies equivalent to the original utility function. When comparing our agents
with the optimal solution and a benchmark symmetric agent, we find that the
Deep Q-Learning algorithm manages to recover the optimal agent.
",0
Inverse Reinforcement Learning: A Control Lyapunov Approach,"Samuel Tesfazgi, Armin Lederer, Sandra Hirche",2021-04-09T17:08:16Z,Reinforcement Learning,"  Inferring the intent of an intelligent agent from demonstrations and
subsequently predicting its behavior, is a critical task in many collaborative
settings. A common approach to solve this problem is the framework of inverse
reinforcement learning (IRL), where the observed agent, e.g., a human
demonstrator, is assumed to behave according to an intrinsic cost function that
reflects its intent and informs its control actions. In this work, we
reformulate the IRL inference problem to learning control Lyapunov functions
(CLF) from demonstrations by exploiting the inverse optimality property, which
states that every CLF is also a meaningful value function. Moreover, the
derived CLF formulation directly guarantees stability of inferred control
policies. We show the flexibility of our proposed method by learning from
goal-directed movement demonstrations in a continuous environment.
",0
Survey on reinforcement learning for language processing,"Victor Uc-Cetina, Nicolas Navarro-Guerrero, Anabel Martin-Gonzalez, Cornelius Weber, Stefan Wermter",2021-04-12T15:33:11Z,Reinforcement Learning,"  In recent years some researchers have explored the use of reinforcement
learning (RL) algorithms as key components in the solution of various natural
language processing tasks. For instance, some of these algorithms leveraging
deep neural learning have found their way into conversational systems. This
paper reviews the state of the art of RL methods for their possible use for
different problems of natural language processing, focusing primarily on
conversational systems, mainly due to their growing relevance. We provide
detailed descriptions of the problems as well as discussions of why RL is
well-suited to solve them. Also, we analyze the advantages and limitations of
these methods. Finally, we elaborate on promising research directions in
natural language processing that might benefit from reinforcement learning.
",75
Podracer architectures for scalable Reinforcement Learning,"Matteo Hessel, Manuel Kroiss, Aidan Clark, Iurii Kemaev, John Quan, Thomas Keck, Fabio Viola, Hado van Hasselt",2021-04-13T15:05:35Z,Reinforcement Learning,"  Supporting state-of-the-art AI research requires balancing rapid prototyping,
ease of use, and quick iteration, with the ability to deploy experiments at a
scale traditionally associated with production systems.Deep learning frameworks
such as TensorFlow, PyTorch and JAX allow users to transparently make use of
accelerators, such as TPUs and GPUs, to offload the more computationally
intensive parts of training and inference in modern deep learning systems.
Popular training pipelines that use these frameworks for deep learning
typically focus on (un-)supervised learning. How to best train reinforcement
learning (RL) agents at scale is still an active research area. In this report
we argue that TPUs are particularly well suited for training RL agents in a
scalable, efficient and reproducible way. Specifically we describe two
architectures designed to make the best use of the resources available on a TPU
Pod (a special configuration in a Google data center that features multiple TPU
devices connected to each other by extremely low latency communication
channels).
",0
Quantum Architecture Search via Deep Reinforcement Learning,"En-Jui Kuo, Yao-Lung L. Fang, Samuel Yen-Chi Chen",2021-04-15T18:53:26Z,Reinforcement Learning,"  Recent advances in quantum computing have drawn considerable attention to
building realistic application for and using quantum computers. However,
designing a suitable quantum circuit architecture requires expert knowledge.
For example, it is non-trivial to design a quantum gate sequence for generating
a particular quantum state with as fewer gates as possible. We propose a
quantum architecture search framework with the power of deep reinforcement
learning (DRL) to address this challenge. In the proposed framework, the DRL
agent can only access the Pauli-$X$, $Y$, $Z$ expectation values and a
predefined set of quantum operations for learning the target quantum state, and
is optimized by the advantage actor-critic (A2C) and proximal policy
optimization (PPO) algorithms. We demonstrate a successful generation of
quantum gate sequences for multi-qubit GHZ states without encoding any
knowledge of quantum physics in the agent. The design of our framework is
rather general and can be employed with other DRL architectures or optimization
methods to study gate synthesis and compilation for many quantum states.
",0
Deep Reinforcement Learning in a Monetary Model,"Mingli Chen, Andreas Joseph, Michael Kumhof, Xinlei Pan, Xuan Zhou",2021-04-19T14:56:44Z,Reinforcement Learning,"  We propose using deep reinforcement learning to solve dynamic stochastic
general equilibrium models. Agents are represented by deep artificial neural
networks and learn to solve their dynamic optimisation problem by interacting
with the model environment, of which they have no a priori knowledge. Deep
reinforcement learning offers a flexible yet principled way to model bounded
rationality within this general class of models. We apply our proposed approach
to a classical model from the adaptive learning literature in macroeconomics
which looks at the interaction of monetary and fiscal policy. We find that,
contrary to adaptive learning, the artificially intelligent household can solve
the model in all policy regimes.
",0
Outcome-Driven Reinforcement Learning via Variational Inference,"Tim G. J. Rudner, Vitchyr H. Pong, Rowan McAllister, Yarin Gal, Sergey Levine",2021-04-20T18:16:21Z,Reinforcement Learning,"  While reinforcement learning algorithms provide automated acquisition of
optimal policies, practical application of such methods requires a number of
design decisions, such as manually designing reward functions that not only
define the task, but also provide sufficient shaping to accomplish it. In this
paper, we view reinforcement learning as inferring policies that achieve
desired outcomes, rather than as a problem of maximizing rewards. To solve this
inference problem, we establish a novel variational inference formulation that
allows us to derive a well-shaped reward function which can be learned directly
from environment interactions. From the corresponding variational objective, we
also derive a new probabilistic Bellman backup operator and use it to develop
an off-policy algorithm to solve goal-directed tasks. We empirically
demonstrate that this method eliminates the need to hand-craft reward functions
for a suite of diverse manipulation and locomotion tasks and leads to effective
goal-directed behaviors.
",0
Adaptive Adversarial Training for Meta Reinforcement Learning,"Shiqi Chen, Zhengyu Chen, Donglin Wang",2021-04-27T16:23:34Z,Reinforcement Learning,"  Meta Reinforcement Learning (MRL) enables an agent to learn from a limited
number of past trajectories and extrapolate to a new task. In this paper, we
attempt to improve the robustness of MRL. We build upon model-agnostic
meta-learning (MAML) and propose a novel method to generate adversarial samples
for MRL by using Generative Adversarial Network (GAN). That allows us to
enhance the robustness of MRL to adversal attacks by leveraging these attacks
during meta training process.
",0
A Reinforcement Learning Environment for Polyhedral Optimizations,"Alexander Brauckmann, Andrés Goens, Jeronimo Castrillon",2021-04-28T12:41:52Z,Reinforcement Learning,"  The polyhedral model allows a structured way of defining semantics-preserving
transformations to improve the performance of a large class of loops. Finding
profitable points in this space is a hard problem which is usually approached
by heuristics that generalize from domain-expert knowledge. Existing problem
formulations in state-of-the-art heuristics depend on the shape of particular
loops, making it hard to leverage generic and more powerful optimization
techniques from the machine learning domain. In this paper, we propose PolyGym,
a shape-agnostic formulation for the space of legal transformations in the
polyhedral model as a Markov Decision Process (MDP). Instead of using
transformations, the formulation is based on an abstract space of possible
schedules. In this formulation, states model partial schedules, which are
constructed by actions that are reusable across different loops. With a simple
heuristic to traverse the space, we demonstrate that our formulation is
powerful enough to match and outperform state-of-the-art heuristics. On the
Polybench benchmark suite, we found transformations that led to a speedup of
3.39x over LLVM O3, which is 1.83x better than the speedup achieved by ISL. Our
generic MDP formulation enables using reinforcement learning to learn
optimization policies over a wide range of loops. This also contributes to the
emerging field of machine learning in compilers, as it exposes a novel problem
formulation that can push the limits of existing methods.
",6
Hypernetwork Dismantling via Deep Reinforcement Learning,"Dengcheng Yan, Wenxin Xie, Yiwen Zhang, Qiang He, Yun Yang",2021-04-29T13:35:29Z,Reinforcement Learning,"  Network dismantling aims to degrade the connectivity of a network by removing
an optimal set of nodes. It has been widely adopted in many real-world
applications such as epidemic control and rumor containment. However,
conventional methods usually focus on simple network modeling with only
pairwise interactions, while group-wise interactions modeled by hypernetwork
are ubiquitous and critical. In this work, we formulate the hypernetwork
dismantling problem as a node sequence decision problem and propose a deep
reinforcement learning (DRL)-based hypernetwork dismantling framework. Besides,
we design a novel inductive hypernetwork embedding method to ensure the
transferability to various real-world hypernetworks. Our framework first
generates small-scale synthetic hypernetworks and embeds the nodes and
hypernetworks into a low dimensional vector space to represent the action and
state space in DRL, respectively. Then trial-and-error dismantling tasks are
conducted by an agent on these synthetic hypernetworks, and the dismantling
strategy is continuously optimized. Finally, the well-optimized strategy is
applied to real-world hypernetwork dismantling tasks. Experimental results on
five real-world hypernetworks demonstrate the effectiveness of our proposed
framework.
",0
Model-Free Quantum Control with Reinforcement Learning,"V. V. Sivak, A. Eickbusch, H. Liu, B. Royer, I. Tsioutsios, M. H. Devoret",2021-04-29T17:53:26Z,Reinforcement Learning,"  Model bias is an inherent limitation of the current dominant approach to
optimal quantum control, which relies on a system simulation for optimization
of control policies. To overcome this limitation, we propose a circuit-based
approach for training a reinforcement learning agent on quantum control tasks
in a model-free way. Given a continuously parameterized control circuit, the
agent learns its parameters through trial-and-error interaction with the
quantum system, using measurement outcomes as the only source of information
about the quantum state. Focusing on control of a harmonic oscillator coupled
to an ancilla qubit, we show how to reward the learning agent using
measurements of experimentally available observables. We train the agent to
prepare various non-classical states using both unitary control and control
with adaptive measurement-based quantum feedback, and to execute logical gates
on encoded qubits. This approach significantly outperforms widely used
model-free methods in terms of sample efficiency. Our numerical work is of
immediate relevance to superconducting circuits and trapped ions platforms
where such training can be implemented in experiment, allowing complete
elimination of model bias and the adaptation of quantum control policies to the
specific system in which they are deployed.
",0
BACKDOORL: Backdoor Attack against Competitive Reinforcement Learning,"Lun Wang, Zaynah Javed, Xian Wu, Wenbo Guo, Xinyu Xing, Dawn Song",2021-05-02T23:47:55Z,Reinforcement Learning,"  Recent research has confirmed the feasibility of backdoor attacks in deep
reinforcement learning (RL) systems. However, the existing attacks require the
ability to arbitrarily modify an agent's observation, constraining the
application scope to simple RL systems such as Atari games. In this paper, we
migrate backdoor attacks to more complex RL systems involving multiple agents
and explore the possibility of triggering the backdoor without directly
manipulating the agent's observation. As a proof of concept, we demonstrate
that an adversary agent can trigger the backdoor of the victim agent with its
own action in two-player competitive RL systems. We prototype and evaluate
BACKDOORL in four competitive environments. The results show that when the
backdoor is activated, the winning rate of the victim drops by 17% to 37%
compared to when not activated.
",65
Robotic Surgery With Lean Reinforcement Learning,"Yotam Barnoy, Molly O'Brien, Will Wang, Gregory Hager",2021-05-03T16:52:26Z,Reinforcement Learning,"  As surgical robots become more common, automating away some of the burden of
complex direct human operation becomes ever more feasible. Model-free
reinforcement learning (RL) is a promising direction toward generalizable
automated surgical performance, but progress has been slowed by the lack of
efficient and realistic learning environments. In this paper, we describe
adding reinforcement learning support to the da Vinci Skill Simulator, a
training simulation used around the world to allow surgeons to learn and
rehearse technical skills. We successfully teach an RL-based agent to perform
sub-tasks in the simulator environment, using either image or state data. As
far as we know, this is the first time an RL-based agent is taught from visual
data in a surgical robotics environment. Additionally, we tackle the sample
inefficiency of RL using a simple-to-implement system which we term
hybrid-batch learning (HBL), effectively adding a second, long-term replay
buffer to the Q-learning process. Additionally, this allows us to bootstrap
learning from images from the data collected using the easier task of learning
from state. We show that HBL decreases our learning times significantly.
",0
Reinforcement Learning for Ridesharing: An Extended Survey,"Zhiwei Qin, Hongtu Zhu, Jieping Ye",2021-05-03T18:09:58Z,Reinforcement Learning,"  In this paper, we present a comprehensive, in-depth survey of the literature
on reinforcement learning approaches to decision optimization problems in a
typical ridesharing system. Papers on the topics of rideshare matching, vehicle
repositioning, ride-pooling, routing, and dynamic pricing are covered. Most of
the literature has appeared in the last few years, and several core challenges
are to continue to be tackled: model complexity, agent coordination, and joint
optimization of multiple levers. Hence, we also introduce popular data sets and
open simulation environments to facilitate further research and development.
Subsequently, we discuss a number of challenges and opportunities for
reinforcement learning research on this important domain.
",0
Data-Efficient Reinforcement Learning for Malaria Control,"Lixin Zou, Long Xia, Linfang Hou, Xiangyu Zhao, Dawei Yin",2021-05-04T16:54:16Z,Reinforcement Learning,"  Sequential decision-making under cost-sensitive tasks is prohibitively
daunting, especially for the problem that has a significant impact on people's
daily lives, such as malaria control, treatment recommendation. The main
challenge faced by policymakers is to learn a policy from scratch by
interacting with a complex environment in a few trials. This work introduces a
practical, data-efficient policy learning method, named Variance-Bonus Monte
Carlo Tree Search~(VB-MCTS), which can copy with very little data and
facilitate learning from scratch in only a few trials. Specifically, the
solution is a model-based reinforcement learning method. To avoid model bias,
we apply Gaussian Process~(GP) regression to estimate the transitions
explicitly. With the GP world model, we propose a variance-bonus reward to
measure the uncertainty about the world. Adding the reward to the planning with
MCTS can result in more efficient and effective exploration. Furthermore, the
derived polynomial sample complexity indicates that VB-MCTS is sample
efficient. Finally, outstanding performance on a competitive world-level RL
competition and extensive experimental results verify its advantage over the
state-of-the-art on the challenging malaria control task.
",0
Solving Sokoban with forward-backward reinforcement learning,"Yaron Shoham, Gal Elidan",2021-05-05T07:37:57Z,Reinforcement Learning,"  Despite seminal advances in reinforcement learning in recent years, many
domains where the rewards are sparse, e.g. given only at task completion,
remain quite challenging. In such cases, it can be beneficial to tackle the
task both from its beginning and end, and make the two ends meet. Existing
approaches that do so, however, are not effective in the common scenario where
the strategy needed near the end goal is very different from the one that is
effective earlier on.
  In this work we propose a novel RL approach for such settings. In short, we
first train a backward-looking agent with a simple relaxed goal, and then
augment the state representation of the forward-looking agent with
straightforward hint features. This allows the learned forward agent to
leverage information from backward plans, without mimicking their policy.
  We demonstrate the efficacy of our approach on the challenging game of
Sokoban, where we substantially surpass learned solvers that generalize across
levels, and are competitive with SOTA performance of the best highly-crafted
systems. Impressively, we achieve these results while learning from a small
number of practice levels and using simple RL techniques.
",4
Reinforcement learning of rare diffusive dynamics,"Avishek Das, Dominic C. Rose, Juan P. Garrahan, David T. Limmer",2021-05-10T13:00:15Z,Reinforcement Learning,"  We present a method to probe rare molecular dynamics trajectories directly
using reinforcement learning. We consider trajectories that are conditioned to
transition between regions of configuration space in finite time, like those
relevant in the study of reactive events, as well as trajectories exhibiting
rare fluctuations of time-integrated quantities in the long time limit, like
those relevant in the calculation of large deviation functions. In both cases,
reinforcement learning techniques are used to optimize an added force that
minimizes the Kullback-Leibler divergence between the conditioned trajectory
ensemble and a driven one. Under the optimized added force, the system evolves
the rare fluctuation as a typical one, affording a variational estimate of its
likelihood in the original trajectory ensemble. Low variance gradients
employing value functions are proposed to increase the convergence of the
optimal force. The method we develop employing these gradients leads to
efficient and accurate estimates of both the optimal force and the likelihood
of the rare event for a variety of model systems.
",0
Adaptive Policy Transfer in Reinforcement Learning,"Girish Joshi, Girish Chowdhary",2021-05-10T22:42:03Z,Reinforcement Learning,"  Efficient and robust policy transfer remains a key challenge for
reinforcement learning to become viable for real-wold robotics. Policy transfer
through warm initialization, imitation, or interacting over a large set of
agents with randomized instances, have been commonly applied to solve a variety
of Reinforcement Learning tasks. However, this seems far from how skill
transfer happens in the biological world: Humans and animals are able to
quickly adapt the learned behaviors between similar tasks and learn new skills
when presented with new situations. Here we seek to answer the question: Will
learning to combine adaptation and exploration lead to a more efficient
transfer of policies between domains? We introduce a principled mechanism that
can ""Adapt-to-Learn"", that is adapt the source policy to learn to solve a
target task with significant transition differences and uncertainties. We show
that the presented method learns to seamlessly combine learning from adaptation
and exploration and leads to a robust policy transfer algorithm with
significantly reduced sample complexity in transferring skills between related
tasks.
",0
Ordering-Based Causal Discovery with Reinforcement Learning,"Xiaoqiang Wang, Yali Du, Shengyu Zhu, Liangjun Ke, Zhitang Chen, Jianye Hao, Jun Wang",2021-05-14T03:49:59Z,Reinforcement Learning,"  It is a long-standing question to discover causal relations among a set of
variables in many empirical sciences. Recently, Reinforcement Learning (RL) has
achieved promising results in causal discovery from observational data.
However, searching the space of directed graphs and enforcing acyclicity by
implicit penalties tend to be inefficient and restrict the existing RL-based
method to small scale problems. In this work, we propose a novel RL-based
approach for causal discovery, by incorporating RL into the ordering-based
paradigm. Specifically, we formulate the ordering search problem as a
multi-step Markov decision process, implement the ordering generating process
with an encoder-decoder architecture, and finally use RL to optimize the
proposed model based on the reward mechanisms designed for~each ordering. A
generated ordering would then be processed using variable selection to obtain
the final causal graph. We analyze the consistency and computational complexity
of the proposed method, and empirically show that a pretrained model can be
exploited to accelerate training. Experimental results on both synthetic and
real data sets shows that the proposed method achieves a much improved
performance over existing RL-based method.
",49
Generic Itemset Mining Based on Reinforcement Learning,"Kazuma Fujioka, Kimiaki Shirahama",2021-05-17T11:57:02Z,Reinforcement Learning,"  One of the biggest problems in itemset mining is the requirement of
developing a data structure or algorithm, every time a user wants to extract a
different type of itemsets. To overcome this, we propose a method, called
Generic Itemset Mining based on Reinforcement Learning (GIM-RL), that offers a
unified framework to train an agent for extracting any type of itemsets. In
GIM-RL, the environment formulates iterative steps of extracting a target type
of itemsets from a dataset. At each step, an agent performs an action to add or
remove an item to or from the current itemset, and then obtains from the
environment a reward that represents how relevant the itemset resulting from
the action is to the target type. Through numerous trial-and-error steps where
various rewards are obtained by diverse actions, the agent is trained to
maximise cumulative rewards so that it acquires the optimal action policy for
forming as many itemsets of the target type as possible. In this framework, an
agent for extracting any type of itemsets can be trained as long as a reward
suitable for the type can be defined. The extensive experiments on mining high
utility itemsets, frequent itemsets and association rules show the general
effectiveness and one remarkable potential (agent transfer) of GIM-RL. We hope
that GIM-RL opens a new research direction towards learning-based itemset
mining.
",0
Behavior-based Neuroevolutionary Training in Reinforcement Learning,"Jörg Stork, Martin Zaefferer, Nils Eisler, Patrick Tichelmann, Thomas Bartz-Beielstein, A. E. Eiben",2021-05-17T15:40:42Z,Reinforcement Learning,"  In addition to their undisputed success in solving classical optimization
problems, neuroevolutionary and population-based algorithms have become an
alternative to standard reinforcement learning methods. However, evolutionary
methods often lack the sample efficiency of standard value-based methods that
leverage gathered state and value experience. If reinforcement learning for
real-world problems with significant resource cost is considered, sample
efficiency is essential. The enhancement of evolutionary algorithms with
experience exploiting methods is thus desired and promises valuable insights.
This work presents a hybrid algorithm that combines topology-changing
neuroevolutionary optimization with value-based reinforcement learning. We
illustrate how the behavior of policies can be used to create distance and loss
functions, which benefit from stored experiences and calculated state values.
They allow us to model behavior and perform a directed search in the behavior
space by gradient-free evolutionary algorithms and surrogate-based
optimization. For this purpose, we consolidate different methods to generate
and optimize agent policies, creating a diverse population. We exemplify the
performance of our algorithm on standard benchmarks and a purpose-built
real-world problem. Our results indicate that combining methods can enhance the
sample efficiency and learning speed for evolutionary approaches.
",0
Reinforcement Learning for Adaptive Video Compressive Sensing,"Sidi Lu, Xin Yuan, Aggelos K Katsaggelos, Weisong Shi",2021-05-18T00:01:27Z,Reinforcement Learning,"  We apply reinforcement learning to video compressive sensing to adapt the
compression ratio. Specifically, video snapshot compressive imaging (SCI),
which captures high-speed video using a low-speed camera is considered in this
work, in which multiple (B) video frames can be reconstructed from a snapshot
measurement. One research gap in previous studies is how to adapt B in the
video SCI system for different scenes. In this paper, we fill this gap
utilizing reinforcement learning (RL). An RL model, as well as various
convolutional neural networks for reconstruction, are learned to achieve
adaptive sensing of video SCI systems. Furthermore, the performance of an
object detection network using directly the video SCI measurements without
reconstruction is also used to perform RL-based adaptive video compressive
sensing. Our proposed adaptive SCI method can thus be implemented in low cost
and real time. Our work takes the technology one step further towards real
applications of video SCI.
",0
Room Clearance with Feudal Hierarchical Reinforcement Learning,"Henry Charlesworth, Adrian Millea, Eddie Pottrill, Rich Riley",2021-05-24T15:05:58Z,Reinforcement Learning,"  Reinforcement learning (RL) is a general framework that allows systems to
learn autonomously through trial-and-error interaction with their environment.
In recent years combining RL with expressive, high-capacity neural network
models has led to impressive performance in a diverse range of domains.
However, dealing with the large state and action spaces often required for
problems in the real world still remains a significant challenge. In this paper
we introduce a new simulation environment, ""Gambit"", designed as a tool to
build scenarios that can drive RL research in a direction useful for military
analysis. Using this environment we focus on an abstracted and simplified room
clearance scenario, where a team of blue agents have to make their way through
a building and ensure that all rooms are cleared of (and remain clear) of enemy
red agents. We implement a multi-agent version of feudal hierarchical RL that
introduces a command hierarchy where a commander at the higher level sends
orders to multiple agents at the lower level who simply have to learn to follow
these orders. We find that breaking the task down in this way allows us to
solve a number of non-trivial floorplans that require the coordination of
multiple agents much more efficiently than the standard baseline RL algorithms
we compare with. We then go on to explore how qualitatively different behaviour
can emerge depending on what we prioritise in the agent's reward function (e.g.
clearing the building quickly vs. prioritising rescuing civilians).
",0
Unbiased Asymmetric Reinforcement Learning under Partial Observability,"Andrea Baisero, Christopher Amato",2021-05-25T05:18:44Z,Reinforcement Learning,"  In partially observable reinforcement learning, offline training gives access
to latent information which is not available during online training and/or
execution, such as the system state. Asymmetric actor-critic methods exploit
such information by training a history-based policy via a state-based critic.
However, many asymmetric methods lack theoretical foundation, and are only
evaluated on limited domains. We examine the theory of asymmetric actor-critic
methods which use state-based critics, and expose fundamental issues which
undermine the validity of a common variant, and limit its ability to address
partial observability. We propose an unbiased asymmetric actor-critic variant
which is able to exploit state information while remaining theoretically sound,
maintaining the validity of the policy gradient theorem, and introducing no
bias and relatively low variance into the training process. An empirical
evaluation performed on domains which exhibit significant partial observability
confirms our analysis, demonstrating that unbiased asymmetric actor-critic
converges to better policies and/or faster than symmetric and biased asymmetric
baselines.
",0
A Generalised Inverse Reinforcement Learning Framework,"Firas Jarboui, Vianney Perchet",2021-05-25T10:30:45Z,Reinforcement Learning,"  The gloabal objective of inverse Reinforcement Learning (IRL) is to estimate
the unknown cost function of some MDP base on observed trajectories generated
by (approximate) optimal policies. The classical approach consists in tuning
this cost function so that associated optimal trajectories (that minimise the
cumulative discounted cost, i.e. the classical RL loss) are 'similar' to the
observed ones. Prior contributions focused on penalising degenerate solutions
and improving algorithmic scalability. Quite orthogonally to them, we question
the pertinence of characterising optimality with respect to the cumulative
discounted cost as it induces an implicit bias against policies with longer
mixing times. State of the art value based RL algorithms circumvent this issue
by solving for the fixed point of the Bellman optimality operator, a stronger
criterion that is not well defined for the inverse problem. To alleviate this
bias in IRL, we introduce an alternative training loss that puts more weights
on future states which yields a reformulation of the (maximum entropy) IRL
problem. The algorithms we devised exhibit enhanced performances (and similar
tractability) than off-the-shelf ones in multiple OpenAI gym environments.
",0
Towards Scalable Verification of Deep Reinforcement Learning,"Guy Amir, Michael Schapira, Guy Katz",2021-05-25T13:34:40Z,Reinforcement Learning,"  Deep neural networks (DNNs) have gained significant popularity in recent
years, becoming the state of the art in a variety of domains. In particular,
deep reinforcement learning (DRL) has recently been employed to train DNNs that
realize control policies for various types of real-world systems. In this work,
we present the whiRL 2.0 tool, which implements a new approach for verifying
complex properties of interest for DRL systems. To demonstrate the benefits of
whiRL 2.0, we apply it to case studies from the communication networks domain
that have recently been used to motivate formal verification of DRL systems,
and which exhibit characteristics that are conducive for scalable verification.
We propose techniques for performing k-induction and semi-automated invariant
inference on such systems, and leverage these techniques for proving safety and
liveness properties that were previously impossible to verify due to the
scalability barriers of prior approaches. Furthermore, we show how our proposed
techniques provide insights into the inner workings and the generalizability of
DRL systems. whiRL 2.0 is publicly available online.
",0
RL-GRIT: Reinforcement Learning for Grammar Inference,Walt Woods,2021-05-17T23:48:39Z,Reinforcement Learning,"  When working to understand usage of a data format, examples of the data
format are often more representative than the format's specification. For
example, two different applications might use very different JSON
representations, or two PDF-writing applications might make use of very
different areas of the PDF specification to realize the same rendered content.
The complexity arising from these distinct origins can lead to large,
difficult-to-understand attack surfaces, presenting a security concern when
considering both exfiltration and data schizophrenia. Grammar inference can aid
in describing the practical language generator behind examples of a data
format. However, most grammar inference research focuses on natural language,
not data formats, and fails to support crucial features such as type recursion.
We propose a novel set of mechanisms for grammar inference, RL-GRIT, and apply
them to understanding de facto data formats. After reviewing existing grammar
inference solutions, it was determined that a new, more flexible scaffold could
be found in Reinforcement Learning (RL). Within this work, we lay out the many
algorithmic changes required to adapt RL from its traditional, sequential-time
environment to the highly interdependent environment of parsing. The result is
an algorithm which can demonstrably learn recursive control structures in
simple data formats, and can extract meaningful structure from fragments of the
PDF format. Whereas prior work in grammar inference focused on either regular
languages or constituency parsing, we show that RL can be used to surpass the
expressiveness of both classes, and offers a clear path to learning
context-sensitive languages. The proposed algorithm can serve as a building
block for understanding the ecosystems of de facto data formats.
",0
AndroidEnv: A Reinforcement Learning Platform for Android,"Daniel Toyama, Philippe Hamel, Anita Gergely, Gheorghe Comanici, Amelia Glaese, Zafarali Ahmed, Tyler Jackson, Shibl Mourad, Doina Precup",2021-05-27T15:20:14Z,Reinforcement Learning,"  We introduce AndroidEnv, an open-source platform for Reinforcement Learning
(RL) research built on top of the Android ecosystem. AndroidEnv allows RL
agents to interact with a wide variety of apps and services commonly used by
humans through a universal touchscreen interface. Since agents train on a
realistic simulation of an Android device, they have the potential to be
deployed on real devices. In this report, we give an overview of the
environment, highlighting the significant features it provides for research,
and we present an empirical evaluation of some popular reinforcement learning
agents on a set of tasks built on this platform.
",0
Adversarial Intrinsic Motivation for Reinforcement Learning,"Ishan Durugkar, Mauricio Tec, Scott Niekum, Peter Stone",2021-05-27T17:51:34Z,Reinforcement Learning,"  Learning with an objective to minimize the mismatch with a reference
distribution has been shown to be useful for generative modeling and imitation
learning. In this paper, we investigate whether one such objective, the
Wasserstein-1 distance between a policy's state visitation distribution and a
target distribution, can be utilized effectively for reinforcement learning
(RL) tasks. Specifically, this paper focuses on goal-conditioned reinforcement
learning where the idealized (unachievable) target distribution has full
measure at the goal. This paper introduces a quasimetric specific to Markov
Decision Processes (MDPs) and uses this quasimetric to estimate the above
Wasserstein-1 distance. It further shows that the policy that minimizes this
Wasserstein-1 distance is the policy that reaches the goal in as few steps as
possible. Our approach, termed Adversarial Intrinsic Motivation (AIM),
estimates this Wasserstein-1 distance through its dual objective and uses it to
compute a supplemental reward function. Our experiments show that this reward
function changes smoothly with respect to transitions in the MDP and directs
the agent's exploration to find the goal efficiently. Additionally, we combine
AIM with Hindsight Experience Replay (HER) and show that the resulting
algorithm accelerates learning significantly on several simulated robotics
tasks when compared to other rewards that encourage exploration or accelerate
learning.
",0
Reinforcement Learning for on-line Sequence Transformation,"Grzegorz Rypeść, Łukasz Lepak, Paweł Wawrzyński",2021-05-28T20:31:25Z,Reinforcement Learning,"  A number of problems in the processing of sound and natural language, as well
as in other areas, can be reduced to simultaneously reading an input sequence
and writing an output sequence of generally different length. There are well
developed methods that produce the output sequence based on the entirely known
input. However, efficient methods that enable such transformations on-line do
not exist. In this paper we introduce an architecture that learns with
reinforcement to make decisions about whether to read a token or write another
token. This architecture is able to transform potentially infinite sequences
on-line. In an experimental study we compare it with state-of-the-art methods
for neural machine translation. While it produces slightly worse translations
than Transformer, it outperforms the autoencoder with attention, even though
our architecture translates texts on-line thereby solving a more difficult
problem than both reference methods.
",0
Goal Misgeneralization in Deep Reinforcement Learning,"Lauro Langosco, Jack Koch, Lee Sharkey, Jacob Pfau, Laurent Orseau, David Krueger",2021-05-28T21:13:34Z,Reinforcement Learning,"  We study goal misgeneralization, a type of out-of-distribution generalization
failure in reinforcement learning (RL). Goal misgeneralization failures occur
when an RL agent retains its capabilities out-of-distribution yet pursues the
wrong goal. For instance, an agent might continue to competently avoid
obstacles, but navigate to the wrong place. In contrast, previous works have
typically focused on capability generalization failures, where an agent fails
to do anything sensible at test time. We formalize this distinction between
capability and goal generalization, provide the first empirical demonstrations
of goal misgeneralization, and present a partial characterization of its
causes.
",0
Quantum Compiling by Deep Reinforcement Learning,"Lorenzo Moro, Matteo G. A. Paris, Marcello Restelli, Enrico Prati",2021-05-31T15:32:15Z,Reinforcement Learning,"  The architecture of circuital quantum computers requires computing layers
devoted to compiling high-level quantum algorithms into lower-level circuits of
quantum gates. The general problem of quantum compiling is to approximate any
unitary transformation that describes the quantum computation, as a sequence of
elements selected from a finite base of universal quantum gates. The existence
of an approximating sequence of one qubit quantum gates is guaranteed by the
Solovay-Kitaev theorem, which implies sub-optimal algorithms to establish it
explicitly. Since a unitary transformation may require significantly different
gate sequences, depending on the base considered, such a problem is of great
complexity and does not admit an efficient approximating algorithm. Therefore,
traditional approaches are time-consuming tasks, unsuitable to be employed
during quantum computation. We exploit the deep reinforcement learning method
as an alternative strategy, which has a significantly different trade-off
between search time and exploitation time. Deep reinforcement learning allows
creating single-qubit operations in real time, after an arbitrary long training
period during which a strategy for creating sequences to approximate unitary
operators is built. The deep reinforcement learning based compiling method
allows for fast computation times, which could in principle be exploited for
real-time quantum compiling.
",0
Decision Transformer: Reinforcement Learning via Sequence Modeling,"Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch",2021-06-02T17:53:39Z,Reinforcement Learning,"  We introduce a framework that abstracts Reinforcement Learning (RL) as a
sequence modeling problem. This allows us to draw upon the simplicity and
scalability of the Transformer architecture, and associated advances in
language modeling such as GPT-x and BERT. In particular, we present Decision
Transformer, an architecture that casts the problem of RL as conditional
sequence modeling. Unlike prior approaches to RL that fit value functions or
compute policy gradients, Decision Transformer simply outputs the optimal
actions by leveraging a causally masked Transformer. By conditioning an
autoregressive model on the desired return (reward), past states, and actions,
our Decision Transformer model can generate future actions that achieve the
desired return. Despite its simplicity, Decision Transformer matches or exceeds
the performance of state-of-the-art model-free offline RL baselines on Atari,
OpenAI Gym, and Key-to-Door tasks.
",1256
LiMIIRL: Lightweight Multiple-Intent Inverse Reinforcement Learning,"Aaron J. Snoswell, Surya P. N. Singh, Nan Ye",2021-06-03T12:00:38Z,Reinforcement Learning,"  Multiple-Intent Inverse Reinforcement Learning (MI-IRL) seeks to find a
reward function ensemble to rationalize demonstrations of different but
unlabelled intents. Within the popular expectation maximization (EM) framework
for learning probabilistic MI-IRL models, we present a warm-start strategy
based on up-front clustering of the demonstrations in feature space. Our
theoretical analysis shows that this warm-start solution produces a
near-optimal reward ensemble, provided the behavior modes satisfy mild
separation conditions. We also propose a MI-IRL performance metric that
generalizes the popular Expected Value Difference measure to directly assesses
learned rewards against the ground-truth reward ensemble. Our metric elegantly
addresses the difficulty of pairing up learned and ground truth rewards via a
min-cost flow formulation, and is efficiently computable. We also develop a
MI-IRL benchmark problem that allows for more comprehensive algorithmic
evaluations. On this problem, we find our MI-IRL warm-start strategy helps
avoid poor quality local minima reward ensembles, resulting in a significant
improvement in behavior clustering. Our extensive sensitivity analysis
demonstrates that the quality of the learned reward ensembles is improved under
various settings, including cases where our theoretical assumptions do not
necessarily hold. Finally, we demonstrate the effectiveness of our methods by
discovering distinct driving styles in a large real-world dataset of driver GPS
trajectories.
",0
Differentiable Architecture Search for Reinforcement Learning,"Yingjie Miao, Xingyou Song, John D. Co-Reyes, Daiyi Peng, Summer Yue, Eugene Brevdo, Aleksandra Faust",2021-06-04T03:08:43Z,Reinforcement Learning,"  In this paper, we investigate the fundamental question: To what extent are
gradient-based neural architecture search (NAS) techniques applicable to RL?
Using the original DARTS as a convenient baseline, we discover that the
discrete architectures found can achieve up to 250% performance compared to
manual architecture designs on both discrete and continuous action space
environments across off-policy and on-policy RL algorithms, at only 3x more
computation time. Furthermore, through numerous ablation studies, we
systematically verify that not only does DARTS correctly upweight operations
during its supernet phrase, but also gradually improves resulting discrete
cells up to 30x more efficiently than random search, suggesting DARTS is
surprisingly an effective tool for improving architectures in RL.
",0
Task-driven Semantic Coding via Reinforcement Learning,"Xin Li, Jun Shi, Zhibo Chen",2021-06-07T11:02:17Z,Reinforcement Learning,"  Task-driven semantic video/image coding has drawn considerable attention with
the development of intelligent media applications, such as license plate
detection, face detection, and medical diagnosis, which focuses on maintaining
the semantic information of videos/images. Deep neural network (DNN)-based
codecs have been studied for this purpose due to their inherent end-to-end
optimization mechanism. However, the traditional hybrid coding framework cannot
be optimized in an end-to-end manner, which makes task-driven semantic fidelity
metric unable to be automatically integrated into the rate-distortion
optimization process. Therefore, it is still attractive and challenging to
implement task-driven semantic coding with the traditional hybrid coding
framework, which should still be widely used in practical industry for a long
time. To solve this challenge, we design semantic maps for different tasks to
extract the pixelwise semantic fidelity for videos/images. Instead of directly
integrating the semantic fidelity metric into traditional hybrid coding
framework, we implement task-driven semantic coding by implementing semantic
bit allocation based on reinforcement learning (RL). We formulate the semantic
bit allocation problem as a Markov decision process (MDP) and utilize one RL
agent to automatically determine the quantization parameters (QPs) for
different coding units (CUs) according to the task-driven semantic fidelity
metric. Extensive experiments on different tasks, such as classification,
detection and segmentation, have demonstrated the superior performance of our
approach by achieving an average bitrate saving of 34.39% to 52.62% over the
High Efficiency Video Coding (H.265/HEVC) anchor under equivalent task-related
semantic fidelity.
",37
XIRL: Cross-embodiment Inverse Reinforcement Learning,"Kevin Zakka, Andy Zeng, Pete Florence, Jonathan Tompson, Jeannette Bohg, Debidatta Dwibedi",2021-06-07T18:45:07Z,Reinforcement Learning,"  We investigate the visual cross-embodiment imitation setting, in which agents
learn policies from videos of other agents (such as humans) demonstrating the
same task, but with stark differences in their embodiments -- shape, actions,
end-effector dynamics, etc. In this work, we demonstrate that it is possible to
automatically discover and learn vision-based reward functions from
cross-embodiment demonstration videos that are robust to these differences.
Specifically, we present a self-supervised method for Cross-embodiment Inverse
Reinforcement Learning (XIRL) that leverages temporal cycle-consistency
constraints to learn deep visual embeddings that capture task progression from
offline videos of demonstrations across multiple expert agents, each performing
the same task differently due to embodiment differences. Prior to our work,
producing rewards from self-supervised embeddings typically required alignment
with a reference trajectory, which may be difficult to acquire under stark
embodiment differences. We show empirically that if the embeddings are aware of
task progress, simply taking the negative distance between the current state
and goal state in the learned embedding space is useful as a reward for
training policies with reinforcement learning. We find our learned reward
function not only works for embodiments seen during training, but also
generalizes to entirely new embodiments. Additionally, when transferring
real-world human demonstrations to a simulated robot, we find that XIRL is more
sample efficient than current best methods. Qualitative results, code, and
datasets are available at https://x-irl.github.io
",0
RewardsOfSum: Exploring Reinforcement Learning Rewards for Summarisation,"Jacob Parnell, Inigo Jauregi Unanue, Massimo Piccardi",2021-06-08T03:30:50Z,Reinforcement Learning,"  To date, most abstractive summarisation models have relied on variants of the
negative log-likelihood (NLL) as their training objective. In some cases,
reinforcement learning has been added to train the models with an objective
that is closer to their evaluation measures (e.g. ROUGE). However, the reward
function to be used within the reinforcement learning approach can play a key
role for performance and is still partially unexplored. For this reason, in
this paper, we propose two reward functions for the task of abstractive
summarisation: the first function, referred to as RwB-Hinge, dynamically
selects the samples for the gradient update. The second function, nicknamed
RISK, leverages a small pool of strong candidates to inform the reward. In the
experiments, we probe the proposed approach by fine-tuning an NLL pre trained
model over nine summarisation datasets of diverse size and nature. The
experimental results show a consistent improvement over the negative
log-likelihood baselines.
",0
Dynamic Sparse Training for Deep Reinforcement Learning,"Ghada Sokar, Elena Mocanu, Decebal Constantin Mocanu, Mykola Pechenizkiy, Peter Stone",2021-06-08T09:57:20Z,Reinforcement Learning,"  Deep reinforcement learning (DRL) agents are trained through trial-and-error
interactions with the environment. This leads to a long training time for dense
neural networks to achieve good performance. Hence, prohibitive computation and
memory resources are consumed. Recently, learning efficient DRL agents has
received increasing attention. Yet, current methods focus on accelerating
inference time. In this paper, we introduce for the first time a dynamic sparse
training approach for deep reinforcement learning to accelerate the training
process. The proposed approach trains a sparse neural network from scratch and
dynamically adapts its topology to the changing data distribution during
training. Experiments on continuous control tasks show that our dynamic sparse
agents achieve higher performance than the equivalent dense methods, reduce the
parameter count and floating-point operations (FLOPs) by 50%, and have a faster
learning speed that enables reaching the performance of dense agents with
40-50% reduction in the training steps.
",43
Pretraining Representations for Data-Efficient Reinforcement Learning,"Max Schwarzer, Nitarshan Rajkumar, Michael Noukhovitch, Ankesh Anand, Laurent Charlin, Devon Hjelm, Philip Bachman, Aaron Courville",2021-06-09T04:14:27Z,Reinforcement Learning,"  Data efficiency is a key challenge for deep reinforcement learning. We
address this problem by using unlabeled data to pretrain an encoder which is
then finetuned on a small amount of task-specific data. To encourage learning
representations which capture diverse aspects of the underlying MDP, we employ
a combination of latent dynamics modelling and unsupervised goal-conditioned
RL. When limited to 100k steps of interaction on Atari games (equivalent to two
hours of human experience), our approach significantly surpasses prior work
combining offline representation pretraining with task-specific finetuning, and
compares favourably with other pretraining methods that require orders of
magnitude more data. Our approach shows particular promise when combined with
larger models as well as more diverse, task-aligned observational data --
approaching human-level performance and data-efficiency on Atari in our best
setting. We provide code associated with this work at
https://github.com/mila-iqia/SGI.
",0
Simplifying Deep Reinforcement Learning via Self-Supervision,"Daochen Zha, Kwei-Herng Lai, Kaixiong Zhou, Xia Hu",2021-06-10T06:29:59Z,Reinforcement Learning,"  Supervised regression to demonstrations has been demonstrated to be a stable
way to train deep policy networks. We are motivated to study how we can take
full advantage of supervised loss functions for stably training deep
reinforcement learning agents. This is a challenging task because it is unclear
how the training data could be collected to enable policy improvement. In this
work, we propose Self-Supervised Reinforcement Learning (SSRL), a simple
algorithm that optimizes policies with purely supervised losses. We demonstrate
that, without policy gradient or value estimation, an iterative procedure of
``labeling"" data and supervised regression is sufficient to drive stable policy
improvement. By selecting and imitating trajectories with high episodic
rewards, SSRL is surprisingly competitive to contemporary algorithms with more
stable performance and less running time, showing the potential of solving
reinforcement learning with supervised learning techniques. The code is
available at https://github.com/daochenzha/SSRL
",0
Verifiable and Compositional Reinforcement Learning Systems,"Cyrus Neary, Christos Verginis, Murat Cubuktepe, Ufuk Topcu",2021-06-07T17:05:14Z,Reinforcement Learning,"  We propose a framework for verifiable and compositional reinforcement
learning (RL) in which a collection of RL subsystems, each of which learns to
accomplish a separate subtask, are composed to achieve an overall task. The
framework consists of a high-level model, represented as a parametric Markov
decision process (pMDP) which is used to plan and to analyze compositions of
subsystems, and of the collection of low-level subsystems themselves. By
defining interfaces between the subsystems, the framework enables automatic
decompositions of task specifications, e.g., reach a target set of states with
a probability of at least 0.95, into individual subtask specifications, i.e.
achieve the subsystem's exit conditions with at least some minimum probability,
given that its entry conditions are met. This in turn allows for the
independent training and testing of the subsystems; if they each learn a policy
satisfying the appropriate subtask specification, then their composition is
guaranteed to satisfy the overall task specification. Conversely, if the
subtask specifications cannot all be satisfied by the learned policies, we
present a method, formulated as the problem of finding an optimal set of
parameters in the pMDP, to automatically update the subtask specifications to
account for the observed shortcomings. The result is an iterative procedure for
defining subtask specifications, and for training the subsystems to meet them.
As an additional benefit, this procedure allows for particularly challenging or
important components of an overall task to be determined automatically, and
focused on, during training. Experimental results demonstrate the presented
framework's novel capabilities.
",0
DECORE: Deep Compression with Reinforcement Learning,"Manoj Alwani, Yang Wang, Vashisht Madhavan",2021-06-11T00:03:41Z,Reinforcement Learning,"  Deep learning has become an increasingly popular and powerful methodology for
modern pattern recognition systems. However, many deep neural networks have
millions or billions of parameters, making them untenable for real-world
applications due to constraints on memory size or latency requirements. As a
result, efficient network compression techniques are often required for the
widespread adoption of deep learning methods. We present DECORE, a
reinforcement learning-based approach to automate the network compression
process. DECORE assigns an agent to each channel in the network along with a
light policy gradient method to learn which neurons or channels to be kept or
removed. Each agent in the network has just one parameter (keep or drop) to
learn, which leads to a much faster training process compared to existing
approaches. DECORE provides state-of-the-art compression results on various
network architectures and various datasets. For example, on the ResNet-110
architecture, it achieves a 64.8% compression and 61.8% FLOPs reduction as
compared to the baseline model without any accuracy loss on the CIFAR-10
dataset. It can reduce the size of regular architectures like the VGG network
by up to 99% with just a small accuracy drop of 2.28%. For a larger dataset
like ImageNet with just 30 epochs of training, it can compress the ResNet-50
architecture by 44.7% and reduce FLOPs by 42.3%, with just a 0.69% drop on
Top-5 accuracy of the uncompressed model. We also demonstrate that DECORE can
be used to search for compressed network architectures based on various
constraints, such as memory and FLOPs.
",0
Safe Reinforcement Learning with Linear Function Approximation,"Sanae Amani, Christos Thrampoulidis, Lin F. Yang",2021-06-11T08:46:57Z,Reinforcement Learning,"  Safety in reinforcement learning has become increasingly important in recent
years. Yet, existing solutions either fail to strictly avoid choosing unsafe
actions, which may lead to catastrophic results in safety-critical systems, or
fail to provide regret guarantees for settings where safety constraints need to
be learned. In this paper, we address both problems by first modeling safety as
an unknown linear cost function of states and actions, which must always fall
below a certain threshold. We then present algorithms, termed SLUCB-QVI and
RSLUCB-QVI, for episodic Markov decision processes (MDPs) with linear function
approximation. We show that SLUCB-QVI and RSLUCB-QVI, while with \emph{no
safety violation}, achieve a
$\tilde{\mathcal{O}}\left(\kappa\sqrt{d^3H^3T}\right)$ regret, nearly matching
that of state-of-the-art unsafe algorithms, where $H$ is the duration of each
episode, $d$ is the dimension of the feature mapping, $\kappa$ is a constant
characterizing the safety constraints, and $T$ is the total number of action
plays. We further present numerical simulations that corroborate our
theoretical findings.
",0
Automatic Risk Adaptation in Distributional Reinforcement Learning,"Frederik Schubert, Theresa Eimer, Bodo Rosenhahn, Marius Lindauer",2021-06-11T11:31:04Z,Reinforcement Learning,"  The use of Reinforcement Learning (RL) agents in practical applications
requires the consideration of suboptimal outcomes, depending on the familiarity
of the agent with its environment. This is especially important in
safety-critical environments, where errors can lead to high costs or damage. In
distributional RL, the risk-sensitivity can be controlled via different
distortion measures of the estimated return distribution. However, these
distortion functions require an estimate of the risk level, which is difficult
to obtain and depends on the current state. In this work, we demonstrate the
suboptimality of a static risk level estimation and propose a method to
dynamically select risk levels at each environment step. Our method ARA
(Automatic Risk Adaptation) estimates the appropriate risk level in both known
and unknown environments using a Random Network Distillation error. We show
reduced failure rates by up to a factor of 7 and improved generalization
performance by up to 14% compared to both risk-aware and risk-agnostic agents
in several locomotion environments.
",0
Offline Reinforcement Learning as Anti-Exploration,"Shideh Rezaeifar, Robert Dadashi, Nino Vieillard, Léonard Hussenot, Olivier Bachem, Olivier Pietquin, Matthieu Geist",2021-06-11T14:41:30Z,Reinforcement Learning,"  Offline Reinforcement Learning (RL) aims at learning an optimal control from
a fixed dataset, without interactions with the system. An agent in this setting
should avoid selecting actions whose consequences cannot be predicted from the
data. This is the converse of exploration in RL, which favors such actions. We
thus take inspiration from the literature on bonus-based exploration to design
a new offline RL agent. The core idea is to subtract a prediction-based
exploration bonus from the reward, instead of adding it for exploration. This
allows the policy to stay close to the support of the dataset. We connect this
approach to a more common regularization of the learned policy towards the
data. Instantiated with a bonus based on the prediction error of a variational
autoencoder, we show that our agent is competitive with the state of the art on
a set of continuous control locomotion and manipulation tasks.
",0
A Minimalist Approach to Offline Reinforcement Learning,"Scott Fujimoto, Shixiang Shane Gu",2021-06-12T20:38:59Z,Reinforcement Learning,"  Offline reinforcement learning (RL) defines the task of learning from a fixed
batch of data. Due to errors in value estimation from out-of-distribution
actions, most offline RL algorithms take the approach of constraining or
regularizing the policy with the actions contained in the dataset. Built on
pre-existing RL algorithms, modifications to make an RL algorithm work offline
comes at the cost of additional complexity. Offline RL algorithms introduce new
hyperparameters and often leverage secondary components such as generative
models, while adjusting the underlying RL algorithm. In this paper we aim to
make a deep RL algorithm work while making minimal changes. We find that we can
match the performance of state-of-the-art offline RL algorithms by simply
adding a behavior cloning term to the policy update of an online RL algorithm
and normalizing the data. The resulting algorithm is a simple to implement and
tune baseline, while more than halving the overall run time by removing the
additional computational overhead of previous methods.
",637
Deep Reinforcement Learning based Group Recommender System,"Zefang Liu, Shuran Wen, Yinzhu Quan",2021-06-13T02:45:48Z,Reinforcement Learning,"  Group recommender systems are widely used in current web applications. In
this paper, we propose a novel group recommender system based on the deep
reinforcement learning. We introduce the MovieLens data at first and generate
one random group dataset, MovieLens-Rand, from it. This randomly generated
dataset is described and analyzed. We also present experimental settings and
two state-of-art baselines, AGREE and GroupIM. The framework of our novel
model, the Deep Reinforcement learning based Group Recommender system (DRGR),
is proposed. Actor-critic networks are implemented with the deep deterministic
policy gradient algorithm. The DRGR model is applied on the MovieLens-Rand
dataset with two baselines. Compared with baselines, we conclude that DRGR
performs better than GroupIM due to long interaction histories but worse than
AGREE because of the self-attention mechanism. We express advantages and
shortcomings of DRGR and also give future improvement directions at the end.
",0
Bellman-consistent Pessimism for Offline Reinforcement Learning,"Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, Alekh Agarwal",2021-06-13T05:50:36Z,Reinforcement Learning,"  The use of pessimism, when reasoning about datasets lacking exhaustive
exploration has recently gained prominence in offline reinforcement learning.
Despite the robustness it adds to the algorithm, overly pessimistic reasoning
can be equally damaging in precluding the discovery of good policies, which is
an issue for the popular bonus-based pessimism. In this paper, we introduce the
notion of Bellman-consistent pessimism for general function approximation:
instead of calculating a point-wise lower bound for the value function, we
implement pessimism at the initial state over the set of functions consistent
with the Bellman equations. Our theoretical guarantees only require Bellman
closedness as standard in the exploratory setting, in which case bonus-based
pessimism fails to provide guarantees. Even in the special case of linear
function approximation where stronger expressivity assumptions hold, our result
improves upon a recent bonus-based approach by $\mathcal{O}(d)$ in its sample
complexity when the action space is finite. Remarkably, our algorithms
automatically adapt to the best bias-variance tradeoff in the hindsight,
whereas most prior approaches require tuning extra hyperparameters a priori.
",234
Deep Reinforcement Learning for Conservation Decisions,"Marcus Lapeyrolerie, Melissa S. Chapman, Kari E. A. Norman, Carl Boettiger",2021-06-15T16:32:48Z,Reinforcement Learning,"  Can machine learning help us make better decisions about a changing planet?
In this paper, we illustrate and discuss the potential of a promising corner of
machine learning known as _reinforcement learning_ (RL) to help tackle the most
challenging conservation decision problems. RL is uniquely well suited to
conservation and global change challenges for three reasons: (1) RL explicitly
focuses on designing an agent who _interacts_ with an environment which is
dynamic and uncertain, (2) RL approaches do not require massive amounts of
data, (3) RL approaches would utilize rather than replace existing models,
simulations, and the knowledge they contain. We provide a conceptual and
technical introduction to RL and its relevance to ecological and conservation
challenges, including examples of a problem in setting fisheries quotas and in
managing ecological tipping points. Four appendices with annotated code provide
a tangible introduction to researchers looking to adopt, evaluate, or extend
these approaches.
",0
Unbiased Methods for Multi-Goal Reinforcement Learning,"Léonard Blier, Yann Ollivier",2021-06-16T15:31:51Z,Reinforcement Learning,"  In multi-goal reinforcement learning (RL) settings, the reward for each goal
is sparse, and located in a small neighborhood of the goal. In large dimension,
the probability of reaching a reward vanishes and the agent receives little
learning signal. Methods such as Hindsight Experience Replay (HER) tackle this
issue by also learning from realized but unplanned-for goals. But HER is known
to introduce bias, and can converge to low-return policies by overestimating
chancy outcomes. First, we vindicate HER by proving that it is actually
unbiased in deterministic environments, such as many optimal control settings.
Next, for stochastic environments in continuous spaces, we tackle sparse
rewards by directly taking the infinitely sparse reward limit. We fully
formalize the problem of multi-goal RL with infinitely sparse Dirac rewards at
each goal. We introduce unbiased deep Q-learning and actor-critic algorithms
that can handle such infinitely sparse rewards, and test them in toy
environments.
",0
Safe Reinforcement Learning Using Advantage-Based Intervention,"Nolan Wagener, Byron Boots, Ching-An Cheng",2021-06-16T20:28:56Z,Reinforcement Learning,"  Many sequential decision problems involve finding a policy that maximizes
total reward while obeying safety constraints. Although much recent research
has focused on the development of safe reinforcement learning (RL) algorithms
that produce a safe policy after training, ensuring safety during training as
well remains an open problem. A fundamental challenge is performing exploration
while still satisfying constraints in an unknown Markov decision process (MDP).
In this work, we address this problem for the chance-constrained setting. We
propose a new algorithm, SAILR, that uses an intervention mechanism based on
advantage functions to keep the agent safe throughout training and optimizes
the agent's policy using off-the-shelf RL algorithms designed for unconstrained
MDPs. Our method comes with strong guarantees on safety during both training
and deployment (i.e., after training and without the intervention mechanism)
and policy performance compared to the optimal safety-constrained policy. In
our experiments, we show that SAILR violates constraints far less during
training than standard safe RL and constrained MDP approaches and converges to
a well-performing policy that can be deployed safely without intervention. Our
code is available at https://github.com/nolanwagener/safe_rl.
",0
Contrastive Reinforcement Learning of Symbolic Reasoning Domains,"Gabriel Poesia, WenXin Dong, Noah Goodman",2021-06-16T21:46:07Z,Reinforcement Learning,"  Abstract symbolic reasoning, as required in domains such as mathematics and
logic, is a key component of human intelligence. Solvers for these domains have
important applications, especially to computer-assisted education. But learning
to solve symbolic problems is challenging for machine learning algorithms.
Existing models either learn from human solutions or use hand-engineered
features, making them expensive to apply in new domains. In this paper, we
instead consider symbolic domains as simple environments where states and
actions are given as unstructured text, and binary rewards indicate whether a
problem is solved. This flexible setup makes it easy to specify new domains,
but search and planning become challenging. We introduce four environments
inspired by the Mathematics Common Core Curriculum, and observe that existing
Reinforcement Learning baselines perform poorly. We then present a novel
learning algorithm, Contrastive Policy Learning (ConPoLe) that explicitly
optimizes the InfoNCE loss, which lower bounds the mutual information between
the current state and next states that continue on a path to the solution.
ConPoLe successfully solves all four domains. Moreover, problem representations
learned by ConPoLe enable accurate prediction of the categories of problems in
a real mathematics curriculum. Our results suggest new directions for
reinforcement learning in symbolic domains, as well as applications to
mathematics education.
",0
Mungojerrie: Reinforcement Learning of Linear-Time Objectives,"Ernst Moritz Hahn, Mateo Perez, Sven Schewe, Fabio Somenzi, Ashutosh Trivedi, Dominik Wojtczak",2021-06-16T22:26:16Z,Reinforcement Learning,"  Reinforcement learning synthesizes controllers without prior knowledge of the
system. At each timestep, a reward is given. The controllers optimize the
discounted sum of these rewards. Applying this class of algorithms requires
designing a reward scheme, which is typically done manually. The designer must
ensure that their intent is accurately captured. This may not be trivial, and
is prone to error. An alternative to this manual programming, akin to
programming directly in assembly, is to specify the objective in a formal
language and have it ""compiled"" to a reward scheme. Mungojerrie
(https://plv.colorado.edu/mungojerrie/) is a tool for testing reward schemes
for $\omega$-regular objectives on finite models. The tool contains
reinforcement learning algorithms and a probabilistic model checker.
Mungojerrie supports models specified in PRISM and $\omega$-automata specified
in HOA.
",0
Many Agent Reinforcement Learning Under Partial Observability,"Keyang He, Prashant Doshi, Bikramjit Banerjee",2021-06-17T21:24:29Z,Reinforcement Learning,"  Recent renewed interest in multi-agent reinforcement learning (MARL) has
generated an impressive array of techniques that leverage deep reinforcement
learning, primarily actor-critic architectures, and can be applied to a limited
range of settings in terms of observability and communication. However, a
continuing limitation of much of this work is the curse of dimensionality when
it comes to representations based on joint actions, which grow exponentially
with the number of agents. In this paper, we squarely focus on this challenge
of scalability. We apply the key insight of action anonymity, which leads to
permutation invariance of joint actions, to two recently presented deep MARL
algorithms, MADDPG and IA2C, and compare these instantiations to another recent
technique that leverages action anonymity, viz., mean-field MARL. We show that
our instantiations can learn the optimal behavior in a broader class of agent
networks than the mean-field method, using a recently introduced pragmatic
domain.
",0
Policy Smoothing for Provably Robust Reinforcement Learning,"Aounon Kumar, Alexander Levine, Soheil Feizi",2021-06-21T21:42:08Z,Reinforcement Learning,"  The study of provable adversarial robustness for deep neural networks (DNNs)
has mainly focused on static supervised learning tasks such as image
classification. However, DNNs have been used extensively in real-world adaptive
tasks such as reinforcement learning (RL), making such systems vulnerable to
adversarial attacks as well. Prior works in provable robustness in RL seek to
certify the behaviour of the victim policy at every time-step against a
non-adaptive adversary using methods developed for the static setting. But in
the real world, an RL adversary can infer the defense strategy used by the
victim agent by observing the states, actions, etc., from previous time-steps
and adapt itself to produce stronger attacks in future steps. We present an
efficient procedure, designed specifically to defend against an adaptive RL
adversary, that can directly certify the total reward without requiring the
policy to be robust at each time-step. Our main theoretical contribution is to
prove an adaptive version of the Neyman-Pearson Lemma -- a key lemma for
smoothing-based certificates -- where the adversarial perturbation at a
particular time can be a stochastic function of current and previous
observations and states as well as previous actions. Building on this result,
we propose policy smoothing where the agent adds a Gaussian noise to its
observation at each time-step before passing it through the policy function.
Our robustness certificates guarantee that the final total reward obtained by
policy smoothing remains above a certain threshold, even though the actions at
intermediate time-steps may change under the attack. Our experiments on various
environments like Cartpole, Pong, Freeway and Mountain Car show that our method
can yield meaningful robustness guarantees in practice.
",49
Reinforcement Learning for Physical Layer Communications,"Philippe Mary, Visa Koivunen, Christophe Moy",2021-06-22T08:02:06Z,Reinforcement Learning,"  In this chapter, we will give comprehensive examples of applying RL in
optimizing the physical layer of wireless communications by defining different
class of problems and the possible solutions to handle them. In Section 9.2, we
present all the basic theory needed to address a RL problem, i.e. Markov
decision process (MDP), Partially observable Markov decision process (POMDP),
but also two very important and widely used algorithms for RL, i.e. the
Q-learning and SARSA algorithms. We also introduce the deep reinforcement
learning (DRL) paradigm and the section ends with an introduction to the
multi-armed bandits (MAB) framework. Section 9.3 focuses on some toy examples
to illustrate how the basic concepts of RL are employed in communication
systems. We present applications extracted from literature with simplified
system models using similar notation as in Section 9.2 of this Chapter. In
Section 9.3, we also focus on modeling RL problems, i.e. how action and state
spaces and rewards are chosen. The Chapter is concluded in Section 9.4 with a
prospective thought on RL trends and it ends with a review of a broader state
of the art in Section 9.5.
",0
Emphatic Algorithms for Deep Reinforcement Learning,"Ray Jiang, Tom Zahavy, Zhongwen Xu, Adam White, Matteo Hessel, Charles Blundell, Hado van Hasselt",2021-06-21T12:11:39Z,Reinforcement Learning,"  Off-policy learning allows us to learn about possible policies of behavior
from experience generated by a different behavior policy. Temporal difference
(TD) learning algorithms can become unstable when combined with function
approximation and off-policy sampling - this is known as the ''deadly triad''.
Emphatic temporal difference (ETD($\lambda$)) algorithm ensures convergence in
the linear case by appropriately weighting the TD($\lambda$) updates. In this
paper, we extend the use of emphatic methods to deep reinforcement learning
agents. We show that naively adapting ETD($\lambda$) to popular deep
reinforcement learning algorithms, which use forward view multi-step returns,
results in poor performance. We then derive new emphatic algorithms for use in
the context of such algorithms, and we demonstrate that they provide noticeable
benefits in small problems designed to highlight the instability of TD methods.
Finally, we observed improved performance when applying these algorithms at
scale on classic Atari games from the Arcade Learning Environment.
",0
Off-Policy Reinforcement Learning with Delayed Rewards,"Beining Han, Zhizhou Ren, Zuofan Wu, Yuan Zhou, Jian Peng",2021-06-22T15:19:48Z,Reinforcement Learning,"  We study deep reinforcement learning (RL) algorithms with delayed rewards. In
many real-world tasks, instant rewards are often not readily accessible or even
defined immediately after the agent performs actions. In this work, we first
formally define the environment with delayed rewards and discuss the challenges
raised due to the non-Markovian nature of such environments. Then, we introduce
a general off-policy RL framework with a new Q-function formulation that can
handle the delayed rewards with theoretical convergence guarantees. For
practical tasks with high dimensional state spaces, we further introduce the
HC-decomposition rule of the Q-function in our framework which naturally leads
to an approximation scheme that helps boost the training efficiency and
stability. We finally conduct extensive experiments to demonstrate the superior
performance of our algorithms over the existing work and their variants.
",0
Compositional Reinforcement Learning from Logical Specifications,"Kishor Jothimurugan, Suguman Bansal, Osbert Bastani, Rajeev Alur",2021-06-25T22:54:28Z,Reinforcement Learning,"  We study the problem of learning control policies for complex tasks given by
logical specifications. Recent approaches automatically generate a reward
function from a given specification and use a suitable reinforcement learning
algorithm to learn a policy that maximizes the expected reward. These
approaches, however, scale poorly to complex tasks that require high-level
planning. In this work, we develop a compositional learning approach, called
DiRL, that interleaves high-level planning and reinforcement learning. First,
DiRL encodes the specification as an abstract graph; intuitively, vertices and
edges of the graph correspond to regions of the state space and simpler
sub-tasks, respectively. Our approach then incorporates reinforcement learning
to learn neural network policies for each edge (sub-task) within a
Dijkstra-style planning algorithm to compute a high-level plan in the graph. An
evaluation of the proposed approach on a set of challenging control benchmarks
with continuous state and action spaces demonstrates that it outperforms
state-of-the-art baselines.
",0
Regret Analysis in Deterministic Reinforcement Learning,"Damianos Tranos, Alexandre Proutiere",2021-06-27T23:41:57Z,Reinforcement Learning,"  We consider Markov Decision Processes (MDPs) with deterministic transitions
and study the problem of regret minimization, which is central to the analysis
and design of optimal learning algorithms. We present logarithmic
problem-specific regret lower bounds that explicitly depend on the system
parameter (in contrast to previous minimax approaches) and thus, truly quantify
the fundamental limit of performance achievable by any learning algorithm.
Deterministic MDPs can be interpreted as graphs and analyzed in terms of their
cycles, a fact which we leverage in order to identify a class of deterministic
MDPs whose regret lower bound can be determined numerically. We further
exemplify this result on a deterministic line search problem, and a
deterministic MDP with state-dependent rewards, whose regret lower bounds we
can state explicitly. These bounds share similarities with the known
problem-specific bound of the multi-armed bandit problem and suggest that
navigation on a deterministic MDP need not have an effect on the performance of
a learning algorithm.
",0
Deep Multiagent Reinforcement Learning: Challenges and Directions,"Annie Wong, Thomas Bäck, Anna V. Kononova, Aske Plaat",2021-06-29T19:53:15Z,Reinforcement Learning,"  This paper surveys the field of deep multiagent reinforcement learning. The
combination of deep neural networks with reinforcement learning has gained
increased traction in recent years and is slowly shifting the focus from
single-agent to multiagent environments. Dealing with multiple agents is
inherently more complex as (a) the future rewards depend on multiple players'
joint actions and (b) the computational complexity increases. We present the
most common multiagent problem representations and their main challenges, and
identify five research areas that address one or more of these challenges:
centralised training and decentralised execution, opponent modelling,
communication, efficient coordination, and reward shaping. We find that many
computational studies rely on unrealistic assumptions or are not generalisable
to other settings; they struggle to overcome the curse of dimensionality or
nonstationarity. Approaches from psychology and sociology capture promising
relevant behaviours, such as communication and coordination, to help agents
achieve better performance in multiagent settings. We suggest that, for
multiagent reinforcement learning to be successful, future research should
address these challenges with an interdisciplinary approach to open up new
possibilities in multiagent reinforcement learning.
",52
Goal-Conditioned Reinforcement Learning with Imagined Subgoals,"Elliot Chane-Sane, Cordelia Schmid, Ivan Laptev",2021-07-01T15:30:59Z,Reinforcement Learning,"  Goal-conditioned reinforcement learning endows an agent with a large variety
of skills, but it often struggles to solve tasks that require more temporally
extended reasoning. In this work, we propose to incorporate imagined subgoals
into policy learning to facilitate learning of complex tasks. Imagined subgoals
are predicted by a separate high-level policy, which is trained simultaneously
with the policy and its critic. This high-level policy predicts intermediate
states halfway to the goal using the value function as a reachability metric.
We don't require the policy to reach these subgoals explicitly. Instead, we use
them to define a prior policy, and incorporate this prior into a KL-constrained
policy iteration scheme to speed up and regularize learning. Imagined subgoals
are used during policy learning, but not during test time, where we only apply
the learned policy. We evaluate our approach on complex robotic navigation and
manipulation tasks and show that it outperforms existing methods by a large
margin.
",0
Distilling Reinforcement Learning Tricks for Video Games,"Anssi Kanervisto, Christian Scheller, Yanick Schraner, Ville Hautamäki",2021-07-01T19:02:38Z,Reinforcement Learning,"  Reinforcement learning (RL) research focuses on general solutions that can be
applied across different domains. This results in methods that RL practitioners
can use in almost any domain. However, recent studies often lack the
engineering steps (""tricks"") which may be needed to effectively use RL, such as
reward shaping, curriculum learning, and splitting a large task into smaller
chunks. Such tricks are common, if not necessary, to achieve state-of-the-art
results and win RL competitions. To ease the engineering efforts, we distill
descriptions of tricks from state-of-the-art results and study how well these
tricks can improve a standard deep Q-learning agent. The long-term goal of this
work is to enable combining proven RL methods with domain-specific tricks by
providing a unified software framework and accompanying insights in multiple
domains.
",0
Reinforcement Learning for Feedback-Enabled Cyber Resilience,"Yunhan Huang, Linan Huang, Quanyan Zhu",2021-07-02T01:08:45Z,Reinforcement Learning,"  Digitization and remote connectivity have enlarged the attack surface and
made cyber systems more vulnerable. As attackers become increasingly
sophisticated and resourceful, mere reliance on traditional cyber protection,
such as intrusion detection, firewalls, and encryption, is insufficient to
secure the cyber systems. Cyber resilience provides a new security paradigm
that complements inadequate protection with resilience mechanisms. A
Cyber-Resilient Mechanism (CRM) adapts to the known or zero-day threats and
uncertainties in real-time and strategically responds to them to maintain
critical functions of the cyber systems in the event of successful attacks.
Feedback architectures play a pivotal role in enabling the online sensing,
reasoning, and actuation process of the CRM. Reinforcement Learning (RL) is an
essential tool that epitomizes the feedback architectures for cyber resilience.
It allows the CRM to provide sequential responses to attacks with limited or
without prior knowledge of the environment and the attacker. In this work, we
review the literature on RL for cyber resilience and discuss cyber resilience
against three major types of vulnerabilities, i.e., posture-related,
information-related, and human-related vulnerabilities. We introduce three
application domains of CRMs: moving target defense, defensive cyber deception,
and assistive human security technologies. The RL algorithms also have
vulnerabilities themselves. We explain the three vulnerabilities of RL and
present attack models where the attacker targets the information exchanged
between the environment and the agent: the rewards, the state observations, and
the action commands. We show that the attacker can trick the RL agent into
learning a nefarious policy with minimum attacking effort. Lastly, we discuss
the future challenges of RL for cyber security and resilience and emerging
applications of RL-based CRMs.
",0
The Least Restriction for Offline Reinforcement Learning,Zizhou Su,2021-07-05T01:50:40Z,Reinforcement Learning,"  Many practical applications of reinforcement learning (RL) constrain the
agent to learn from a fixed offline dataset of logged interactions, which has
already been gathered, without offering further possibility for data
collection. However, commonly used off-policy RL algorithms, such as the Deep Q
Network and the Deep Deterministic Policy Gradient, are incapable of learning
without data correlated to the distribution under the current policy, making
them ineffective for this offline setting. As the first step towards useful
offline RL algorithms, we analysis the reason of instability in standard
off-policy RL algorithms. It is due to the bootstrapping error. The key to
avoiding this error, is ensuring that the agent's action space does not go out
of the fixed offline dataset. Based on our consideration, a creative offline RL
framework, the Least Restriction (LR), is proposed in this paper. The LR
regards selecting an action as taking a sample from the probability
distribution. It merely set a little limit for action selection, which not only
avoid the action being out of the offline dataset but also remove all the
unreasonable restrictions in earlier approaches (e.g. Batch-Constrained Deep
Q-Learning). In the further, we will demonstrate that the LR, is able to learn
robustly from different offline datasets, including random and suboptimal
demonstrations, on a range of practical control tasks.
",0
Meta-Reinforcement Learning for Heuristic Planning,"Ricardo Luna Gutierrez, Matteo Leonetti",2021-07-06T13:25:52Z,Reinforcement Learning,"  In Meta-Reinforcement Learning (meta-RL) an agent is trained on a set of
tasks to prepare for and learn faster in new, unseen, but related tasks. The
training tasks are usually hand-crafted to be representative of the expected
distribution of test tasks and hence all used in training. We show that given a
set of training tasks, learning can be both faster and more effective (leading
to better performance in the test tasks), if the training tasks are
appropriately selected. We propose a task selection algorithm,
Information-Theoretic Task Selection (ITTS), based on information theory, which
optimizes the set of tasks used for training in meta-RL, irrespectively of how
they are generated. The algorithm establishes which training tasks are both
sufficiently relevant for the test tasks, and different enough from one
another. We reproduce different meta-RL experiments from the literature and
show that ITTS improves the final performance in all of them.
",0
Survey of Self-Play in Reinforcement Learning,"Anthony DiGiovanni, Ethan C. Zell",2021-07-06T19:33:53Z,Reinforcement Learning,"  In reinforcement learning (RL), the term self-play describes a kind of
multi-agent learning (MAL) that deploys an algorithm against copies of itself
to test compatibility in various stochastic environments. As is typical in MAL,
the literature draws heavily from well-established concepts in classical game
theory and so this survey quickly reviews some fundamental concepts. In what
follows, we present a brief survey of self-play literature, its major themes,
criteria, and techniques, and then conclude with an assessment of current
shortfalls of the literature as well as suggestions for future directions.
",0
RRL: Resnet as representation for Reinforcement Learning,"Rutav Shah, Vikash Kumar",2021-07-07T17:59:07Z,Reinforcement Learning,"  The ability to autonomously learn behaviors via direct interactions in
uninstrumented environments can lead to generalist robots capable of enhancing
productivity or providing care in unstructured settings like homes. Such
uninstrumented settings warrant operations only using the robot's
proprioceptive sensor such as onboard cameras, joint encoders, etc which can be
challenging for policy learning owing to the high dimensionality and partial
observability issues. We propose RRL: Resnet as representation for
Reinforcement Learning -- a straightforward yet effective approach that can
learn complex behaviors directly from proprioceptive inputs. RRL fuses features
extracted from pre-trained Resnet into the standard reinforcement learning
pipeline and delivers results comparable to learning directly from the state.
In a simulated dexterous manipulation benchmark, where the state of the art
methods fail to make significant progress, RRL delivers contact rich behaviors.
The appeal of RRL lies in its simplicity in bringing together progress from the
fields of Representation Learning, Imitation Learning, and Reinforcement
Learning. Its effectiveness in learning behaviors directly from visual inputs
with performance and sample efficiency matching learning directly from the
state, even in complex high dimensional domains, is far from obvious.
",0
CoBERL: Contrastive BERT for Reinforcement Learning,"Andrea Banino, Adrià Puidomenech Badia, Jacob Walker, Tim Scholtes, Jovana Mitrovic, Charles Blundell",2021-07-12T13:54:18Z,Reinforcement Learning,"  Many reinforcement learning (RL) agents require a large amount of experience
to solve tasks. We propose Contrastive BERT for RL (CoBERL), an agent that
combines a new contrastive loss and a hybrid LSTM-transformer architecture to
tackle the challenge of improving data efficiency. CoBERL enables efficient,
robust learning from pixels across a wide range of domains. We use
bidirectional masked prediction in combination with a generalization of recent
contrastive methods to learn better representations for transformers in RL,
without the need of hand engineered data augmentations. We find that CoBERL
consistently improves performance across the full Atari suite, a set of control
tasks and a challenging 3D environment.
",0
Model Selection for Generic Reinforcement Learning,"Avishek Ghosh, Sayak Ray Chowdhury, Kannan Ramchandran",2021-07-13T05:00:38Z,Reinforcement Learning,"  We address the problem of model selection for the finite horizon episodic
Reinforcement Learning (RL) problem where the transition kernel $P^*$ belongs
to a family of models $\mathcal{P}^*$ with finite metric entropy. In the model
selection framework, instead of $\mathcal{P}^*$, we are given $M$ nested
families of transition kernels $\cP_1 \subset \cP_2 \subset \ldots \subset
\cP_M$. We propose and analyze a novel algorithm, namely \emph{Adaptive
Reinforcement Learning (General)} (\texttt{ARL-GEN}) that adapts to the
smallest such family where the true transition kernel $P^*$ lies.
\texttt{ARL-GEN} uses the Upper Confidence Reinforcement Learning
(\texttt{UCRL}) algorithm with value targeted regression as a blackbox and puts
a model selection module at the beginning of each epoch. Under a mild
separability assumption on the model classes, we show that \texttt{ARL-GEN}
obtains a regret of
$\Tilde{\mathcal{O}}(d_{\mathcal{E}}^*H^2+\sqrt{d_{\mathcal{E}}^* \mathbb{M}^*
H^2 T})$, with high probability, where $H$ is the horizon length, $T$ is the
total number of steps, $d_{\mathcal{E}}^*$ is the Eluder dimension and
$\mathbb{M}^*$ is the metric entropy corresponding to $\mathcal{P}^*$. Note
that this regret scaling matches that of an oracle that knows $\mathcal{P}^*$
in advance. We show that the cost of model selection for \texttt{ARL-GEN} is an
additive term in the regret having a weak dependence on $T$. Subsequently, we
remove the separability assumption and consider the setup of linear mixture
MDPs, where the transition kernel $P^*$ has a linear function approximation.
With this low rank structure, we propose novel adaptive algorithms for model
selection, and obtain (order-wise) regret identical to that of an oracle with
knowledge of the true model class.
",0
Safer Reinforcement Learning through Transferable Instinct Networks,"Djordje Grbic, Sebastian Risi",2021-07-14T13:22:04Z,Reinforcement Learning,"  Random exploration is one of the main mechanisms through which reinforcement
learning (RL) finds well-performing policies. However, it can lead to
undesirable or catastrophic outcomes when learning online in safety-critical
environments. In fact, safe learning is one of the major obstacles towards
real-world agents that can learn during deployment. One way of ensuring that
agents respect hard limitations is to explicitly configure boundaries in which
they can operate. While this might work in some cases, we do not always have
clear a-priori information which states and actions can lead dangerously close
to hazardous states. Here, we present an approach where an additional policy
can override the main policy and offer a safer alternative action. In our
instinct-regulated RL (IR^2L) approach, an ""instinctual"" network is trained to
recognize undesirable situations, while guarding the learning policy against
entering them. The instinct network is pre-trained on a single task where it is
safe to make mistakes, and transferred to environments in which learning a new
task safely is critical. We demonstrate IR^2L in the OpenAI Safety gym domain,
in which it receives a significantly lower number of safety violations during
training than a baseline RL approach while reaching similar task performance.
",0
Deep Adaptive Multi-Intention Inverse Reinforcement Learning,"Ariyan Bighashdel, Panagiotis Meletis, Pavol Jancura, Gijs Dubbelman",2021-07-14T13:33:01Z,Reinforcement Learning,"  This paper presents a deep Inverse Reinforcement Learning (IRL) framework
that can learn an a priori unknown number of nonlinear reward functions from
unlabeled experts' demonstrations. For this purpose, we employ the tools from
Dirichlet processes and propose an adaptive approach to simultaneously account
for both complex and unknown number of reward functions. Using the conditional
maximum entropy principle, we model the experts' multi-intention behaviors as a
mixture of latent intention distributions and derive two algorithms to estimate
the parameters of the deep reward network along with the number of experts'
intentions from unlabeled demonstrations. The proposed algorithms are evaluated
on three benchmarks, two of which have been specifically extended in this study
for multi-intention IRL, and compared with well-known baselines. We demonstrate
through several experiments the advantages of our algorithms over the existing
approaches and the benefits of online inferring, rather than fixing beforehand,
the number of expert's intentions.
",0
Reinforcement Learning for Education: Opportunities and Challenges,"Adish Singla, Anna N. Rafferty, Goran Radanovic, Neil T. Heffernan",2021-07-15T21:27:45Z,Reinforcement Learning,"  This survey article has grown out of the RL4ED workshop organized by the
authors at the Educational Data Mining (EDM) 2021 conference. We organized this
workshop as part of a community-building effort to bring together researchers
and practitioners interested in the broad areas of reinforcement learning (RL)
and education (ED). This article aims to provide an overview of the workshop
activities and summarize the main research directions in the area of RL for ED.
",0
Demonstration-Guided Reinforcement Learning with Learned Skills,"Karl Pertsch, Youngwoon Lee, Yue Wu, Joseph J. Lim",2021-07-21T17:59:34Z,Reinforcement Learning,"  Demonstration-guided reinforcement learning (RL) is a promising approach for
learning complex behaviors by leveraging both reward feedback and a set of
target task demonstrations. Prior approaches for demonstration-guided RL treat
every new task as an independent learning problem and attempt to follow the
provided demonstrations step-by-step, akin to a human trying to imitate a
completely unseen behavior by following the demonstrator's exact muscle
movements. Naturally, such learning will be slow, but often new behaviors are
not completely unseen: they share subtasks with behaviors we have previously
learned. In this work, we aim to exploit this shared subtask structure to
increase the efficiency of demonstration-guided RL. We first learn a set of
reusable skills from large offline datasets of prior experience collected
across many tasks. We then propose Skill-based Learning with Demonstrations
(SkiLD), an algorithm for demonstration-guided RL that efficiently leverages
the provided demonstrations by following the demonstrated skills instead of the
primitive actions, resulting in substantial performance improvements over prior
demonstration-guided RL approaches. We validate the effectiveness of our
approach on long-horizon maze navigation and complex robot manipulation tasks.
",0
Accelerating Quadratic Optimization with Reinforcement Learning,"Jeffrey Ichnowski, Paras Jain, Bartolomeo Stellato, Goran Banjac, Michael Luo, Francesco Borrelli, Joseph E. Gonzalez, Ion Stoica, Ken Goldberg",2021-07-22T17:59:10Z,Reinforcement Learning,"  First-order methods for quadratic optimization such as OSQP are widely used
for large-scale machine learning and embedded optimal control, where many
related problems must be rapidly solved. These methods face two persistent
challenges: manual hyperparameter tuning and convergence time to high-accuracy
solutions. To address these, we explore how Reinforcement Learning (RL) can
learn a policy to tune parameters to accelerate convergence. In experiments
with well-known QP benchmarks we find that our RL policy, RLQP, significantly
outperforms state-of-the-art QP solvers by up to 3x. RLQP generalizes
surprisingly well to previously unseen problems with varying dimension and
structure from different applications, including the QPLIB, Netlib LP and
Maros-Meszaros problems. Code for RLQP is available at
https://github.com/berkeleyautomation/rlqp.
",0
Autonomous Reinforcement Learning via Subgoal Curricula,"Archit Sharma, Abhishek Gupta, Sergey Levine, Karol Hausman, Chelsea Finn",2021-07-27T16:39:45Z,Reinforcement Learning,"  Reinforcement learning (RL) promises to enable autonomous acquisition of
complex behaviors for diverse agents. However, the success of current
reinforcement learning algorithms is predicated on an often under-emphasised
requirement -- each trial needs to start from a fixed initial state
distribution. Unfortunately, resetting the environment to its initial state
after each trial requires substantial amount of human supervision and extensive
instrumentation of the environment which defeats the goal of autonomous
acquisition of complex behaviors. In this work, we propose Value-accelerated
Persistent Reinforcement Learning (VaPRL), which generates a curriculum of
initial states such that the agent can bootstrap on the success of easier tasks
to efficiently learn harder tasks. The agent also learns to reach the initial
states proposed by the curriculum, minimizing the reliance on human
interventions into the learning. We observe that VaPRL reduces the
interventions required by three orders of magnitude compared to episodic RL
while outperforming prior state-of-the art methods for reset-free RL both in
terms of sample efficiency and asymptotic performance on a variety of simulated
robotics problems.
",0
Non-Markovian Reinforcement Learning using Fractional Dynamics,"Gaurav Gupta, Chenzhong Yin, Jyotirmoy V. Deshmukh, Paul Bogdan",2021-07-29T07:35:13Z,Reinforcement Learning,"  Reinforcement learning (RL) is a technique to learn the control policy for an
agent that interacts with a stochastic environment. In any given state, the
agent takes some action, and the environment determines the probability
distribution over the next state as well as gives the agent some reward. Most
RL algorithms typically assume that the environment satisfies Markov
assumptions (i.e. the probability distribution over the next state depends only
on the current state). In this paper, we propose a model-based RL technique for
a system that has non-Markovian dynamics. Such environments are common in many
real-world applications such as in human physiology, biological systems,
material science, and population dynamics. Model-based RL (MBRL) techniques
typically try to simultaneously learn a model of the environment from the data,
as well as try to identify an optimal policy for the learned model. We propose
a technique where the non-Markovianity of the system is modeled through a
fractional dynamical system. We show that we can quantify the difference in the
performance of an MBRL algorithm that uses bounded horizon model predictive
control from the optimal policy. Finally, we demonstrate our proposed framework
on a pharmacokinetic model of human blood glucose dynamics and show that our
fractional models can capture distant correlations on real-world datasets.
",8
Lyapunov-based uncertainty-aware safe reinforcement learning,"Ashkan B. Jeddi, Nariman L. Dehghani, Abdollah Shafieezadeh",2021-07-29T13:08:15Z,Reinforcement Learning,"  Reinforcement learning (RL) has shown a promising performance in learning
optimal policies for a variety of sequential decision-making tasks. However, in
many real-world RL problems, besides optimizing the main objectives, the agent
is expected to satisfy a certain level of safety (e.g., avoiding collisions in
autonomous driving). While RL problems are commonly formalized as Markov
decision processes (MDPs), safety constraints are incorporated via constrained
Markov decision processes (CMDPs). Although recent advances in safe RL have
enabled learning safe policies in CMDPs, these safety requirements should be
satisfied during both training and in the deployment process. Furthermore, it
is shown that in memory-based and partially observable environments, these
methods fail to maintain safety over unseen out-of-distribution observations.
To address these limitations, we propose a Lyapunov-based uncertainty-aware
safe RL model. The introduced model adopts a Lyapunov function that converts
trajectory-based constraints to a set of local linear constraints. Furthermore,
to ensure the safety of the agent in highly uncertain environments, an
uncertainty quantification method is developed that enables identifying
risk-averse actions through estimating the probability of constraint
violations. Moreover, a Transformers model is integrated to provide the agent
with memory to process long time horizons of information via the self-attention
mechanism. The proposed model is evaluated in grid-world navigation tasks where
safety is defined as avoiding static and dynamic obstacles in fully and
partially observable environments. The results of these experiments show a
significant improvement in the performance of the agent both in achieving
optimality and satisfying safety constraints.
",0
Inverse Reinforcement Learning for Strategy Identification,"Mark Rucker, Stephen Adams, Roy Hayes, Peter A. Beling",2021-07-31T17:22:52Z,Reinforcement Learning,"  In adversarial environments, one side could gain an advantage by identifying
the opponent's strategy. For example, in combat games, if an opponents strategy
is identified as overly aggressive, one could lay a trap that exploits the
opponent's aggressive nature. However, an opponent's strategy is not always
apparent and may need to be estimated from observations of their actions. This
paper proposes to use inverse reinforcement learning (IRL) to identify
strategies in adversarial environments. Specifically, the contributions of this
work are 1) the demonstration of this concept on gaming combat data generated
from three pre-defined strategies and 2) the framework for using IRL to achieve
strategy identification. The numerical experiments demonstrate that the
recovered rewards can be identified using a variety of techniques. In this
paper, the recovered reward are visually displayed, clustered using
unsupervised learning, and classified using a supervised learner.
",0
Offline Decentralized Multi-Agent Reinforcement Learning,"Jiechuan Jiang, Zongqing Lu",2021-08-04T03:53:33Z,Reinforcement Learning,"  In many real-world multi-agent cooperative tasks, due to high cost and risk,
agents cannot continuously interact with the environment and collect
experiences during learning, but have to learn from offline datasets. However,
the transition dynamics in the dataset of each agent can be much different from
the ones induced by the learned policies of other agents in execution, creating
large errors in value estimates. Consequently, agents learn uncoordinated
low-performing policies. In this paper, we propose a framework for offline
decentralized multi-agent reinforcement learning, which exploits value
deviation and transition normalization to deliberately modify the transition
probabilities. Value deviation optimistically increases the transition
probabilities of high-value next states, and transition normalization
normalizes the transition probabilities of next states. They together enable
agents to learn high-performing and coordinated policies. Theoretically, we
prove the convergence of Q-learning under the altered non-stationary transition
dynamics. Empirically, we show that the framework can be easily built on many
existing offline reinforcement learning algorithms and achieve substantial
improvement in a variety of multi-agent tasks.
",0
Quantum Reinforcement Learning: the Maze problem,"Nicola Dalla Pozza, Lorenzo Buffoni, Stefano Martina, Filippo Caruso",2021-08-10T07:49:58Z,Reinforcement Learning,"  Quantum Machine Learning (QML) is a young but rapidly growing field where
quantum information meets machine learning. Here, we will introduce a new QML
model generalizing the classical concept of Reinforcement Learning to the
quantum domain, i.e. Quantum Reinforcement Learning (QRL). In particular we
apply this idea to the maze problem, where an agent has to learn the optimal
set of actions in order to escape from a maze with the highest success
probability. To perform the strategy optimization, we consider an hybrid
protocol where QRL is combined with classical deep neural networks. In
particular, we find that the agent learns the optimal strategy in both the
classical and quantum regimes, and we also investigate its behaviour in a noisy
environment. It turns out that the quantum speedup does robustly allow the
agent to exploit useful actions also at very short time scales, with key roles
played by the quantum coherence and the external noise. This new framework has
the high potential to be applied to perform different tasks (e.g. high
transmission/processing rates and quantum error correction) in the
new-generation Noisy Intermediate-Scale Quantum (NISQ) devices whose topology
engineering is starting to become a new and crucial control knob for practical
applications in real-world problems. This work is dedicated to the memory of
Peter Wittek.
",0
Gap-Dependent Unsupervised Exploration for Reinforcement Learning,"Jingfeng Wu, Vladimir Braverman, Lin F. Yang",2021-08-11T20:42:46Z,Reinforcement Learning,"  For the problem of task-agnostic reinforcement learning (RL), an agent first
collects samples from an unknown environment without the supervision of reward
signals, then is revealed with a reward and is asked to compute a corresponding
near-optimal policy. Existing approaches mainly concern the worst-case
scenarios, in which no structural information of the reward/transition-dynamics
is utilized. Therefore the best sample upper bound is
$\propto\widetilde{\mathcal{O}}(1/\epsilon^2)$, where $\epsilon>0$ is the
target accuracy of the obtained policy, and can be overly pessimistic. To
tackle this issue, we provide an efficient algorithm that utilizes a gap
parameter, $\rho>0$, to reduce the amount of exploration. In particular, for an
unknown finite-horizon Markov decision process, the algorithm takes only
$\widetilde{\mathcal{O}} (1/\epsilon \cdot (H^3SA / \rho + H^4 S^2 A) )$
episodes of exploration, and is able to obtain an $\epsilon$-optimal policy for
a post-revealed reward with sub-optimality gap at least $\rho$, where $S$ is
the number of states, $A$ is the number of actions, and $H$ is the length of
the horizon, obtaining a nearly \emph{quadratic saving} in terms of $\epsilon$.
We show that, information-theoretically, this bound is nearly tight for $\rho <
\Theta(1/(HS))$ and $H>1$. We further show that
$\propto\widetilde{\mathcal{O}}(1)$ sample bound is possible for $H=1$ (i.e.,
multi-armed bandit) or with a sampling simulator, establishing a stark
separation between those settings and the RL setting.
",0
Aspect Sentiment Triplet Extraction Using Reinforcement Learning,"Samson Yu Bai Jian, Tapas Nayak, Navonil Majumder, Soujanya Poria",2021-08-13T07:38:48Z,Reinforcement Learning,"  Aspect Sentiment Triplet Extraction (ASTE) is the task of extracting triplets
of aspect terms, their associated sentiments, and the opinion terms that
provide evidence for the expressed sentiments. Previous approaches to ASTE
usually simultaneously extract all three components or first identify the
aspect and opinion terms, then pair them up to predict their sentiment
polarities. In this work, we present a novel paradigm, ASTE-RL, by regarding
the aspect and opinion terms as arguments of the expressed sentiment in a
hierarchical reinforcement learning (RL) framework. We first focus on
sentiments expressed in a sentence, then identify the target aspect and opinion
terms for that sentiment. This takes into account the mutual interactions among
the triplet's components while improving exploration and sample efficiency.
Furthermore, this hierarchical RLsetup enables us to deal with multiple and
overlapping triplets. In our experiments, we evaluate our model on existing
datasets from laptop and restaurant domains and show that it achieves
state-of-the-art performance. The implementation of this work is publicly
available at https://github.com/declare-lab/ASTE-RL.
",0
"Plug and Play, Model-Based Reinforcement Learning","Majid Abdolshah, Hung Le, Thommen Karimpanal George, Sunil Gupta, Santu Rana, Svetha Venkatesh",2021-08-20T01:20:15Z,Reinforcement Learning,"  Sample-efficient generalisation of reinforcement learning approaches have
always been a challenge, especially, for complex scenes with many components.
In this work, we introduce Plug and Play Markov Decision Processes, an
object-based representation that allows zero-shot integration of new objects
from known object classes. This is achieved by representing the global
transition dynamics as a union of local transition functions, each with respect
to one active object in the scene. Transition dynamics from an object class can
be pre-learnt and thus would be ready to use in a new environment. Each active
object is also endowed with its reward function. Since there is no central
reward function, addition or removal of objects can be handled efficiently by
only updating the reward functions of objects involved. A new transfer learning
mechanism is also proposed to adapt reward function in such cases. Experiments
show that our representation can achieve sample-efficiency in a variety of
set-ups.
",0
A Boosting Approach to Reinforcement Learning,"Nataly Brukhim, Elad Hazan, Karan Singh",2021-08-22T16:00:45Z,Reinforcement Learning,"  Reducing reinforcement learning to supervised learning is a well-studied and
effective approach that leverages the benefits of compact function
approximation to deal with large-scale Markov decision processes.
Independently, the boosting methodology (e.g. AdaBoost) has proven to be
indispensable in designing efficient and accurate classification algorithms by
combining inaccurate rules-of-thumb.
  In this paper, we take a further step: we reduce reinforcement learning to a
sequence of weak learning problems. Since weak learners perform only marginally
better than random guesses, such subroutines constitute a weaker assumption
than the availability of an accurate supervised learning oracle. We prove that
the sample complexity and running time bounds of the proposed method do not
explicitly depend on the number of states.
  While existing results on boosting operate on convex losses, the value
function over policies is non-convex. We show how to use a non-convex variant
of the Frank-Wolfe method for boosting, that additionally improves upon the
known sample complexity and running time even for reductions to supervised
learning.
",0
Adversary agent reinforcement learning for pursuit-evasion,X. Huang,2021-08-25T01:44:06Z,Reinforcement Learning,"  A reinforcement learning environment with adversary agents is proposed in
this work for pursuit-evasion game in the presence of fog of war, which is of
both scientific significance and practical importance in aerospace
applications. One of the most popular learning environments, StarCraft, is
adopted here and the associated mini-games are analyzed to identify the current
limitation for training adversary agents. The key contribution includes the
analysis of the potential performance of an agent by incorporating control and
differential game theory into the specific reinforcement learning environment,
and the development of an adversary agents challenge (SAAC) environment by
extending the current StarCraft mini-games. The subsequent study showcases the
use of this learning environment and the effectiveness of an adversary agent
for evasion units. Overall, the proposed SAAC environment should benefit
pursuit-evasion studies with rapidly-emerging reinforcement learning
technologies. Last but not least, the corresponding tutorial code can be found
at GitHub.
",0
Sensor-Based Navigation Using Hierarchical Reinforcement Learning,"Christopher Gebauer, Nils Dengler, Maren Bennewitz",2021-08-30T14:29:57Z,Reinforcement Learning,"  Robotic systems are nowadays capable of solving complex navigation tasks.
However, their capabilities are limited to the knowledge of the designer and
consequently lack generalizability to initially unconsidered situations. This
makes deep reinforcement learning (DRL) especially interesting, as these
algorithms promise a self-learning system only relying on feedback from the
environment. In this paper, we consider the problem of lidar-based robot
navigation in continuous action space using DRL without providing any
goal-oriented or global information. By relying solely on local sensor data to
solve navigation tasks, we design an agent that assigns its own waypoints based
on intrinsic motivation. Our agent is able to learn goal-directed navigation
behavior even when facing only sparse feedback, i.e., delayed rewards when
reaching the target. To address this challenge and the complexity of the
continuous action space, we deploy a hierarchical agent structure in which the
exploration is distributed across multiple layers. Within the hierarchical
structure, our agent self-assigns internal goals and learns to extract
reasonable waypoints to reach the desired target position only based on local
sensor data. In our experiments, we demonstrate the navigation capabilities of
our agent in two environments and show that the hierarchical structure
seriously improves the performance in terms of success rate and success
weighted by path length in comparison to a flat structure. Furthermore, we
provide a real-robot experiment to illustrate that the trained agent can be
easily transferred to a real-world scenario.
",0
Variational Quantum Reinforcement Learning via Evolutionary Optimization,"Samuel Yen-Chi Chen, Chih-Min Huang, Chia-Wei Hsing, Hsi-Sheng Goan, Ying-Jer Kao",2021-09-01T16:36:04Z,Reinforcement Learning,"  Recent advance in classical reinforcement learning (RL) and quantum
computation (QC) points to a promising direction of performing RL on a quantum
computer. However, potential applications in quantum RL are limited by the
number of qubits available in the modern quantum devices. Here we present two
frameworks of deep quantum RL tasks using a gradient-free evolution
optimization: First, we apply the amplitude encoding scheme to the Cart-Pole
problem; Second, we propose a hybrid framework where the quantum RL agents are
equipped with hybrid tensor network-variational quantum circuit (TN-VQC)
architecture to handle inputs with dimensions exceeding the number of qubits.
This allows us to perform quantum RL on the MiniGrid environment with
147-dimensional inputs. We demonstrate the quantum advantage of parameter
saving using the amplitude encoding. The hybrid TN-VQC architecture provides a
natural way to perform efficient compression of the input dimension, enabling
further quantum RL applications on noisy intermediate-scale quantum devices.
",0
Self-timed Reinforcement Learning using Tsetlin Machine,"Adrian Wheeldon, Alex Yakovlev, Rishad Shafik",2021-09-02T11:24:23Z,Reinforcement Learning,"  We present a hardware design for the learning datapath of the Tsetlin machine
algorithm, along with a latency analysis of the inference datapath. In order to
generate a low energy hardware which is suitable for pervasive artificial
intelligence applications, we use a mixture of asynchronous design techniques -
including Petri nets, signal transition graphs, dual-rail and bundled-data. The
work builds on previous design of the inference hardware, and includes an
in-depth breakdown of the automaton feedback, probability generation and
Tsetlin automata. Results illustrate the advantages of asynchronous design in
applications such as personalized healthcare and battery-powered internet of
things devices, where energy is limited and latency is an important figure of
merit. Challenges of static timing analysis in asynchronous circuits are also
addressed.
",0
Delving into Macro Placement with Reinforcement Learning,"Zixuan Jiang, Ebrahim Songhori, Shen Wang, Anna Goldie, Azalia Mirhoseini, Joe Jiang, Young-Joon Lee, David Z. Pan",2021-09-06T16:30:01Z,Reinforcement Learning,"  In physical design, human designers typically place macros via trial and
error, which is a Markov decision process. Reinforcement learning (RL) methods
have demonstrated superhuman performance on the macro placement. In this paper,
we propose an extension to this prior work (Mirhoseini et al., 2020). We first
describe the details of the policy and value network architecture. We replace
the force-directed method with DREAMPlace for placing standard cells in the RL
environment. We also compare our improved method with other academic placers on
public benchmarks.
",0
Guiding Global Placement With Reinforcement Learning,"Robert Kirby, Kolby Nottingham, Rajarshi Roy, Saad Godil, Bryan Catanzaro",2021-09-06T17:54:45Z,Reinforcement Learning,"  Recent advances in GPU accelerated global and detail placement have reduced
the time to solution by an order of magnitude. This advancement allows us to
leverage data driven optimization (such as Reinforcement Learning) in an effort
to improve the final quality of placement results. In this work we augment
state-of-the-art, force-based global placement solvers with a reinforcement
learning agent trained to improve the final detail placed Half Perimeter Wire
Length (HPWL).
  We propose novel control schemes with either global or localized control of
the placement process. We then train reinforcement learning agents to use these
controls to guide placement to improved solutions. In both cases, the augmented
optimizer finds improved placement solutions.
  Our trained agents achieve an average 1% improvement in final detail place
HPWL across a range of academic benchmarks and more than 1% in global place
HPWL on real industry designs.
",0
User Tampering in Reinforcement Learning Recommender Systems,"Charles Evans, Atoosa Kasirzadeh",2021-09-09T07:53:23Z,Reinforcement Learning,"  In this paper, we introduce new formal methods and provide empirical evidence
to highlight a unique safety concern prevalent in reinforcement learning
(RL)-based recommendation algorithms -- 'user tampering.' User tampering is a
situation where an RL-based recommender system may manipulate a media user's
opinions through its suggestions as part of a policy to maximize long-term user
engagement. We use formal techniques from causal modeling to critically analyze
prevailing solutions proposed in the literature for implementing scalable
RL-based recommendation systems, and we observe that these methods do not
adequately prevent user tampering. Moreover, we evaluate existing mitigation
strategies for reward tampering issues, and show that these methods are
insufficient in addressing the distinct phenomenon of user tampering within the
context of recommendations. We further reinforce our findings with a simulation
study of an RL-based recommendation system focused on the dissemination of
political content. Our study shows that a Q-learning algorithm consistently
learns to exploit its opportunities to polarize simulated users with its early
recommendations in order to have more consistent success with subsequent
recommendations that align with this induced polarization. Our findings
emphasize the necessity for developing safer RL-based recommendation systems
and suggest that achieving such safety would require a fundamental shift in the
design away from the approaches we have seen in the recent literature.
",0
Estimation of Warfarin Dosage with Reinforcement Learning,Arpita Vats,2021-09-15T20:27:18Z,Reinforcement Learning,"  In this paper, it has attempted to use Reinforcement learning to model the
proper dosage of Warfarin for patients.The paper first examines two baselines:
a fixed model of 35 mg/week dosages and a linear model that relies on patient
data. We implemented a LinUCB bandit that improved performance measured on
regret and percent incorrect. On top of the LinUCB bandit, we experimented with
online supervised learning and reward reshaping to boost performance. Our
results clearly beat the baselines and show the promise of using multi-armed
bandits and artificial intelligence to aid physicians in deciding proper
dosages.
",0
Hindsight Foresight Relabeling for Meta-Reinforcement Learning,"Michael Wan, Jian Peng, Tanmay Gangwani",2021-09-18T23:49:14Z,Reinforcement Learning,"  Meta-reinforcement learning (meta-RL) algorithms allow for agents to learn
new behaviors from small amounts of experience, mitigating the sample
inefficiency problem in RL. However, while meta-RL agents can adapt quickly to
new tasks at test time after experiencing only a few trajectories, the
meta-training process is still sample-inefficient. Prior works have found that
in the multi-task RL setting, relabeling past transitions and thus sharing
experience among tasks can improve sample efficiency and asymptotic
performance. We apply this idea to the meta-RL setting and devise a new
relabeling method called Hindsight Foresight Relabeling (HFR). We construct a
relabeling distribution using the combination of ""hindsight"", which is used to
relabel trajectories using reward functions from the training task
distribution, and ""foresight"", which takes the relabeled trajectories and
computes the utility of each trajectory for each task. HFR is easy to implement
and readily compatible with existing meta-RL algorithms. We find that HFR
improves performance when compared to other relabeling methods on a variety of
meta-RL tasks.
",0
Lifelong Robotic Reinforcement Learning by Retaining Experiences,"Annie Xie, Chelsea Finn",2021-09-19T18:00:51Z,Reinforcement Learning,"  Multi-task learning ideally allows robots to acquire a diverse repertoire of
useful skills. However, many multi-task reinforcement learning efforts assume
the robot can collect data from all tasks at all times. In reality, the tasks
that the robot learns arrive sequentially, depending on the user and the
robot's current environment. In this work, we study a practical sequential
multi-task RL problem that is motivated by the practical constraints of
physical robotic systems, and derive an approach that effectively leverages the
data and policies learned for previous tasks to cumulatively grow the robot's
skill-set. In a series of simulated robotic manipulation experiments, our
approach requires less than half the samples than learning each task from
scratch, while avoiding impractical round-robin data collection. On a Franka
Emika Panda robot arm, our approach incrementally learns ten challenging tasks,
including bottle capping and block insertion.
",0
AutoPhoto: Aesthetic Photo Capture using Reinforcement Learning,"Hadi AlZayer, Hubert Lin, Kavita Bala",2021-09-21T02:52:34Z,Reinforcement Learning,"  The process of capturing a well-composed photo is difficult and it takes
years of experience to master. We propose a novel pipeline for an autonomous
agent to automatically capture an aesthetic photograph by navigating within a
local region in a scene. Instead of classical optimization over heuristics such
as the rule-of-thirds, we adopt a data-driven aesthetics estimator to assess
photo quality. A reinforcement learning framework is used to optimize the model
with respect to the learned aesthetics metric. We train our model in simulation
with indoor scenes, and we demonstrate that our system can capture aesthetic
photos in both simulation and real world environments on a ground robot. To our
knowledge, this is the first system that can automatically explore an
environment to capture an aesthetic photo with respect to a learned aesthetic
estimator.
",0
Autonomous Blimp Control using Deep Reinforcement Learning,"Yu Tang Liu, Eric Price, Pascal Goldschmid, Michael J. Black, Aamir Ahmad",2021-09-22T13:22:40Z,Reinforcement Learning,"  Aerial robot solutions are becoming ubiquitous for an increasing number of
tasks. Among the various types of aerial robots, blimps are very well suited to
perform long-duration tasks while being energy efficient, relatively silent and
safe. To address the blimp navigation and control task, in our recent work, we
have developed a software-in-the-loop simulation and a PID-based controller for
large blimps in the presence of wind disturbance. However, blimps have a
deformable structure and their dynamics are inherently non-linear and
time-delayed, often resulting in large trajectory tracking errors. Moreover,
the buoyancy of a blimp is constantly changing due to changes in the ambient
temperature and pressure. In the present paper, we explore a deep reinforcement
learning (DRL) approach to address these issues. We train only in simulation,
while keeping conditions as close as possible to the real-world scenario. We
derive a compact state representation to reduce the training time and a
discrete action space to enforce control smoothness. Our initial results in
simulation show a significant potential of DRL in solving the blimp control
task and robustness against moderate wind and parameter uncertainty. Extensive
experiments are presented to study the robustness of our approach. We also
openly provide the source code of our approach.
",0
The $f$-Divergence Reinforcement Learning Framework,"Chen Gong, Qiang He, Yunpeng Bai, Zhou Yang, Xiaoyu Chen, Xinwen Hou, Xianjie Zhang, Yu Liu, Guoliang Fan",2021-09-24T10:20:46Z,Reinforcement Learning,"  The framework of deep reinforcement learning (DRL) provides a powerful and
widely applicable mathematical formalization for sequential decision-making.
This paper present a novel DRL framework, termed \emph{$f$-Divergence
Reinforcement Learning (FRL)}. In FRL, the policy evaluation and policy
improvement phases are simultaneously performed by minimizing the
$f$-divergence between the learning policy and sampling policy, which is
distinct from conventional DRL algorithms that aim to maximize the expected
cumulative rewards. We theoretically prove that minimizing such $f$-divergence
can make the learning policy converge to the optimal policy. Besides, we
convert the process of training agents in FRL framework to a saddle-point
optimization problem with a specific $f$ function through Fenchel conjugate,
which forms new methods for policy evaluation and policy improvement. Through
mathematical proofs and empirical evaluation, we demonstrate that the FRL
framework has two advantages: (1) policy evaluation and policy improvement
processes are performed simultaneously and (2) the issues of overestimating
value function are naturally alleviated. To evaluate the effectiveness of the
FRL framework, we conduct experiments on Atari 2600 video games and show that
agents trained in the FRL framework match or surpass the baseline DRL
algorithms.
",0
"Stackelberg Actor-Critic: Game-Theoretic Reinforcement Learning
  Algorithms","Liyuan Zheng, Tanner Fiez, Zane Alumbaugh, Benjamin Chasnov, Lillian J. Ratliff",2021-09-25T06:18:41Z,Reinforcement Learning,"  The hierarchical interaction between the actor and critic in actor-critic
based reinforcement learning algorithms naturally lends itself to a
game-theoretic interpretation. We adopt this viewpoint and model the actor and
critic interaction as a two-player general-sum game with a leader-follower
structure known as a Stackelberg game. Given this abstraction, we propose a
meta-framework for Stackelberg actor-critic algorithms where the leader player
follows the total derivative of its objective instead of the usual individual
gradient. From a theoretical standpoint, we develop a policy gradient theorem
for the refined update and provide a local convergence guarantee for the
Stackelberg actor-critic algorithms to a local Stackelberg equilibrium. From an
empirical standpoint, we demonstrate via simple examples that the learning
dynamics we study mitigate cycling and accelerate convergence compared to the
usual gradient dynamics given cost structures induced by actor-critic
formulations. Finally, extensive experiments on OpenAI gym environments show
that Stackelberg actor-critic algorithms always perform at least as well and
often significantly outperform the standard actor-critic algorithm
counterparts.
",0
A First-Occupancy Representation for Reinforcement Learning,"Ted Moskovitz, Spencer R. Wilson, Maneesh Sahani",2021-09-28T16:48:16Z,Reinforcement Learning,"  Both animals and artificial agents benefit from state representations that
support rapid transfer of learning across tasks and which enable them to
efficiently traverse their environments to reach rewarding states. The
successor representation (SR), which measures the expected cumulative,
discounted state occupancy under a fixed policy, enables efficient transfer to
different reward structures in an otherwise constant Markovian environment and
has been hypothesized to underlie aspects of biological behavior and neural
activity. However, in the real world, rewards may move or only be available for
consumption once, may shift location, or agents may simply aim to reach goal
states as rapidly as possible without the constraint of artificially imposed
task horizons. In such cases, the most behaviorally-relevant representation
would carry information about when the agent was likely to first reach states
of interest, rather than how often it should expect to visit them over a
potentially infinite time span. To reflect such demands, we introduce the
first-occupancy representation (FR), which measures the expected temporal
discount to the first time a state is accessed. We demonstrate that the FR
facilitates exploration, the selection of efficient paths to desired states,
allows the agent, under certain conditions, to plan provably optimal
trajectories defined by a sequence of subgoals, and induces similar behavior to
animals avoiding threatening stimuli.
",11
Online Robust Reinforcement Learning with Model Uncertainty,"Yue Wang, Shaofeng Zou",2021-09-29T16:17:47Z,Reinforcement Learning,"  Robust reinforcement learning (RL) is to find a policy that optimizes the
worst-case performance over an uncertainty set of MDPs. In this paper, we focus
on model-free robust RL, where the uncertainty set is defined to be centering
at a misspecified MDP that generates a single sample trajectory sequentially
and is assumed to be unknown. We develop a sample-based approach to estimate
the unknown uncertainty set and design a robust Q-learning algorithm (tabular
case) and robust TDC algorithm (function approximation setting), which can be
implemented in an online and incremental fashion. For the robust Q-learning
algorithm, we prove that it converges to the optimal robust Q function, and for
the robust TDC algorithm, we prove that it converges asymptotically to some
stationary points. Unlike the results in [Roy et al., 2017], our algorithms do
not need any additional conditions on the discount factor to guarantee the
convergence. We further characterize the finite-time error bounds of the two
algorithms and show that both the robust Q-learning and robust TDC algorithms
converge as fast as their vanilla counterparts(within a constant factor). Our
numerical experiments further demonstrate the robustness of our algorithms. Our
approach can be readily extended to robustify many other algorithms, e.g., TD,
SARSA, and other GTD algorithms.
",0
Surveillance Evasion Through Bayesian Reinforcement Learning,"Dongping Qi, David Bindel, Alexander Vladimirsky",2021-09-30T02:29:21Z,Reinforcement Learning,"  We consider a task of surveillance-evading path-planning in a continuous
setting. An Evader strives to escape from a 2D domain while minimizing the risk
of detection (and immediate capture). The probability of detection is
path-dependent and determined by the spatially inhomogeneous surveillance
intensity, which is fixed but a priori unknown and gradually learned in the
multi-episodic setting. We introduce a Bayesian reinforcement learning
algorithm that relies on a Gaussian Process regression (to model the
surveillance intensity function based on the information from prior episodes),
numerical methods for Hamilton-Jacobi PDEs (to plan the best continuous
trajectories based on the current model), and Confidence Bounds (to balance the
exploration vs exploitation). We use numerical experiments and regret metrics
to highlight the significant advantages of our approach compared to traditional
graph-based algorithms of reinforcement learning.
",0
Reinforcement Learning with Information-Theoretic Actuation,"Elliot Catt, Marcus Hutter, Joel Veness",2021-09-30T14:10:59Z,Reinforcement Learning,"  Reinforcement Learning formalises an embodied agent's interaction with the
environment through observations, rewards and actions. But where do the actions
come from? Actions are often considered to represent something external, such
as the movement of a limb, a chess piece, or more generally, the output of an
actuator. In this work we explore and formalize a contrasting view, namely that
actions are best thought of as the output of a sequence of internal choices
with respect to an action model. This view is particularly well-suited for
leveraging the recent advances in large sequence models as prior knowledge for
multi-task reinforcement learning problems. Our main contribution in this work
is to show how to augment the standard MDP formalism with a sequential notion
of internal action using information-theoretic techniques, and that this leads
to self-consistent definitions of both internal and external action value
functions.
",0
Coordinated Reinforcement Learning for Optimizing Mobile Networks,"Maxime Bouton, Hasan Farooq, Julien Forgeat, Shruti Bothe, Meral Shirazipour, Per Karlsson",2021-09-30T14:46:18Z,Reinforcement Learning,"  Mobile networks are composed of many base stations and for each of them many
parameters must be optimized to provide good services. Automatically and
dynamically optimizing all these entities is challenging as they are sensitive
to variations in the environment and can affect each other through
interferences. Reinforcement learning (RL) algorithms are good candidates to
automatically learn base station configuration strategies from incoming data
but they are often hard to scale to many agents. In this work, we demonstrate
how to use coordination graphs and reinforcement learning in a complex
application involving hundreds of cooperating agents. We show how mobile
networks can be modeled using coordination graphs and how network optimization
problems can be solved efficiently using multi- agent reinforcement learning.
The graph structure occurs naturally from expert knowledge about the network
and allows to explicitly learn coordinating behaviors between the antennas
through edge value functions represented by neural networks. We show
empirically that coordinated reinforcement learning outperforms other methods.
The use of local RL updates and parameter sharing can handle a large number of
agents without sacrificing coordination which makes it well suited to optimize
the ever denser networks brought by 5G and beyond.
",0
Decentralized Safe Reinforcement Learning for Voltage Control,"Wenqi Cui, Jiayi Li, Baosen Zhang",2021-10-03T23:33:12Z,Reinforcement Learning,"  Inverter-based distributed energy resources provide the possibility for fast
time-scale voltage control by quickly adjusting their reactive power. The
power-electronic interfaces allow these resources to realize almost arbitrary
control law, but designing these decentralized controllers is nontrivial.
Reinforcement learning (RL) approaches are becoming increasingly popular to
search for policy parameterized by neural networks. It is difficult, however,
to enforce that the learned controllers are safe, in the sense that they may
introduce instabilities into the system.
  This paper proposes a safe learning approach for voltage control. We prove
that the system is guaranteed to be exponentially stable if each controller
satisfies certain Lipschitz constraints. The set of Lipschitz bound is
optimized to enlarge the search space for neural network controllers. We
explicitly engineer the structure of neural network controllers such that they
satisfy the Lipschitz constraints by design. A decentralized RL framework is
constructed to train local neural network controller at each bus in a
model-free setting.
",0
Automating Privilege Escalation with Deep Reinforcement Learning,"Kalle Kujanpää, Willie Victor, Alexander Ilin",2021-10-04T12:20:46Z,Reinforcement Learning,"  AI-based defensive solutions are necessary to defend networks and information
assets against intelligent automated attacks. Gathering enough realistic data
for training machine learning-based defenses is a significant practical
challenge. An intelligent red teaming agent capable of performing realistic
attacks can alleviate this problem. However, there is little scientific
evidence demonstrating the feasibility of fully automated attacks using machine
learning. In this work, we exemplify the potential threat of malicious actors
using deep reinforcement learning to train automated agents. We present an
agent that uses a state-of-the-art reinforcement learning algorithm to perform
local privilege escalation. Our results show that the autonomous agent can
escalate privileges in a Windows 7 environment using a wide variety of
different techniques depending on the environment configuration it encounters.
Hence, our agent is usable for generating realistic attack sensor data for
training and evaluating intrusion detection systems.
",0
Dropout Q-Functions for Doubly Efficient Reinforcement Learning,"Takuya Hiraoka, Takahisa Imagawa, Taisei Hashimoto, Takashi Onishi, Yoshimasa Tsuruoka",2021-10-05T13:28:11Z,Reinforcement Learning,"  Randomized ensembled double Q-learning (REDQ) (Chen et al., 2021b) has
recently achieved state-of-the-art sample efficiency on continuous-action
reinforcement learning benchmarks. This superior sample efficiency is made
possible by using a large Q-function ensemble. However, REDQ is much less
computationally efficient than non-ensemble counterparts such as Soft
Actor-Critic (SAC) (Haarnoja et al., 2018a). To make REDQ more computationally
efficient, we propose a method of improving computational efficiency called
DroQ, which is a variant of REDQ that uses a small ensemble of dropout
Q-functions. Our dropout Q-functions are simple Q-functions equipped with
dropout connection and layer normalization. Despite its simplicity of
implementation, our experimental results indicate that DroQ is doubly (sample
and computationally) efficient. It achieved comparable sample efficiency with
REDQ, much better computational efficiency than REDQ, and comparable
computational efficiency with that of SAC.
",81
The Information Geometry of Unsupervised Reinforcement Learning,"Benjamin Eysenbach, Ruslan Salakhutdinov, Sergey Levine",2021-10-06T13:08:36Z,Reinforcement Learning,"  How can a reinforcement learning (RL) agent prepare to solve downstream tasks
if those tasks are not known a priori? One approach is unsupervised skill
discovery, a class of algorithms that learn a set of policies without access to
a reward function. Such algorithms bear a close resemblance to representation
learning algorithms (e.g., contrastive learning) in supervised learning, in
that both are pretraining algorithms that maximize some approximation to a
mutual information objective. While prior work has shown that the set of skills
learned by such methods can accelerate downstream RL tasks, prior work offers
little analysis into whether these skill learning algorithms are optimal, or
even what notion of optimality would be appropriate to apply to them. In this
work, we show that unsupervised skill discovery algorithms based on mutual
information maximization do not learn skills that are optimal for every
possible reward function. However, we show that the distribution over skills
provides an optimal initialization minimizing regret against
adversarially-chosen reward functions, assuming a certain type of adaptation
procedure. Our analysis also provides a geometric perspective on these skill
learning methods.
",0
Optimized Recommender Systems with Deep Reinforcement Learning,Lucas Farris,2021-10-06T19:54:55Z,Reinforcement Learning,"  Recommender Systems have been the cornerstone of online retailers.
Traditionally they were based on rules, relevance scores, ranking algorithms,
and supervised learning algorithms, but now it is feasible to use reinforcement
learning algorithms to generate meaningful recommendations. This work
investigates and develops means to setup a reproducible testbed, and evaluate
different state of the art algorithms in a realistic environment. It entails a
proposal, literature review, methodology, results, and comments.
",0
Reinforcement Learning in Reward-Mixing MDPs,"Jeongyeol Kwon, Yonathan Efroni, Constantine Caramanis, Shie Mannor",2021-10-07T18:55:49Z,Reinforcement Learning,"  Learning a near optimal policy in a partially observable system remains an
elusive challenge in contemporary reinforcement learning. In this work, we
consider episodic reinforcement learning in a reward-mixing Markov decision
process (MDP). There, a reward function is drawn from one of multiple possible
reward models at the beginning of every episode, but the identity of the chosen
reward model is not revealed to the agent. Hence, the latent state space, for
which the dynamics are Markovian, is not given to the agent. We study the
problem of learning a near optimal policy for two reward-mixing MDPs. Unlike
existing approaches that rely on strong assumptions on the dynamics, we make no
assumptions and study the problem in full generality. Indeed, with no further
assumptions, even for two switching reward-models, the problem requires several
new ideas beyond existing algorithmic and analysis techniques for efficient
exploration. We provide the first polynomial-time algorithm that finds an
$\epsilon$-optimal policy after exploring $\tilde{O}(poly(H,\epsilon^{-1})
\cdot S^2 A^2)$ episodes, where $H$ is time-horizon and $S, A$ are the number
of states and actions respectively. This is the first efficient algorithm that
does not require any assumptions in partially observed environments where the
observation space is smaller than the latent state space.
",0
Offline Meta-Reinforcement Learning for Industrial Insertion,"Tony Z. Zhao, Jianlan Luo, Oleg Sushkov, Rugile Pevceviciute, Nicolas Heess, Jon Scholz, Stefan Schaal, Sergey Levine",2021-10-08T17:41:58Z,Reinforcement Learning,"  Reinforcement learning (RL) can in principle let robots automatically adapt
to new tasks, but current RL methods require a large number of trials to
accomplish this. In this paper, we tackle rapid adaptation to new tasks through
the framework of meta-learning, which utilizes past tasks to learn to adapt
with a specific focus on industrial insertion tasks. Fast adaptation is crucial
because prohibitively large number of on-robot trials will potentially damage
hardware pieces. Additionally, effective adaptation is also feasible in that
experience among different insertion applications can be largely leveraged by
each other. In this setting, we address two specific challenges when applying
meta-learning. First, conventional meta-RL algorithms require lengthy online
meta-training. We show that this can be replaced with appropriately chosen
offline data, resulting in an offline meta-RL method that only requires
demonstrations and trials from each of the prior tasks, without the need to run
costly meta-RL procedures online. Second, meta-RL methods can fail to
generalize to new tasks that are too different from those seen at meta-training
time, which poses a particular challenge in industrial applications, where high
success rates are critical. We address this by combining contextual
meta-learning with direct online finetuning: if the new task is similar to
those seen in the prior data, then the contextual meta-learner adapts
immediately, and if it is too different, it gradually adapts through
finetuning. We show that our approach is able to quickly adapt to a variety of
different insertion tasks, with a success rate of 100% using only a fraction of
the samples needed for learning the tasks from scratch. Experiment videos and
details are available at
https://sites.google.com/view/offline-metarl-insertion.
",63
Reinforcement Learning for Systematic FX Trading,"Gabriel Borrageiro, Nick Firoozye, Paolo Barucca",2021-10-10T09:44:29Z,Reinforcement Learning,"  We explore online inductive transfer learning, with a feature representation
transfer from a radial basis function network formed of Gaussian mixture model
hidden processing units to a direct, recurrent reinforcement learning agent.
This agent is put to work in an experiment, trading the major spot market
currency pairs, where we accurately account for transaction and funding costs.
These sources of profit and loss, including the price trends that occur in the
currency markets, are made available to the agent via a quadratic utility, who
learns to target a position directly. We improve upon earlier work by targeting
a risk position in an online transfer learning context. Our agent achieves an
annualised portfolio information ratio of 0.52 with a compound return of 9.3\%,
net of execution and funding cost, over a 7-year test set; this is despite
forcing the model to trade at the close of the trading day at 5 pm EST when
trading costs are statistically the most expensive.
",0
Bid Optimization using Maximum Entropy Reinforcement Learning,"Mengjuan Liu, Jinyu Liu, Zhengning Hu, Yuchen Ge, Xuyun Nie",2021-10-11T06:53:53Z,Reinforcement Learning,"  Real-time bidding (RTB) has become a critical way of online advertising. In
RTB, an advertiser can participate in bidding ad impressions to display its
advertisements. The advertiser determines every impression's bidding price
according to its bidding strategy. Therefore, a good bidding strategy can help
advertisers improve cost efficiency. This paper focuses on optimizing a single
advertiser's bidding strategy using reinforcement learning (RL) in RTB.
Unfortunately, it is challenging to optimize the bidding strategy through RL at
the granularity of impression due to the highly dynamic nature of the RTB
environment. In this paper, we first utilize a widely accepted linear bidding
function to compute every impression's base price and optimize it by a mutable
adjustment factor derived from the RTB auction environment, to avoid optimizing
every impression's bidding price directly. Specifically, we use the maximum
entropy RL algorithm (Soft Actor-Critic) to optimize the adjustment factor
generation policy at the impression-grained level. Finally, the empirical study
on a public dataset demonstrates that the proposed bidding strategy has
superior performance compared with the baselines.
",5
Offline Reinforcement Learning with Implicit Q-Learning,"Ilya Kostrikov, Ashvin Nair, Sergey Levine",2021-10-12T17:05:05Z,Reinforcement Learning,"  Offline reinforcement learning requires reconciling two conflicting aims:
learning a policy that improves over the behavior policy that collected the
dataset, while at the same time minimizing the deviation from the behavior
policy so as to avoid errors due to distributional shift. This trade-off is
critical, because most current offline reinforcement learning methods need to
query the value of unseen actions during training to improve the policy, and
therefore need to either constrain these actions to be in-distribution, or else
regularize their values. We propose an offline RL method that never needs to
evaluate actions outside of the dataset, but still enables the learned policy
to improve substantially over the best behavior in the data through
generalization. The main insight in our work is that, instead of evaluating
unseen actions from the latest policy, we can approximate the policy
improvement step implicitly by treating the state value function as a random
variable, with randomness determined by the action (while still integrating
over the dynamics to avoid excessive optimism), and then taking a state
conditional upper expectile of this random variable to estimate the value of
the best actions in that state. This leverages the generalization capacity of
the function approximator to estimate the value of the best available action at
a given state without ever directly querying a Q-function with this unseen
action. Our algorithm alternates between fitting this upper expectile value
function and backing it up into a Q-function. Then, we extract the policy via
advantage-weighted behavioral cloning. We dub our method implicit Q-learning
(IQL). IQL demonstrates the state-of-the-art performance on D4RL, a standard
benchmark for offline reinforcement learning. We also demonstrate that IQL
achieves strong performance fine-tuning using online interaction after offline
initialization.
",0
Feudal Reinforcement Learning by Reading Manuals,"Kai Wang, Zhonghao Wang, Mo Yu, Humphrey Shi",2021-10-13T03:50:15Z,Reinforcement Learning,"  Reading to act is a prevalent but challenging task which requires the ability
to reason from a concise instruction. However, previous works face the semantic
mismatch between the low-level actions and the high-level language descriptions
and require the human-designed curriculum to work properly. In this paper, we
present a Feudal Reinforcement Learning (FRL) model consisting of a manager
agent and a worker agent. The manager agent is a multi-hop plan generator
dealing with high-level abstract information and generating a series of
sub-goals in a backward manner. The worker agent deals with the low-level
perceptions and actions to achieve the sub-goals one by one. In comparison, our
FRL model effectively alleviate the mismatching between text-level inference
and low-level perceptions and actions; and is general to various forms of
environments, instructions and manuals; and our multi-hop plan generator can
significantly boost for challenging tasks where multi-step reasoning form the
texts is critical to resolve the instructed goals. We showcase our approach
achieves competitive performance on two challenging tasks, Read to Fight
Monsters (RTFM) and Messenger, without human-designed curriculum learning.
",0
Offline Reinforcement Learning with Soft Behavior Regularization,"Haoran Xu, Xianyuan Zhan, Jianxiong Li, Honglei Yin",2021-10-14T14:29:44Z,Reinforcement Learning,"  Most prior approaches to offline reinforcement learning (RL) utilize
\textit{behavior regularization}, typically augmenting existing off-policy
actor critic algorithms with a penalty measuring divergence between the policy
and the offline data. However, these approaches lack guaranteed performance
improvement over the behavior policy. In this work, we start from the
performance difference between the learned policy and the behavior policy, we
derive a new policy learning objective that can be used in the offline setting,
which corresponds to the advantage function value of the behavior policy,
multiplying by a state-marginal density ratio. We propose a practical way to
compute the density ratio and demonstrate its equivalence to a state-dependent
behavior regularization. Unlike state-independent regularization used in prior
approaches, this \textit{soft} regularization allows more freedom of policy
deviation at high confidence states, leading to better performance and
stability. We thus term our resulting algorithm Soft Behavior-regularized Actor
Critic (SBAC). Our experimental results show that SBAC matches or outperforms
the state-of-the-art on a set of continuous control locomotion and manipulation
tasks.
",0
On-Policy Model Errors in Reinforcement Learning,"Lukas P. Fröhlich, Maksym Lefarov, Melanie N. Zeilinger, Felix Berkenkamp",2021-10-15T10:15:53Z,Reinforcement Learning,"  Model-free reinforcement learning algorithms can compute policy gradients
given sampled environment transitions, but require large amounts of data. In
contrast, model-based methods can use the learned model to generate new data,
but model errors and bias can render learning unstable or suboptimal. In this
paper, we present a novel method that combines real-world data and a learned
model in order to get the best of both worlds. The core idea is to exploit the
real-world data for on-policy predictions and use the learned model only to
generalize to different actions. Specifically, we use the data as
time-dependent on-policy correction terms on top of a learned model, to retain
the ability to generate data without accumulating errors over long prediction
horizons. We motivate this method theoretically and show that it counteracts an
error term for model-based policy improvement. Experiments on MuJoCo- and
PyBullet-benchmarks show that our method can drastically improve existing
model-based approaches without introducing additional tuning parameters.
",0
Neural Network Pruning Through Constrained Reinforcement Learning,"Shehryar Malik, Muhammad Umair Haider, Omer Iqbal, Murtaza Taj",2021-10-16T11:57:38Z,Reinforcement Learning,"  Network pruning reduces the size of neural networks by removing (pruning)
neurons such that the performance drop is minimal. Traditional pruning
approaches focus on designing metrics to quantify the usefulness of a neuron
which is often quite tedious and sub-optimal. More recent approaches have
instead focused on training auxiliary networks to automatically learn how
useful each neuron is however, they often do not take computational limitations
into account. In this work, we propose a general methodology for pruning neural
networks. Our proposed methodology can prune neural networks to respect
pre-defined computational budgets on arbitrary, possibly non-differentiable,
functions. Furthermore, we only assume the ability to be able to evaluate these
functions for different inputs, and hence they do not need to be fully
specified beforehand. We achieve this by proposing a novel pruning strategy via
constrained reinforcement learning algorithms. We prove the effectiveness of
our approach via comparison with state-of-the-art methods on standard image
classification datasets. Specifically, we reduce 83-92.90 of total parameters
on various variants of VGG while achieving comparable or better performance
than that of original networks. We also achieved 75.09 reduction in parameters
on ResNet18 without incurring any loss in accuracy.
",0
Provable Hierarchy-Based Meta-Reinforcement Learning,"Kurtland Chua, Qi Lei, Jason D. Lee",2021-10-18T17:56:02Z,Reinforcement Learning,"  Hierarchical reinforcement learning (HRL) has seen widespread interest as an
approach to tractable learning of complex modular behaviors. However, existing
work either assume access to expert-constructed hierarchies, or use
hierarchy-learning heuristics with no provable guarantees. To address this gap,
we analyze HRL in the meta-RL setting, where a learner learns latent
hierarchical structure during meta-training for use in a downstream task. We
consider a tabular setting where natural hierarchical structure is embedded in
the transition dynamics. Analogous to supervised meta-learning theory, we
provide ""diversity conditions"" which, together with a tractable optimism-based
algorithm, guarantee sample-efficient recovery of this natural hierarchy.
Furthermore, we provide regret bounds on a learner using the recovered
hierarchy to solve a meta-test task. Our bounds incorporate common notions in
HRL literature such as temporal and state/action abstractions, suggesting that
our setting and analysis capture important features of HRL in practice.
",0
Aesthetic Photo Collage with Deep Reinforcement Learning,"Mingrui Zhang, Mading Li, Li Chen, Jiahao Yu",2021-10-19T07:34:48Z,Reinforcement Learning,"  Photo collage aims to automatically arrange multiple photos on a given canvas
with high aesthetic quality. Existing methods are based mainly on handcrafted
feature optimization, which cannot adequately capture high-level human
aesthetic senses. Deep learning provides a promising way, but owing to the
complexity of collage and lack of training data, a solution has yet to be
found. In this paper, we propose a novel pipeline for automatic generation of
aspect ratio specified collage and the reinforcement learning technique is
introduced in collage for the first time. Inspired by manual collages, we model
the collage generation as sequential decision process to adjust spatial
positions, orientation angles, placement order and the global layout. To
instruct the agent to improve both the overall layout and local details, the
reward function is specially designed for collage, considering subjective and
objective factors. To overcome the lack of training data, we pretrain our deep
aesthetic network on a large scale image aesthetic dataset (CPC) for general
aesthetic feature extraction and propose an attention fusion module for
structural collage feature representation. We test our model against competing
methods on two movie datasets and our results outperform others in aesthetic
quality evaluation. Further user study is also conducted to demonstrate the
effectiveness.
",0
False Correlation Reduction for Offline Reinforcement Learning,"Zhihong Deng, Zuyue Fu, Lingxiao Wang, Zhuoran Yang, Chenjia Bai, Tianyi Zhou, Zhaoran Wang, Jing Jiang",2021-10-24T15:34:03Z,Reinforcement Learning,"  Offline reinforcement learning (RL) harnesses the power of massive datasets
for resolving sequential decision problems. Most existing papers only discuss
defending against out-of-distribution (OOD) actions while we investigate a
broader issue, the false correlations between epistemic uncertainty and
decision-making, an essential factor that causes suboptimality. In this paper,
we propose falSe COrrelation REduction (SCORE) for offline RL, a practically
effective and theoretically provable algorithm. We empirically show that SCORE
achieves the SoTA performance with 3.1x acceleration on various tasks in a
standard benchmark (D4RL). The proposed algorithm introduces an annealing
behavior cloning regularizer to help produce a high-quality estimation of
uncertainty which is critical for eliminating false correlations from
suboptimality. Theoretically, we justify the rationality of the proposed method
and prove its convergence to the optimal policy with a sublinear rate under
mild assumptions.
",0
Uniformly Conservative Exploration in Reinforcement Learning,"Wanqiao Xu, Jason Yecheng Ma, Kan Xu, Hamsa Bastani, Osbert Bastani",2021-10-25T15:57:16Z,Reinforcement Learning,"  A key challenge to deploying reinforcement learning in practice is avoiding
excessive (harmful) exploration in individual episodes. We propose a natural
constraint on exploration -- \textit{uniformly} outperforming a conservative
policy (adaptively estimated from all data observed thus far), up to a
per-episode exploration budget. We design a novel algorithm that uses a UCB
reinforcement learning policy for exploration, but overrides it as needed to
satisfy our exploration constraint with high probability. Importantly, to
ensure unbiased exploration across the state space, our algorithm adaptively
determines when to explore. We prove that our approach remains conservative
while minimizing regret in the tabular setting. We experimentally validate our
results on a sepsis treatment task and an HIV treatment task, demonstrating
that our algorithm can learn while ensuring good performance compared to the
baseline policy for every patient; the latter task also demonstrates that our
approach extends to continuous state spaces via deep reinforcement learning.
",3
LTL-Based Non-Markovian Inverse Reinforcement Learning,"Mohammad Afzal, Sankalp Gambhir, Ashutosh Gupta, Krishna S, Ashutosh Trivedi, Alvaro Velasquez",2021-10-26T12:13:15Z,Reinforcement Learning,"  The successes of reinforcement learning in recent years are underpinned by
the characterization of suitable reward functions. However, in settings where
such rewards are non-intuitive, difficult to define, or otherwise error-prone
in their definition, it is useful to instead learn the reward signal from
expert demonstrations. This is the crux of inverse reinforcement learning
(IRL). While eliciting learning requirements in the form of scalar reward
signals has been shown to effective, such representations lack explainability
and lead to opaque learning. We aim to mitigate this situation by presenting a
novel IRL method for eliciting declarative learning requirements in the form of
a popular formal logic -- Linear Temporal Logic (LTL) -- from a set of traces
given by the expert policy. A key novelty of the proposed approach is
quantitative semantics of satisfaction of an LTL formula by a word that,
following Occam's razor principle, incentivizes simpler explanations. Given a
sample $S=(P,N)$ consisting of positive traces $P$ and negative traces $N$, the
proposed algorithms automate the search for a formula $\varphi$ which provides
the simplest explanation (in the $GF$ fragment of LTL) of the samples. We have
implemented this approach as an open-source tool QuantLearn to perform
logic-based non-Markovian IRL. Our results demonstrate the feasibility of the
proposed approach in eliciting intuitive LTL-based reward signals from noisy
data.
",0
Stabilising viscous extensional flows using Reinforcement Learning,"Marco Vona, Eric Lauga",2021-10-27T18:01:09Z,Reinforcement Learning,"  The four-roll mill, wherein four identical cylinders undergo rotation of
identical magnitude but alternate signs, was originally proposed by GI Taylor
to create local extensional flows and study their ability to deform small
liquid drops. Since an extensional flow has an unstable eigendirection, a drop
located at the flow stagnation point will have a tendency to escape. This
unstable dynamics can however be stabilised using, e.g., a modulation of the
rotation rates of the cylinders. Here we use Reinforcement Learning, a branch
of Machine Learning devoted to the optimal selection of actions based on
cumulative rewards, in order to devise a stabilisation algorithm for the
four-roll mill flow. The flow is modelled as the linear superposition of four
two-dimensional rotlets and the drop is treated as a rigid spherical particle
smaller than all other length scales in the problem. Unlike previous attempts
to devise control, we take a probabilistic approach whereby speed adjustments
are drawn from a probability density function whose shape is improved over time
via a form of gradient ascent know as Actor-Critic method. With enough
training, our algorithm is able to precisely control the drop and keep it close
to the stagnation point for as long as needed. We explore the impact of the
physical and learning parameters on the effectiveness of the control and
demonstrate the robustness of the algorithm against thermal noise. We finally
show that Reinforcement Learning can provide a control algorithm effective for
all initial positions and that can be adapted to limit the magnitude of the
flow extension near the position of the drop.
",2
Adaptive Discretization in Online Reinforcement Learning,"Sean R. Sinclair, Siddhartha Banerjee, Christina Lee Yu",2021-10-29T15:06:15Z,Reinforcement Learning,"  Discretization based approaches to solving online reinforcement learning
problems have been studied extensively in practice on applications ranging from
resource allocation to cache management. Two major questions in designing
discretization-based algorithms are how to create the discretization and when
to refine it. While there have been several experimental results investigating
heuristic solutions to these questions, there has been little theoretical
treatment. In this paper we provide a unified theoretical analysis of
tree-based hierarchical partitioning methods for online reinforcement learning,
providing model-free and model-based algorithms. We show how our algorithms are
able to take advantage of inherent structure of the problem by providing
guarantees that scale with respect to the 'zooming dimension' instead of the
ambient dimension, an instance-dependent quantity measuring the benignness of
the optimal $Q_h^\star$ function.
  Many applications in computing systems and operations research requires
algorithms that compete on three facets: low sample complexity, mild storage
requirements, and low computational burden. Our algorithms are easily adapted
to operating constraints, and our theory provides explicit bounds across each
of the three facets. This motivates its use in practical applications as our
approach automatically adapts to underlying problem structure even when very
little is known a priori about the system.
",0
Context Meta-Reinforcement Learning via Neuromodulation,"Eseoghene Ben-Iwhiwhu, Jeffery Dick, Nicholas A. Ketz, Praveen K. Pilly, Andrea Soltoggio",2021-10-30T01:05:40Z,Reinforcement Learning,"  Meta-reinforcement learning (meta-RL) algorithms enable agents to adapt
quickly to tasks from few samples in dynamic environments. Such a feat is
achieved through dynamic representations in an agent's policy network (obtained
via reasoning about task context, model parameter updates, or both). However,
obtaining rich dynamic representations for fast adaptation beyond simple
benchmark problems is challenging due to the burden placed on the policy
network to accommodate different policies. This paper addresses the challenge
by introducing neuromodulation as a modular component to augment a standard
policy network that regulates neuronal activities in order to produce efficient
dynamic representations for task adaptation. The proposed extension to the
policy network is evaluated across multiple discrete and continuous control
environments of increasing complexity. To prove the generality and benefits of
the extension in meta-RL, the neuromodulated network was applied to two
state-of-the-art meta-RL algorithms (CAVIA and PEARL). The result demonstrates
that meta-RL augmented with neuromodulation produces significantly better
result and richer dynamic representations in comparison to the baselines.
",0
Adjacency constraint for efficient hierarchical reinforcement learning,"Tianren Zhang, Shangqi Guo, Tian Tan, Xiaolin Hu, Feng Chen",2021-10-30T09:26:45Z,Reinforcement Learning,"  Goal-conditioned Hierarchical Reinforcement Learning (HRL) is a promising
approach for scaling up reinforcement learning (RL) techniques. However, it
often suffers from training inefficiency as the action space of the high-level,
i.e., the goal space, is large. Searching in a large goal space poses
difficulty for both high-level subgoal generation and low-level policy
learning. In this paper, we show that this problem can be effectively
alleviated by restricting the high-level action space from the whole goal space
to a $k$-step adjacent region of the current state using an adjacency
constraint. We theoretically prove that in a deterministic Markov Decision
Process (MDP), the proposed adjacency constraint preserves the optimal
hierarchical policy, while in a stochastic MDP the adjacency constraint induces
a bounded state-value suboptimality determined by the MDP's transition
structure. We further show that this constraint can be practically implemented
by training an adjacency network that can discriminate between adjacent and
non-adjacent subgoals. Experimental results on discrete and continuous control
tasks including challenging simulated robot locomotion and manipulation tasks
show that incorporating the adjacency constraint significantly boosts the
performance of state-of-the-art goal-conditioned HRL approaches.
",0
Model-Free Risk-Sensitive Reinforcement Learning,"Grégoire Delétang, Jordi Grau-Moya, Markus Kunesch, Tim Genewein, Rob Brekelmans, Shane Legg, Pedro A. Ortega",2021-11-04T14:27:46Z,Reinforcement Learning,"  We extend temporal-difference (TD) learning in order to obtain
risk-sensitive, model-free reinforcement learning algorithms. This extension
can be regarded as modification of the Rescorla-Wagner rule, where the
(sigmoidal) stimulus is taken to be either the event of over- or
underestimating the TD target. As a result, one obtains a stochastic
approximation rule for estimating the free energy from i.i.d. samples generated
by a Gaussian distribution with unknown mean and variance. Since the Gaussian
free energy is known to be a certainty-equivalent sensitive to the mean and the
variance, the learning rule has applications in risk-sensitive decision-making.
",0
B-Pref: Benchmarking Preference-Based Reinforcement Learning,"Kimin Lee, Laura Smith, Anca Dragan, Pieter Abbeel",2021-11-04T17:32:06Z,Reinforcement Learning,"  Reinforcement learning (RL) requires access to a reward function that
incentivizes the right behavior, but these are notoriously hard to specify for
complex tasks. Preference-based RL provides an alternative: learning policies
using a teacher's preferences without pre-defined rewards, thus overcoming
concerns associated with reward engineering. However, it is difficult to
quantify the progress in preference-based RL due to the lack of a commonly
adopted benchmark. In this paper, we introduce B-Pref: a benchmark specially
designed for preference-based RL. A key challenge with such a benchmark is
providing the ability to evaluate candidate algorithms quickly, which makes
relying on real human input for evaluation prohibitive. At the same time,
simulating human input as giving perfect preferences for the ground truth
reward function is unrealistic. B-Pref alleviates this by simulating teachers
with a wide array of irrationalities, and proposes metrics not solely for
performance but also for robustness to these potential irrationalities. We
showcase the utility of B-Pref by using it to analyze algorithmic design
choices, such as selecting informative queries, for state-of-the-art
preference-based RL algorithms. We hope that B-Pref can serve as a common
starting point to study preference-based RL more systematically. Source code is
available at https://github.com/rll-research/B-Pref.
",0
d3rlpy: An Offline Deep Reinforcement Learning Library,"Takuma Seno, Michita Imai",2021-11-06T03:09:39Z,Reinforcement Learning,"  In this paper, we introduce d3rlpy, an open-sourced offline deep
reinforcement learning (RL) library for Python. d3rlpy supports a set of
offline deep RL algorithms as well as off-policy online algorithms via a fully
documented plug-and-play API. To address a reproducibility issue, we conduct a
large-scale benchmark with D4RL and Atari 2600 dataset to ensure implementation
quality and provide experimental scripts and full tables of results. The d3rlpy
source code can be found on GitHub: \url{https://github.com/takuseno/d3rlpy}.
",0
Robust Deep Reinforcement Learning for Quadcopter Control,"Aditya M. Deshpande, Ali A. Minai, Manish Kumar",2021-11-06T16:35:13Z,Reinforcement Learning,"  Deep reinforcement learning (RL) has made it possible to solve complex
robotics problems using neural networks as function approximators. However, the
policies trained on stationary environments suffer in terms of generalization
when transferred from one environment to another. In this work, we use Robust
Markov Decision Processes (RMDP) to train the drone control policy, which
combines ideas from Robust Control and RL. It opts for pessimistic optimization
to handle potential gaps between policy transfer from one environment to
another. The trained control policy is tested on the task of quadcopter
positional control. RL agents were trained in a MuJoCo simulator. During
testing, different environment parameters (unseen during the training) were
used to validate the robustness of the trained policy for transfer from one
environment to another. The robust policy outperformed the standard agents in
these environments, suggesting that the added robustness increases generality
and can adapt to non-stationary environments.
  Codes: https://github.com/adipandas/gym_multirotor
",14
Reinforcement Learning for Mixed Autonomy Intersections,"Zhongxia Yan, Cathy Wu",2021-11-08T18:03:18Z,Reinforcement Learning,"  We propose a model-free reinforcement learning method for controlling mixed
autonomy traffic in simulated traffic networks with through-traffic-only
two-way and four-way intersections. Our method utilizes multi-agent policy
decomposition which allows decentralized control based on local observations
for an arbitrary number of controlled vehicles. We demonstrate that, even
without reward shaping, reinforcement learning learns to coordinate the
vehicles to exhibit traffic signal-like behaviors, achieving near-optimal
throughput with 33-50% controlled vehicles. With the help of multi-task
learning and transfer learning, we show that this behavior generalizes across
inflow rates and size of the traffic network. Our code, models, and videos of
results are available at
https://github.com/ZhongxiaYan/mixed_autonomy_intersections.
",0
Interactive Inverse Reinforcement Learning for Cooperative Games,"Thomas Kleine Buening, Anne-Marie George, Christos Dimitrakakis",2021-11-08T18:24:52Z,Reinforcement Learning,"  We study the problem of designing autonomous agents that can learn to
cooperate effectively with a potentially suboptimal partner while having no
access to the joint reward function. This problem is modeled as a cooperative
episodic two-agent Markov decision process. We assume control over only the
first of the two agents in a Stackelberg formulation of the game, where the
second agent is acting so as to maximise expected utility given the first
agent's policy. How should the first agent act in order to learn the joint
reward function as quickly as possible and so that the joint policy is as close
to optimal as possible? We analyse how knowledge about the reward function can
be gained in this interactive two-agent scenario. We show that when the
learning agent's policies have a significant effect on the transition function,
the reward function can be learned efficiently.
",0
A Dataset Perspective on Offline Reinforcement Learning,"Kajetan Schweighofer, Andreas Radler, Marius-Constantin Dinu, Markus Hofmarcher, Vihang Patil, Angela Bitto-Nemling, Hamid Eghbal-zadeh, Sepp Hochreiter",2021-11-08T18:48:43Z,Reinforcement Learning,"  The application of Reinforcement Learning (RL) in real world environments can
be expensive or risky due to sub-optimal policies during training. In Offline
RL, this problem is avoided since interactions with an environment are
prohibited. Policies are learned from a given dataset, which solely determines
their performance. Despite this fact, how dataset characteristics influence
Offline RL algorithms is still hardly investigated. The dataset characteristics
are determined by the behavioral policy that samples this dataset. Therefore,
we define characteristics of behavioral policies as exploratory for yielding
high expected information in their interaction with the Markov Decision Process
(MDP) and as exploitative for having high expected return. We implement two
corresponding empirical measures for the datasets sampled by the behavioral
policy in deterministic MDPs. The first empirical measure SACo is defined by
the normalized unique state-action pairs and captures exploration. The second
empirical measure TQ is defined by the normalized average trajectory return and
captures exploitation. Empirical evaluations show the effectiveness of TQ and
SACo. In large-scale experiments using our proposed measures, we show that the
unconstrained off-policy Deep Q-Network family requires datasets with high SACo
to find a good policy. Furthermore, experiments show that policy constraint
algorithms perform well on datasets with high TQ and SACo. Finally, the
experiments show, that purely dataset-constrained Behavioral Cloning performs
competitively to the best Offline RL algorithms for datasets with high TQ.
",0
Dueling RL: Reinforcement Learning with Trajectory Preferences,"Aldo Pacchiano, Aadirupa Saha, Jonathan Lee",2021-11-08T22:17:36Z,Reinforcement Learning,"  We consider the problem of preference based reinforcement learning (PbRL),
where, unlike traditional reinforcement learning, an agent receives feedback
only in terms of a 1 bit (0/1) preference over a trajectory pair instead of
absolute rewards for them. The success of the traditional RL framework
crucially relies on the underlying agent-reward model, which, however, depends
on how accurately a system designer can express an appropriate reward function
and often a non-trivial task. The main novelty of our framework is the ability
to learn from preference-based trajectory feedback that eliminates the need to
hand-craft numeric reward models. This paper sets up a formal framework for the
PbRL problem with non-markovian rewards, where the trajectory preferences are
encoded by a generalized linear model of dimension $d$. Assuming the transition
model is known, we then propose an algorithm with almost optimal regret
guarantee of $\tilde {\mathcal{O}}\left( SH d \log (T / \delta) \sqrt{T}
\right)$. We further, extend the above algorithm to the case of unknown
transition dynamics, and provide an algorithm with near optimal regret
guarantee $\widetilde{\mathcal{O}}((\sqrt{d} + H^2 + |\mathcal{S}|)\sqrt{dT}
+\sqrt{|\mathcal{S}||\mathcal{A}|TH} )$. To the best of our knowledge, our work
is one of the first to give tight regret guarantees for preference based RL
problems with trajectory preferences.
",61
DriverGym: Democratising Reinforcement Learning for Autonomous Driving,"Parth Kothari, Christian Perone, Luca Bergamini, Alexandre Alahi, Peter Ondruska",2021-11-12T11:47:08Z,Reinforcement Learning,"  Despite promising progress in reinforcement learning (RL), developing
algorithms for autonomous driving (AD) remains challenging: one of the critical
issues being the absence of an open-source platform capable of training and
effectively validating the RL policies on real-world data. We propose
DriverGym, an open-source OpenAI Gym-compatible environment specifically
tailored for developing RL algorithms for autonomous driving. DriverGym
provides access to more than 1000 hours of expert logged data and also supports
reactive and data-driven agent behavior. The performance of an RL policy can be
easily validated on real-world data using our extensive and flexible
closed-loop evaluation protocol. In this work, we also provide behavior cloning
baselines using supervised learning and RL, trained in DriverGym. We make
DriverGym code, as well as all the baselines publicly available to further
stimulate development from the community.
",0
Optimism and Delays in Episodic Reinforcement Learning,"Benjamin Howson, Ciara Pike-Burke, Sarah Filippi",2021-11-15T09:06:04Z,Reinforcement Learning,"  There are many algorithms for regret minimisation in episodic reinforcement
learning. This problem is well-understood from a theoretical perspective,
providing that the sequences of states, actions and rewards associated with
each episode are available to the algorithm updating the policy immediately
after every interaction with the environment. However, feedback is almost
always delayed in practice. In this paper, we study the impact of delayed
feedback in episodic reinforcement learning from a theoretical perspective and
propose two general-purpose approaches to handling the delays. The first
involves updating as soon as new information becomes available, whereas the
second waits before using newly observed information to update the policy. For
the class of optimistic algorithms and either approach, we show that the regret
increases by an additive term involving the number of states, actions, episode
length, the expected delay and an algorithm-dependent constant. We empirically
investigate the impact of various delay distributions on the regret of
optimistic algorithms to validate our theoretical results.
",0
Versatile Inverse Reinforcement Learning via Cumulative Rewards,"Niklas Freymuth, Philipp Becker, Gerhard Neumann",2021-11-15T10:49:15Z,Reinforcement Learning,"  Inverse Reinforcement Learning infers a reward function from expert
demonstrations, aiming to encode the behavior and intentions of the expert.
Current approaches usually do this with generative and uni-modal models,
meaning that they encode a single behavior. In the common setting, where there
are various solutions to a problem and the experts show versatile behavior this
severely limits the generalization capabilities of these methods. We propose a
novel method for Inverse Reinforcement Learning that overcomes these problems
by formulating the recovered reward as a sum of iteratively trained
discriminators. We show on simulated tasks that our approach is able to recover
general, high-quality reward functions and produces policies of the same
quality as behavioral cloning approaches designed for versatile behavior.
",0
Generating GPU Compiler Heuristics using Reinforcement Learning,"Ian Colbert, Jake Daly, Norm Rubin",2021-11-23T18:15:34Z,Reinforcement Learning,"  GPU compilers are complex software programs with many optimizations specific
to target hardware. These optimizations are often controlled by heuristics
hand-designed by compiler experts using time- and resource-intensive processes.
In this paper, we developed a GPU compiler autotuning framework that uses
off-policy deep reinforcement learning to generate heuristics that improve the
frame rates of graphics applications. Furthermore, we demonstrate the
resilience of these learned heuristics to frequent compiler updates by
analyzing their stability across a year of code check-ins without retraining.
We show that our machine learning-based compiler autotuning framework matches
or surpasses the frame rates for 98% of graphics benchmarks with an average
uplift of 1.6% up to 15.8%.
",0
A note on stabilizing reinforcement learning,"Pavel Osinenko, Grigory Yaremenko, Ilya Osokin",2021-11-24T07:58:14Z,Reinforcement Learning,"  Reinforcement learning is a general methodology of adaptive optimal control
that has attracted much attention in various fields ranging from video game
industry to robot manipulators. Despite its remarkable performance
demonstrations, plain reinforcement learning controllers do not guarantee
stability which compromises their applicability in industry. To provide such
guarantees, measures have to be taken. This gives rise to what could generally
be called stabilizing reinforcement learning. Concrete approaches range from
employment of human overseers to filter out unsafe actions to formally verified
shields and fusion with classical stabilizing controllers. A line of attack
that utilizes elements of adaptive control has become fairly popular in the
recent years. In this note, we critically address such an approach in a fairly
general actor-critic setup for nonlinear time-continuous environments. The
actor network utilizes a so-called robustifying term that is supposed to
compensate for the neural network errors. The corresponding stability analysis
is based on the value function itself. We indicate a problem in such a
stability analysis and provide a counterexample to the overall control scheme.
Implications for such a line of attack in stabilizing reinforcement learning
are discussed. Furthermore, unfortunately the said problem possess no fix
without a substantial reconsideration of the whole approach. As a positive
message, we derive a stochastic critic neural network weight convergence
analysis provided that the environment was stabilized.
",0
Final Adaptation Reinforcement Learning for N-Player Games,"Wolfgang Konen, Samineh Bagheri",2021-11-29T08:36:39Z,Reinforcement Learning,"  This paper covers n-tuple-based reinforcement learning (RL) algorithms for
games. We present new algorithms for TD-, SARSA- and Q-learning which work
seamlessly on various games with arbitrary number of players. This is achieved
by taking a player-centered view where each player propagates his/her rewards
back to previous rounds. We add a new element called Final Adaptation RL (FARL)
to all these algorithms. Our main contribution is that FARL is a vitally
important ingredient to achieve success with the player-centered view in
various games. We report results on seven board games with 1, 2 and 3 players,
including Othello, ConnectFour and Hex. In most cases it is found that FARL is
important to learn a near-perfect playing strategy. All algorithms are
available in the GBG framework on GitHub.
",0
Maximum Entropy Model-based Reinforcement Learning,"Oleg Svidchenko, Aleksei Shpilman",2021-12-02T13:07:29Z,Reinforcement Learning,"  Recent advances in reinforcement learning have demonstrated its ability to
solve hard agent-environment interaction tasks on a super-human level. However,
the application of reinforcement learning methods to practical and real-world
tasks is currently limited due to most RL state-of-art algorithms' sample
inefficiency, i.e., the need for a vast number of training episodes. For
example, OpenAI Five algorithm that has beaten human players in Dota 2 has
trained for thousands of years of game time. Several approaches exist that
tackle the issue of sample inefficiency, that either offers a more efficient
usage of already gathered experience or aim to gain a more relevant and diverse
experience via a better exploration of an environment. However, to our
knowledge, no such approach exists for model-based algorithms, that showed
their high sample efficiency in solving hard control tasks with
high-dimensional state space. This work connects exploration techniques and
model-based reinforcement learning. We have designed a novel exploration method
that takes into account features of the model-based approach. We also
demonstrate through experiments that our method significantly improves the
performance of the model-based algorithm Dreamer.
",0
Architecting and Visualizing Deep Reinforcement Learning Models,"Alexander Neuwirth, Derek Riley",2021-12-02T17:48:26Z,Reinforcement Learning,"  To meet the growing interest in Deep Reinforcement Learning (DRL), we sought
to construct a DRL-driven Atari Pong agent and accompanying visualization tool.
Existing approaches do not support the flexibility required to create an
interactive exhibit with easily-configurable physics and a human-controlled
player. Therefore, we constructed a new Pong game environment, discovered and
addressed a number of unique data deficiencies that arise when applying DRL to
a new environment, architected and tuned a policy gradient based DRL model,
developed a real-time network visualization, and combined these elements into
an interactive display to help build intuition and awareness of the mechanics
of DRL inference.
",0
Safe Reinforcement Learning for Grid Voltage Control,"Thanh Long Vu, Sayak Mukherjee, Renke Huang, Qiuhua Huang",2021-12-02T18:34:50Z,Reinforcement Learning,"  Under voltage load shedding has been considered as a standard approach to
recover the voltage stability of the electric power grid under emergency
conditions, yet this scheme usually trips a massive amount of load
inefficiently. Reinforcement learning (RL) has been adopted as a promising
approach to circumvent the issues; however, RL approach usually cannot
guarantee the safety of the systems under control. In this paper, we discuss a
couple of novel safe RL approaches, namely constrained optimization approach
and Barrier function-based approach, that can safely recover voltage under
emergency events. This method is general and can be applied to other
safety-critical control problems. Numerical simulations on the 39-bus IEEE
benchmark are performed to demonstrate the effectiveness of the proposed safe
RL emergency control.
",0
Towards Interactive Reinforcement Learning with Intrinsic Feedback,"Benjamin Poole, Minwoo Lee",2021-12-02T19:29:26Z,Reinforcement Learning,"  Reinforcement learning (RL) and brain-computer interfaces (BCI) have
experienced significant growth over the past decade. With rising interest in
human-in-the-loop (HITL), incorporating human input with RL algorithms has
given rise to the sub-field of interactive RL. Adjacently, the field of BCI has
long been interested in extracting informative brain signals from neural
activity for use in human-computer interactions. A key link between these
fields lies in the interpretation of neural activity as feedback such that
interactive RL approaches can be employed. We denote this new and emerging
medium of feedback as intrinsic feedback. Despite intrinsic feedback's ability
to be conveyed automatically and even unconsciously, proper exploration
surrounding this key link has largely gone unaddressed by both communities.
Thus, to help facilitate a deeper understanding and a more effective
utilization, we provide a tutorial-style review covering the motivations,
approaches, and open problems of intrinsic feedback and its foundational
concepts.
",0
Reinforcement Learning-Based Automatic Berthing System,Daesoo Lee,2021-12-03T12:34:50Z,Reinforcement Learning,"  Previous studies on automatic berthing systems based on artificial neural
network (ANN) showed great berthing performance by training the ANN with ship
berthing data as training data. However, because the ANN requires a large
amount of training data to yield robust performance, the ANN-based automatic
berthing system is somewhat limited due to the difficulty in obtaining the
berthing data. In this study, to overcome this difficulty, the automatic
berthing system based on one of the reinforcement learning (RL) algorithms,
proximal policy optimization (PPO), is proposed because the RL algorithms can
learn an optimal control policy through trial-and-error by interacting with a
given environment and does not require any pre-obtained training data, where
the control policy in the proposed PPO-based automatic berthing system controls
revolutions per second (RPS) and rudder angle of a ship. Finally, it is shown
that the proposed PPO-based automatic berthing system eliminates the need for
obtaining the training dataset and shows great potential for the actual
berthing application.
",0
Deep differentiable reinforcement learning and optimal trading,Thibault Jaisson,2021-12-06T11:35:53Z,Reinforcement Learning,"  In many reinforcement learning applications, the underlying environment
reward and transition functions are explicitly known differentiable functions.
This enables us to use recent research which applies machine learning tools to
stochastic control to find optimal action functions. In this paper, we define
differentiable reinforcement learning as a particular case of this research. We
find that incorporating deep learning in this framework leads to more accurate
and stable solutions than those obtained from more generic actor critic
algorithms. We apply this deep differentiable reinforcement learning (DDRL)
algorithm to the problem of one asset optimal trading strategies in various
environments where the market dynamics are known. Thanks to the stability of
this method, we are able to efficiently find optimal strategies for complex
multi-scale market models. We also extend these methods to simultaneously find
optimal action functions for a wide range of environment parameters. This makes
it applicable to real life financial signals and portfolio optimization where
the expected return has multiple time scales. In the case of a slow and a fast
alpha signal, we find that the optimal trading strategy consists in using the
fast signal to time the trades associated to the slow signal.
",4
Hierarchical Reinforcement Learning with Timed Subgoals,"Nico Gürtler, Dieter Büchler, Georg Martius",2021-12-06T15:11:19Z,Reinforcement Learning,"  Hierarchical reinforcement learning (HRL) holds great potential for
sample-efficient learning on challenging long-horizon tasks. In particular,
letting a higher level assign subgoals to a lower level has been shown to
enable fast learning on difficult problems. However, such subgoal-based methods
have been designed with static reinforcement learning environments in mind and
consequently struggle with dynamic elements beyond the immediate control of the
agent even though they are ubiquitous in real-world problems. In this paper, we
introduce Hierarchical reinforcement learning with Timed Subgoals (HiTS), an
HRL algorithm that enables the agent to adapt its timing to a dynamic
environment by not only specifying what goal state is to be reached but also
when. We discuss how communicating with a lower level in terms of such timed
subgoals results in a more stable learning problem for the higher level. Our
experiments on a range of standard benchmarks and three new challenging dynamic
reinforcement learning environments show that our method is capable of
sample-efficient learning where an existing state-of-the-art subgoal-based HRL
method fails to learn stable solutions.
",0
Recent Advances in Reinforcement Learning in Finance,"Ben Hambly, Renyuan Xu, Huining Yang",2021-12-08T19:55:26Z,Reinforcement Learning,"  The rapid changes in the finance industry due to the increasing amount of
data have revolutionized the techniques on data processing and data analysis
and brought new theoretical and computational challenges. In contrast to
classical stochastic control theory and other analytical approaches for solving
financial decision-making problems that heavily reply on model assumptions, new
developments from reinforcement learning (RL) are able to make full use of the
large amount of financial data with fewer model assumptions and to improve
decisions in complex financial environments. This survey paper aims to review
the recent developments and use of RL approaches in finance. We give an
introduction to Markov decision processes, which is the setting for many of the
commonly used RL approaches. Various algorithms are then introduced with a
focus on value and policy based methods that do not require any model
assumptions. Connections are made with neural networks to extend the framework
to encompass deep RL algorithms. Our survey concludes by discussing the
application of these RL algorithms in a variety of decision-making problems in
finance, including optimal execution, portfolio optimization, option pricing
and hedging, market making, smart order routing, and robo-advising.
",0
VMAgent: Scheduling Simulator for Reinforcement Learning,"Junjie Sheng, Shengliang Cai, Haochuan Cui, Wenhao Li, Yun Hua, Bo Jin, Wenli Zhou, Yiqiu Hu, Lei Zhu, Qian Peng, Hongyuan Zha, Xiangfeng Wang",2021-12-09T09:18:38Z,Reinforcement Learning,"  A novel simulator called VMAgent is introduced to help RL researchers better
explore new methods, especially for virtual machine scheduling. VMAgent is
inspired by practical virtual machine (VM) scheduling tasks and provides an
efficient simulation platform that can reflect the real situations of cloud
computing. Three scenarios (fading, recovering, and expansion) are concluded
from practical cloud computing and corresponds to many reinforcement learning
challenges (high dimensional state and action spaces, high non-stationarity,
and life-long demand). VMAgent provides flexible configurations for RL
researchers to design their customized scheduling environments considering
different problem features. From the VM scheduling perspective, VMAgent also
helps to explore better learning-based scheduling solutions.
",0
Reinforcement Learning with Almost Sure Constraints,"Agustin Castellano, Hancheng Min, Juan Bazerque, Enrique Mallada",2021-12-09T20:07:53Z,Reinforcement Learning,"  In this work we address the problem of finding feasible policies for
Constrained Markov Decision Processes under probability one constraints. We
argue that stationary policies are not sufficient for solving this problem, and
that a rich class of policies can be found by endowing the controller with a
scalar quantity, so called budget, that tracks how close the agent is to
violating the constraint. We show that the minimal budget required to act
safely can be obtained as the smallest fixed point of a Bellman-like operator,
for which we analyze its convergence properties. We also show how to learn this
quantity when the true kernel of the Markov decision process is not known,
while providing sample-complexity bounds. The utility of knowing this minimal
budget relies in that it can aid in the search of optimal or near-optimal
policies by shrinking down the region of the state space the agent must
navigate. Simulations illustrate the different nature of probability one
constraints against the typically used constraints in expectation.
",0
Quantum Architecture Search via Continual Reinforcement Learning,"Esther Ye, Samuel Yen-Chi Chen",2021-12-10T19:07:56Z,Reinforcement Learning,"  Quantum computing has promised significant improvement in solving difficult
computational tasks over classical computers. Designing quantum circuits for
practical use, however, is not a trivial objective and requires expert-level
knowledge. To aid this endeavor, this paper proposes a machine learning-based
method to construct quantum circuit architectures. Previous works have
demonstrated that classical deep reinforcement learning (DRL) algorithms can
successfully construct quantum circuit architectures without encoded physics
knowledge. However, these DRL-based works are not generalizable to settings
with changing device noises, thus requiring considerable amounts of training
resources to keep the RL models up-to-date. With this in mind, we incorporated
continual learning to enhance the performance of our algorithm. In this paper,
we present the Probabilistic Policy Reuse with deep Q-learning (PPR-DQL)
framework to tackle this circuit design challenge. By conducting numerical
simulations over various noise patterns, we demonstrate that the RL agent with
PPR was able to find the quantum gate sequence to generate the two-qubit Bell
state faster than the agent that was trained from scratch. The proposed
framework is general and can be applied to other quantum gate synthesis or
control problems -- including the automatic calibration of quantum devices.
",0
Edge-Compatible Reinforcement Learning for Recommendations,"James E. Kostas, Philip S. Thomas, Georgios Theocharous",2021-12-10T20:12:39Z,Reinforcement Learning,"  Most reinforcement learning (RL) recommendation systems designed for edge
computing must either synchronize during recommendation selection or depend on
an unprincipled patchwork collection of algorithms. In this work, we build on
asynchronous coagent policy gradient algorithms \citep{kostas2020asynchronous}
to propose a principled solution to this problem. The class of algorithms that
we propose can be distributed over the internet and run asynchronously and in
real-time. When a given edge fails to respond to a request for data with
sufficient speed, this is not a problem; the algorithm is designed to function
and learn in the edge setting, and network issues are part of this setting. The
result is a principled, theoretically grounded RL algorithm designed to be
distributed in and learn in this asynchronous environment. In this work, we
describe this algorithm and a proposed class of architectures in detail, and
demonstrate that they work well in practice in the asynchronous setting, even
as the network quality degrades.
",0
Federated Reinforcement Learning at the Edge,Konstantinos Gatsis,2021-12-11T03:28:59Z,Reinforcement Learning,"  Modern cyber-physical architectures use data collected from systems at
different physical locations to learn appropriate behaviors and adapt to
uncertain environments. However, an important challenge arises as communication
exchanges at the edge of networked systems are costly due to limited resources.
This paper considers a setup where multiple agents need to communicate
efficiently in order to jointly solve a reinforcement learning problem over
time-series data collected in a distributed manner. This is posed as learning
an approximate value function over a communication network. An algorithm for
achieving communication efficiency is proposed, supported with theoretical
guarantees, practical implementations, and numerical evaluations. The approach
is based on the idea of communicating only when sufficiently informative data
is collected.
",0
NEORL: NeuroEvolution Optimization with Reinforcement Learning,"Majdi I. Radaideh, Katelin Du, Paul Seurin, Devin Seyler, Xubo Gu, Haijia Wang, Koroush Shirvan",2021-12-01T17:55:45Z,Reinforcement Learning,"  We present an open-source Python framework for NeuroEvolution Optimization
with Reinforcement Learning (NEORL) developed at the Massachusetts Institute of
Technology. NEORL offers a global optimization interface of state-of-the-art
algorithms in the field of evolutionary computation, neural networks through
reinforcement learning, and hybrid neuroevolution algorithms. NEORL features
diverse set of algorithms, user-friendly interface, parallel computing support,
automatic hyperparameter tuning, detailed documentation, and demonstration of
applications in mathematical and real-world engineering optimization. NEORL
encompasses various optimization problems from combinatorial, continuous, mixed
discrete/continuous, to high-dimensional, expensive, and constrained
engineering optimization. NEORL is tested in variety of engineering
applications relevant to low carbon energy research in addressing solutions to
climate change. The examples include nuclear reactor control and fuel cell
power production. The results demonstrate NEORL competitiveness against other
algorithms and optimization frameworks in the literature, and a potential tool
to solve large-scale optimization problems. More examples and benchmarking of
NEORL can be found here: https://neorl.readthedocs.io/en/latest/index.html
",3
"Conjugated Discrete Distributions for Distributional Reinforcement
  Learning","Björn Lindenberg, Jonas Nordqvist, Karl-Olof Lindahl",2021-12-14T14:14:49Z,Other,"  In this work we continue to build upon recent advances in reinforcement
learning for finite Markov processes. A common approach among previous existing
algorithms, both single-actor and distributed, is to either clip rewards or to
apply a transformation method on Q-functions to handle a large variety of
magnitudes in real discounted returns. We theoretically show that one of the
most successful methods may not yield an optimal policy if we have a
non-deterministic process. As a solution, we argue that distributional
reinforcement learning lends itself to remedy this situation completely. By the
introduction of a conjugated distributional operator we may handle a large
class of transformations for real returns with guaranteed theoretical
convergence. We propose an approximating single-actor algorithm based on this
operator that trains agents directly on unaltered rewards using a proper
distributional metric given by the Cram\'er distance. To evaluate its
performance in a stochastic setting we train agents on a suite of 55 Atari 2600
games using sticky-actions and obtain state-of-the-art performance compared to
other well-known algorithms in the Dopamine framework.
",0
Representation and Invariance in Reinforcement Learning,"Samuel Alexander, Arthur Paul Pedersen",2021-12-14T21:33:12Z,Reinforcement Learning,"  Researchers have formalized reinforcement learning (RL) in different ways. If
an agent in one RL framework is to run within another RL framework's
environments, the agent must first be converted, or mapped, into that other
framework. Whether or not this is possible depends on not only the RL
frameworks in question and but also how intelligence itself is measured. In
this paper, we lay foundations for studying relative-intelligence-preserving
mappability between RL frameworks. We define two types of mappings, called weak
and strong translations, between RL frameworks and prove that existence of
these mappings enables two types of intelligence comparison according to the
mappings preserving relative intelligence. We investigate the existence or lack
thereof of these mappings between: (i) RL frameworks where agents go first and
RL frameworks where environments go first; and (ii) twelve different RL
frameworks differing in terms of whether or not agents or environments are
required to be deterministic. In the former case, we consider various natural
mappings between agent-first and environment-first RL and vice versa; we show
some positive results (some such mappings are strong or weak translations) and
some negative results (some such mappings are not). In the latter case, we
completely characterize which of the twelve RL-framework pairs admit weak
translations, under the assumption of integer-valued rewards and some
additional mild assumptions.
",0
Unsupervised Reinforcement Learning in Multiple Environments,"Mirco Mutti, Mattia Mancassola, Marcello Restelli",2021-12-16T09:54:37Z,Reinforcement Learning,"  Several recent works have been dedicated to unsupervised reinforcement
learning in a single environment, in which a policy is first pre-trained with
unsupervised interactions, and then fine-tuned towards the optimal policy for
several downstream supervised tasks defined over the same environment. Along
this line, we address the problem of unsupervised reinforcement learning in a
class of multiple environments, in which the policy is pre-trained with
interactions from the whole class, and then fine-tuned for several tasks in any
environment of the class. Notably, the problem is inherently multi-objective as
we can trade off the pre-training objective between environments in many ways.
In this work, we foster an exploration strategy that is sensitive to the most
adverse cases within the class. Hence, we cast the exploration problem as the
maximization of the mean of a critical percentile of the state visitation
entropy induced by the exploration strategy over the class of environments.
Then, we present a policy gradient algorithm, $\alpha$MEPOL, to optimize the
introduced objective through mediated interactions with the class. Finally, we
empirically demonstrate the ability of the algorithm in learning to explore
challenging classes of continuous environments and we show that reinforcement
learning greatly benefits from the pre-trained exploration strategy w.r.t.
learning from scratch.
",0
Inherently Explainable Reinforcement Learning in Natural Language,"Xiangyu Peng, Mark O. Riedl, Prithviraj Ammanabrolu",2021-12-16T14:24:35Z,Reinforcement Learning,"  We focus on the task of creating a reinforcement learning agent that is
inherently explainable -- with the ability to produce immediate local
explanations by thinking out loud while performing a task and analyzing entire
trajectories post-hoc to produce causal explanations. This Hierarchically
Explainable Reinforcement Learning agent (HEX-RL), operates in Interactive
Fictions, text-based game environments in which an agent perceives and acts
upon the world using textual natural language. These games are usually
structured as puzzles or quests with long-term dependencies in which an agent
must complete a sequence of actions to succeed -- providing ideal environments
in which to test an agent's ability to explain its actions. Our agent is
designed to treat explainability as a first-class citizen, using an extracted
symbolic knowledge graph-based state representation coupled with a Hierarchical
Graph Attention mechanism that points to the facts in the internal graph
representation that most influenced the choice of actions. Experiments show
that this agent provides significantly improved explanations over strong
baselines, as rated by human participants generally unfamiliar with the
environment, while also matching state-of-the-art task performance.
",0
Autonomous Reinforcement Learning: Formalism and Benchmarking,"Archit Sharma, Kelvin Xu, Nikhil Sardana, Abhishek Gupta, Karol Hausman, Sergey Levine, Chelsea Finn",2021-12-17T16:28:06Z,Reinforcement Learning,"  Reinforcement learning (RL) provides a naturalistic framing for learning
through trial and error, which is appealing both because of its simplicity and
effectiveness and because of its resemblance to how humans and animals acquire
skills through experience. However, real-world embodied learning, such as that
performed by humans and animals, is situated in a continual, non-episodic
world, whereas common benchmark tasks in RL are episodic, with the environment
resetting between trials to provide the agent with multiple attempts. This
discrepancy presents a major challenge when attempting to take RL algorithms
developed for episodic simulated environments and run them on real-world
platforms, such as robots. In this paper, we aim to address this discrepancy by
laying out a framework for Autonomous Reinforcement Learning (ARL):
reinforcement learning where the agent not only learns through its own
experience, but also contends with lack of human supervision to reset between
trials. We introduce a simulated benchmark EARL around this framework,
containing a set of diverse and challenging simulated tasks reflective of the
hurdles introduced to learning when only a minimal reliance on extrinsic
intervention can be assumed. We show that standard approaches to episodic RL
and existing approaches struggle as interventions are minimized, underscoring
the need for developing new algorithms for reinforcement learning with a
greater focus on autonomy.
",0
Direct Behavior Specification via Constrained Reinforcement Learning,"Julien Roy, Roger Girgis, Joshua Romoff, Pierre-Luc Bacon, Christopher Pal",2021-12-22T21:12:28Z,Reinforcement Learning,"  The standard formulation of Reinforcement Learning lacks a practical way of
specifying what are admissible and forbidden behaviors. Most often,
practitioners go about the task of behavior specification by manually
engineering the reward function, a counter-intuitive process that requires
several iterations and is prone to reward hacking by the agent. In this work,
we argue that constrained RL, which has almost exclusively been used for safe
RL, also has the potential to significantly reduce the amount of work spent for
reward specification in applied RL projects. To this end, we propose to specify
behavioral preferences in the CMDP framework and to use Lagrangian methods to
automatically weigh each of these behavioral constraints. Specifically, we
investigate how CMDPs can be adapted to solve goal-based tasks while adhering
to several constraints simultaneously. We evaluate this framework on a set of
continuous control tasks relevant to the application of Reinforcement Learning
for NPC design in video games.
",0
Newsvendor Model with Deep Reinforcement Learning,Dylan K. Goetting,2021-12-22T05:52:44Z,Reinforcement Learning,"  I present a deep reinforcement learning (RL) solution to the mathematical
problem known as the Newsvendor model, which seeks to optimize profit given a
probabilistic demand distribution. To reflect a more realistic and complex
situation, the demand distribution can change for different days of the week,
thus changing the optimum behavior. I used a Twin-Delayed Deep Deterministic
Policy Gradient agent (written as completely original code) with both an actor
and critic network to solve this problem. The agent was able to learn optimal
behavior consistent with the analytical solution of the problem, and could
identify separate probability distributions for different days of the week and
behave accordingly.
",0
Rediscovering Affordance: A Reinforcement Learning Perspective,"Yi-Chi Liao, Kashyap Todi, Aditya Acharya, Antti Keurulainen, Andrew Howes, Antti Oulasvirta",2021-12-24T00:25:03Z,Reinforcement Learning,"  Affordance refers to the perception of possible actions allowed by an object.
Despite its relevance to human-computer interaction, no existing theory
explains the mechanisms that underpin affordance-formation; that is, how
affordances are discovered and adapted via interaction. We propose an
integrative theory of affordance-formation based on the theory of reinforcement
learning in cognitive sciences. The key assumption is that users learn to
associate promising motor actions to percepts via experience when reinforcement
signals (success/failure) are present. They also learn to categorize actions
(e.g., ""rotating"" a dial), giving them the ability to name and reason about
affordance. Upon encountering novel widgets, their ability to generalize these
actions determines their ability to perceive affordances. We implement this
theory in a virtual robot model, which demonstrates human-like adaptation of
affordance in interactive widgets tasks. While its predictions align with
trends in human data, humans are able to adapt affordances faster, suggesting
the existence of additional mechanisms.
",0
A Survey on Interpretable Reinforcement Learning,"Claire Glanois, Paul Weng, Matthieu Zimmer, Dong Li, Tianpei Yang, Jianye Hao, Wulong Liu",2021-12-24T17:26:57Z,Reinforcement Learning,"  Although deep reinforcement learning has become a promising machine learning
approach for sequential decision-making problems, it is still not mature enough
for high-stake domains such as autonomous driving or medical applications. In
such contexts, a learned policy needs for instance to be interpretable, so that
it can be inspected before any deployment (e.g., for safety and verifiability
reasons). This survey provides an overview of various approaches to achieve
higher interpretability in reinforcement learning (RL). To that aim, we
distinguish interpretability (as a property of a model) and explainability (as
a post-hoc operation, with the intervention of a proxy) and discuss them in the
context of RL with an emphasis on the former notion. In particular, we argue
that interpretable RL may embrace different facets: interpretable inputs,
interpretable (transition/reward) models, and interpretable decision-making.
Based on this scheme, we summarize and analyze recent work related to
interpretable RL with an emphasis on papers published in the past 10 years. We
also discuss briefly some related research areas and point to some potential
promising research directions.
",0
Reinforcement Learning with Dynamic Convex Risk Measures,"Anthony Coache, Sebastian Jaimungal",2021-12-26T16:41:05Z,Reinforcement Learning,"  We develop an approach for solving time-consistent risk-sensitive stochastic
optimization problems using model-free reinforcement learning (RL).
Specifically, we assume agents assess the risk of a sequence of random
variables using dynamic convex risk measures. We employ a time-consistent
dynamic programming principle to determine the value of a particular policy,
and develop policy gradient update rules that aid in obtaining optimal
policies. We further develop an actor-critic style algorithm using neural
networks to optimize over policies. Finally, we demonstrate the performance and
flexibility of our approach by applying it to three optimization problems:
statistical arbitrage trading strategies, financial hedging, and obstacle
avoidance robot control.
",0
Alpha-Mini: Minichess Agent with Deep Reinforcement Learning,"Michael Sun, Robert Tan",2021-12-22T19:16:17Z,Reinforcement Learning,"  We train an agent to compete in the game of Gardner minichess, a downsized
variation of chess played on a 5x5 board. We motivated and applied a SOTA
actor-critic method Proximal Policy Optimization with Generalized Advantage
Estimation. Our initial task centered around training the agent against a
random agent. Once we obtained reasonable performance, we then adopted a
version of iterative policy improvement adopted by AlphaGo to pit the agent
against increasingly stronger versions of itself, and evaluate the resulting
performance gain. The final agent achieves a near (.97) perfect win rate
against a random agent. We also explore the effects of pretraining the network
using a collection of positions obtained via self-play.
",0
Single-Shot Pruning for Offline Reinforcement Learning,"Samin Yeasar Arnob, Riyasat Ohib, Sergey Plis, Doina Precup",2021-12-31T18:10:02Z,Reinforcement Learning,"  Deep Reinforcement Learning (RL) is a powerful framework for solving complex
real-world problems. Large neural networks employed in the framework are
traditionally associated with better generalization capabilities, but their
increased size entails the drawbacks of extensive training duration,
substantial hardware resources, and longer inference times. One way to tackle
this problem is to prune neural networks leaving only the necessary parameters.
State-of-the-art concurrent pruning techniques for imposing sparsity perform
demonstrably well in applications where data distributions are fixed. However,
they have not yet been substantially explored in the context of RL. We close
the gap between RL and single-shot pruning techniques and present a general
pruning approach to the Offline RL. We leverage a fixed dataset to prune neural
networks before the start of RL training. We then run experiments varying the
network sparsity level and evaluating the validity of pruning at initialization
techniques in continuous control tasks. Our results show that with 95% of the
network weights pruned, Offline-RL algorithms can still retain performance in
the majority of our experiments. To the best of our knowledge, no prior work
utilizing pruning in RL retained performance at such high levels of sparsity.
  Moreover, pruning at initialization techniques can be easily integrated into
any existing Offline-RL algorithms without changing the learning objective.
",19
Offline Reinforcement Learning for Road Traffic Control,"Mayuresh Kunjir, Sanjay Chawla",2022-01-07T09:55:21Z,Reinforcement Learning,"  Traffic signal control is an important problem in urban mobility with a
significant potential of economic and environmental impact. While there is a
growing interest in Reinforcement Learning (RL) for traffic signal control, the
work so far has focussed on learning through simulations which could lead to
inaccuracies due to simplifying assumptions. Instead, real experience data on
traffic is available and could be exploited at minimal costs. Recent progress
in offline or batch RL has enabled just that. Model-based offline RL methods,
in particular, have been shown to generalize from the experience data much
better than others.
  We build a model-based learning framework which infers a Markov Decision
Process (MDP) from a dataset collected using a cyclic traffic signal control
policy that is both commonplace and easy to gather. The MDP is built with
pessimistic costs to manage out-of-distribution scenarios using an adaptive
shaping of rewards which is shown to provide better regularization compared to
the prior related work in addition to being PAC-optimal. Our model is evaluated
on a complex signalized roundabout showing that it is possible to build highly
performant traffic control policies in a data efficient manner.
",0
Verified Probabilistic Policies for Deep Reinforcement Learning,"Edoardo Bacci, David Parker",2022-01-10T23:55:04Z,Reinforcement Learning,"  Deep reinforcement learning is an increasingly popular technique for
synthesising policies to control an agent's interaction with its environment.
There is also growing interest in formally verifying that such policies are
correct and execute safely. Progress has been made in this area by building on
existing work for verification of deep neural networks and of continuous-state
dynamical systems. In this paper, we tackle the problem of verifying
probabilistic policies for deep reinforcement learning, which are used to, for
example, tackle adversarial environments, break symmetries and manage
trade-offs. We propose an abstraction approach, based on interval Markov
decision processes, that yields probabilistic guarantees on a policy's
execution, and present techniques to build and solve these models using
abstract interpretation, mixed-integer linear programming, entropy-based
refinement and probabilistic model checking. We implement our approach and
illustrate its effectiveness on a selection of reinforcement learning
benchmarks.
",4
The Recurrent Reinforcement Learning Crypto Agent,"Gabriel Borrageiro, Nick Firoozye, Paolo Barucca",2022-01-12T21:00:43Z,Reinforcement Learning,"  We demonstrate a novel application of online transfer learning for a digital
assets trading agent. This agent uses a powerful feature space representation
in the form of an echo state network, the output of which is made available to
a direct, recurrent reinforcement learning agent. The agent learns to trade the
XBTUSD (Bitcoin versus US Dollars) perpetual swap derivatives contract on
BitMEX on an intraday basis. By learning from the multiple sources of impact on
the quadratic risk-adjusted utility that it seeks to maximise, the agent avoids
excessive over-trading, captures a funding profit, and can predict the market's
direction. Overall, our crypto agent realises a total return of 350\%, net of
transaction costs, over roughly five years, 71\% of which is down to funding
profit. The annualised information ratio that it achieves is 1.46.
",0
Reinforcement Learning based Air Combat Maneuver Generation,"Muhammed Murat Ozbek, Emre Koyuncu",2022-01-14T15:55:44Z,Reinforcement Learning,"  The advent of artificial intelligence technology paved the way of many
researches to be made within air combat sector. Academicians and many other
researchers did a research on a prominent research direction called autonomous
maneuver decision of UAV. Elaborative researches produced some outcomes, but
decisions that include Reinforcement Learning(RL) came out to be more
efficient. There have been many researches and experiments done to make an
agent reach its target in an optimal way, most prominent are Genetic
Algorithm(GA) , A star, RRT and other various optimization techniques have been
used. But Reinforcement Learning is the well known one for its success. In
DARPHA Alpha Dogfight Trials, reinforcement learning prevailed against a real
veteran F16 human pilot who was trained by Boeing. This successor model was
developed by Heron Systems. After this accomplishment, reinforcement learning
bring tremendous attention on itself. In this research we aimed our UAV which
has a dubin vehicle dynamic property to move to the target in two dimensional
space in an optimal path using Twin Delayed Deep Deterministic Policy Gradients
(TD3) and used in experience replay Hindsight Experience Replay(HER).We did
tests on two different environments and used simulations.
",0
Demystifying Reinforcement Learning in Time-Varying Systems,"Pouya Hamadanian, Malte Schwarzkopf, Siddartha Sen, Mohammad Alizadeh",2022-01-14T17:04:11Z,Reinforcement Learning,"  Recent research has turned to Reinforcement Learning (RL) to solve
challenging decision problems, as an alternative to hand-tuned heuristics. RL
can learn good policies without the need for modeling the environment's
dynamics. Despite this promise, RL remains an impractical solution for many
real-world systems problems. A particularly challenging case occurs when the
environment changes over time, i.e. it exhibits non-stationarity. In this work,
we characterize the challenges introduced by non-stationarity, shed light on
the range of approaches to them and develop a robust framework for addressing
them to train RL agents in live systems. Such agents must explore and learn new
environments, without hurting the system's performance, and remember them over
time. To this end, our framework (i) identifies different environments
encountered by the live system, (ii) triggers exploration when necessary, (iii)
takes precautions to retain knowledge from prior environments, and (iv) employs
safeguards to protect the system's performance when the RL agent makes
mistakes. We apply our framework to two systems problems, straggler mitigation
and adaptive video streaming, and evaluate it against a variety of alternative
approaches using real-world and synthetic data. We show that all components of
the framework are necessary to cope with non-stationarity and provide guidance
on alternative design choices for each component.
",0
"Conservative Distributional Reinforcement Learning with Safety
  Constraints","Hengrui Zhang, Youfang Lin, Sheng Han, Shuo Wang, Kai Lv",2022-01-18T19:45:43Z,Reinforcement Learning,"  Safety exploration can be regarded as a constrained Markov decision problem
where the expected long-term cost is constrained. Previous off-policy
algorithms convert the constrained optimization problem into the corresponding
unconstrained dual problem by introducing the Lagrangian relaxation technique.
However, the cost function of the above algorithms provides inaccurate
estimations and causes the instability of the Lagrange multiplier learning. In
this paper, we present a novel off-policy reinforcement learning algorithm
called Conservative Distributional Maximum a Posteriori Policy Optimization
(CDMPO). At first, to accurately judge whether the current situation satisfies
the constraints, CDMPO adapts distributional reinforcement learning method to
estimate the Q-function and C-function. Then, CDMPO uses a conservative value
function loss to reduce the number of violations of constraints during the
exploration process. In addition, we utilize Weighted Average Proportional
Integral Derivative (WAPID) to update the Lagrange multiplier stably. Empirical
results show that the proposed method has fewer violations of constraints in
the early exploration process. The final test results also illustrate that our
method has better risk control.
",0
Goal-Conditioned Reinforcement Learning: Problems and Solutions,"Minghuan Liu, Menghui Zhu, Weinan Zhang",2022-01-20T17:06:42Z,Reinforcement Learning,"  Goal-conditioned reinforcement learning (GCRL), related to a set of complex
RL problems, trains an agent to achieve different goals under particular
scenarios. Compared to the standard RL solutions that learn a policy solely
depending on the states or observations, GCRL additionally requires the agent
to make decisions according to different goals. In this survey, we provide a
comprehensive overview of the challenges and algorithms for GCRL. Firstly, we
answer what the basic problems are studied in this field. Then, we explain how
goals are represented and present how existing solutions are designed from
different points of view. Finally, we make the conclusion and discuss potential
future prospects that recent researches focus on.
",0
Dynamic Channel Access via Meta-Reinforcement Learning,"Ziyang Lu, M. Cenk Gursoy",2021-12-24T15:04:43Z,Reinforcement Learning,"  In this paper, we address the channel access problem in a dynamic wireless
environment via meta-reinforcement learning. Spectrum is a scarce resource in
wireless communications, especially with the dramatic increase in the number of
devices in networks. Recently, inspired by the success of deep reinforcement
learning (DRL), extensive studies have been conducted in addressing wireless
resource allocation problems via DRL. However, training DRL algorithms usually
requires a massive amount of data collected from the environment for each
specific task and the well-trained model may fail if there is a small variation
in the environment. In this work, in order to address these challenges, we
propose a meta-DRL framework that incorporates the method of Model-Agnostic
Meta-Learning (MAML). In the proposed framework, we train a common
initialization for similar channel selection tasks. From the initialization, we
show that only a few gradient descents are required for adapting to different
tasks drawn from the same distribution. We demonstrate the performance
improvements via simulation results.
",0
Pearl: Parallel Evolutionary and Reinforcement Learning Library,"Rohan Tangri, Danilo P. Mandic, Anthony G. Constantinides",2022-01-24T10:22:30Z,Reinforcement Learning,"  Reinforcement learning is increasingly finding success across domains where
the problem can be represented as a Markov decision process. Evolutionary
computation algorithms have also proven successful in this domain, exhibiting
similar performance to the generally more complex reinforcement learning.
Whilst there exist many open-source reinforcement learning and evolutionary
computation libraries, no publicly available library combines the two
approaches for enhanced comparison, cooperation, or visualization. To this end,
we have created Pearl (https://github.com/LondonNode/Pearl), an open source
Python library designed to allow researchers to rapidly and conveniently
perform optimized reinforcement learning, evolutionary computation and
combinations of the two. The key features within Pearl include: modular and
expandable components, opinionated module settings, Tensorboard integration,
custom callbacks and comprehensive visualizations.
",0
Deep Reinforcement Learning with Spiking Q-learning,"Ding Chen, Peixi Peng, Tiejun Huang, Yonghong Tian",2022-01-21T16:42:11Z,Reinforcement Learning,"  With the help of special neuromorphic hardware, spiking neural networks
(SNNs) are expected to realize artificial intelligence (AI) with less energy
consumption. It provides a promising energy-efficient way for realistic control
tasks by combining SNNs with deep reinforcement learning (RL). There are only a
few existing SNN-based RL methods at present. Most of them either lack
generalization ability or employ Artificial Neural Networks (ANNs) to estimate
value function in training. The former needs to tune numerous hyper-parameters
for each scenario, and the latter limits the application of different types of
RL algorithm and ignores the large energy consumption in training. To develop a
robust spike-based RL method, we draw inspiration from non-spiking interneurons
found in insects and propose the deep spiking Q-network (DSQN), using the
membrane voltage of non-spiking neurons as the representation of Q-value, which
can directly learn robust policies from high-dimensional sensory inputs using
end-to-end RL. Experiments conducted on 17 Atari games demonstrate the DSQN is
effective and even outperforms the ANN-based deep Q-network (DQN) in most
games. Moreover, the experiments show superior learning stability and
robustness to adversarial attacks of DSQN.
",0
Hyperparameter Tuning for Deep Reinforcement Learning Applications,"Mariam Kiran, Melis Ozyildirim",2022-01-26T20:43:13Z,Reinforcement Learning,"  Reinforcement learning (RL) applications, where an agent can simply learn
optimal behaviors by interacting with the environment, are quickly gaining
tremendous success in a wide variety of applications from controlling simple
pendulums to complex data centers. However, setting the right hyperparameters
can have a huge impact on the deployed solution performance and reliability in
the inference models, produced via RL, used for decision-making. Hyperparameter
search itself is a laborious process that requires many iterations and
computationally expensive to find the best settings that produce the best
neural network architectures. In comparison to other neural network
architectures, deep RL has not witnessed much hyperparameter tuning, due to its
algorithm complexity and simulation platforms needed. In this paper, we propose
a distributed variable-length genetic algorithm framework to systematically
tune hyperparameters for various RL applications, improving training time and
robustness of the architecture, via evolution. We demonstrate the scalability
of our approach on many RL problems (from simple gyms to complex applications)
and compared with Bayesian approach. Our results show that with more
generations, optimal solutions that require fewer training episodes and are
computationally cheap while being more robust for deployment. Our results are
imperative to advance deep reinforcement learning controllers for real-world
problems.
",0
Excavation Reinforcement Learning Using Geometric Representation,"Qingkai Lu, Yifan Zhu, Liangjun Zhang",2022-01-27T02:59:56Z,Reinforcement Learning,"  Excavation of irregular rigid objects in clutter, such as fragmented rocks
and wood blocks, is very challenging due to their complex interaction dynamics
and highly variable geometries. In this paper, we adopt reinforcement learning
(RL) to tackle this challenge and learn policies to plan for a sequence of
excavation trajectories for irregular rigid objects, given point clouds of
excavation scenes. Moreover, we separately learn a compact representation of
the point cloud on geometric tasks that do not require human labeling. We show
that using the representation reduces training time for RL, while achieving
similar asymptotic performance compare to an end-to-end RL algorithm. When
using a policy trained in simulation directly on a real scene, we show that the
policy trained with the representation outperforms end-to-end RL. To our best
knowledge, this paper presents the first application of RL to plan a sequence
of excavation trajectories of irregular rigid objects in clutter.
",0
Quantile-Based Policy Optimization for Reinforcement Learning,"Jinyang Jiang, Jiaqiao Hu, Yijie Peng",2022-01-27T12:01:36Z,Reinforcement Learning,"  Classical reinforcement learning (RL) aims to optimize the expected
cumulative rewards. In this work, we consider the RL setting where the goal is
to optimize the quantile of the cumulative rewards. We parameterize the policy
controlling actions by neural networks and propose a novel policy gradient
algorithm called Quantile-Based Policy Optimization (QPO) and its variant
Quantile-Based Proximal Policy Optimization (QPPO) to solve deep RL problems
with quantile objectives. QPO uses two coupled iterations running at different
time scales for simultaneously estimating quantiles and policy parameters and
is shown to converge to the global optimal policy under certain conditions. Our
numerical results demonstrate that the proposed algorithms outperform the
existing baseline algorithms under the quantile criterion.
",0
Generative Adversarial Exploration for Reinforcement Learning,"Weijun Hong, Menghui Zhu, Minghuan Liu, Weinan Zhang, Ming Zhou, Yong Yu, Peng Sun",2022-01-27T17:34:47Z,Reinforcement Learning,"  Exploration is crucial for training the optimal reinforcement learning (RL)
policy, where the key is to discriminate whether a state visiting is novel.
Most previous work focuses on designing heuristic rules or distance metrics to
check whether a state is novel without considering such a discrimination
process that can be learned. In this paper, we propose a novel method called
generative adversarial exploration (GAEX) to encourage exploration in RL via
introducing an intrinsic reward output from a generative adversarial network,
where the generator provides fake samples of states that help discriminator
identify those less frequently visited states. Thus the agent is encouraged to
visit those states which the discriminator is less confident to judge as
visited. GAEX is easy to implement and of high training efficiency. In our
experiments, we apply GAEX into DQN and the DQN-GAEX algorithm achieves
convincing performance on challenging exploration problems, including the game
Venture, Montezuma's Revenge and Super Mario Bros, without further fine-tuning
on complicate learning algorithms. To our knowledge, this is the first work to
employ GAN in RL exploration problems.
",0
Dynamic Temporal Reconciliation by Reinforcement learning,"Himanshi Charotia, Abhishek Garg, Gaurav Dhama, Naman Maheshwari",2022-01-28T07:15:23Z,Reinforcement Learning,"  Planning based on long and short term time series forecasts is a common
practice across many industries. In this context, temporal aggregation and
reconciliation techniques have been useful in improving forecasts, reducing
model uncertainty, and providing a coherent forecast across different time
horizons. However, an underlying assumption spanning all these techniques is
the complete availability of data across all levels of the temporal hierarchy,
while this offers mathematical convenience but most of the time low frequency
data is partially completed and it is not available while forecasting. On the
other hand, high frequency data can significantly change in a scenario like the
COVID pandemic and this change can be used to improve forecasts that will
otherwise significantly diverge from long term actuals. We propose a dynamic
reconciliation method whereby we formulate the problem of informing low
frequency forecasts based on high frequency actuals as a Markov Decision
Process (MDP) allowing for the fact that we do not have complete information
about the dynamics of the process. This allows us to have the best long term
estimates based on the most recent data available even if the low frequency
cycles have only been partially completed. The MDP has been solved using a Time
Differenced Reinforcement learning (TDRL) approach with customizable actions
and improves the long terms forecasts dramatically as compared to relying
solely on historical low frequency data. The result also underscores the fact
that while low frequency forecasts can improve the high frequency forecasts as
mentioned in the temporal reconciliation literature (based on the assumption
that low frequency forecasts have lower noise to signal ratio) the high
frequency forecasts can also be used to inform the low frequency forecasts.
",0
Mask-based Latent Reconstruction for Reinforcement Learning,"Tao Yu, Zhizheng Zhang, Cuiling Lan, Yan Lu, Zhibo Chen",2022-01-28T13:07:11Z,Reinforcement Learning,"  For deep reinforcement learning (RL) from pixels, learning effective state
representations is crucial for achieving high performance. However, in
practice, limited experience and high-dimensional inputs prevent effective
representation learning. To address this, motivated by the success of
mask-based modeling in other research fields, we introduce mask-based
reconstruction to promote state representation learning in RL. Specifically, we
propose a simple yet effective self-supervised method, Mask-based Latent
Reconstruction (MLR), to predict complete state representations in the latent
space from the observations with spatially and temporally masked pixels. MLR
enables better use of context information when learning state representations
to make them more informative, which facilitates the training of RL agents.
Extensive experiments show that our MLR significantly improves the sample
efficiency in RL and outperforms the state-of-the-art sample-efficient RL
methods on multiple continuous and discrete control benchmarks. Our code is
available at https://github.com/microsoft/Mask-based-Latent-Reconstruction.
",35
Can Wikipedia Help Offline Reinforcement Learning?,"Machel Reid, Yutaro Yamada, Shixiang Shane Gu",2022-01-28T13:55:35Z,Reinforcement Learning,"  Fine-tuning reinforcement learning (RL) models has been challenging because
of a lack of large scale off-the-shelf datasets as well as high variance in
transferability among different environments. Recent work has looked at
tackling offline RL from the perspective of sequence modeling with improved
results as result of the introduction of the Transformer architecture. However,
when the model is trained from scratch, it suffers from slow convergence
speeds. In this paper, we look to take advantage of this formulation of
reinforcement learning as sequence modeling and investigate the transferability
of pre-trained sequence models on other domains (vision, language) when
finetuned on offline RL tasks (control, games). To this end, we also propose
techniques to improve transfer between these domains. Results show consistent
performance gains in terms of both convergence speed and reward on a variety of
environments, accelerating training by 3-6x and achieving state-of-the-art
performance in a variety of tasks using Wikipedia-pretrained and GPT2 language
models. We hope that this work not only brings light to the potentials of
leveraging generic sequence modeling techniques and pre-trained models for RL,
but also inspires future work on sharing knowledge between generative modeling
tasks of completely different domains.
",88
"Explaining Reinforcement Learning Policies through Counterfactual
  Trajectories","Julius Frost, Olivia Watkins, Eric Weiner, Pieter Abbeel, Trevor Darrell, Bryan Plummer, Kate Saenko",2022-01-29T00:52:37Z,Reinforcement Learning,"  In order for humans to confidently decide where to employ RL agents for
real-world tasks, a human developer must validate that the agent will perform
well at test-time. Some policy interpretability methods facilitate this by
capturing the policy's decision making in a set of agent rollouts. However,
even the most informative trajectories of training time behavior may give
little insight into the agent's behavior out of distribution. In contrast, our
method conveys how the agent performs under distribution shifts by showing the
agent's behavior across a wider trajectory distribution. We generate these
trajectories by guiding the agent to more diverse unseen states and showing the
agent's behavior there. In a user study, we demonstrate that our method enables
users to score better than baseline methods on one of two agent validation
tasks.
",0
Sequential Search with Off-Policy Reinforcement Learning,"Dadong Miao, Yanan Wang, Guoyu Tang, Lin Liu, Sulong Xu, Bo Long, Yun Xiao, Lingfei Wu, Yunjiang Jiang",2022-02-01T06:52:40Z,Reinforcement Learning,"  Recent years have seen a significant amount of interests in Sequential
Recommendation (SR), which aims to understand and model the sequential user
behaviors and the interactions between users and items over time. Surprisingly,
despite the huge success Sequential Recommendation has achieved, there is
little study on Sequential Search (SS), a twin learning task that takes into
account a user's current and past search queries, in addition to behavior on
historical query sessions. The SS learning task is even more important than the
counterpart SR task for most of E-commence companies due to its much larger
online serving demands as well as traffic volume.
  To this end, we propose a highly scalable hybrid learning model that consists
of an RNN learning framework leveraging all features in short-term user-item
interactions, and an attention model utilizing selected item-only features from
long-term interactions. As a novel optimization step, we fit multiple short
user sequences in a single RNN pass within a training batch, by solving a
greedy knapsack problem on the fly. Moreover, we explore the use of off-policy
reinforcement learning in multi-session personalized search ranking.
Specifically, we design a pairwise Deep Deterministic Policy Gradient model
that efficiently captures users' long term reward in terms of pairwise
classification error. Extensive ablation experiments demonstrate significant
improvement each component brings to its state-of-the-art baseline, on a
variety of offline and online metrics.
",0
Coordinated Frequency Control through Safe Reinforcement Learning,"Yi Zhou, Liangcai Zhou, Di Shi, Xiaoying Zhao",2022-01-30T06:40:32Z,Reinforcement Learning,"  With widespread deployment of renewables, the electric power grids are
experiencing increasing dynamics and uncertainties, with its secure operation
being threatened. Existing frequency control schemes based on day-ahead offline
analysis and minute-level online sensitivity calculations are difficult to
adapt to rapidly changing system states. In particular, they are unable to
facilitate coordinated control of system frequency and power flows. A refined
approach and tools are urgently needed to assist system operators to make
timely decisions. This paper proposes a novel model-free coordinated frequency
control framework based on safe reinforcement learning, with multiple control
objectives considered. The load frequency control problem is modeled as a
constrained Markov decision process, which can be solved by an AI agent
continuously interacting with the grid to achieve sub-second decision making.
Extensive numerical experiments conducted at East China Power Grid demonstrate
the effectiveness and promise of the proposed method.
",0
Distributional Reinforcement Learning with Regularized Wasserstein Loss,"Ke Sun, Yingnan Zhao, Wulong Liu, Bei Jiang, Linglong Kong",2022-02-01T21:27:51Z,Reinforcement Learning,"  The empirical success of distributional reinforcement learning (RL) highly
relies on the choice of distribution divergence equipped with an appropriate
distribution representation. In this paper, we propose \textit{Sinkhorn
distributional RL (SinkhornDRL)}, which leverages Sinkhorn divergence, a
regularized Wasserstein loss, to minimize the difference between current and
target Bellman return distributions. Theoretically, we prove the contraction
properties of SinkhornDRL, aligning with the interpolation nature of Sinkhorn
divergence between Wasserstein distance and Maximum Mean Discrepancy (MMD). The
introduced SinkhornDRL enriches the family of distributional RL algorithms,
contributing to interpreting the algorithm behaviors compared with existing
approaches by our investigation into their relationships. Empirically, we show
that SinkhornDRL consistently outperforms or matches existing algorithms on the
Atari games suite and particularly stands out in the multi-dimensional reward
setting. \thanks{Code is available in
\url{https://github.com/datake/SinkhornDistRL}.}.
",0
Reinforcement learning of optimal active particle navigation,"Mahdi Nasiri, Benno Liebchen",2022-02-01T23:47:59Z,Reinforcement Learning,"  The development of self-propelled particles at the micro- and the nanoscale
has sparked a huge potential for future applications in active matter physics,
microsurgery, and targeted drug delivery. However, while the latter
applications provoke the quest on how to optimally navigate towards a target,
such as e.g. a cancer cell, there is still no simple way known to determine the
optimal route in sufficiently complex environments. Here we develop a machine
learning-based approach that allows us, for the first time, to determine the
asymptotically optimal path of a self-propelled agent which can freely steer in
complex environments. Our method hinges on policy gradient-based deep
reinforcement learning techniques and, crucially, does not require any reward
shaping or heuristics. The presented method provides a powerful alternative to
current analytical methods to calculate optimal trajectories and opens a route
towards a universal path planner for future intelligent active particles.
",0
Challenging Common Assumptions in Convex Reinforcement Learning,"Mirco Mutti, Riccardo De Santi, Piersilvio De Bartolomeis, Marcello Restelli",2022-02-03T10:47:10Z,Reinforcement Learning,"  The classic Reinforcement Learning (RL) formulation concerns the maximization
of a scalar reward function. More recently, convex RL has been introduced to
extend the RL formulation to all the objectives that are convex functions of
the state distribution induced by a policy. Notably, convex RL covers several
relevant applications that do not fall into the scalar formulation, including
imitation learning, risk-averse RL, and pure exploration. In classic RL, it is
common to optimize an infinite trials objective, which accounts for the state
distribution instead of the empirical state visitation frequencies, even though
the actual number of trajectories is always finite in practice. This is
theoretically sound since the infinite trials and finite trials objectives can
be proved to coincide and thus lead to the same optimal policy. In this paper,
we show that this hidden assumption does not hold in the convex RL setting. In
particular, we show that erroneously optimizing the infinite trials objective
in place of the actual finite trials one, as it is usually done, can lead to a
significant approximation error. Since the finite trials setting is the default
in both simulated and real-world RL, we believe shedding light on this issue
will lead to better approaches and methodologies for convex RL, impacting
relevant research areas such as imitation learning, risk-averse RL, and pure
exploration among others.
",17
Meta-Reinforcement Learning with Self-Modifying Networks,"Mathieu Chalvidal, Thomas Serre, Rufin VanRullen",2022-02-04T19:54:10Z,Reinforcement Learning,"  Deep Reinforcement Learning has demonstrated the potential of neural networks
tuned with gradient descent for solving complex tasks in well-delimited
environments. However, these neural systems are slow learners producing
specialized agents with no mechanism to continue learning beyond their training
curriculum. On the contrary, biological synaptic plasticity is persistent and
manifold, and has been hypothesized to play a key role in executive functions
such as working memory and cognitive flexibility, potentially supporting more
efficient and generic learning abilities. Inspired by this, we propose to build
networks with dynamic weights, able to continually perform self-reflexive
modification as a function of their current synaptic state and action-reward
feedback, rather than a fixed network configuration. The resulting model,
MetODS (for Meta-Optimized Dynamical Synapses) is a broadly applicable
meta-reinforcement learning system able to learn efficient and powerful control
rules in the agent policy space. A single layer with dynamic synapses can
perform one-shot learning, generalizes navigation principles to unseen
environments and manifests a strong ability to learn adaptive motor policies.
",0
Reinforcement Learning for Shared Autonomy Drone Landings,"Kal Backman, Dana Kulić, Hoam Chung",2022-02-07T03:57:30Z,Reinforcement Learning,"  Novice pilots find it difficult to operate and land unmanned aerial vehicles
(UAVs), due to the complex UAV dynamics, challenges in depth perception, lack
of expertise with the control interface and additional disturbances from the
ground effect. Therefore we propose a shared autonomy approach to assist pilots
in safely landing a UAV under conditions where depth perception is difficult
and safe landing zones are limited. Our approach comprises of two modules: a
perception module that encodes information onto a compressed latent
representation using two RGB-D cameras and a policy module that is trained with
the reinforcement learning algorithm TD3 to discern the pilot's intent and to
provide control inputs that augment the user's input to safely land the UAV.
The policy module is trained in simulation using a population of simulated
users. Simulated users are sampled from a parametric model with four
parameters, which model a pilot's tendency to conform to the assistant,
proficiency, aggressiveness and speed. We conduct a user study (n = 28) where
human participants were tasked with landing a physical UAV on one of several
platforms under challenging viewing conditions. The assistant, trained with
only simulated user data, improved task success rate from 51.4% to 98.2%
despite being unaware of the human participants' goal or the structure of the
environment a priori. With the proposed assistant, regardless of prior piloting
experience, participants performed with a proficiency greater than the most
experienced unassisted participants.
",0
Optimizing Warfarin Dosing using Deep Reinforcement Learning,"Sadjad Anzabi Zadeh, W. Nick Street, Barrett W. Thomas",2022-02-07T19:58:54Z,Reinforcement Learning,"  Warfarin is a widely used anticoagulant, and has a narrow therapeutic range.
Dosing of warfarin should be individualized, since slight overdosing or
underdosing can have catastrophic or even fatal consequences. Despite much
research on warfarin dosing, current dosing protocols do not live up to
expectations, especially for patients sensitive to warfarin. We propose a deep
reinforcement learning-based dosing model for warfarin. To overcome the issue
of relatively small sample sizes in dosing trials, we use a Pharmacokinetic/
Pharmacodynamic (PK/PD) model of warfarin to simulate dose-responses of virtual
patients. Applying the proposed algorithm on virtual test patients shows that
this model outperforms a set of clinically accepted dosing protocols by a wide
margin. We tested the robustness of our dosing protocol on a second PK/PD model
and showed that its performance is comparable to the set of baseline protocols.
",0
Offline Reinforcement Learning for Mobile Notifications,"Yiping Yuan, Ajith Muralidharan, Preetam Nandy, Miao Cheng, Prakruthi Prabhakar",2022-02-04T22:22:22Z,Reinforcement Learning,"  Mobile notification systems have taken a major role in driving and
maintaining user engagement for online platforms. They are interesting
recommender systems to machine learning practitioners with more sequential and
long-term feedback considerations. Most machine learning applications in
notification systems are built around response-prediction models, trying to
attribute both short-term impact and long-term impact to a notification
decision. However, a user's experience depends on a sequence of notifications
and attributing impact to a single notification is not always accurate, if not
impossible. In this paper, we argue that reinforcement learning is a better
framework for notification systems in terms of performance and iteration speed.
We propose an offline reinforcement learning framework to optimize sequential
notification decisions for driving user engagement. We describe a
state-marginalized importance sampling policy evaluation approach, which can be
used to evaluate the policy offline and tune learning hyperparameters. Through
simulations that approximate the notifications ecosystem, we demonstrate the
performance and benefits of the offline evaluation approach as a part of the
reinforcement learning modeling approach. Finally, we collect data through
online exploration in the production system, train an offline Double Deep
Q-Network and launch a successful policy online. We also discuss the practical
considerations and results obtained by deploying these policies for a
large-scale recommendation system use-case.
",0
Financial Vision Based Reinforcement Learning Trading Strategy,"Yun-Cheng Tsai, Fu-Min Szu, Jun-Hao Chen, Samuel Yen-Chi Chen",2022-02-03T02:14:38Z,Reinforcement Learning,"  Recent advances in artificial intelligence (AI) for quantitative trading have
led to its general superhuman performance in significant trading performance.
However, the potential risk of AI trading is a ""black box"" decision. Some AI
computing mechanisms are complex and challenging to understand. If we use AI
without proper supervision, AI may lead to wrong choices and make huge losses.
Hence, we need to ask about the AI ""black box"", including why did AI decide to
do this or not? Why can people trust AI or not? How can people fix their
mistakes? These problems also highlight the challenges that AI technology can
explain in the trading field.
",0
Computational-Statistical Gaps in Reinforcement Learning,"Daniel Kane, Sihan Liu, Shachar Lovett, Gaurav Mahajan",2022-02-11T04:48:35Z,Reinforcement Learning,"  Reinforcement learning with function approximation has recently achieved
tremendous results in applications with large state spaces. This empirical
success has motivated a growing body of theoretical work proposing necessary
and sufficient conditions under which efficient reinforcement learning is
possible. From this line of work, a remarkably simple minimal sufficient
condition has emerged for sample efficient reinforcement learning: MDPs with
optimal value function $V^*$ and $Q^*$ linear in some known low-dimensional
features. In this setting, recent works have designed sample efficient
algorithms which require a number of samples polynomial in the feature
dimension and independent of the size of state space. They however leave
finding computationally efficient algorithms as future work and this is
considered a major open problem in the community.
  In this work, we make progress on this open problem by presenting the first
computational lower bound for RL with linear function approximation: unless
NP=RP, no randomized polynomial time algorithm exists for deterministic
transition MDPs with a constant number of actions and linear optimal value
functions. To prove this, we show a reduction from Unique-Sat, where we convert
a CNF formula into an MDP with deterministic transitions, constant number of
actions and low dimensional linear optimal value functions. This result also
exhibits the first computational-statistical gap in reinforcement learning with
linear function approximation, as the underlying statistical problem is
information-theoretically solvable with a polynomial number of queries, but no
computationally efficient algorithm exists unless NP=RP. Finally, we also prove
a quasi-polynomial time lower bound under the Randomized Exponential Time
Hypothesis.
",5
Supported Policy Optimization for Offline Reinforcement Learning,"Jialong Wu, Haixu Wu, Zihan Qiu, Jianmin Wang, Mingsheng Long",2022-02-13T07:38:36Z,Reinforcement Learning,"  Policy constraint methods to offline reinforcement learning (RL) typically
utilize parameterization or regularization that constrains the policy to
perform actions within the support set of the behavior policy. The elaborative
designs of parameterization methods usually intrude into the policy networks,
which may bring extra inference cost and cannot take full advantage of
well-established online methods. Regularization methods reduce the divergence
between the learned policy and the behavior policy, which may mismatch the
inherent density-based definition of support set thereby failing to avoid the
out-of-distribution actions effectively. This paper presents Supported Policy
OpTimization (SPOT), which is directly derived from the theoretical
formalization of the density-based support constraint. SPOT adopts a VAE-based
density estimator to explicitly model the support set of behavior policy and
presents a simple but effective density-based regularization term, which can be
plugged non-intrusively into off-the-shelf off-policy RL algorithms. SPOT
achieves the state-of-the-art performance on standard benchmarks for offline
RL. Benefiting from the pluggable design, offline pretrained models from SPOT
can also be applied to perform online fine-tuning seamlessly.
",0
Interpretable Reinforcement Learning with Multilevel Subgoal Discovery,"Alexander Demin, Denis Ponomaryov",2022-02-15T14:04:44Z,Reinforcement Learning,"  We propose a novel Reinforcement Learning model for discrete environments,
which is inherently interpretable and supports the discovery of deep subgoal
hierarchies. In the model, an agent learns information about environment in the
form of probabilistic rules, while policies for (sub)goals are learned as
combinations thereof. No reward function is required for learning; an agent
only needs to be given a primary goal to achieve. Subgoals of a goal G from the
hierarchy are computed as descriptions of states, which if previously achieved
increase the total efficiency of the available policies for G. These state
descriptions are introduced as new sensor predicates into the rule language of
the agent, which allows for sensing important intermediate states and for
updating environment rules and policies accordingly.
",0
Sequential Bayesian experimental designs via reinforcement learning,Hikaru Asano,2022-02-14T04:29:04Z,Reinforcement Learning,"  Bayesian experimental design (BED) has been used as a method for conducting
efficient experiments based on Bayesian inference. The existing methods,
however, mostly focus on maximizing the expected information gain (EIG); the
cost of experiments and sample efficiency are often not taken into account. In
order to address this issue and enhance practical applicability of BED, we
provide a new approach Sequential Experimental Design via Reinforcement
Learning to construct BED in a sequential manner by applying reinforcement
learning in this paper. Here, reinforcement learning is a branch of machine
learning in which an agent learns a policy to maximize its reward by
interacting with the environment. The characteristics of interacting with the
environment are similar to the sequential experiment, and reinforcement
learning is indeed a method that excels at sequential decision making.
  By proposing a new real-world-oriented experimental environment, our approach
aims to maximize the EIG while keeping the cost of experiments and sample
efficiency in mind simultaneously. We conduct numerical experiments for three
different examples. It is confirmed that our method outperforms the existing
methods in various indices such as the EIG and sampling efficiency, indicating
that our proposed method and experimental environment can make a significant
contribution to application of BED to the real world.
",0
Robust Reinforcement Learning via Genetic Curriculum,"Yeeho Song, Jeff Schneider",2022-02-17T01:14:20Z,Reinforcement Learning,"  Achieving robust performance is crucial when applying deep reinforcement
learning (RL) in safety critical systems. Some of the state of the art
approaches try to address the problem with adversarial agents, but these agents
often require expert supervision to fine tune and prevent the adversary from
becoming too challenging to the trainee agent. While other approaches involve
automatically adjusting environment setups during training, they have been
limited to simple environments where low-dimensional encodings can be used.
Inspired by these approaches, we propose genetic curriculum, an algorithm that
automatically identifies scenarios in which the agent currently fails and
generates an associated curriculum to help the agent learn to solve the
scenarios and acquire more robust behaviors. As a non-parametric optimizer, our
approach uses a raw, non-fixed encoding of scenarios, reducing the need for
expert supervision and allowing our algorithm to adapt to the changing
performance of the agent. Our empirical studies show improvement in robustness
over the existing state of the art algorithms, providing training curricula
that result in agents being 2 - 8x times less likely to fail without
sacrificing cumulative reward. We include an ablation study and share insights
on why our algorithm outperforms prior approaches.
",0
A Survey of Explainable Reinforcement Learning,"Stephanie Milani, Nicholay Topin, Manuela Veloso, Fei Fang",2022-02-17T03:45:09Z,Reinforcement Learning,"  Explainable reinforcement learning (XRL) is an emerging subfield of
explainable machine learning that has attracted considerable attention in
recent years. The goal of XRL is to elucidate the decision-making process of
learning agents in sequential decision-making settings. In this survey, we
propose a novel taxonomy for organizing the XRL literature that prioritizes the
RL setting. We overview techniques according to this taxonomy. We point out
gaps in the literature, which we use to motivate and outline a roadmap for
future work.
",0
TransDreamer: Reinforcement Learning with Transformer World Models,"Chang Chen, Yi-Fu Wu, Jaesik Yoon, Sungjin Ahn",2022-02-19T00:30:52Z,Reinforcement Learning,"  The Dreamer agent provides various benefits of Model-Based Reinforcement
Learning (MBRL) such as sample efficiency, reusable knowledge, and safe
planning. However, its world model and policy networks inherit the limitations
of recurrent neural networks and thus an important question is how an MBRL
framework can benefit from the recent advances of transformers and what the
challenges are in doing so. In this paper, we propose a transformer-based MBRL
agent, called TransDreamer. We first introduce the Transformer State-Space
Model, a world model that leverages a transformer for dynamics predictions. We
then share this world model with a transformer-based policy network and obtain
stability in training a transformer-based RL agent. In experiments, we apply
the proposed model to 2D visual RL and 3D first-person visual RL tasks both
requiring long-range memory access for memory-based reasoning. We show that the
proposed model outperforms Dreamer in these complex tasks.
",0
Shaping Advice in Deep Reinforcement Learning,"Baicen Xiao, Bhaskar Ramasubramanian, Radha Poovendran",2022-02-19T01:42:04Z,Reinforcement Learning,"  Reinforcement learning involves agents interacting with an environment to
complete tasks. When rewards provided by the environment are sparse, agents may
not receive immediate feedback on the quality of actions that they take,
thereby affecting learning of policies. In this paper, we propose to methods to
augment the reward signal from the environment with an additional reward termed
shaping advice in both single and multi-agent reinforcement learning. The
shaping advice is specified as a difference of potential functions at
consecutive time-steps. Each potential function is a function of observations
and actions of the agents. The use of potential functions is underpinned by an
insight that the total potential when starting from any state and returning to
the same state is always equal to zero. We show through theoretical analyses
and experimental validation that the shaping advice does not distract agents
from completing tasks specified by the environment reward. Theoretically, we
prove that the convergence of policy gradients and value functions when using
shaping advice implies the convergence of these quantities in the absence of
shaping advice. We design two algorithms- Shaping Advice in Single-agent
reinforcement learning (SAS) and Shaping Advice in Multi-agent reinforcement
learning (SAM). Shaping advice in SAS and SAM needs to be specified only once
at the start of training, and can easily be provided by non-experts.
Experimentally, we evaluate SAS and SAM on two tasks in single-agent
environments and three tasks in multi-agent environments that have sparse
rewards. We observe that using shaping advice results in agents learning
policies to complete tasks faster, and obtain higher rewards than algorithms
that do not use shaping advice.
",0
Reinforcement Learning in Practice: Opportunities and Challenges,Yuxi Li,2022-02-23T03:58:46Z,Reinforcement Learning,"  This article is a gentle discussion about the field of reinforcement learning
in practice, about opportunities and challenges, touching a broad range of
topics, with perspectives and without technical details. The article is based
on both historical and recent research papers, surveys, tutorials, talks,
blogs, books, (panel) discussions, and workshops/conferences. Various groups of
readers, like researchers, engineers, students, managers, investors, officers,
and people wanting to know more about the field, may find the article
interesting.
  In this article, we first give a brief introduction to reinforcement learning
(RL), and its relationship with deep learning, machine learning and AI. Then we
discuss opportunities of RL, in particular, products and services, games,
bandits, recommender systems, robotics, transportation, finance and economics,
healthcare, education, combinatorial optimization, computer systems, and
science and engineering. Then we discuss challenges, in particular, 1)
foundation, 2) representation, 3) reward, 4) exploration, 5) model, simulation,
planning, and benchmarks, 6) off-policy/offline learning, 7) learning to learn
a.k.a. meta-learning, 8) explainability and interpretability, 9) constraints,
10) software development and deployment, 11) business perspectives, and 12)
more challenges. We conclude with a discussion, attempting to answer: ""Why has
RL not been widely adopted in practice yet?"" and ""When is RL helpful?"".
",0
Consistent Dropout for Policy Gradient Reinforcement Learning,"Matthew Hausknecht, Nolan Wagener",2022-02-23T23:00:40Z,Reinforcement Learning,"  Dropout has long been a staple of supervised learning, but is rarely used in
reinforcement learning. We analyze why naive application of dropout is
problematic for policy-gradient learning algorithms and introduce consistent
dropout, a simple technique to address this instability. We demonstrate
consistent dropout enables stable training with A2C and PPO in both continuous
and discrete action environments across a wide range of dropout probabilities.
Finally, we show that consistent dropout enables the online training of complex
architectures such as GPT without needing to disable the model's native
dropout.
",0
Avalanche RL: a Continual Reinforcement Learning Library,"Nicolò Lucchesi, Antonio Carta, Vincenzo Lomonaco, Davide Bacciu",2022-02-28T10:01:22Z,Reinforcement Learning,"  Continual Reinforcement Learning (CRL) is a challenging setting where an
agent learns to interact with an environment that is constantly changing over
time (the stream of experiences). In this paper, we describe Avalanche RL, a
library for Continual Reinforcement Learning which allows to easily train
agents on a continuous stream of tasks. Avalanche RL is based on PyTorch and
supports any OpenAI Gym environment. Its design is based on Avalanche, one of
the more popular continual learning libraries, which allow us to reuse a large
number of continual learning strategies and improve the interaction between
reinforcement learning and continual learning researchers. Additionally, we
propose Continual Habitat-Lab, a novel benchmark and a high-level library which
enables the usage of the photorealistic simulator Habitat-Sim for CRL research.
Overall, Avalanche RL attempts to unify under a common framework continual
reinforcement learning applications, which we hope will foster the growth of
the field.
",0
A Theory of Abstraction in Reinforcement Learning,David Abel,2022-03-01T12:46:28Z,Reinforcement Learning,"  Reinforcement learning defines the problem facing agents that learn to make
good decisions through action and observation alone. To be effective problem
solvers, such agents must efficiently explore vast worlds, assign credit from
delayed feedback, and generalize to new experiences, all while making use of
limited data, computational resources, and perceptual bandwidth. Abstraction is
essential to all of these endeavors. Through abstraction, agents can form
concise models of their environment that support the many practices required of
a rational, adaptive decision maker. In this dissertation, I present a theory
of abstraction in reinforcement learning. I first offer three desiderata for
functions that carry out the process of abstraction: they should 1) preserve
representation of near-optimal behavior, 2) be learned and constructed
efficiently, and 3) lower planning or learning time. I then present a suite of
new algorithms and analysis that clarify how agents can learn to abstract
according to these desiderata. Collectively, these results provide a partial
path toward the discovery and use of abstraction that minimizes the complexity
of effective reinforcement learning.
",25
Hierarchical Reinforcement Learning with AI Planning Models,"Junkyu Lee, Michael Katz, Don Joven Agravante, Miao Liu, Geraud Nangue Tasse, Tim Klinger, Shirin Sohrabi",2022-03-01T18:38:41Z,Reinforcement Learning,"  Two common approaches to sequential decision-making are AI planning (AIP) and
reinforcement learning (RL). Each has strengths and weaknesses. AIP is
interpretable, easy to integrate with symbolic knowledge, and often efficient,
but requires an up-front logical domain specification and is sensitive to
noise; RL only requires specification of rewards and is robust to noise but is
sample inefficient and not easily supplied with external knowledge. We propose
an integrative approach that combines high-level planning with RL, retaining
interpretability, transfer, and efficiency, while allowing for robust learning
of the lower-level plan actions. Our approach defines options in hierarchical
reinforcement learning (HRL) from AIP operators by establishing a
correspondence between the state transition model of AI planning problem and
the abstract state transition system of a Markov Decision Process (MDP).
Options are learned by adding intrinsic rewards to encourage consistency
between the MDP and AIP transition models. We demonstrate the benefit of our
integrated approach by comparing the performance of RL and HRL algorithms in
both MiniGrid and N-rooms environments, showing the advantage of our method
over the existing ones.
",0
Reliable validation of Reinforcement Learning Benchmarks,"Matthias Müller-Brockhausen, Aske Plaat, Mike Preuss",2022-03-02T12:55:27Z,Reinforcement Learning,"  Reinforcement Learning (RL) is one of the most dynamic research areas in Game
AI and AI as a whole, and a wide variety of games are used as its prominent
test problems. However, it is subject to the replicability crisis that
currently affects most algorithmic AI research. Benchmarking in Reinforcement
Learning could be improved through verifiable results. There are numerous
benchmark environments whose scores are used to compare different algorithms,
such as Atari. Nevertheless, reviewers must trust that figures represent
truthful values, as it is difficult to reproduce an exact training curve. We
propose improving this situation by providing access to the original
experimental data to validate study results. To that end, we rely on the
concept of minimal traces. These allow re-simulation of action sequences in
deterministic RL environments and, in turn, enable reviewers to verify, re-use,
and manually inspect experimental results without needing large compute
clusters. It also permits validation of presented reward graphs, an inspection
of individual episodes, and re-use of result data (baselines) for proper
comparison in follow-up papers. We offer plug-and-play code that works with Gym
so that our measures fit well in the existing RL and reproducibility
eco-system. Our approach is freely available, easy to use, and adds minimal
overhead, as minimal traces allow a data compression ratio of up to $\approx
10^4:1$ (94GB to 8MB for Atari Pong) compared to a regular MDP trace used in
offline RL datasets. The paper presents proof-of-concept results for a variety
of games.
",0
Quantum Reinforcement Learning via Policy Iteration,"El Amine Cherrat, Iordanis Kerenidis, Anupam Prakash",2022-03-03T18:08:17Z,Reinforcement Learning,"  Quantum computing has shown the potential to substantially speed up machine
learning applications, in particular for supervised and unsupervised learning.
Reinforcement learning, on the other hand, has become essential for solving
many decision making problems and policy iteration methods remain the
foundation of such approaches. In this paper, we provide a general framework
for performing quantum reinforcement learning via policy iteration. We validate
our framework by designing and analyzing: \emph{quantum policy evaluation}
methods for infinite horizon discounted problems by building quantum states
that approximately encode the value function of a policy $\pi$; and
\emph{quantum policy improvement} methods by post-processing measurement
outcomes on these quantum states. Last, we study the theoretical and
experimental performance of our quantum algorithms on two environments from
OpenAI's Gym.
",0
GraspARL: Dynamic Grasping via Adversarial Reinforcement Learning,"Tianhao Wu, Fangwei Zhong, Yiran Geng, Hongchen Wang, Yongjian Zhu, Yizhou Wang, Hao Dong",2022-03-04T03:25:09Z,Reinforcement Learning,"  Grasping moving objects, such as goods on a belt or living animals, is an
important but challenging task in robotics. Conventional approaches rely on a
set of manually defined object motion patterns for training, resulting in poor
generalization to unseen object trajectories. In this work, we introduce an
adversarial reinforcement learning framework for dynamic grasping, namely
GraspARL. To be specific. we formulate the dynamic grasping problem as a
'move-and-grasp' game, where the robot is to pick up the object on the mover
and the adversarial mover is to find a path to escape it. Hence, the two agents
play a min-max game and are trained by reinforcement learning. In this way, the
mover can auto-generate diverse moving trajectories while training. And the
robot trained with the adversarial trajectories can generalize to various
motion patterns. Empirical results on the simulator and real-world scenario
demonstrate the effectiveness of each and good generalization of our method.
",6
Intrinsically-Motivated Reinforcement Learning: A Brief Introduction,Mingqi Yuan,2022-03-03T12:39:58Z,Reinforcement Learning,"  Reinforcement learning (RL) is one of the three basic paradigms of machine
learning. It has demonstrated impressive performance in many complex tasks like
Go and StarCraft, which is increasingly involved in smart manufacturing and
autonomous driving. However, RL consistently suffers from the
exploration-exploitation dilemma. In this paper, we investigated the problem of
improving exploration in RL and introduced the intrinsically-motivated RL. In
sharp contrast to the classic exploration strategies, intrinsically-motivated
RL utilizes the intrinsic learning motivation to provide sustainable
exploration incentives. We carefully classified the existing intrinsic reward
methods and analyzed their practical drawbacks. Moreover, we proposed a new
intrinsic reward method via R\'enyi state entropy maximization, which overcomes
the drawbacks of the preceding methods and provides powerful exploration
incentives. Finally, extensive simulation demonstrated that the proposed module
achieve superior performance with higher efficiency and robustness.
",0
Safe Reinforcement Learning for Legged Locomotion,"Tsung-Yen Yang, Tingnan Zhang, Linda Luu, Sehoon Ha, Jie Tan, Wenhao Yu",2022-03-05T01:49:16Z,Reinforcement Learning,"  Designing control policies for legged locomotion is complex due to the
under-actuated and non-continuous robot dynamics. Model-free reinforcement
learning provides promising tools to tackle this challenge. However, a major
bottleneck of applying model-free reinforcement learning in real world is
safety. In this paper, we propose a safe reinforcement learning framework that
switches between a safe recovery policy that prevents the robot from entering
unsafe states, and a learner policy that is optimized to complete the task. The
safe recovery policy takes over the control when the learner policy violates
safety constraints, and hands over the control back when there are no future
safety violations. We design the safe recovery policy so that it ensures safety
of legged locomotion while minimally intervening in the learning process.
Furthermore, we theoretically analyze the proposed framework and provide an
upper bound on the task performance. We verify the proposed framework in four
locomotion tasks on a simulated and real quadrupedal robot: efficient gait,
catwalk, two-leg balance, and pacing. On average, our method achieves 48.6%
fewer falls and comparable or better rewards than the baseline methods in
simulation. When deployed it on real-world quadruped robot, our training
pipeline enables 34% improvement in energy efficiency for the efficient gait,
40.9% narrower of the feet placement in the catwalk, and two times more jumping
duration in the two-leg balance. Our method achieves less than five falls over
the duration of 115 minutes of hardware time.
",30
On Credit Assignment in Hierarchical Reinforcement Learning,"Joery A. de Vries, Thomas M. Moerland, Aske Plaat",2022-03-07T11:13:09Z,Reinforcement Learning,"  Hierarchical Reinforcement Learning (HRL) has held longstanding promise to
advance reinforcement learning. Yet, it has remained a considerable challenge
to develop practical algorithms that exhibit some of these promises. To improve
our fundamental understanding of HRL, we investigate hierarchical credit
assignment from the perspective of conventional multistep reinforcement
learning. We show how e.g., a 1-step `hierarchical backup' can be seen as a
conventional multistep backup with $n$ skip connections over time connecting
each subsequent state to the first independent of actions inbetween.
Furthermore, we find that generalizing hierarchy to multistep return estimation
methods requires us to consider how to partition the environment trace, in
order to construct backup paths. We leverage these insight to develop a new
hierarchical algorithm Hier$Q_k(\lambda)$, for which we demonstrate that
hierarchical credit assignment alone can already boost agent performance (i.e.,
when eliminating generalization or exploration). Altogether, our work yields
fundamental insight into the nature of hierarchical backups and distinguishes
this as an additional basis for reinforcement learning research.
",0
Deep Reinforcement Learning for Entity Alignment,"Lingbing Guo, Yuqiang Han, Qiang Zhang, Huajun Chen",2022-03-07T11:49:40Z,Reinforcement Learning,"  Embedding-based methods have attracted increasing attention in recent entity
alignment (EA) studies. Although great promise they can offer, there are still
several limitations. The most notable is that they identify the aligned
entities based on cosine similarity, ignoring the semantics underlying the
embeddings themselves. Furthermore, these methods are shortsighted,
heuristically selecting the closest entity as the target and allowing multiple
entities to match the same candidate. To address these limitations, we model
entity alignment as a sequential decision-making task, in which an agent
sequentially decides whether two entities are matched or mismatched based on
their representation vectors. The proposed reinforcement learning (RL)-based
entity alignment framework can be flexibly adapted to most embedding-based EA
methods. The experimental results demonstrate that it consistently advances the
performance of several state-of-the-art methods, with a maximum improvement of
31.1% on Hits@1.
",0
Reinforcement Learning for Location-Aware Scheduling,"Stelios Stavroulakis, Biswa Sengupta",2022-03-07T15:51:00Z,Reinforcement Learning,"  Recent techniques in dynamical scheduling and resource management have found
applications in warehouse environments due to their ability to organize and
prioritize tasks in a higher temporal resolution. The rise of deep
reinforcement learning, as a learning paradigm, has enabled decentralized agent
populations to discover complex coordination strategies. However, training
multiple agents simultaneously introduce many obstacles in training as
observation and action spaces become exponentially large. In our work, we
experimentally quantify how various aspects of the warehouse environment (e.g.,
floor plan complexity, information about agents' live location, level of task
parallelizability) affect performance and execution priority. To achieve
efficiency, we propose a compact representation of the state and action space
for location-aware multi-agent systems, wherein each agent has knowledge of
only self and task coordinates, hence only partial observability of the
underlying Markov Decision Process. Finally, we show how agents trained in
certain environments maintain performance in completely unseen settings and
also correlate performance degradation with floor plan geometry.
",0
Graph Reinforcement Learning for Radio Resource Allocation,"Jianyu Zhao, Chenyang Yang",2022-03-08T08:02:54Z,Reinforcement Learning,"  Deep reinforcement learning (DRL) for resource allocation has been
investigated extensively owing to its ability of handling model-free and
end-to-end problems. Yet the high training complexity of DRL hinders its
practical use in dynamic wireless systems. To reduce the training cost, we
resort to graph reinforcement learning for exploiting two kinds of relational
priors inherent in many problems in wireless communications: topology
information and permutation properties. To design graph reinforcement learning
framework systematically for harnessing the two priors, we first conceive a
method to transform state matrix into state graph, and then propose a general
method for graph neural networks to satisfy desirable permutation properties.
To demonstrate how to apply the proposed methods, we take deep deterministic
policy gradient (DDPG) as an example for optimizing two representative resource
allocation problems. One is predictive power allocation that minimizes the
energy consumed for ensuring the quality-ofservice of each user that requests
video streaming. The other is link scheduling that maximizes the sum-rate for
device-to-device communications. Simulation results show that the graph DDPG
algorithm converges much faster and needs much lower space complexity than
existing DDPG algorithms to achieve the same learning performance.
",0
Deep Binary Reinforcement Learning for Scalable Verification,"Christopher Lazarus, Mykel J. Kochenderfer",2022-03-11T01:20:23Z,Reinforcement Learning,"  The use of neural networks as function approximators has enabled many
advances in reinforcement learning (RL). The generalization power of neural
networks combined with advances in RL algorithms has reignited the field of
artificial intelligence. Despite their power, neural networks are considered
black boxes, and their use in safety-critical settings remains a challenge.
Recently, neural network verification has emerged as a way to certify safety
properties of networks. Verification is a hard problem, and it is difficult to
scale to large networks such as the ones used in deep reinforcement learning.
We provide an approach to train RL policies that are more easily verifiable. We
use binarized neural networks (BNNs), a type of network with mostly binary
parameters. We present an RL algorithm tailored specifically for BNNs. After
training BNNs for the Atari environments, we verify robustness properties.
",0
Precise atom manipulation through deep reinforcement learning,"I-Ju Chen, Markus Aapro, Abraham Kipnis, Alexander Ilin, Peter Liljeroth, Adam S. Foster",2022-03-14T10:24:43Z,Reinforcement Learning,"  Atomic-scale manipulation in scanning tunneling microscopy has enabled the
creation of quantum states of matter based on artificial structures and extreme
miniaturization of computational circuitry based on individual atoms. The
ability to autonomously arrange atomic structures with precision will enable
the scaling up of nanoscale fabrication and expand the range of artificial
structures hosting exotic quantum states. However, the \textit{a priori}
unknown manipulation parameters, the possibility of spontaneous tip apex
changes, and the difficulty of modeling tip-atom interactions make it
challenging to select manipulation parameters that can achieve atomic precision
throughout extended operations. Here we use deep reinforcement learning (DRL)
to control the real-world atom manipulation process. Several state-of-the-art
reinforcement learning techniques are used jointly to boost data efficiency.
The reinforcement learning agent learns to manipulate Ag adatoms on Ag(111)
surfaces with optimal precision and is integrated with path planning algorithms
to complete an autonomous atomic assembly system. The results demonstrate that
state-of-the-art deep reinforcement learning can offer effective solutions to
real-world challenges in nanofabrication and powerful approaches to
increasingly complex scientific experiments at the atomic scale.
",0
Orchestrated Value Mapping for Reinforcement Learning,"Mehdi Fatemi, Arash Tavakoli",2022-03-14T15:13:44Z,Reinforcement Learning,"  We present a general convergent class of reinforcement learning algorithms
that is founded on two distinct principles: (1) mapping value estimates to a
different space using arbitrary functions from a broad class, and (2) linearly
decomposing the reward signal into multiple channels. The first principle
enables incorporating specific properties into the value estimator that can
enhance learning. The second principle, on the other hand, allows for the value
function to be represented as a composition of multiple utility functions. This
can be leveraged for various purposes, e.g. dealing with highly varying reward
scales, incorporating a priori knowledge about the sources of reward, and
ensemble learning. Combining the two principles yields a general blueprint for
instantiating convergent algorithms by orchestrating diverse mapping functions
over multiple reward channels. This blueprint generalizes and subsumes
algorithms such as Q-Learning, Log Q-Learning, and Q-Decomposition. In
addition, our convergence proof for this general class relaxes certain required
assumptions in some of these algorithms. Based on our theory, we discuss
several interesting configurations as special cases. Finally, to illustrate the
potential of the design space that our theory opens up, we instantiate a
particular algorithm and evaluate its performance on the Atari suite.
",0
L2Explorer: A Lifelong Reinforcement Learning Assessment Environment,"Erik C. Johnson, Eric Q. Nguyen, Blake Schreurs, Chigozie S. Ewulum, Chace Ashcraft, Neil M. Fendley, Megan M. Baker, Alexander New, Gautam K. Vallabha",2022-03-14T19:20:26Z,Reinforcement Learning,"  Despite groundbreaking progress in reinforcement learning for robotics,
gameplay, and other complex domains, major challenges remain in applying
reinforcement learning to the evolving, open-world problems often found in
critical application spaces. Reinforcement learning solutions tend to
generalize poorly when exposed to new tasks outside of the data distribution
they are trained on, prompting an interest in continual learning algorithms. In
tandem with research on continual learning algorithms, there is a need for
challenge environments, carefully designed experiments, and metrics to assess
research progress. We address the latter need by introducing a framework for
continual reinforcement-learning development and assessment using Lifelong
Learning Explorer (L2Explorer), a new, Unity-based, first-person 3D exploration
environment that can be continuously reconfigured to generate a range of tasks
and task variants structured into complex and evolving evaluation curricula. In
contrast to procedurally generated worlds with randomized components, we have
developed a systematic approach to defining curricula in response to controlled
changes with accompanying metrics to assess transfer, performance recovery, and
data efficiency. Taken together, the L2Explorer environment and evaluation
approach provides a framework for developing future evaluation methodologies in
open-world settings and rigorously evaluating approaches to lifelong learning.
",0
Semi-Markov Offline Reinforcement Learning for Healthcare,"Mehdi Fatemi, Mary Wu, Jeremy Petch, Walter Nelson, Stuart J. Connolly, Alexander Benz, Anthony Carnicelli, Marzyeh Ghassemi",2022-03-17T14:51:21Z,Reinforcement Learning,"  Reinforcement learning (RL) tasks are typically framed as Markov Decision
Processes (MDPs), assuming that decisions are made at fixed time intervals.
However, many applications of great importance, including healthcare, do not
satisfy this assumption, yet they are commonly modelled as MDPs after an
artificial reshaping of the data. In addition, most healthcare (and similar)
problems are offline by nature, allowing for only retrospective studies. To
address both challenges, we begin by discussing the Semi-MDP (SMDP) framework,
which formally handles actions of variable timings. We next present a formal
way to apply SMDP modifications to nearly any given value-based offline RL
method. We use this theory to introduce three SMDP-based offline RL algorithms,
namely, SDQN, SDDQN, and SBCQ. We then experimentally demonstrate that only
these SMDP-based algorithms learn the optimal policy in variable-time
environments, whereas their MDP counterparts do not. Finally, we apply our new
algorithms to a real-world offline dataset pertaining to warfarin dosing for
stroke prevention and demonstrate similar results.
",0
Privacy-Preserving Reinforcement Learning Beyond Expectation,"Arezoo Rajabi, Bhaskar Ramasubramanian, Abdullah Al Maruf, Radha Poovendran",2022-03-18T21:28:29Z,Reinforcement Learning,"  Cyber and cyber-physical systems equipped with machine learning algorithms
such as autonomous cars share environments with humans. In such a setting, it
is important to align system (or agent) behaviors with the preferences of one
or more human users. We consider the case when an agent has to learn behaviors
in an unknown environment. Our goal is to capture two defining characteristics
of humans: i) a tendency to assess and quantify risk, and ii) a desire to keep
decision making hidden from external parties. We incorporate cumulative
prospect theory (CPT) into the objective of a reinforcement learning (RL)
problem for the former. For the latter, we use differential privacy. We design
an algorithm to enable an RL agent to learn policies to maximize a CPT-based
objective in a privacy-preserving manner and establish guarantees on the
privacy of value functions learned by the algorithm when rewards are
sufficiently close. This is accomplished through adding a calibrated noise
using a Gaussian process mechanism at each step. Through empirical evaluations,
we highlight a privacy-utility tradeoff and demonstrate that the RL agent is
able to learn behaviors that are aligned with that of a human user in the same
environment in a privacy-preserving manner
",1
Teachable Reinforcement Learning via Advice Distillation,"Olivia Watkins, Trevor Darrell, Pieter Abbeel, Jacob Andreas, Abhishek Gupta",2022-03-19T03:22:57Z,Reinforcement Learning,"  Training automated agents to complete complex tasks in interactive
environments is challenging: reinforcement learning requires careful
hand-engineering of reward functions, imitation learning requires specialized
infrastructure and access to a human expert, and learning from intermediate
forms of supervision (like binary preferences) is time-consuming and extracts
little information from each human intervention. Can we overcome these
challenges by building agents that learn from rich, interactive feedback
instead? We propose a new supervision paradigm for interactive learning based
on ""teachable"" decision-making systems that learn from structured advice
provided by an external teacher. We begin by formalizing a class of
human-in-the-loop decision making problems in which multiple forms of
teacher-provided advice are available to a learner. We then describe a simple
learning algorithm for these problems that first learns to interpret advice,
then learns from advice to complete tasks even in the absence of human
supervision. In puzzle-solving, navigation, and locomotion domains, we show
that agents that learn from advice can acquire new skills with significantly
less human supervision than standard reinforcement learning algorithms and
often less than imitation learning.
",0
Explainability in reinforcement learning: perspective and position,"Agneza Krajna, Mario Brcic, Tomislav Lipic, Juraj Doncevic",2022-03-22T09:00:13Z,Reinforcement Learning,"  Artificial intelligence (AI) has been embedded into many aspects of people's
daily lives and it has become normal for people to have AI make decisions for
them. Reinforcement learning (RL) models increase the space of solvable
problems with respect to other machine learning paradigms. Some of the most
interesting applications are in situations with non-differentiable expected
reward function, operating in unknown or underdefined environment, as well as
for algorithmic discovery that surpasses performance of any teacher, whereby
agent learns from experimental experience through simple feedback. The range of
applications and their social impact is vast, just to name a few: genomics,
game-playing (chess, Go, etc.), general optimization, financial investment,
governmental policies, self-driving cars, recommendation systems, etc. It is
therefore essential to improve the trust and transparency of RL-based systems
through explanations. Most articles dealing with explainability in artificial
intelligence provide methods that concern supervised learning and there are
very few articles dealing with this in the area of RL. The reasons for this are
the credit assignment problem, delayed rewards, and the inability to assume
that data is independently and identically distributed (i.i.d.). This position
paper attempts to give a systematic overview of existing methods in the
explainable RL area and propose a novel unified taxonomy, building and
expanding on the existing ones. The position section describes pragmatic
aspects of how explainability can be observed. The gap between the parties
receiving and generating the explanation is especially emphasized. To reduce
the gap and achieve honesty and truthfulness of explanations, we set up three
pillars: proactivity, risk attitudes, and epistemological constraints. To this
end, we illustrate our proposal on simple variants of the shortest path
problem.
",0
Bellman Residual Orthogonalization for Offline Reinforcement Learning,"Andrea Zanette, Martin J. Wainwright",2022-03-24T01:04:17Z,Reinforcement Learning,"  We propose and analyze a reinforcement learning principle that approximates
the Bellman equations by enforcing their validity only along an user-defined
space of test functions. Focusing on applications to model-free offline RL with
function approximation, we exploit this principle to derive confidence
intervals for off-policy evaluation, as well as to optimize over policies
within a prescribed policy class. We prove an oracle inequality on our policy
optimization procedure in terms of a trade-off between the value and
uncertainty of an arbitrary comparator policy. Different choices of test
function spaces allow us to tackle different problems within a common
framework. We characterize the loss of efficiency in moving from on-policy to
off-policy data using our procedures, and establish connections to
concentrability coefficients studied in past work. We examine in depth the
implementation of our methods with linear function approximation, and provide
theoretical guarantees with polynomial-time implementations even when Bellman
closure does not hold.
",0
MERLIN -- Malware Evasion with Reinforcement LearnINg,"Tony Quertier, Benjamin Marais, Stéphane Morucci, Bertrand Fournel",2022-03-24T10:58:47Z,Reinforcement Learning,"  In addition to signature-based and heuristics-based detection techniques,
machine learning (ML) is widely used to generalize to new, never-before-seen
malicious software (malware). However, it has been demonstrated that ML models
can be fooled by tricking the classifier into returning the incorrect label.
These studies, for instance, usually rely on a prediction score that is fragile
to gradient-based attacks. In the context of a more realistic situation where
an attacker has very little information about the outputs of a malware
detection engine, modest evasion rates are achieved. In this paper, we propose
a method using reinforcement learning with DQN and REINFORCE algorithms to
challenge two state-of-the-art ML-based detection engines (MalConv \& EMBER)
and a commercial AV classified by Gartner as a leader AV. Our method combines
several actions, modifying a Windows portable execution (PE) file without
breaking its functionalities. Our method also identifies which actions perform
better and compiles a detailed vulnerability report to help mitigate the
evasion. We demonstrate that REINFORCE achieves very good evasion rates even on
a commercial AV with limited available information.
",0
Optimizing Airborne Wind Energy with Reinforcement Learning,"N. Orzan, C. Leone, A. Mazzolini, J. Oyero, A. Celani",2022-03-27T10:28:16Z,Reinforcement Learning,"  Airborne Wind Energy is a lightweight technology that allows power extraction
from the wind using airborne devices such as kites and gliders, where the
airfoil orientation can be dynamically controlled in order to maximize
performance. The dynamical complexity of turbulent aerodynamics makes this
optimization problem unapproachable by conventional methods such as classical
control theory, which rely on accurate and tractable analytical models of the
dynamical system at hand. Here we propose to attack this problem through
Reinforcement Learning, a technique that -- by repeated trial-and-error
interactions with the environment -- learns to associate observations with
profitable actions without requiring prior knowledge of the system. We show
that in a simulated environment Reinforcement Learning finds an efficient way
to control a kite so that it can tow a vehicle for long distances. The
algorithm we use is based on a small set of intuitive observations and its
physically transparent interpretation allows to describe the approximately
optimal strategy as a simple list of manoeuvring instructions.
",0
Marginalized Operators for Off-policy Reinforcement Learning,"Yunhao Tang, Mark Rowland, Rémi Munos, Michal Valko",2022-03-30T09:59:59Z,Reinforcement Learning,"  In this work, we propose marginalized operators, a new class of off-policy
evaluation operators for reinforcement learning. Marginalized operators
strictly generalize generic multi-step operators, such as Retrace, as special
cases. Marginalized operators also suggest a form of sample-based estimates
with potential variance reduction, compared to sample-based estimates of the
original multi-step operators. We show that the estimates for marginalized
operators can be computed in a scalable way, which also generalizes prior
results on marginalized importance sampling as special cases. Finally, we
empirically demonstrate that marginalized operators provide performance gains
to off-policy evaluation and downstream policy optimization algorithms.
",0
Reinforcement Learning Guided by Provable Normative Compliance,Emery Neufeld,2022-03-30T13:10:55Z,Reinforcement Learning,"  Reinforcement learning (RL) has shown promise as a tool for engineering safe,
ethical, or legal behaviour in autonomous agents. Its use typically relies on
assigning punishments to state-action pairs that constitute unsafe or unethical
choices. Despite this assignment being a crucial step in this approach,
however, there has been limited discussion on generalizing the process of
selecting punishments and deciding where to apply them. In this paper, we adopt
an approach that leverages an existing framework -- the normative supervisor of
(Neufeld et al., 2021) -- during training. This normative supervisor is used to
dynamically translate states and the applicable normative system into
defeasible deontic logic theories, feed these theories to a theorem prover, and
use the conclusions derived to decide whether or not to assign a punishment to
the agent. We use multi-objective RL (MORL) to balance the ethical objective of
avoiding violations with a non-ethical objective; we will demonstrate that our
approach works for a multiplicity of MORL techniques, and show that it is
effective regardless of the magnitude of the punishment we assign.
",0
Factored Adaptation for Non-Stationary Reinforcement Learning,"Fan Feng, Biwei Huang, Kun Zhang, Sara Magliacane",2022-03-30T18:14:01Z,Reinforcement Learning,"  Dealing with non-stationarity in environments (e.g., in the transition
dynamics) and objectives (e.g., in the reward functions) is a challenging
problem that is crucial in real-world applications of reinforcement learning
(RL). While most current approaches model the changes as a single shared
embedding vector, we leverage insights from the recent causality literature to
model non-stationarity in terms of individual latent change factors, and causal
graphs across different environments. In particular, we propose Factored
Adaptation for Non-Stationary RL (FANS-RL), a factored adaption approach that
learns jointly both the causal structure in terms of a factored MDP, and a
factored representation of the individual time-varying change factors. We prove
that under standard assumptions, we can completely recover the causal graph
representing the factored transition and reward function, as well as a partial
structure between the individual change factors and the state components.
Through our general framework, we can consider general non-stationary scenarios
with different function types and changing frequency, including changes across
episodes and within episodes. Experimental results demonstrate that FANS-RL
outperforms existing approaches in terms of return, compactness of the latent
state representation, and robustness to varying degrees of non-stationarity.
",0
Building Decision Forest via Deep Reinforcement Learning,"Guixuan Wen, Kaigui Wu",2022-04-01T09:37:35Z,Reinforcement Learning,"  Ensemble learning methods whose base classifier is a decision tree usually
belong to the bagging or boosting. However, no previous work has ever built the
ensemble classifier by maximizing long-term returns to the best of our
knowledge. This paper proposes a decision forest building method called
MA-H-SAC-DF for binary classification via deep reinforcement learning. First,
the building process is modeled as a decentralized partial observable Markov
decision process, and a set of cooperative agents jointly constructs all base
classifiers. Second, the global state and local observations are defined based
on informations of the parent node and the current location. Last, the
state-of-the-art deep reinforcement method Hybrid SAC is extended to a
multi-agent system under the CTDE architecture to find an optimal decision
forest building policy. The experiments indicate that MA-H-SAC-DF has the same
performance as random forest, Adaboost, and GBDT on balanced datasets and
outperforms them on imbalanced datasets.
",0
Hierarchical Reinforcement Learning under Mixed Observability,"Hai Nguyen, Zhihan Yang, Andrea Baisero, Xiao Ma, Robert Platt, Christopher Amato",2022-04-02T16:41:17Z,Reinforcement Learning,"  The framework of mixed observable Markov decision processes (MOMDP) models
many robotic domains in which some state variables are fully observable while
others are not. In this work, we identify a significant subclass of MOMDPs
defined by how actions influence the fully observable components of the state
and how those, in turn, influence the partially observable components and the
rewards. This unique property allows for a two-level hierarchical approach we
call HIerarchical Reinforcement Learning under Mixed Observability (HILMO),
which restricts partial observability to the top level while the bottom level
remains fully observable, enabling higher learning efficiency. The top level
produces desired goals to be reached by the bottom level until the task is
solved. We further develop theoretical guarantees to show that our approach can
achieve optimal and quasi-optimal behavior under mild assumptions. Empirical
results on long-horizon continuous control tasks demonstrate the efficacy and
efficiency of our approach in terms of improved success rate, sample
efficiency, and wall-clock training time. We also deploy policies learned in
simulation on a real robot.
",4
Value Gradient weighted Model-Based Reinforcement Learning,"Claas Voelcker, Victor Liao, Animesh Garg, Amir-massoud Farahmand",2022-04-04T13:28:31Z,Reinforcement Learning,"  Model-based reinforcement learning (MBRL) is a sample efficient technique to
obtain control policies, yet unavoidable modeling errors often lead performance
deterioration. The model in MBRL is often solely fitted to reconstruct
dynamics, state observations in particular, while the impact of model error on
the policy is not captured by the training objective. This leads to a mismatch
between the intended goal of MBRL, enabling good policy and value learning, and
the target of the loss function employed in practice, future state prediction.
Naive intuition would suggest that value-aware model learning would fix this
problem and, indeed, several solutions to this objective mismatch problem have
been proposed based on theoretical analysis. However, they tend to be inferior
in practice to commonly used maximum likelihood (MLE) based approaches. In this
paper we propose the Value-gradient weighted Model Learning (VaGraM), a novel
method for value-aware model learning which improves the performance of MBRL in
challenging settings, such as small model capacity and the presence of
distracting state dimensions. We analyze both MLE and value-aware approaches
and demonstrate how they fail to account for exploration and the behavior of
function approximation when learning value-aware models and highlight the
additional goals that must be met to stabilize optimization in the deep
learning setting. We verify our analysis by showing that our loss function is
able to achieve high returns on the Mujoco benchmark suite while being more
robust than maximum likelihood based approaches.
",0
RL4ReAl: Reinforcement Learning for Register Allocation,"S. VenkataKeerthy, Siddharth Jain, Anilava Kundu, Rohit Aggarwal, Albert Cohen, Ramakrishna Upadrasta",2022-04-05T06:30:03Z,Reinforcement Learning,"  We aim to automate decades of research and experience in register allocation,
leveraging machine learning. We tackle this problem by embedding a multi-agent
reinforcement learning algorithm within LLVM, training it with the state of the
art techniques. We formalize the constraints that precisely define the problem
for a given instruction-set architecture, while ensuring that the generated
code preserves semantic correctness. We also develop a gRPC based framework
providing a modular and efficient compiler interface for training and
inference. Our approach is architecture independent: we show experimental
results targeting Intel x86 and ARM AArch64. Our results match or out-perform
the heavily tuned, production-grade register allocators of LLVM.
",0
Automating Reinforcement Learning with Example-based Resets,"Jigang Kim, J. hyeon Park, Daesol Cho, H. Jin Kim",2022-04-05T08:12:42Z,Reinforcement Learning,"  Deep reinforcement learning has enabled robots to learn motor skills from
environmental interactions with minimal to no prior knowledge. However,
existing reinforcement learning algorithms assume an episodic setting, in which
the agent resets to a fixed initial state distribution at the end of each
episode, to successfully train the agents from repeated trials. Such reset
mechanism, while trivial for simulated tasks, can be challenging to provide for
real-world robotics tasks. Resets in robotic systems often require extensive
human supervision and task-specific workarounds, which contradicts the goal of
autonomous robot learning. In this paper, we propose an extension to
conventional reinforcement learning towards greater autonomy by introducing an
additional agent that learns to reset in a self-supervised manner. The reset
agent preemptively triggers a reset to prevent manual resets and implicitly
imposes a curriculum for the forward agent. We apply our method to learn from
scratch on a suite of simulated and real-world continuous control tasks and
demonstrate that the reset agent successfully learns to reduce manual resets
whilst also allowing the forward policy to improve gradually over time.
",0
Automating Staged Rollout with Reinforcement Learning,"Shadow Pritchard, Vidhyashree Nagaraju, Lance Fiondella",2022-04-01T21:22:39Z,Reinforcement Learning,"  Staged rollout is a strategy of incrementally releasing software updates to
portions of the user population in order to accelerate defect discovery without
incurring catastrophic outcomes such as system wide outages. Some past studies
have examined how to quantify and automate staged rollout, but stop short of
simultaneously considering multiple product or process metrics explicitly. This
paper demonstrates the potential to automate staged rollout with
multi-objective reinforcement learning in order to dynamically balance
stakeholder needs such as time to deliver new features and downtime incurred by
failures due to latent defects.
",0
Federated Reinforcement Learning with Environment Heterogeneity,"Hao Jin, Yang Peng, Wenhao Yang, Shusen Wang, Zhihua Zhang",2022-04-06T07:21:00Z,Reinforcement Learning,"  We study a Federated Reinforcement Learning (FedRL) problem in which $n$
agents collaboratively learn a single policy without sharing the trajectories
they collected during agent-environment interaction. We stress the constraint
of environment heterogeneity, which means $n$ environments corresponding to
these $n$ agents have different state transitions. To obtain a value function
or a policy function which optimizes the overall performance in all
environments, we propose two federated RL algorithms, \texttt{QAvg} and
\texttt{PAvg}. We theoretically prove that these algorithms converge to
suboptimal solutions, while such suboptimality depends on how heterogeneous
these $n$ environments are. Moreover, we propose a heuristic that achieves
personalization by embedding the $n$ environments into $n$ vectors. The
personalization heuristic not only improves the training but also allows for
better generalization to new environments.
",50
Reinforcement Learning Agents in Colonel Blotto,Joseph Christian G. Noel,2022-04-04T16:18:01Z,Reinforcement Learning,"  Models and games are simplified representations of the world. There are many
different kinds of models, all differing in complexity and which aspect of the
world they allow us to further our understanding of. In this paper we focus on
a specific instance of agent-based models, which uses reinforcement learning
(RL) to train the agent how to act in its environment. Reinforcement learning
agents are usually also Markov processes, which is another type of model that
can be used. We test this reinforcement learning agent in a Colonel Blotto
environment1, and measure its performance against Random agents as its
opponent. We find that the RL agent handily beats a single opponent, and still
performs quite well when the number of opponents are increased. We also analyze
the RL agent and look at what strategies it has arrived by looking at the
actions that it has given the highest and lowest Q-values. Interestingly, the
optimal strategy for playing multiple opponents is almost the complete opposite
of the optimal strategy for playing a single opponent.
",0
Hardware Trojan Insertion Using Reinforcement Learning,"Amin Sarihi, Ahmad Patooghy, Peter Jamieson, Abdel-Hameed A. Badawy",2022-04-09T01:50:03Z,Reinforcement Learning,"  This paper utilizes Reinforcement Learning (RL) as a means to automate the
Hardware Trojan (HT) insertion process to eliminate the inherent human biases
that limit the development of robust HT detection methods. An RL agent explores
the design space and finds circuit locations that are best for keeping inserted
HTs hidden. To achieve this, a digital circuit is converted to an environment
in which an RL agent inserts HTs such that the cumulative reward is maximized.
Our toolset can insert combinational HTs into the ISCAS-85 benchmark suite with
variations in HT size and triggering conditions. Experimental results show that
the toolset achieves high input coverage rates (100\% in two benchmark
circuits) that confirms its effectiveness. Also, the inserted HTs have shown a
minimal footprint and rare activation probability.
",0
Dynamic Dialogue Policy for Continual Reinforcement Learning,"Christian Geishauser, Carel van Niekerk, Nurul Lubis, Michael Heck, Hsien-Chin Lin, Shutong Feng, Milica Gašić",2022-04-12T16:30:40Z,Reinforcement Learning,"  Continual learning is one of the key components of human learning and a
necessary requirement of artificial intelligence. As dialogue can potentially
span infinitely many topics and tasks, a task-oriented dialogue system must
have the capability to continually learn, dynamically adapting to new
challenges while preserving the knowledge it already acquired. Despite the
importance, continual reinforcement learning of the dialogue policy has
remained largely unaddressed. The lack of a framework with training protocols,
baseline models and suitable metrics, has so far hindered research in this
direction. In this work we fill precisely this gap, enabling research in
dialogue policy optimisation to go from static to dynamic learning. We provide
a continual learning algorithm, baseline architectures and metrics for
assessing continual learning models. Moreover, we propose the dynamic dialogue
policy transformer (DDPT), a novel dynamic architecture that can integrate new
knowledge seamlessly, is capable of handling large state spaces and obtains
significant zero-shot performance when being exposed to unseen domains, without
any growth in network parameter size.
",0
Reinforcement learning on graphs: A survey,"Mingshuo Nie, Dongming Chen, Dongqi Wang",2022-04-13T01:25:58Z,Reinforcement Learning,"  Graph mining tasks arise from many different application domains, ranging
from social networks, transportation to E-commerce, etc., which have been
receiving great attention from the theoretical and algorithmic design
communities in recent years, and there has been some pioneering work employing
the research-rich Reinforcement Learning (RL) techniques to address graph data
mining tasks. However, these graph mining methods and RL models are dispersed
in different research areas, which makes it hard to compare them. In this
survey, we provide a comprehensive overview of RL and graph mining methods and
generalize these methods to Graph Reinforcement Learning (GRL) as a unified
formulation. We further discuss the applications of GRL methods across various
domains and summarize the method descriptions, open-source codes, and benchmark
datasets of GRL methods. Furthermore, we propose important directions and
challenges to be solved in the future. As far as we know, this is the latest
work on a comprehensive survey of GRL, this work provides a global view and a
learning resource for scholars. In addition, we create an online open-source
for both interested scholars who want to enter this rapidly developing domain
and experts who would like to compare GRL methods.
",0
Optimizing Tensor Network Contraction Using Reinforcement Learning,"Eli A. Meirom, Haggai Maron, Shie Mannor, Gal Chechik",2022-04-18T21:45:13Z,Reinforcement Learning,"  Quantum Computing (QC) stands to revolutionize computing, but is currently
still limited. To develop and test quantum algorithms today, quantum circuits
are often simulated on classical computers. Simulating a complex quantum
circuit requires computing the contraction of a large network of tensors. The
order (path) of contraction can have a drastic effect on the computing cost,
but finding an efficient order is a challenging combinatorial optimization
problem.
  We propose a Reinforcement Learning (RL) approach combined with Graph Neural
Networks (GNN) to address the contraction ordering problem. The problem is
extremely challenging due to the huge search space, the heavy-tailed reward
distribution, and the challenging credit assignment. We show how a carefully
implemented RL-agent that uses a GNN as the basic policy construct can address
these challenges and obtain significant improvements over state-of-the-art
techniques in three varieties of circuits, including the largest scale networks
used in contemporary QC.
",11
Reinforcement Learning Generation of 4-Qubits Entangled States,"Sara Giordano, Miguel A. Martin-Delgado",2022-04-26T14:46:58Z,Reinforcement Learning,"  We have devised an artificial intelligence algorithm with machine
reinforcement learning (Q-learning) to construct remarkable entangled states
with 4 qubits. This way, the algorithm is able to generate representative
states for some of the 49 true SLOCC classes of the four-qubit entanglement
states. In particular, it is possible to reach at least one true SLOCC class
for each of the nine entanglement families. The quantum circuits synthesized by
the algorithm may be useful for the experimental realization of these important
classes of entangled states and to draw conclusions about the intrinsic
properties of our universe. We introduce a graphical tool called the state-link
graph (SLG) to represent the construction of the Quality matrix (Q-matrix) used
by the algorithm to build a given objective state belonging to the
corresponding entanglement class. This allows us to discover the necessary
connections between specific entanglement features and the role of certain
quantum gates that the algorithm needs to include in the quantum gate set of
actions. The quantum circuits found are optimal by construction with respect to
the quantum gate-set chosen. These SLGs make the algorithm simple, intuitive
and a useful resource for the automated construction of entangled states with a
low number of qubits.
",0
Network Topology Optimization via Deep Reinforcement Learning,"Zhuoran Li, Xing Wang, Ling Pan, Lin Zhu, Zhendong Wang, Junlan Feng, Chao Deng, Longbo Huang",2022-04-19T07:45:07Z,Reinforcement Learning,"  Topology impacts important network performance metrics, including link
utilization, throughput and latency, and is of central importance to network
operators. However, due to the combinatorial nature of network topology, it is
extremely difficult to obtain an optimal solution, especially since topology
planning in networks also often comes with management-specific constraints. As
a result, local optimization with hand-tuned heuristic methods from human
experts are often adopted in practice. Yet, heuristic methods cannot cover the
global topology design space while taking into account constraints, and cannot
guarantee to find good solutions.
  In this paper, we propose a novel deep reinforcement learning (DRL)
algorithm, called Advantage Actor Critic-Graph Searching (A2C-GS), for network
topology optimization. A2C-GS consists of three novel components, including a
verifier to validate the correctness of a generated network topology, a graph
neural network (GNN) to efficiently approximate topology rating, and a DRL
actor layer to conduct a topology search. A2C-GS can efficiently search over
large topology space and output topology with satisfying performance. We
conduct a case study based on a real network scenario, and our experimental
results demonstrate the superior performance of A2C-GS in terms of both
efficiency and performance.
",0
Deep-Attack over the Deep Reinforcement Learning,"Yang Li, Quan Pan, Erik Cambria",2022-05-02T10:58:19Z,Reinforcement Learning,"  Recent adversarial attack developments have made reinforcement learning more
vulnerable, and different approaches exist to deploy attacks against it, where
the key is how to choose the right timing of the attack. Some work tries to
design an attack evaluation function to select critical points that will be
attacked if the value is greater than a certain threshold. This approach makes
it difficult to find the right place to deploy an attack without considering
the long-term impact. In addition, there is a lack of appropriate indicators of
assessment during attacks. To make the attacks more intelligent as well as to
remedy the existing problems, we propose the reinforcement learning-based
attacking framework by considering the effectiveness and stealthy
spontaneously, while we also propose a new metric to evaluate the performance
of the attack model in these two aspects. Experimental results show the
effectiveness of our proposed model and the goodness of our proposed evaluation
metric. Furthermore, we validate the transferability of the model, and also its
robustness under the adversarial training.
",0
Exploration in Deep Reinforcement Learning: A Survey,"Pawel Ladosz, Lilian Weng, Minwoo Kim, Hyondong Oh",2022-05-02T12:03:44Z,Reinforcement Learning,"  This paper reviews exploration techniques in deep reinforcement learning.
Exploration techniques are of primary importance when solving sparse reward
problems. In sparse reward problems, the reward is rare, which means that the
agent will not find the reward often by acting randomly. In such a scenario, it
is challenging for reinforcement learning to learn rewards and actions
association. Thus more sophisticated exploration methods need to be devised.
This review provides a comprehensive overview of existing exploration
approaches, which are categorized based on the key contributions as follows
reward novel states, reward diverse behaviours, goal-based methods,
probabilistic methods, imitation-based methods, safe exploration and
random-based methods. Then, the unsolved challenges are discussed to provide
valuable future research directions. Finally, the approaches of different
categories are compared in terms of complexity, computational effort and
overall performance.
",0
Processing Network Controls via Deep Reinforcement Learning,Mark Gluzman,2022-05-01T04:34:21Z,Reinforcement Learning,"  Novel advanced policy gradient (APG) algorithms, such as proximal policy
optimization (PPO), trust region policy optimization, and their variations,
have become the dominant reinforcement learning (RL) algorithms because of
their ease of implementation and good practical performance. This dissertation
is concerned with theoretical justification and practical application of the
APG algorithms for solving processing network control optimization problems.
Processing network control problems are typically formulated as Markov decision
process (MDP) or semi-Markov decision process (SMDP) problems that have several
unconventional for RL features: infinite state spaces, unbounded costs,
long-run average cost objectives. Policy improvement bounds play a crucial role
in the theoretical justification of the APG algorithms. In this thesis we
refine existing bounds for MDPs with finite state spaces and prove novel policy
improvement bounds for classes of MDPs and SMDPs used to model processing
network operations. We consider two examples of processing network control
problems and customize the PPO algorithm to solve them. First, we consider
parallel-server and multiclass queueing networks controls. Second, we consider
the drivers repositioning problem in a ride-hailing service system. For both
examples the PPO algorithm with auxiliary modifications consistently generates
control policies that outperform state-of-art heuristics.
",0
Search-Based Testing of Reinforcement Learning,"Martin Tappler, Filip Cano Córdoba, Bernhard K. Aichernig, Bettina Könighofer",2022-05-07T12:40:45Z,Reinforcement Learning,"  Evaluation of deep reinforcement learning (RL) is inherently challenging.
Especially the opaqueness of learned policies and the stochastic nature of both
agents and environments make testing the behavior of deep RL agents difficult.
We present a search-based testing framework that enables a wide range of novel
analysis capabilities for evaluating the safety and performance of deep RL
agents. For safety testing, our framework utilizes a search algorithm that
searches for a reference trace that solves the RL task. The backtracking states
of the search, called boundary states, pose safety-critical situations. We
create safety test-suites that evaluate how well the RL agent escapes
safety-critical situations near these boundary states. For robust performance
testing, we create a diverse set of traces via fuzz testing. These fuzz traces
are used to bring the agent into a wide variety of potentially unknown states
from which the average performance of the agent is compared to the average
performance of the fuzz traces. We apply our search-based testing approach on
RL for Nintendo's Super Mario Bros.
",19
Modularity in NEAT Reinforcement Learning Networks,"Humphrey Munn, Marcus Gallagher",2022-05-13T05:18:18Z,Reinforcement Learning,"  Modularity is essential to many well-performing structured systems, as it is
a useful means of managing complexity [8]. An analysis of modularity in neural
networks produced by machine learning algorithms can offer valuable insight
into the workings of such algorithms and how modularity can be leveraged to
improve performance. However, this property is often overlooked in the
neuroevolutionary literature, so the modular nature of many learning algorithms
is unknown. This property was assessed on the popular algorithm ""NeuroEvolution
of Augmenting Topologies"" (NEAT) for standard simulation benchmark control
problems due to NEAT's ability to optimise network topology. This paper shows
that NEAT networks seem to rapidly increase in modularity over time with the
rate and convergence dependent on the problem. Interestingly, NEAT tends
towards increasingly modular networks even when network fitness converges. It
was shown that the ideal level of network modularity in the explored parameter
space is highly dependent on other network variables, dispelling theories that
modularity has a straightforward relationship to network performance. This is
further proven in this paper by demonstrating that rewarding modularity
directly did not improve fitness.
",0
Policy Gradient Method For Robust Reinforcement Learning,"Yue Wang, Shaofeng Zou",2022-05-15T17:35:17Z,Reinforcement Learning,"  This paper develops the first policy gradient method with global optimality
guarantee and complexity analysis for robust reinforcement learning under model
mismatch. Robust reinforcement learning is to learn a policy robust to model
mismatch between simulator and real environment. We first develop the robust
policy (sub-)gradient, which is applicable for any differentiable parametric
policy class. We show that the proposed robust policy gradient method converges
to the global optimum asymptotically under direct policy parameterization. We
further develop a smoothed robust policy gradient method and show that to
achieve an $\epsilon$-global optimum, the complexity is $\mathcal
O(\epsilon^{-3})$. We then extend our methodology to the general model-free
setting and design the robust actor-critic method with differentiable
parametric policy class and value function. We further characterize its
asymptotic convergence and sample complexity under the tabular setting.
Finally, we provide simulation results to demonstrate the robustness of our
methods.
",0
Rethinking Reinforcement Learning based Logic Synthesis,"Chao Wang, Chen Chen, Dong Li, Bin Wang",2022-05-16T12:15:32Z,Reinforcement Learning,"  Recently, reinforcement learning has been used to address logic synthesis by
formulating the operator sequence optimization problem as a Markov decision
process. However, through extensive experiments, we find out that the learned
policy makes decisions independent from the circuit features (i.e., states) and
yields an operator sequence that is permutation invariant to some extent in
terms of operators. Based on these findings, we develop a new RL-based method
that can automatically recognize critical operators and generate common
operator sequences generalizable to unseen circuits. Our algorithm is verified
on both the EPFL benchmark, a private dataset and a circuit at industrial
scale. Experimental results demonstrate that it achieves a good balance among
delay, area and runtime, and is practical for industrial usage.
",4
Attacking and Defending Deep Reinforcement Learning Policies,Chao Wang,2022-05-16T12:47:54Z,Reinforcement Learning,"  Recent studies have shown that deep reinforcement learning (DRL) policies are
vulnerable to adversarial attacks, which raise concerns about applications of
DRL to safety-critical systems. In this work, we adopt a principled way and
study the robustness of DRL policies to adversarial attacks from the
perspective of robust optimization. Within the framework of robust
optimization, optimal adversarial attacks are given by minimizing the expected
return of the policy, and correspondingly a good defense mechanism should be
realized by improving the worst-case performance of the policy. Considering
that attackers generally have no access to the training environment, we propose
a greedy attack algorithm, which tries to minimize the expected return of the
policy without interacting with the environment, and a defense algorithm, which
performs adversarial training in a max-min form. Experiments on Atari game
environments show that our attack algorithm is more effective and leads to
worse return of the policy than existing attack algorithms, and our defense
algorithm yields policies more robust than existing defense methods to a range
of adversarial attacks (including our proposed attack algorithm).
",0
The Primacy Bias in Deep Reinforcement Learning,"Evgenii Nikishin, Max Schwarzer, Pierluca D'Oro, Pierre-Luc Bacon, Aaron Courville",2022-05-16T16:48:36Z,Reinforcement Learning,"  This work identifies a common flaw of deep reinforcement learning (RL)
algorithms: a tendency to rely on early interactions and ignore useful evidence
encountered later. Because of training on progressively growing datasets, deep
RL agents incur a risk of overfitting to earlier experiences, negatively
affecting the rest of the learning process. Inspired by cognitive science, we
refer to this effect as the primacy bias. Through a series of experiments, we
dissect the algorithmic aspects of deep RL that exacerbate this bias. We then
propose a simple yet generally-applicable mechanism that tackles the primacy
bias by periodically resetting a part of the agent. We apply this mechanism to
algorithms in both discrete (Atari 100k) and continuous action (DeepMind
Control Suite) domains, consistently improving their performance.
",0
Moral reinforcement learning using actual causation,Tue Herlau,2022-05-17T09:25:51Z,Reinforcement Learning,"  Reinforcement learning systems will to a greater and greater extent make
decisions that significantly impact the well-being of humans, and it is
therefore essential that these systems make decisions that conform to our
expectations of morally good behavior. The morally good is often defined in
causal terms, as in whether one's actions have in fact caused a particular
outcome, and whether the outcome could have been anticipated. We propose an
online reinforcement learning method that learns a policy under the constraint
that the agent should not be the cause of harm. This is accomplished by
defining cause using the theory of actual causation and assigning blame to the
agent when its actions are the actual cause of an undesirable outcome. We
conduct experiments on a toy ethical dilemma in which a natural choice of
reward function leads to clearly undesirable behavior, but our method learns a
policy that avoids being the cause of harmful behavior, demonstrating the
soundness of our approach. Allowing an agent to learn while observing causal
moral distinctions such as blame, opens the possibility to learning policies
that better conform to our moral judgments.
",0
Data Valuation for Offline Reinforcement Learning,"Amir Abolfazli, Gregory Palmer, Daniel Kudenko",2022-05-19T13:21:40Z,Reinforcement Learning,"  The success of deep reinforcement learning (DRL) hinges on the availability
of training data, which is typically obtained via a large number of environment
interactions. In many real-world scenarios, costs and risks are associated with
gathering these data. The field of offline reinforcement learning addresses
these issues through outsourcing the collection of data to a domain expert or a
carefully monitored program and subsequently searching for a batch-constrained
optimal policy. With the emergence of data markets, an alternative to
constructing a dataset in-house is to purchase external data. However, while
state-of-the-art offline reinforcement learning approaches have shown a lot of
promise, they currently rely on carefully constructed datasets that are well
aligned with the intended target domains. This raises questions regarding the
transferability and robustness of an offline reinforcement learning agent
trained on externally acquired data. In this paper, we empirically evaluate the
ability of the current state-of-the-art offline reinforcement learning
approaches to coping with the source-target domain mismatch within two MuJoCo
environments, finding that current state-of-the-art offline reinforcement
learning algorithms underperform in the target domain. To address this, we
propose data valuation for offline reinforcement learning (DVORL), which allows
us to identify relevant and high-quality transitions, improving the performance
and transferability of policies learned by offline reinforcement learning
algorithms. The results show that our method outperforms offline reinforcement
learning baselines on two MuJoCo environments.
",0
ARLO: A Framework for Automated Reinforcement Learning,"Marco Mussi, Davide Lombarda, Alberto Maria Metelli, Francesco Trovò, Marcello Restelli",2022-05-20T19:16:04Z,Reinforcement Learning,"  Automated Reinforcement Learning (AutoRL) is a relatively new area of
research that is gaining increasing attention. The objective of AutoRL consists
in easing the employment of Reinforcement Learning (RL) techniques for the
broader public by alleviating some of its main challenges, including data
collection, algorithm selection, and hyper-parameter tuning. In this work, we
propose a general and flexible framework, namely ARLO: Automated Reinforcement
Learning Optimizer, to construct automated pipelines for AutoRL. Based on this,
we propose a pipeline for offline and one for online RL, discussing the
components, interaction, and highlighting the difference between the two
settings. Furthermore, we provide a Python implementation of such pipelines,
released as an open-source library. Our implementation has been tested on an
illustrative LQG domain and on classic MuJoCo environments, showing the ability
to reach competitive performances requiring limited human intervention. We also
showcase the full pipeline on a realistic dam environment, automatically
performing the feature selection and the model generation tasks.
",2
Cooperative Reinforcement Learning on Traffic Signal Control,"Chi-Chun Chao, Jun-Wei Hsieh, Bor-Shiun Wang",2022-05-23T13:25:15Z,Reinforcement Learning,"  Traffic signal control is a challenging real-world problem aiming to minimize
overall travel time by coordinating vehicle movements at road intersections.
Existing traffic signal control systems in use still rely heavily on
oversimplified information and rule-based methods. Specifically, the
periodicity of green/red light alternations can be considered as a prior for
better planning of each agent in policy optimization. To better learn such
adaptive and predictive priors, traditional
  RL-based methods can only return a fixed length from predefined action pool
with only local agents. If there is no cooperation between these agents, some
agents often make conflicts to other agents and thus decrease the whole
throughput. This paper proposes a cooperative, multi-objective architecture
with age-decaying weights to better estimate multiple reward terms for traffic
signal control optimization, which termed COoperative Multi-Objective
Multi-Agent Deep Deterministic Policy Gradient (COMMA-DDPG). Two types of
agents running to maximize rewards of different goals - one for local traffic
optimization at each intersection and the other for global traffic waiting time
optimization. The global agent is used to guide the local agents as a means for
aiding faster learning but not used in the inference phase. We also provide an
analysis of solution existence together with convergence proof for the proposed
RL optimization. Evaluation is performed using real-world traffic data
collected using traffic cameras from an Asian country. Our method can
effectively reduce the total delayed time by 60\%. Results demonstrate its
superiority when compared to SoTA methods.
",0
Multimodal Knowledge Alignment with Reinforcement Learning,"Youngjae Yu, Jiwan Chung, Heeseung Yun, Jack Hessel, JaeSung Park, Ximing Lu, Prithviraj Ammanabrolu, Rowan Zellers, Ronan Le Bras, Gunhee Kim, Yejin Choi",2022-05-25T10:12:17Z,Reinforcement Learning,"  Large language models readily adapt to novel settings, even without
task-specific training data. Can their zero-shot capacity be extended to
multimodal inputs? In this work, we propose ESPER which extends language-only
zero-shot models to unseen multimodal tasks, like image and audio captioning.
Our key novelty is to use reinforcement learning to align multimodal inputs to
language model generations without direct supervision: for example, in the
image case our reward optimization relies only on cosine similarity derived
from CLIP, and thus requires no additional explicitly paired (image, caption)
data. Because the parameters of the language model are left unchanged, the
model maintains its capacity for zero-shot generalization. Experiments
demonstrate that ESPER outperforms baselines and prior work on a variety of
zero-shot tasks; these include a new benchmark we collect+release, ESP dataset,
which tasks models with generating several diversely-styled captions for each
image.
",32
Impartial Games: A Challenge for Reinforcement Learning,"Bei Zhou, Søren Riis",2022-05-25T14:02:02Z,Reinforcement Learning,"  While AlphaZero-style reinforcement learning (RL) algorithms excel in various
board games, in this paper we show that they face challenges on impartial games
where players share pieces. We present a concrete example of a game - namely
the children's game of Nim - and other impartial games that seem to be a
stumbling block for AlphaZero-style and similar self-play reinforcement
learning algorithms.
  Our work is built on the challenges posed by the intricacies of data
distribution on the ability of neural networks to learn parity functions,
exacerbated by the noisy labels issue. Our findings are consistent with recent
studies showing that AlphaZero-style algorithms are vulnerable to adversarial
attacks and adversarial perturbations, showing the difficulty of learning to
master the games in all legal states.
  We show that Nim can be learned on small boards, but the learning progress of
AlphaZero-style algorithms dramatically slows down when the board size
increases. Intuitively, the difference between impartial games like Nim and
partisan games like Chess and Go can be explained by the fact that if a small
part of the board is covered for impartial games it is typically not possible
to predict whether the position is won or lost as there is often zero
correlation between the visible part of a partly blanked-out position and its
correct evaluation. This situation starkly contrasts partisan games where a
partly blanked-out board position typically provides abundant or at least
non-trifle information about the value of the fully uncovered position.
",0
Constrained Reinforcement Learning for Short Video Recommendation,"Qingpeng Cai, Ruohan Zhan, Chi Zhang, Jie Zheng, Guangwei Ding, Pinghua Gong, Dong Zheng, Peng Jiang",2022-05-26T09:36:20Z,Reinforcement Learning,"  The wide popularity of short videos on social media poses new opportunities
and challenges to optimize recommender systems on the video-sharing platforms.
Users provide complex and multi-faceted responses towards recommendations,
including watch time and various types of interactions with videos. As a
result, established recommendation algorithms that concern a single objective
are not adequate to meet this new demand of optimizing comprehensive user
experiences. In this paper, we formulate the problem of short video
recommendation as a constrained Markov Decision Process (MDP), where platforms
want to optimize the main goal of user watch time in long term, with the
constraint of accommodating the auxiliary responses of user interactions such
as sharing/downloading videos.
  To solve the constrained MDP, we propose a two-stage reinforcement learning
approach based on actor-critic framework. At stage one, we learn individual
policies to optimize each auxiliary response. At stage two, we learn a policy
to (i) optimize the main response and (ii) stay close to policies learned at
the first stage, which effectively guarantees the performance of this main
policy on the auxiliaries. Through extensive simulations, we demonstrate
effectiveness of our approach over alternatives in both optimizing the main
goal as well as balancing the others. We further show the advantage of our
approach in live experiments of short video recommendations, where it
significantly outperforms other baselines in terms of watch time and
interactions from video views. Our approach has been fully launched in the
production system to optimize user experiences on the platform.
",0
Off-Beat Multi-Agent Reinforcement Learning,"Wei Qiu, Weixun Wang, Rundong Wang, Bo An, Yujing Hu, Svetlana Obraztsova, Zinovi Rabinovich, Jianye Hao, Yingfeng Chen, Changjie Fan",2022-05-27T02:21:04Z,Reinforcement Learning,"  We investigate model-free multi-agent reinforcement learning (MARL) in
environments where off-beat actions are prevalent, i.e., all actions have
pre-set execution durations. During execution durations, the environment
changes are influenced by, but not synchronised with, action execution. Such a
setting is ubiquitous in many real-world problems. However, most MARL methods
assume actions are executed immediately after inference, which is often
unrealistic and can lead to catastrophic failure for multi-agent coordination
with off-beat actions. In order to fill this gap, we develop an algorithmic
framework for MARL with off-beat actions. We then propose a novel episodic
memory, LeGEM, for model-free MARL algorithms. LeGEM builds agents' episodic
memories by utilizing agents' individual experiences. It boosts multi-agent
learning by addressing the challenging temporal credit assignment problem
raised by the off-beat actions via our novel reward redistribution scheme,
alleviating the issue of non-Markovian reward. We evaluate LeGEM on various
multi-agent scenarios with off-beat actions, including Stag-Hunter Game, Quarry
Game, Afforestation Game, and StarCraft II micromanagement tasks. Empirical
results show that LeGEM significantly boosts multi-agent coordination and
achieves leading performance and improved sample efficiency.
",0
Scalable Multi-Agent Model-Based Reinforcement Learning,"Vladimir Egorov, Aleksei Shpilman",2022-05-25T08:35:00Z,Reinforcement Learning,"  Recent Multi-Agent Reinforcement Learning (MARL) literature has been largely
focused on Centralized Training with Decentralized Execution (CTDE) paradigm.
CTDE has been a dominant approach for both cooperative and mixed environments
due to its capability to efficiently train decentralized policies. While in
mixed environments full autonomy of the agents can be a desirable outcome,
cooperative environments allow agents to share information to facilitate
coordination. Approaches that leverage this technique are usually referred as
communication methods, as full autonomy of agents is compromised for better
performance. Although communication approaches have shown impressive results,
they do not fully leverage this additional information during training phase.
In this paper, we propose a new method called MAMBA which utilizes Model-Based
Reinforcement Learning (MBRL) to further leverage centralized training in
cooperative environments. We argue that communication between agents is enough
to sustain a world model for each agent during execution phase while imaginary
rollouts can be used for training, removing the necessity to interact with the
environment. These properties yield sample efficient algorithm that can scale
gracefully with the number of agents. We empirically confirm that MAMBA
achieves good performance while reducing the number of interactions with the
environment up to an orders of magnitude compared to Model-Free
state-of-the-art approaches in challenging domains of SMAC and Flatland.
",0
Optimizing measurement-based cooling by reinforcement learning,"Jia-shun Yan, Jun Jing",2022-06-01T06:07:10Z,Reinforcement Learning,"  Conditional cooling-by-measurement holds a significant advantage over its
unconditional (nonselective) counterpart in the average-population-reduction
rate. However, it has a clear weakness with respect to the limited success
probability of finding the detector in the measured state. In this work, we
propose an optimized architecture to cool down a target resonator, which is
initialized as a thermal state, using an interpolation of conditional and
unconditional measurement strategies. An optimal measurement-interval
$\tau_{\rm opt}^u$ for unconditional measurement is analytically derived for
the first time, which is inversely proportional to the collective dominant Rabi
frequency $\Omega_d$ as a function of the resonator's population in the end of
the last round. A cooling algorithm under global optimization by the
reinforcement learning results in the maximum value for the cooperative cooling
performance, an indicator to measure the comprehensive cooling efficiency for
arbitrary cooling-by-measurement architecture. In particular, the average
population of the target resonator under only $16$ rounds of measurements can
be reduced by four orders in magnitude with a success probability about $30\%$.
",3
Offline Reinforcement Learning with Differential Privacy,"Dan Qiao, Yu-Xiang Wang",2022-06-02T00:45:04Z,Reinforcement Learning,"  The offline reinforcement learning (RL) problem is often motivated by the
need to learn data-driven decision policies in financial, legal and healthcare
applications. However, the learned policy could retain sensitive information of
individuals in the training data (e.g., treatment and outcome of patients),
thus susceptible to various privacy risks. We design offline RL algorithms with
differential privacy guarantees which provably prevent such risks. These
algorithms also enjoy strong instance-dependent learning bounds under both
tabular and linear Markov decision process (MDP) settings. Our theory and
simulation suggest that the privacy guarantee comes at (almost) no drop in
utility comparing to the non-private counterpart for a medium-size dataset.
",15
Equivariant Reinforcement Learning for Quadrotor UAV,"Beomyeol Yu, Taeyoung Lee",2022-06-02T18:17:29Z,Reinforcement Learning,"  This paper presents an equivariant reinforcement learning framework for
quadrotor unmanned aerial vehicles. Successful training of reinforcement
learning often requires numerous interactions with the environments, which
hinders its applicability especially when the available computational resources
are limited, or when there is no reliable simulation model. We identified an
equivariance property of the quadrotor dynamics such that the dimension of the
state required in the training is reduced by one, thereby improving the
sampling efficiency of reinforcement learning substantially. This is
illustrated by numerical examples with popular reinforcement learning
techniques of TD3 and SAC.
",0
Reinforcement Learning with Neural Radiance Fields,"Danny Driess, Ingmar Schubert, Pete Florence, Yunzhu Li, Marc Toussaint",2022-06-03T15:22:08Z,Reinforcement Learning,"  It is a long-standing problem to find effective representations for training
reinforcement learning (RL) agents. This paper demonstrates that learning state
representations with supervision from Neural Radiance Fields (NeRFs) can
improve the performance of RL compared to other learned representations or even
low-dimensional, hand-engineered state information. Specifically, we propose to
train an encoder that maps multiple image observations to a latent space
describing the objects in the scene. The decoder built from a
latent-conditioned NeRF serves as the supervision signal to learn the latent
space. An RL algorithm then operates on the learned latent space as its state
representation. We call this NeRF-RL. Our experiments indicate that NeRF as
supervision leads to a latent space better suited for the downstream RL tasks
involving robotic object manipulations like hanging mugs on hooks, pushing
objects, or opening doors. Video: https://dannydriess.github.io/nerf-rl
",0
Learning Dynamics and Generalization in Reinforcement Learning,"Clare Lyle, Mark Rowland, Will Dabney, Marta Kwiatkowska, Yarin Gal",2022-06-05T08:49:16Z,Reinforcement Learning,"  Solving a reinforcement learning (RL) problem poses two competing challenges:
fitting a potentially discontinuous value function, and generalizing well to
new observations. In this paper, we analyze the learning dynamics of temporal
difference algorithms to gain novel insight into the tension between these two
objectives. We show theoretically that temporal difference learning encourages
agents to fit non-smooth components of the value function early in training,
and at the same time induces the second-order effect of discouraging
generalization. We corroborate these findings in deep RL agents trained on a
range of environments, finding that neural networks trained using temporal
difference algorithms on dense reward tasks exhibit weaker generalization
between states than randomly initialized networks and networks trained with
policy gradient methods. Finally, we investigate how post-training policy
distillation may avoid this pitfall, and show that this approach improves
generalization to novel environments in the ProcGen suite and improves
robustness to input perturbations.
",0
Variational Meta Reinforcement Learning for Social Robotics,"Anand Ballou, Xavier Alameda-Pineda, Chris Reinke",2022-06-07T12:08:59Z,Reinforcement Learning,"  With the increasing presence of robots in our every-day environments,
improving their social skills is of utmost importance. Nonetheless, social
robotics still faces many challenges. One bottleneck is that robotic behaviors
need to be often adapted as social norms depend strongly on the environment.
For example, a robot should navigate more carefully around patients in a
hospital compared to workers in an office. In this work, we investigate
meta-reinforcement learning (meta-RL) as a potential solution. Here, robot
behaviors are learned via reinforcement learning where a reward function needs
to be chosen so that the robot learns an appropriate behavior for a given
environment. We propose to use a variational meta-RL procedure that quickly
adapts the robots' behavior to new reward functions. As a result, given a new
environment different reward functions can be quickly evaluated and an
appropriate one selected. The procedure learns a vectorized representation for
reward functions and a meta-policy that can be conditioned on such a
representation. Given observations from a new reward function, the procedure
identifies its representation and conditions the meta-policy to it. While
investigating the procedures' capabilities, we realized that it suffers from
posterior collapse where only a subset of the dimensions in the representation
encode useful information resulting in a reduced performance. Our second
contribution, a radial basis function (RBF) layer, partially mitigates this
negative effect. The RBF layer lifts the representation to a higher dimensional
space, which is more easily exploitable for the meta-policy. We demonstrate the
interest of the RBF layer and the usage of meta-RL for social robotics on four
robotic simulation tasks.
",0
Regret Bounds for Information-Directed Reinforcement Learning,"Botao Hao, Tor Lattimore",2022-06-09T17:36:17Z,Reinforcement Learning,"  Information-directed sampling (IDS) has revealed its potential as a
data-efficient algorithm for reinforcement learning (RL). However, theoretical
understanding of IDS for Markov Decision Processes (MDPs) is still limited. We
develop novel information-theoretic tools to bound the information ratio and
cumulative information gain about the learning target. Our theoretical results
shed light on the importance of choosing the learning target such that the
practitioners can balance the computation and regret bounds. As a consequence,
we derive prior-free Bayesian regret bounds for vanilla-IDS which learns the
whole environment under tabular finite-horizon MDPs. In addition, we propose a
computationally-efficient regularized-IDS that maximizes an additive form
rather than the ratio form and show that it enjoys the same regret bound as
vanilla-IDS. With the aid of rate-distortion theory, we improve the regret
bound by learning a surrogate, less informative environment. Furthermore, we
extend our analysis to linear MDPs and prove similar regret bounds for Thompson
sampling as a by-product.
",0
Mildly Conservative Q-Learning for Offline Reinforcement Learning,"Jiafei Lyu, Xiaoteng Ma, Xiu Li, Zongqing Lu",2022-06-09T19:44:35Z,Reinforcement Learning,"  Offline reinforcement learning (RL) defines the task of learning from a
static logged dataset without continually interacting with the environment. The
distribution shift between the learned policy and the behavior policy makes it
necessary for the value function to stay conservative such that
out-of-distribution (OOD) actions will not be severely overestimated. However,
existing approaches, penalizing the unseen actions or regularizing with the
behavior policy, are too pessimistic, which suppresses the generalization of
the value function and hinders the performance improvement. This paper explores
mild but enough conservatism for offline learning while not harming
generalization. We propose Mildly Conservative Q-learning (MCQ), where OOD
actions are actively trained by assigning them proper pseudo Q values. We
theoretically show that MCQ induces a policy that behaves at least as well as
the behavior policy and no erroneous overestimation will occur for OOD actions.
Experimental results on the D4RL benchmarks demonstrate that MCQ achieves
remarkable performance compared with prior work. Furthermore, MCQ shows
superior generalization ability when transferring from offline to online, and
significantly outperforms baselines. Our code is publicly available at
https://github.com/dmksjfl/MCQ.
",0
Multifidelity Reinforcement Learning with Control Variates,"Sami Khairy, Prasanna Balaprakash",2022-06-10T15:01:37Z,Reinforcement Learning,"  In many computational science and engineering applications, the output of a
system of interest corresponding to a given input can be queried at different
levels of fidelity with different costs. Typically, low-fidelity data is cheap
and abundant, while high-fidelity data is expensive and scarce. In this work we
study the reinforcement learning (RL) problem in the presence of multiple
environments with different levels of fidelity for a given control task. We
focus on improving the RL agent's performance with multifidelity data.
Specifically, a multifidelity estimator that exploits the cross-correlations
between the low- and high-fidelity returns is proposed to reduce the variance
in the estimation of the state-action value function. The proposed estimator,
which is based on the method of control variates, is used to design a
multifidelity Monte Carlo RL (MFMCRL) algorithm that improves the learning of
the agent in the high-fidelity environment. The impacts of variance reduction
on policy evaluation and policy improvement are theoretically analyzed by using
probability bounds. Our theoretical analysis and numerical experiments
demonstrate that for a finite budget of high-fidelity data samples, our
proposed MFMCRL agent attains superior performance compared with that of a
standard RL agent that uses only the high-fidelity environment data for
learning the optimal policy.
",0
Large-Scale Retrieval for Reinforcement Learning,"Peter C. Humphreys, Arthur Guez, Olivier Tieleman, Laurent Sifre, Théophane Weber, Timothy Lillicrap",2022-06-10T18:25:30Z,Reinforcement Learning,"  Effective decision making involves flexibly relating past experiences and
relevant contextual information to a novel situation. In deep reinforcement
learning (RL), the dominant paradigm is for an agent to amortise information
that helps decision making into its network weights via gradient descent on
training losses. Here, we pursue an alternative approach in which agents can
utilise large-scale context sensitive database lookups to support their
parametric computations. This allows agents to directly learn in an end-to-end
manner to utilise relevant information to inform their outputs. In addition,
new information can be attended to by the agent, without retraining, by simply
augmenting the retrieval dataset. We study this approach for offline RL in 9x9
Go, a challenging game for which the vast combinatorial state space privileges
generalisation over direct matching to past experiences. We leverage fast,
approximate nearest neighbor techniques in order to retrieve relevant data from
a set of tens of millions of expert demonstration states. Attending to this
information provides a significant boost to prediction accuracy and game-play
performance over simply using these demonstrations as training trajectories,
providing a compelling demonstration of the value of large-scale retrieval in
offline RL agents.
",23
Contrastive Learning as Goal-Conditioned Reinforcement Learning,"Benjamin Eysenbach, Tianjun Zhang, Ruslan Salakhutdinov, Sergey Levine",2022-06-15T14:34:15Z,Reinforcement Learning,"  In reinforcement learning (RL), it is easier to solve a task if given a good
representation. While deep RL should automatically acquire such good
representations, prior work often finds that learning representations in an
end-to-end fashion is unstable and instead equip RL algorithms with additional
representation learning parts (e.g., auxiliary losses, data augmentation). How
can we design RL algorithms that directly acquire good representations? In this
paper, instead of adding representation learning parts to an existing RL
algorithm, we show (contrastive) representation learning methods can be cast as
RL algorithms in their own right. To do this, we build upon prior work and
apply contrastive representation learning to action-labeled trajectories, in
such a way that the (inner product of) learned representations exactly
corresponds to a goal-conditioned value function. We use this idea to
reinterpret a prior RL method as performing contrastive learning, and then use
the idea to propose a much simpler method that achieves similar performance.
Across a range of goal-conditioned RL tasks, we demonstrate that contrastive RL
methods achieve higher success rates than prior non-contrastive methods,
including in the offline RL setting. We also show that contrastive RL
outperforms prior methods on image-based tasks, without using data augmentation
or auxiliary objectives.
",106
Bootstrapped Transformer for Offline Reinforcement Learning,"Kerong Wang, Hanye Zhao, Xufang Luo, Kan Ren, Weinan Zhang, Dongsheng Li",2022-06-17T05:57:47Z,Reinforcement Learning,"  Offline reinforcement learning (RL) aims at learning policies from previously
collected static trajectory data without interacting with the real environment.
Recent works provide a novel perspective by viewing offline RL as a generic
sequence generation problem, adopting sequence models such as Transformer
architecture to model distributions over trajectories, and repurposing beam
search as a planning algorithm. However, the training datasets utilized in
general offline RL tasks are quite limited and often suffer from insufficient
distribution coverage, which could be harmful to training sequence generation
models yet has not drawn enough attention in the previous works. In this paper,
we propose a novel algorithm named Bootstrapped Transformer, which incorporates
the idea of bootstrapping and leverages the learned model to self-generate more
offline data to further boost the sequence model training. We conduct extensive
experiments on two offline RL benchmarks and demonstrate that our model can
largely remedy the existing offline RL training limitations and beat other
strong baseline methods. We also analyze the generated pseudo data and the
revealed characteristics may shed some light on offline RL training. The codes
are available at https://seqml.github.io/bootorl.
",0
A Survey on Model-based Reinforcement Learning,"Fan-Ming Luo, Tian Xu, Hang Lai, Xiong-Hui Chen, Weinan Zhang, Yang Yu",2022-06-19T05:28:03Z,Reinforcement Learning,"  Reinforcement learning (RL) solves sequential decision-making problems via a
trial-and-error process interacting with the environment. While RL achieves
outstanding success in playing complex video games that allow huge
trial-and-error, making errors is always undesired in the real world. To
improve the sample efficiency and thus reduce the errors, model-based
reinforcement learning (MBRL) is believed to be a promising direction, which
builds environment models in which the trial-and-errors can take place without
real costs. In this survey, we take a review of MBRL with a focus on the recent
progress in deep RL. For non-tabular environments, there is always a
generalization error between the learned environment model and the real
environment. As such, it is of great importance to analyze the discrepancy
between policy training in the environment model and that in the real
environment, which in turn guides the algorithm design for better model
learning, model usage, and policy training. Besides, we also discuss the recent
advances of model-based techniques in other forms of RL, including offline RL,
goal-conditioned RL, multi-agent RL, and meta-RL. Moreover, we discuss the
applicability and advantages of MBRL in real-world tasks. Finally, we end this
survey by discussing the promising prospects for the future development of
MBRL. We think that MBRL has great potential and advantages in real-world
applications that were overlooked, and we hope this survey could attract more
research on MBRL.
",0
Benchmarking Constraint Inference in Inverse Reinforcement Learning,"Guiliang Liu, Yudong Luo, Ashish Gaurav, Kasra Rezaee, Pascal Poupart",2022-06-20T09:22:20Z,Reinforcement Learning,"  When deploying Reinforcement Learning (RL) agents into a physical system, we
must ensure that these agents are well aware of the underlying constraints. In
many real-world problems, however, the constraints are often hard to specify
mathematically and unknown to the RL agents. To tackle these issues, Inverse
Constrained Reinforcement Learning (ICRL) empirically estimates constraints
from expert demonstrations. As an emerging research topic, ICRL does not have
common benchmarks, and previous works tested algorithms under hand-crafted
environments with manually-generated expert demonstrations. In this paper, we
construct an ICRL benchmark in the context of RL application domains, including
robot control, and autonomous driving. For each environment, we design relevant
constraints and train expert agents to generate demonstration data. Besides,
unlike existing baselines that learn a deterministic constraint, we propose a
variational ICRL method to model a posterior distribution of candidate
constraints. We conduct extensive experiments on these algorithms under our
benchmark and show how they can facilitate studying important research
challenges for ICRL. The benchmark, including the instructions for reproducing
ICRL algorithms, is available at
https://github.com/Guiliang/ICRL-benchmarks-public.
",0
Multi-Agent Car Parking using Reinforcement Learning,Omar Tanner,2022-06-22T16:50:04Z,Reinforcement Learning,"  As the industry of autonomous driving grows, so does the potential
interaction of groups of autonomous cars. Combined with the advancement of
Artificial Intelligence and simulation, such groups can be simulated, and
safety-critical models can be learned controlling the cars within. This study
applies reinforcement learning to the problem of multi-agent car parking, where
groups of cars aim to efficiently park themselves, while remaining safe and
rational. Utilising robust tools and machine learning frameworks, we design and
implement a flexible car parking environment in the form of a Markov decision
process with independent learners, exploiting multi-agent communication. We
implement a suite of tools to perform experiments at scale, obtaining models
parking up to 7 cars with over a 98.1% success rate, significantly beating
existing single-agent models. We also obtain several results relating to
competitive and collaborative behaviours exhibited by the cars in our
environment, with varying densities and levels of communication. Notably, we
discover a form of collaboration that cannot arise without competition, and a
'leaky' form of collaboration whereby agents collaborate without sufficient
state. Such work has numerous potential applications in the autonomous driving
and fleet management industries, and provides several useful techniques and
benchmarks for the application of reinforcement learning to multi-agent car
parking.
",0
Risk Perspective Exploration in Distributional Reinforcement Learning,"Jihwan Oh, Joonkee Kim, Se-Young Yun",2022-06-28T17:37:34Z,Reinforcement Learning,"  Distributional reinforcement learning demonstrates state-of-the-art
performance in continuous and discrete control settings with the features of
variance and risk, which can be used to explore. However, the exploration
method employing the risk property is hard to find, although numerous
exploration methods in Distributional RL employ the variance of return
distribution per action. In this paper, we present risk scheduling approaches
that explore risk levels and optimistic behaviors from a risk perspective. We
demonstrate the performance enhancement of the DMIX algorithm using risk
scheduling in a multi-agent setting with comprehensive experiments.
",3
Deep Reinforcement Learning with Swin Transformers,"Li Meng, Morten Goodwin, Anis Yazidi, Paal Engelstad",2022-06-30T13:20:48Z,Reinforcement Learning,"  Transformers are neural network models that utilize multiple layers of
self-attention heads and have exhibited enormous potential in natural language
processing tasks. Meanwhile, there have been efforts to adapt transformers to
visual tasks of machine learning, including Vision Transformers and Swin
Transformers. Although some researchers use Vision Transformers for
reinforcement learning tasks, their experiments remain at a small scale due to
the high computational cost. This article presents the first online
reinforcement learning scheme that is based on Swin Transformers: Swin DQN. In
contrast to existing research, our novel approach demonstrate the superior
performance with experiments on 49 games in the Arcade Learning Environment.
The results show that our approach achieves significantly higher maximal
evaluation scores than the baseline method in 45 of all the 49 games (92%), and
higher mean evaluation scores than the baseline method in 40 of all the 49
games (82%).
",0
Modular Lifelong Reinforcement Learning via Neural Composition,"Jorge A. Mendez, Harm van Seijen, Eric Eaton",2022-07-01T13:48:29Z,Reinforcement Learning,"  Humans commonly solve complex problems by decomposing them into easier
subproblems and then combining the subproblem solutions. This type of
compositional reasoning permits reuse of the subproblem solutions when tackling
future tasks that share part of the underlying compositional structure. In a
continual or lifelong reinforcement learning (RL) setting, this ability to
decompose knowledge into reusable components would enable agents to quickly
learn new RL tasks by leveraging accumulated compositional structures. We
explore a particular form of composition based on neural modules and present a
set of RL problems that intuitively admit compositional solutions. Empirically,
we demonstrate that neural composition indeed captures the underlying structure
of this space of problems. We further propose a compositional lifelong RL
method that leverages accumulated neural components to accelerate the learning
of future tasks while retaining performance on previous tasks via off-line RL
over replayed experiences.
",0
Safe Reinforcement Learning via Confidence-Based Filters,"Sebastian Curi, Armin Lederer, Sandra Hirche, Andreas Krause",2022-07-04T11:43:23Z,Reinforcement Learning,"  Ensuring safety is a crucial challenge when deploying reinforcement learning
(RL) to real-world systems. We develop confidence-based safety filters, a
control-theoretic approach for certifying state safety constraints for nominal
policies learned via standard RL techniques, based on probabilistic dynamics
models. Our approach is based on a reformulation of state constraints in terms
of cost functions, reducing safety verification to a standard RL task. By
exploiting the concept of hallucinating inputs, we extend this formulation to
determine a ""backup"" policy that is safe for the unknown system with high
probability. Finally, the nominal policy is minimally adjusted at every time
step during a roll-out towards the backup policy, such that safe recovery can
be guaranteed afterwards. We provide formal safety guarantees, and empirically
demonstrate the effectiveness of our approach.
",0
CompoSuite: A Compositional Reinforcement Learning Benchmark,"Jorge A. Mendez, Marcel Hussing, Meghna Gummadi, Eric Eaton",2022-07-08T22:01:52Z,Reinforcement Learning,"  We present CompoSuite, an open-source simulated robotic manipulation
benchmark for compositional multi-task reinforcement learning (RL). Each
CompoSuite task requires a particular robot arm to manipulate one individual
object to achieve a task objective while avoiding an obstacle. This
compositional definition of the tasks endows CompoSuite with two remarkable
properties. First, varying the robot/object/objective/obstacle elements leads
to hundreds of RL tasks, each of which requires a meaningfully different
behavior. Second, RL approaches can be evaluated specifically for their ability
to learn the compositional structure of the tasks. This latter capability to
functionally decompose problems would enable intelligent agents to identify and
exploit commonalities between learning tasks to handle large varieties of
highly diverse problems. We benchmark existing single-task, multi-task, and
compositional learning algorithms on various training settings, and assess
their capability to compositionally generalize to unseen tasks. Our evaluation
exposes the shortcomings of existing RL approaches with respect to
compositionality and opens new avenues for investigation.
",0
PAC Reinforcement Learning for Predictive State Representations,"Wenhao Zhan, Masatoshi Uehara, Wen Sun, Jason D. Lee",2022-07-12T17:57:17Z,Reinforcement Learning,"  In this paper we study online Reinforcement Learning (RL) in partially
observable dynamical systems. We focus on the Predictive State Representations
(PSRs) model, which is an expressive model that captures other well-known
models such as Partially Observable Markov Decision Processes (POMDP). PSR
represents the states using a set of predictions of future observations and is
defined entirely using observable quantities. We develop a novel model-based
algorithm for PSRs that can learn a near optimal policy in sample complexity
scaling polynomially with respect to all the relevant parameters of the
systems. Our algorithm naturally works with function approximation to extend to
systems with potentially large state and observation spaces. We show that given
a realizable model class, the sample complexity of learning the near optimal
policy only scales polynomially with respect to the statistical complexity of
the model class, without any explicit polynomial dependence on the size of the
state and observation spaces. Notably, our work is the first work that shows
polynomial sample complexities to compete with the globally optimal policy in
PSRs. Finally, we demonstrate how our general theorem can be directly used to
derive sample complexity bounds for special models including $m$-step weakly
revealing and $m$-step decodable tabular POMDPs, POMDPs with low-rank latent
transition, and POMDPs with linear emission and latent transition.
",0
GriddlyJS: A Web IDE for Reinforcement Learning,"Christopher Bamford, Minqi Jiang, Mikayel Samvelyan, Tim Rocktäschel",2022-07-13T10:26:38Z,Reinforcement Learning,"  Progress in reinforcement learning (RL) research is often driven by the
design of new, challenging environments -- a costly undertaking requiring
skills orthogonal to that of a typical machine learning researcher. The
complexity of environment development has only increased with the rise of
procedural-content generation (PCG) as the prevailing paradigm for producing
varied environments capable of testing the robustness and generalization of RL
agents. Moreover, existing environments often require complex build processes,
making reproducing results difficult. To address these issues, we introduce
GriddlyJS, a web-based Integrated Development Environment (IDE) based on the
Griddly engine. GriddlyJS allows researchers to visually design and debug
arbitrary, complex PCG grid-world environments using a convenient graphical
interface, as well as visualize, evaluate, and record the performance of
trained agent models. By connecting the RL workflow to the advanced
functionality enabled by modern web standards, GriddlyJS allows publishing
interactive agent-environment demos that reproduce experimental results
directly to the web. To demonstrate the versatility of GriddlyJS, we use it to
quickly develop a complex compositional puzzle-solving environment alongside
arbitrary human-designed environment configurations and their solutions for use
in automatic curriculum learning and offline RL. The GriddlyJS IDE is open
source and freely available at https://griddly.ai.
",0
Skill-based Model-based Reinforcement Learning,"Lucy Xiaoyang Shi, Joseph J. Lim, Youngwoon Lee",2022-07-15T16:06:33Z,Reinforcement Learning,"  Model-based reinforcement learning (RL) is a sample-efficient way of learning
complex behaviors by leveraging a learned single-step dynamics model to plan
actions in imagination. However, planning every action for long-horizon tasks
is not practical, akin to a human planning out every muscle movement. Instead,
humans efficiently plan with high-level skills to solve complex tasks. From
this intuition, we propose a Skill-based Model-based RL framework (SkiMo) that
enables planning in the skill space using a skill dynamics model, which
directly predicts the skill outcomes, rather than predicting all small details
in the intermediate states, step by step. For accurate and efficient long-term
planning, we jointly learn the skill dynamics model and a skill repertoire from
prior experience. We then harness the learned skill dynamics model to
accurately simulate and plan over long horizons in the skill space, which
enables efficient downstream learning of long-horizon, sparse reward tasks.
Experimental results in navigation and manipulation domains show that SkiMo
extends the temporal horizon of model-based approaches and improves the sample
efficiency for both model-based RL and skill-based RL. Code and videos are
available at https://clvrai.com/skimo
",0
Optimizing Data Collection in Deep Reinforcement Learning,"James Gleeson, Daniel Snider, Yvonne Yang, Moshe Gabel, Eyal de Lara, Gennady Pekhimenko",2022-07-15T20:22:31Z,Reinforcement Learning,"  Reinforcement learning (RL) workloads take a notoriously long time to train
due to the large number of samples collected at run-time from simulators.
Unfortunately, cluster scale-up approaches remain expensive, and commonly used
CPU implementations of simulators induce high overhead when switching back and
forth between GPU computations. We explore two optimizations that increase RL
data collection efficiency by increasing GPU utilization: (1) GPU
vectorization: parallelizing simulation on the GPU for increased hardware
parallelism, and (2) simulator kernel fusion: fusing multiple simulation steps
to run in a single GPU kernel launch to reduce global memory bandwidth
requirements. We find that GPU vectorization can achieve up to $1024\times$
speedup over commonly used CPU simulators. We profile the performance of
different implementations and show that for a simple simulator, ML compiler
implementations (XLA) of GPU vectorization outperform a DNN framework (PyTorch)
by $13.4\times$ by reducing CPU overhead from repeated Python to DL backend API
calls. We show that simulator kernel fusion speedups with a simple simulator
are $11.3\times$ and increase by up to $1024\times$ as simulator complexity
increases in terms of memory bandwidth requirements. We show that the speedups
from simulator kernel fusion are orthogonal and combinable with GPU
vectorization, leading to a multiplicative speedup.
",0
Active Exploration for Inverse Reinforcement Learning,"David Lindner, Andreas Krause, Giorgia Ramponi",2022-07-18T14:45:55Z,Reinforcement Learning,"  Inverse Reinforcement Learning (IRL) is a powerful paradigm for inferring a
reward function from expert demonstrations. Many IRL algorithms require a known
transition model and sometimes even a known expert policy, or they at least
require access to a generative model. However, these assumptions are too strong
for many real-world applications, where the environment can be accessed only
through sequential interaction. We propose a novel IRL algorithm: Active
exploration for Inverse Reinforcement Learning (AceIRL), which actively
explores an unknown environment and expert policy to quickly learn the expert's
reward function and identify a good policy. AceIRL uses previous observations
to construct confidence intervals that capture plausible reward functions and
find exploration policies that focus on the most informative regions of the
environment. AceIRL is the first approach to active IRL with sample-complexity
bounds that does not require a generative model of the environment. AceIRL
matches the sample complexity of active IRL with a generative model in the
worst case. Additionally, we establish a problem-dependent bound that relates
the sample complexity of AceIRL to the suboptimality gap of a given IRL
problem. We empirically evaluate AceIRL in simulations and find that it
significantly outperforms more naive exploration strategies.
",0
"A framework for online, stabilizing reinforcement learning","Grigory Yaremenko, Georgiy Malaniya, Pavel Osinenko",2022-07-18T16:23:26Z,Reinforcement Learning,"  Online reinforcement learning is concerned with training an agent on-the-fly
via dynamic interaction with the environment. Here, due to the specifics of the
application, it is not generally possible to perform long pre-training, as it
is commonly done in off-line, model-free approaches, which are akin to dynamic
programming. Such applications may be found more frequently in industry, rather
than in pure digital fields, such as cloud services, video games, database
management, etc., where reinforcement learning has been demonstrating success.
Online reinforcement learning, in contrast, is more akin to classical control,
which utilizes some model knowledge about the environment. Stability of the
closed-loop (agent plus the environment) is a major challenge for such online
approaches. In this paper, we tackle this problem by a special fusion of online
reinforcement learning with elements of classical control, namely, based on the
Lyapunov theory of stability. The idea is to start the agent at once, without
pre-training, and learn approximately optimal policy under specially designed
constraints, which guarantee stability. The resulting approach was tested in an
extensive experimental study with a mobile robot. A nominal parking controller
was used as a baseline. It was observed that the suggested agent could always
successfully park the robot, while significantly improving the cost. While many
approaches may be exploited for mobile robot control, we suggest that the
experiments showed the promising potential of online reinforcement learning
agents based on Lyapunov-like constraints. The presented methodology may be
utilized in safety-critical, industrial applications where stability is
necessary.
",0
Actor-Critic based Improper Reinforcement Learning,"Mohammadi Zaki, Avinash Mohan, Aditya Gopalan, Shie Mannor",2022-07-19T05:55:02Z,Reinforcement Learning,"  We consider an improper reinforcement learning setting where a learner is
given $M$ base controllers for an unknown Markov decision process, and wishes
to combine them optimally to produce a potentially new controller that can
outperform each of the base ones. This can be useful in tuning across
controllers, learnt possibly in mismatched or simulated environments, to obtain
a good controller for a given target environment with relatively few trials.
  Towards this, we propose two algorithms: (1) a Policy Gradient-based
approach; and (2) an algorithm that can switch between a simple Actor-Critic
(AC) based scheme and a Natural Actor-Critic (NAC) scheme depending on the
available information. Both algorithms operate over a class of improper
mixtures of the given controllers. For the first case, we derive convergence
rate guarantees assuming access to a gradient oracle. For the AC-based approach
we provide convergence rate guarantees to a stationary point in the basic AC
case and to a global optimum in the NAC case. Numerical results on (i) the
standard control theoretic benchmark of stabilizing an cartpole; and (ii) a
constrained queueing task show that our improper policy optimization algorithm
can stabilize the system even when the base policies at its disposal are
unstable.
",0
Halftoning with Multi-Agent Deep Reinforcement Learning,"Haitian Jiang, Dongliang Xiong, Xiaowen Jiang, Aiguo Yin, Li Ding, Kai Huang",2022-07-23T04:16:03Z,Reinforcement Learning,"  Deep neural networks have recently succeeded in digital halftoning using
vanilla convolutional layers with high parallelism. However, existing deep
methods fail to generate halftones with a satisfying blue-noise property and
require complex training schemes. In this paper, we propose a halftoning method
based on multi-agent deep reinforcement learning, called HALFTONERS, which
learns a shared policy to generate high-quality halftone images. Specifically,
we view the decision of each binary pixel value as an action of a virtual
agent, whose policy is trained by a low-variance policy gradient. Moreover, the
blue-noise property is achieved by a novel anisotropy suppressing loss
function. Experiments show that our halftoning method produces high-quality
halftones while staying relatively fast.
",3
Online Reinforcement Learning for Periodic MDP,"Ayush Aniket, Arpan Chattopadhyay",2022-07-25T10:37:09Z,Reinforcement Learning,"  We study learning in periodic Markov Decision Process(MDP), a special type of
non-stationary MDP where both the state transition probabilities and reward
functions vary periodically, under the average reward maximization setting. We
formulate the problem as a stationary MDP by augmenting the state space with
the period index, and propose a periodic upper confidence bound reinforcement
learning-2 (PUCRL2) algorithm. We show that the regret of PUCRL2 varies
linearly with the period and as sub-linear with the horizon length. Numerical
results demonstrate the efficacy of PUCRL2.
",0
Offline Reinforcement Learning at Multiple Frequencies,"Kaylee Burns, Tianhe Yu, Chelsea Finn, Karol Hausman",2022-07-26T17:54:49Z,Reinforcement Learning,"  Leveraging many sources of offline robot data requires grappling with the
heterogeneity of such data. In this paper, we focus on one particular aspect of
heterogeneity: learning from offline data collected at different control
frequencies. Across labs, the discretization of controllers, sampling rates of
sensors, and demands of a task of interest may differ, giving rise to a mixture
of frequencies in an aggregated dataset. We study how well offline
reinforcement learning (RL) algorithms can accommodate data with a mixture of
frequencies during training. We observe that the $Q$-value propagates at
different rates for different discretizations, leading to a number of learning
challenges for off-the-shelf offline RL. We present a simple yet effective
solution that enforces consistency in the rate of $Q$-value updates to
stabilize learning. By scaling the value of $N$ in $N$-step returns with the
discretization size, we effectively balance $Q$-value propagation, leading to
more stable convergence. On three simulated robotic control problems, we
empirically find that this simple approach outperforms na\""ive mixing by 50% on
average.
",0
Graph Inverse Reinforcement Learning from Diverse Videos,"Sateesh Kumar, Jonathan Zamora, Nicklas Hansen, Rishabh Jangir, Xiaolong Wang",2022-07-28T17:51:31Z,Reinforcement Learning,"  Research on Inverse Reinforcement Learning (IRL) from third-person videos has
shown encouraging results on removing the need for manual reward design for
robotic tasks. However, most prior works are still limited by training from a
relatively restricted domain of videos. In this paper, we argue that the true
potential of third-person IRL lies in increasing the diversity of videos for
better scaling. To learn a reward function from diverse videos, we propose to
perform graph abstraction on the videos followed by temporal matching in the
graph space to measure the task progress. Our insight is that a task can be
described by entity interactions that form a graph, and this graph abstraction
can help remove irrelevant information such as textures, resulting in more
robust reward functions. We evaluate our approach, GraphIRL, on
cross-embodiment learning in X-MAGICAL and learning from human demonstrations
for real-robot manipulation. We show significant improvements in robustness to
diverse video demonstrations over previous approaches, and even achieve better
results than manual reward design on a real robot pushing task. Videos are
available at https://sateeshkumar21.github.io/GraphIRL .
",0
RangL: A Reinforcement Learning Competition Platform,"Viktor Zobernig, Richard A. Saldanha, Jinke He, Erica van der Sar, Jasper van Doorn, Jia-Chen Hua, Lachlan R. Mason, Aleksander Czechowski, Drago Indjic, Tomasz Kosmala, Alessandro Zocca, Sandjai Bhulai, Jorge Montalvo Arvizu, Claude Klöckl, John Moriarty",2022-07-28T09:44:21Z,Reinforcement Learning,"  The RangL project hosted by The Alan Turing Institute aims to encourage the
wider uptake of reinforcement learning by supporting competitions relating to
real-world dynamic decision problems. This article describes the reusable code
repository developed by the RangL team and deployed for the 2022 Pathways to
Net Zero Challenge, supported by the UK Net Zero Technology Centre. The winning
solutions to this particular Challenge seek to optimize the UK's energy
transition policy to net zero carbon emissions by 2050. The RangL repository
includes an OpenAI Gym reinforcement learning environment and code that
supports both submission to, and evaluation in, a remote instance of the open
source EvalAI platform as well as all winning learning agent strategies. The
repository is an illustrative example of RangL's capability to provide a
reusable structure for future challenges.
",0
Deep Reinforcement Learning for Multi-Agent Interaction,"Ibrahim H. Ahmed, Cillian Brewitt, Ignacio Carlucho, Filippos Christianos, Mhairi Dunion, Elliot Fosong, Samuel Garcin, Shangmin Guo, Balint Gyevnar, Trevor McInroe, Georgios Papoudakis, Arrasy Rahman, Lukas Schäfer, Massimiliano Tamborski, Giuseppe Vecchio, Cheng Wang, Stefano V. Albrecht",2022-08-02T21:55:56Z,Reinforcement Learning,"  The development of autonomous agents which can interact with other agents to
accomplish a given task is a core area of research in artificial intelligence
and machine learning. Towards this goal, the Autonomous Agents Research Group
develops novel machine learning algorithms for autonomous systems control, with
a specific focus on deep reinforcement learning and multi-agent reinforcement
learning. Research problems include scalable learning of coordinated agent
policies and inter-agent communication; reasoning about the behaviours, goals,
and composition of other agents from limited observations; and sample-efficient
learning based on intrinsic motivation, curriculum learning, causal inference,
and representation learning. This article provides a broad overview of the
ongoing research portfolio of the group and discusses open problems for future
directions.
",0
Object Detection with Deep Reinforcement Learning,"Manoosh Samiei, Ruofeng Li",2022-08-09T02:34:53Z,Reinforcement Learning,"  Object localization has been a crucial task in computer vision field. Methods
of localizing objects in an image have been proposed based on the features of
the attended pixels. Recently researchers have proposed methods to formulate
object localization as a dynamic decision process, which can be solved by a
reinforcement learning approach. In this project, we implement a novel active
object localization algorithm based on deep reinforcement learning. We compare
two different action settings for this MDP: a hierarchical method and a dynamic
method. We further perform some ablation studies on the performance of the
models by investigating different hyperparameters and various architecture
changes.
",0
Automating DBSCAN via Deep Reinforcement Learning,"Ruitong Zhang, Hao Peng, Yingtong Dou, Jia Wu, Qingyun Sun, Jingyi Zhang, Philip S. Yu",2022-08-09T04:40:11Z,Reinforcement Learning,"  DBSCAN is widely used in many scientific and engineering fields because of
its simplicity and practicality. However, due to its high sensitivity
parameters, the accuracy of the clustering result depends heavily on practical
experience. In this paper, we first propose a novel Deep Reinforcement Learning
guided automatic DBSCAN parameters search framework, namely DRL-DBSCAN. The
framework models the process of adjusting the parameter search direction by
perceiving the clustering environment as a Markov decision process, which aims
to find the best clustering parameters without manual assistance. DRL-DBSCAN
learns the optimal clustering parameter search policy for different feature
distributions via interacting with the clusters, using a weakly-supervised
reward training policy network. In addition, we also present a recursive search
mechanism driven by the scale of the data to efficiently and controllably
process large parameter spaces. Extensive experiments are conducted on five
artificial and real-world datasets based on the proposed four working modes.
The results of offline and online tasks show that the DRL-DBSCAN not only
consistently improves DBSCAN clustering accuracy by up to 26% and 25%
respectively, but also can stably find the dominant parameters with high
computational efficiency. The code is available at
https://github.com/RingBDStack/DRL-DBSCAN.
",0
Robust Reinforcement Learning using Offline Data,"Kishan Panaganti, Zaiyan Xu, Dileep Kalathil, Mohammad Ghavamzadeh",2022-08-10T03:47:45Z,Reinforcement Learning,"  The goal of robust reinforcement learning (RL) is to learn a policy that is
robust against the uncertainty in model parameters. Parameter uncertainty
commonly occurs in many real-world RL applications due to simulator modeling
errors, changes in the real-world system dynamics over time, and adversarial
disturbances. Robust RL is typically formulated as a max-min problem, where the
objective is to learn the policy that maximizes the value against the worst
possible models that lie in an uncertainty set. In this work, we propose a
robust RL algorithm called Robust Fitted Q-Iteration (RFQI), which uses only an
offline dataset to learn the optimal robust policy. Robust RL with offline data
is significantly more challenging than its non-robust counterpart because of
the minimization over all models present in the robust Bellman operator. This
poses challenges in offline data collection, optimization over the models, and
unbiased estimation. In this work, we propose a systematic approach to overcome
these challenges, resulting in our RFQI algorithm. We prove that RFQI learns a
near-optimal robust policy under standard assumptions and demonstrate its
superior performance on standard benchmark problems.
",52
Making Reinforcement Learning Work on Swimmer,"Maël Franceschetti, Coline Lacoux, Ryan Ohouens, Antonin Raffin, Olivier Sigaud",2022-08-16T08:00:43Z,Reinforcement Learning,"  The SWIMMER environment is a standard benchmark in reinforcement learning
(RL). In particular, it is often used in papers comparing or combining RL
methods with direct policy search methods such as genetic algorithms or
evolution strategies. A lot of these papers report poor performance on SWIMMER
from RL methods and much better performance from direct policy search methods.
In this technical report we show that the low performance of RL methods on
SWIMMER simply comes from the inadequate tuning of an important
hyper-parameter, the discount factor. Furthermore we show that, by setting this
hyper-parameter to a correct value, the issue can be easily fixed. Finally, for
a set of often used RL algorithms, we provide a set of successful
hyper-parameters obtained with the Stable Baselines3 library and its RL Zoo.
",0
Spectral Decomposition Representation for Reinforcement Learning,"Tongzheng Ren, Tianjun Zhang, Lisa Lee, Joseph E. Gonzalez, Dale Schuurmans, Bo Dai",2022-08-19T19:01:30Z,Reinforcement Learning,"  Representation learning often plays a critical role in reinforcement learning
by managing the curse of dimensionality. A representative class of algorithms
exploits a spectral decomposition of the stochastic transition dynamics to
construct representations that enjoy strong theoretical properties in an
idealized setting. However, current spectral methods suffer from limited
applicability because they are constructed for state-only aggregation and
derived from a policy-dependent transition kernel, without considering the
issue of exploration. To address these issues, we propose an alternative
spectral method, Spectral Decomposition Representation (SPEDER), that extracts
a state-action abstraction from the dynamics without inducing spurious
dependence on the data collection policy, while also balancing the
exploration-versus-exploitation trade-off during learning. A theoretical
analysis establishes the sample efficiency of the proposed algorithm in both
the online and offline settings. In addition, an experimental investigation
demonstrates superior performance over current state-of-the-art algorithms
across several benchmarks.
",0
Weighted Maximum Entropy Inverse Reinforcement Learning,"The Viet Bui, Tien Mai, Patrick Jaillet",2022-08-20T06:02:07Z,Reinforcement Learning,"  We study inverse reinforcement learning (IRL) and imitation learning (IM),
the problems of recovering a reward or policy function from expert's
demonstrated trajectories. We propose a new way to improve the learning process
by adding a weight function to the maximum entropy framework, with the
motivation of having the ability to learn and recover the stochasticity (or the
bounded rationality) of the expert policy. Our framework and algorithms allow
to learn both a reward (or policy) function and the structure of the entropy
terms added to the Markov Decision Processes, thus enhancing the learning
procedure. Our numerical experiments using human and simulated demonstrations
and with discrete and continuous IRL/IM tasks show that our approach
outperforms prior algorithms.
",0
Quantum Multi-Agent Meta Reinforcement Learning,"Won Joon Yun, Jihong Park, Joongheon Kim",2022-08-22T22:46:52Z,Reinforcement Learning,"  Although quantum supremacy is yet to come, there has recently been an
increasing interest in identifying the potential of quantum machine learning
(QML) in the looming era of practical quantum computing. Motivated by this, in
this article we re-design multi-agent reinforcement learning (MARL) based on
the unique characteristics of quantum neural networks (QNNs) having two
separate dimensions of trainable parameters: angle parameters affecting the
output qubit states, and pole parameters associated with the output measurement
basis. Exploiting this dyadic trainability as meta-learning capability, we
propose quantum meta MARL (QM2ARL) that first applies angle training for
meta-QNN learning, followed by pole training for few-shot or local-QNN
training. To avoid overfitting, we develop an angle-to-pole regularization
technique injecting noise into the pole domain during angle training.
Furthermore, by exploiting the pole as the memory address of each trained QNN,
we introduce the concept of pole memory allowing one to save and load trained
QNNs using only two-parameter pole values. We theoretically prove the
convergence of angle training under the angle-to-pole regularization, and by
simulation corroborate the effectiveness of QM2ARL in achieving high reward and
fast convergence, as well as of the pole memory in fast adaptation to a
time-varying environment.
",0
Visual processing in context of reinforcement learning,Hlynur Davíð Hlynsson,2022-08-26T09:30:51Z,Reinforcement Learning,"  Although deep reinforcement learning (RL) has recently enjoyed many
successes, its methods are still data inefficient, which makes solving numerous
problems prohibitively expensive in terms of data. We aim to remedy this by
taking advantage of the rich supervisory signal in unlabeled data for learning
state representations. This thesis introduces three different representation
learning algorithms that have access to different subsets of the data sources
that traditional RL algorithms use:
  (i) GRICA is inspired by independent component analysis (ICA) and trains a
deep neural network to output statistically independent features of the input.
GrICA does so by minimizing the mutual information between each feature and the
other features. Additionally, GrICA only requires an unsorted collection of
environment states.
  (ii) Latent Representation Prediction (LARP) requires more context: in
addition to requiring a state as an input, it also needs the previous state and
an action that connects them. This method learns state representations by
predicting the representation of the environment's next state given a current
state and action. The predictor is used with a graph search algorithm.
  (iii) RewPred learns a state representation by training a deep neural network
to learn a smoothed version of the reward function. The representation is used
for preprocessing inputs to deep RL, while the reward predictor is used for
reward shaping. This method needs only state-reward pairs from the environment
for learning the representation.
  We discover that every method has their strengths and weaknesses, and
conclude from our experiments that including unsupervised representation
learning in RL problem-solving pipelines can speed up learning.
",0
Play with Emotion: Affect-Driven Reinforcement Learning,"Matthew Barthet, Ahmed Khalifa, Antonios Liapis, Georgios N. Yannakakis",2022-08-26T12:28:24Z,Reinforcement Learning,"  This paper introduces a paradigm shift by viewing the task of affect modeling
as a reinforcement learning (RL) process. According to the proposed paradigm,
RL agents learn a policy (i.e. affective interaction) by attempting to maximize
a set of rewards (i.e. behavioral and affective patterns) via their experience
with their environment (i.e. context). Our hypothesis is that RL is an
effective paradigm for interweaving affect elicitation and manifestation with
behavioral and affective demonstrations. Importantly, our second
hypothesis-building on Damasio's somatic marker hypothesis-is that emotion can
be the facilitator of decision-making. We test our hypotheses in a racing game
by training Go-Blend agents to model human demonstrations of arousal and
behavior; Go-Blend is a modified version of the Go-Explore algorithm which has
recently showcased supreme performance in hard exploration tasks. We first vary
the arousal-based reward function and observe agents that can effectively
display a palette of affect and behavioral patterns according to the specified
reward. Then we use arousal-based state selection mechanisms in order to bias
the strategies that Go-Blend explores. Our findings suggest that Go-Blend not
only is an efficient affect modeling paradigm but, more importantly,
affect-driven RL improves exploration and yields higher performing agents,
validating Damasio's hypothesis in the domain of games.
",0
DETERRENT: Detecting Trojans using Reinforcement Learning,"Vasudev Gohil, Satwik Patnaik, Hao Guo, Dileep Kalathil,  Jeyavijayan,  Rajendran",2022-08-26T22:09:47Z,Reinforcement Learning,"  Insertion of hardware Trojans (HTs) in integrated circuits is a pernicious
threat. Since HTs are activated under rare trigger conditions, detecting them
using random logic simulations is infeasible. In this work, we design a
reinforcement learning (RL) agent that circumvents the exponential search space
and returns a minimal set of patterns that is most likely to detect HTs.
Experimental results on a variety of benchmarks demonstrate the efficacy and
scalability of our RL agent, which obtains a significant reduction
($169\times$) in the number of test patterns required while maintaining or
improving coverage ($95.75\%$) compared to the state-of-the-art techniques.
",0
Categorical semantics of compositional reinforcement learning,"Georgios Bakirtzis, Michail Savvas, Ufuk Topcu",2022-08-29T15:51:36Z,Reinforcement Learning,"  Reinforcement learning (RL) often requires decomposing a problem into
subtasks and composing learned behaviors on these tasks. Compositionality in RL
has the potential to create modular subtask units that interface with other
system capabilities. However, generating compositional models requires the
characterization of minimal assumptions for the robustness of the compositional
feature. We develop a framework for a \emph{compositional theory} of RL using a
categorical point of view. Given the categorical representation of
compositionality, we investigate sufficient conditions under which
learning-by-parts results in the same optimal policy as learning on the whole.
In particular, our approach introduces a category $\mathsf{MDP}$, whose objects
are Markov decision processes (MDPs) acting as models of tasks. We show that
$\mathsf{MDP}$ admits natural compositional operations, such as certain fiber
products and pushouts. These operations make explicit compositional phenomena
in RL and unify existing constructions, such as puncturing hazardous states in
composite MDPs and incorporating state-action symmetry. We also model
sequential task completion by introducing the language of zig-zag diagrams that
is an immediate application of the pushout operation in $\mathsf{MDP}$.
",0
Model-Based Reinforcement Learning with SINDy,"Rushiv Arora, Bruno Castro da Silva, Eliot Moss",2022-08-30T19:03:48Z,Reinforcement Learning,"  We draw on the latest advancements in the physics community to propose a
novel method for discovering the governing non-linear dynamics of physical
systems in reinforcement learning (RL). We establish that this method is
capable of discovering the underlying dynamics using significantly fewer
trajectories (as little as one rollout with $\leq 30$ time steps) than state of
the art model learning algorithms. Further, the technique learns a model that
is accurate enough to induce near-optimal policies given significantly fewer
trajectories than those required by model-free algorithms. It brings the
benefits of model-based RL without requiring a model to be developed in
advance, for systems that have physics-based dynamics.
  To establish the validity and applicability of this algorithm, we conduct
experiments on four classic control tasks. We found that an optimal policy
trained on the discovered dynamics of the underlying system can generalize
well. Further, the learned policy performs well when deployed on the actual
physical system, thus bridging the model to real system gap. We further compare
our method to state-of-the-art model-based and model-free approaches, and show
that our method requires fewer trajectories sampled on the true physical system
compared other methods. Additionally, we explored approximate dynamics models
and found that they also can perform well.
",0
Deep reinforcement learning for quantum multiparameter estimation,"Valeria Cimini, Mauro Valeri, Emanuele Polino, Simone Piacentini, Francesco Ceccarelli, Giacomo Corrielli, Nicolò Spagnolo, Roberto Osellame, Fabio Sciarrino",2022-09-01T18:01:56Z,Reinforcement Learning,"  Estimation of physical quantities is at the core of most scientific research
and the use of quantum devices promises to enhance its performances. In real
scenarios, it is fundamental to consider that the resources are limited and
Bayesian adaptive estimation represents a powerful approach to efficiently
allocate, during the estimation process, all the available resources. However,
this framework relies on the precise knowledge of the system model, retrieved
with a fine calibration that often results computationally and experimentally
demanding. Here, we introduce a model-free and deep learning-based approach to
efficiently implement realistic Bayesian quantum metrology tasks accomplishing
all the relevant challenges, without relying on any a-priori knowledge on the
system. To overcome this need, a neural network is trained directly on
experimental data to learn the multiparameter Bayesian update. Then, the system
is set at its optimal working point through feedbacks provided by a
reinforcement learning algorithm trained to reconstruct and enhance experiment
heuristics of the investigated quantum sensor. Notably, we prove experimentally
the achievement of higher estimation performances than standard methods,
demonstrating the strength of the combination of these two black-box algorithms
on an integrated photonic circuit. This work represents an important step
towards fully artificial intelligence-based quantum metrology.
",0
Dialogue Evaluation with Offline Reinforcement Learning,"Nurul Lubis, Christian Geishauser, Hsien-Chin Lin, Carel van Niekerk, Michael Heck, Shutong Feng, Milica Gašić",2022-09-02T08:32:52Z,Reinforcement Learning,"  Task-oriented dialogue systems aim to fulfill user goals through natural
language interactions. They are ideally evaluated with human users, which
however is unattainable to do at every iteration of the development phase.
Simulated users could be an alternative, however their development is
nontrivial. Therefore, researchers resort to offline metrics on existing
human-human corpora, which are more practical and easily reproducible. They are
unfortunately limited in reflecting real performance of dialogue systems. BLEU
for instance is poorly correlated with human judgment, and existing
corpus-based metrics such as success rate overlook dialogue context mismatches.
There is still a need for a reliable metric for task-oriented systems with good
generalization and strong correlation with human judgements. In this paper, we
propose the use of offline reinforcement learning for dialogue evaluation based
on a static corpus. Such an evaluator is typically called a critic and utilized
for policy optimization. We go one step further and show that offline RL
critics can be trained on a static corpus for any dialogue system as external
evaluators, allowing dialogue performance comparisons across various types of
systems. This approach has the benefit of being corpus- and model-independent,
while attaining strong correlation with human judgements, which we confirm via
an interactive user trial.
",0
Intrinsic fluctuations of reinforcement learning promote cooperation,"Wolfram Barfuss, Janusz Meylahn",2022-09-01T09:14:47Z,Reinforcement Learning,"  In this work, we ask for and answer what makes classical temporal-difference
reinforcement learning with epsilon-greedy strategies cooperative. Cooperating
in social dilemma situations is vital for animals, humans, and machines. While
evolutionary theory revealed a range of mechanisms promoting cooperation, the
conditions under which agents learn to cooperate are contested. Here, we
demonstrate which and how individual elements of the multi-agent learning
setting lead to cooperation. We use the iterated Prisoner's dilemma with
one-period memory as a testbed. Each of the two learning agents learns a
strategy that conditions the following action choices on both agents' action
choices of the last round. We find that next to a high caring for future
rewards, a low exploration rate, and a small learning rate, it is primarily
intrinsic stochastic fluctuations of the reinforcement learning process which
double the final rate of cooperation to up to 80%. Thus, inherent noise is not
a necessary evil of the iterative learning process. It is a critical asset for
the learning of cooperation. However, we also point out the trade-off between a
high likelihood of cooperative behavior and achieving this in a reasonable
amount of time. Our findings are relevant for purposefully designing
cooperative algorithms and regulating undesired collusive effects.
",0
Natural Policy Gradients In Reinforcement Learning Explained,W. J. A. van Heeswijk,2022-09-05T08:06:29Z,Reinforcement Learning,"  Traditional policy gradient methods are fundamentally flawed. Natural
gradients converge quicker and better, forming the foundation of contemporary
Reinforcement Learning such as Trust Region Policy Optimization (TRPO) and
Proximal Policy Optimization (PPO). This lecture note aims to clarify the
intuition behind natural policy gradients, focusing on the thought process and
the key mathematical constructs.
",0
Improving Assistive Robotics with Deep Reinforcement Learning,"Yash Jakhotiya, Iman Haque",2022-09-05T22:43:39Z,Reinforcement Learning,"  Assistive Robotics is a class of robotics concerned with aiding humans in
daily care tasks that they may be inhibited from doing due to disabilities or
age. While research has demonstrated that classical control methods can be used
to design policies to complete these tasks, these methods can be difficult to
generalize to a variety of instantiations of a task. Reinforcement learning can
provide a solution to this issue, wherein robots are trained in simulation and
their policies are transferred to real-world machines. In this work, we
replicate a published baseline for training robots on three tasks in the
Assistive Gym environment, and we explore the usage of a Recurrent Neural
Network and Phasic Policy Gradient learning to augment the original work. Our
baseline implementation meets or exceeds the baseline of the original work,
however, we found that our explorations into the new methods was not as
effective as we anticipated. We discuss the results of our baseline and some
thoughts on why our new methods were not as successful.
",0
Reward Delay Attacks on Deep Reinforcement Learning,"Anindya Sarkar, Jiarui Feng, Yevgeniy Vorobeychik, Christopher Gill, Ning Zhang",2022-09-08T02:40:44Z,Reinforcement Learning,"  Most reinforcement learning algorithms implicitly assume strong synchrony. We
present novel attacks targeting Q-learning that exploit a vulnerability
entailed by this assumption by delaying the reward signal for a limited time
period. We consider two types of attack goals: targeted attacks, which aim to
cause a target policy to be learned, and untargeted attacks, which simply aim
to induce a policy with a low reward. We evaluate the efficacy of the proposed
attacks through a series of experiments. Our first observation is that
reward-delay attacks are extremely effective when the goal is simply to
minimize reward. Indeed, we find that even naive baseline reward-delay attacks
are also highly successful in minimizing the reward. Targeted attacks, on the
other hand, are more challenging, although we nevertheless demonstrate that the
proposed approaches remain highly effective at achieving the attacker's
targets. In addition, we introduce a second threat model that captures a
minimal mitigation that ensures that rewards cannot be used out of sequence. We
find that this mitigation remains insufficient to ensure robustness to attacks
that delay, but preserve the order, of rewards.
",0
Meta-Reinforcement Learning via Language Instructions,"Zhenshan Bing, Alexander Koch, Xiangtong Yao, Kai Huang, Alois Knoll",2022-09-11T19:42:48Z,Reinforcement Learning,"  Although deep reinforcement learning has recently been very successful at
learning complex behaviors, it requires a tremendous amount of data to learn a
task. One of the fundamental reasons causing this limitation lies in the nature
of the trial-and-error learning paradigm of reinforcement learning, where the
agent communicates with the environment and progresses in the learning only
relying on the reward signal. This is implicit and rather insufficient to learn
a task well. On the contrary, humans are usually taught new skills via natural
language instructions. Utilizing language instructions for robotic motion
control to improve the adaptability is a recently emerged topic and
challenging. In this paper, we present a meta-RL algorithm that addresses the
challenge of learning skills with language instructions in multiple
manipulation tasks. On the one hand, our algorithm utilizes the language
instructions to shape its interpretation of the task, on the other hand, it
still learns to solve task in a trial-and-error process. We evaluate our
algorithm on the robotic manipulation benchmark (Meta-World) and it
significantly outperforms state-of-the-art methods in terms of training and
testing task success rates. Codes are available at
\url{https://tumi6robot.wixsite.com/million}.
",11
Measuring Interventional Robustness in Reinforcement Learning,"Katherine Avery, Jack Kenney, Pracheta Amaranath, Erica Cai, David Jensen",2022-09-19T14:50:05Z,Reinforcement Learning,"  Recent work in reinforcement learning has focused on several characteristics
of learned policies that go beyond maximizing reward. These properties include
fairness, explainability, generalization, and robustness. In this paper, we
define interventional robustness (IR), a measure of how much variability is
introduced into learned policies by incidental aspects of the training
procedure, such as the order of training data or the particular exploratory
actions taken by agents. A training procedure has high IR when the agents it
produces take very similar actions under intervention, despite variation in
these incidental aspects of the training procedure. We develop an intuitive,
quantitative measure of IR and calculate it for eight algorithms in three Atari
environments across dozens of interventions and states. From these experiments,
we find that IR varies with the amount of training and type of algorithm and
that high performance does not imply high IR, as one might expect.
",0
Locally Constrained Representations in Reinforcement Learning,"Somjit Nath, Rushiv Arora, Samira Ebrahimi Kahou",2022-09-20T03:36:39Z,Reinforcement Learning,"  The success of Reinforcement Learning (RL) heavily relies on the ability to
learn robust representations from the observations of the environment. In most
cases, the representations learned purely by the reinforcement learning loss
can differ vastly across states depending on how the value functions change.
However, the representations learned need not be very specific to the task at
hand. Relying only on the RL objective may yield representations that vary
greatly across successive time steps. In addition, since the RL loss has a
changing target, the representations learned would depend on how good the
current values/policies are. Thus, disentangling the representations from the
main task would allow them to focus not only on the task-specific features but
also the environment dynamics. To this end, we propose locally constrained
representations, where an auxiliary loss forces the state representations to be
predictable by the representations of the neighboring states. This encourages
the representations to be driven not only by the value/policy learning but also
by an additional loss that constrains the representations from over-fitting to
the value loss. We evaluate the proposed method on several known benchmarks and
observe strong performance. Especially in continuous control tasks, our
experiments show a significant performance improvement.
",0
Safe Reinforcement Learning with Contrastive Risk Prediction,"Hanping Zhang, Yuhong Guo",2022-09-10T18:54:38Z,Reinforcement Learning,"  As safety violations can lead to severe consequences in real-world robotic
applications, the increasing deployment of Reinforcement Learning (RL) in
robotic domains has propelled the study of safe exploration for reinforcement
learning (safe RL). In this work, we propose a risk preventive training method
for safe RL, which learns a statistical contrastive classifier to predict the
probability of a state-action pair leading to unsafe states. Based on the
predicted risk probabilities, we can collect risk preventive trajectories and
reshape the reward function with risk penalties to induce safe RL policies. We
conduct experiments in robotic simulation environments. The results show the
proposed approach has comparable performance with the state-of-the-art
model-based methods and outperforms conventional model-free safe RL approaches.
",0
Model-Free Reinforcement Learning for Asset Allocation,"Adebayo Oshingbesan, Eniola Ajiboye, Peruth Kamashazi, Timothy Mbaka",2022-09-21T16:00:24Z,Reinforcement Learning,"  Asset allocation (or portfolio management) is the task of determining how to
optimally allocate funds of a finite budget into a range of financial
instruments/assets such as stocks. This study investigated the performance of
reinforcement learning (RL) when applied to portfolio management using
model-free deep RL agents. We trained several RL agents on real-world stock
prices to learn how to perform asset allocation. We compared the performance of
these RL agents against some baseline agents. We also compared the RL agents
among themselves to understand which classes of agents performed better. From
our analysis, RL agents can perform the task of portfolio management since they
significantly outperformed two of the baseline agents (random allocation and
uniform allocation). Four RL agents (A2C, SAC, PPO, and TRPO) outperformed the
best baseline, MPT, overall. This shows the abilities of RL agents to uncover
more profitable trading strategies. Furthermore, there were no significant
performance differences between value-based and policy-based RL agents.
Actor-critic agents performed better than other types of agents. Also,
on-policy agents performed better than off-policy agents because they are
better at policy evaluation and sample efficiency is not a significant problem
in portfolio management. This study shows that RL agents can substantially
improve asset allocation since they outperform strong baselines. On-policy,
actor-critic RL agents showed the most promise based on our analysis.
",0
Learning swimming via deep reinforcement learning,"Jin Zhang, Lei Zhou, Bochao Cao",2022-09-22T11:35:10Z,Reinforcement Learning,"  For decades, people have been seeking for fishlike flapping motions that can
realize underwater propulsion with low energy cost. Complexity of the
nonstationary flow field around the flapping body makes this problem very
difficult. In earlier studies, motion patterns are usually prescribed as
certain periodic functions which constrains the following optimization process
in a small subdomain of the whole motion space. In this work, to avoid this
motion constraint, a variational autoencoder (VAE) is designed to compress
various flapping motions into a simple action vector. Then we let a flapping
airfoil continuously interact with water tunnel environment and adjust its
action accordingly through a reinforcement learning (RL) framework. By this
automatic close-looped experiment, we obtain several motion patterns that can
result in high hydrodynamic efficiency comparing to pure harmonic motions with
the same thrust level. And we find that, after numerous trials and errors, RL
trainings in current experiment always converge to motion patterns that are
close to harmonic motions. In other words, current work proves that harmonic
motion with appropriate amplitude and frequency is always an optimal choice for
efficient underwater propulsion. Furthermore, the RL framework proposed here
can be also extended to the study of other complex swimming problems, which
might pave the way for the creation of a robotic fish that can swim like a real
fish.
",0
Explainable Reinforcement Learning via Model Transforms,"Mira Finkelstein, Lucy Liu, Nitsan Levy Schlot, Yoav Kolumbus, David C. Parkes, Jeffrey S. Rosenshein, Sarah Keren",2022-09-24T13:18:06Z,Reinforcement Learning,"  Understanding emerging behaviors of reinforcement learning (RL) agents may be
difficult since such agents are often trained in complex environments using
highly complex decision making procedures. This has given rise to a variety of
approaches to explainability in RL that aim to reconcile discrepancies that may
arise between the behavior of an agent and the behavior that is anticipated by
an observer. Most recent approaches have relied either on domain knowledge that
may not always be available, on an analysis of the agent's policy, or on an
analysis of specific elements of the underlying environment, typically modeled
as a Markov Decision Process (MDP). Our key claim is that even if the
underlying model is not fully known (e.g., the transition probabilities have
not been accurately learned) or is not maintained by the agent (i.e., when
using model-free methods), the model can nevertheless be exploited to
automatically generate explanations. For this purpose, we suggest using formal
MDP abstractions and transforms, previously used in the literature for
expediting the search for optimal policies, to automatically produce
explanations. Since such transforms are typically based on a symbolic
representation of the environment, they can provide meaningful explanations for
gaps between the anticipated and actual agent behavior. We formally define the
explainability problem, suggest a class of transforms that can be used for
explaining emergent behaviors, and suggest methods that enable efficient search
for an explanation. We demonstrate the approach on a set of standard
benchmarks.
",0
Deep Reinforcement Learning for Adaptive Mesh Refinement,"Corbin Foucart, Aaron Charous, Pierre F. J. Lermusiaux",2022-09-25T23:45:34Z,Reinforcement Learning,"  Finite element discretizations of problems in computational physics often
rely on adaptive mesh refinement (AMR) to preferentially resolve regions
containing important features during simulation. However, these spatial
refinement strategies are often heuristic and rely on domain-specific knowledge
or trial-and-error. We treat the process of adaptive mesh refinement as a
local, sequential decision-making problem under incomplete information,
formulating AMR as a partially observable Markov decision process. Using a deep
reinforcement learning approach, we train policy networks for AMR strategy
directly from numerical simulation. The training process does not require an
exact solution or a high-fidelity ground truth to the partial differential
equation at hand, nor does it require a pre-computed training dataset. The
local nature of our reinforcement learning formulation allows the policy
network to be trained inexpensively on much smaller problems than those on
which they are deployed. The methodology is not specific to any particular
partial differential equation, problem dimension, or numerical discretization,
and can flexibly incorporate diverse problem physics. To that end, we apply the
approach to a diverse set of partial differential equations, using a variety of
high-order discontinuous Galerkin and hybridizable discontinuous Galerkin
finite element discretizations. We show that the resultant deep reinforcement
learning policies are competitive with common AMR heuristics, generalize well
across problem classes, and strike a favorable balance between accuracy and
cost such that they often lead to a higher accuracy per problem degree of
freedom.
",0
Reinforcement Learning with Non-Exponential Discounting,"Matthias Schultheis, Constantin A. Rothkopf, Heinz Koeppl",2022-09-27T14:13:16Z,Reinforcement Learning,"  Commonly in reinforcement learning (RL), rewards are discounted over time
using an exponential function to model time preference, thereby bounding the
expected long-term reward. In contrast, in economics and psychology, it has
been shown that humans often adopt a hyperbolic discounting scheme, which is
optimal when a specific task termination time distribution is assumed. In this
work, we propose a theory for continuous-time model-based reinforcement
learning generalized to arbitrary discount functions. This formulation covers
the case in which there is a non-exponential random termination time. We derive
a Hamilton-Jacobi-Bellman (HJB) equation characterizing the optimal policy and
describe how it can be solved using a collocation method, which uses deep
learning for function approximation. Further, we show how the inverse RL
problem can be approached, in which one tries to recover properties of the
discount function given decision data. We validate the applicability of our
proposed approach on two simulated problems. Our approach opens the way for the
analysis of human discounting in sequential decision-making tasks.
",0
Disentangling Transfer in Continual Reinforcement Learning,"Maciej Wołczyk, Michał Zając, Razvan Pascanu, Łukasz Kuciński, Piotr Miłoś",2022-09-28T08:01:09Z,Reinforcement Learning,"  The ability of continual learning systems to transfer knowledge from
previously seen tasks in order to maximize performance on new tasks is a
significant challenge for the field, limiting the applicability of continual
learning solutions to realistic scenarios. Consequently, this study aims to
broaden our understanding of transfer and its driving forces in the specific
case of continual reinforcement learning. We adopt SAC as the underlying RL
algorithm and Continual World as a suite of continuous control tasks. We
systematically study how different components of SAC (the actor and the critic,
exploration, and data) affect transfer efficacy, and we provide recommendations
regarding various modeling options. The best set of choices, dubbed ClonEx-SAC,
is evaluated on the recent Continual World benchmark. ClonEx-SAC achieves 87%
final success rate compared to 80% of PackNet, the best method in the
benchmark. Moreover, the transfer grows from 0.18 to 0.54 according to the
metric provided by Continual World.
",0
Does Zero-Shot Reinforcement Learning Exist?,"Ahmed Touati, Jérémy Rapin, Yann Ollivier",2022-09-29T16:54:05Z,Reinforcement Learning,"  A zero-shot RL agent is an agent that can solve any RL task in a given
environment, instantly with no additional planning or learning, after an
initial reward-free learning phase. This marks a shift from the reward-centric
RL paradigm towards ""controllable"" agents that can follow arbitrary
instructions in an environment. Current RL agents can solve families of related
tasks at best, or require planning anew for each task. Strategies for
approximate zero-shot RL ave been suggested using successor features (SFs)
[BBQ+ 18] or forward-backward (FB) representations [TO21], but testing has been
limited.
  After clarifying the relationships between these schemes, we introduce
improved losses and new SF models, and test the viability of zero-shot RL
schemes systematically on tasks from the Unsupervised RL benchmark [LYL+21]. To
disentangle universal representation learning from exploration, we work in an
offline setting and repeat the tests on several existing replay buffers.
  SFs appear to suffer from the choice of the elementary state features. SFs
with Laplacian eigenfunctions do well, while SFs based on auto-encoders,
inverse curiosity, transition models, low-rank transition matrix, contrastive
learning, or diversity (APS), perform unconsistently. In contrast, FB
representations jointly learn the elementary and successor features from a
single, principled criterion. They perform best and consistently across the
board, reaching 85% of supervised RL performance with a good replay buffer, in
a zero-shot manner.
",0
Reinforcement Learning Algorithms: An Overview and Classification,"Fadi AlMahamid, Katarina Grolinger",2022-09-29T16:58:42Z,Reinforcement Learning,"  The desire to make applications and machines more intelligent and the
aspiration to enable their operation without human interaction have been
driving innovations in neural networks, deep learning, and other machine
learning techniques. Although reinforcement learning has been primarily used in
video games, recent advancements and the development of diverse and powerful
reinforcement algorithms have enabled the reinforcement learning community to
move from playing video games to solving complex real-life problems in
autonomous systems such as self-driving cars, delivery drones, and automated
robotics. Understanding the environment of an application and the algorithms'
limitations plays a vital role in selecting the appropriate reinforcement
learning algorithm that successfully solves the problem on hand in an efficient
manner. Consequently, in this study, we identify three main environment types
and classify reinforcement learning algorithms according to those environment
types. Moreover, within each category, we identify relationships between
algorithms. The overview of each algorithm provides insight into the
algorithms' foundations and reviews similarities and differences among
algorithms. This study provides a perspective on the field and helps
practitioners and researchers to select the appropriate algorithm for their use
case.
",0
ASPiRe:Adaptive Skill Priors for Reinforcement Learning,"Mengda Xu, Manuela Veloso, Shuran Song",2022-09-30T03:22:27Z,Reinforcement Learning,"  We introduce ASPiRe (Adaptive Skill Prior for RL), a new approach that
leverages prior experience to accelerate reinforcement learning. Unlike
existing methods that learn a single skill prior from a large and diverse
dataset, our framework learns a library of different distinction skill priors
(i.e., behavior priors) from a collection of specialized datasets, and learns
how to combine them to solve a new task. This formulation allows the algorithm
to acquire a set of specialized skill priors that are more reusable for
downstream tasks; however, it also brings up additional challenges of how to
effectively combine these unstructured sets of skill priors to form a new prior
for new tasks. Specifically, it requires the agent not only to identify which
skill prior(s) to use but also how to combine them (either sequentially or
concurrently) to form a new prior. To achieve this goal, ASPiRe includes
Adaptive Weight Module (AWM) that learns to infer an adaptive weight assignment
between different skill priors and uses them to guide policy learning for
downstream tasks via weighted Kullback-Leibler divergences. Our experiments
demonstrate that ASPiRe can significantly accelerate the learning of new
downstream tasks in the presence of multiple priors and show improvement on
competitive baselines.
",0
Policy Gradients for Probabilistic Constrained Reinforcement Learning,"Weiqin Chen, Dharmashankar Subramanian, Santiago Paternain",2022-10-02T18:16:33Z,Reinforcement Learning,"  This paper considers the problem of learning safe policies in the context of
reinforcement learning (RL). In particular, we consider the notion of
probabilistic safety. This is, we aim to design policies that maintain the
state of the system in a safe set with high probability. This notion differs
from cumulative constraints often considered in the literature. The challenge
of working with probabilistic safety is the lack of expressions for their
gradients. Indeed, policy optimization algorithms rely on gradients of the
objective function and the constraints. To the best of our knowledge, this work
is the first one providing such explicit gradient expressions for probabilistic
constraints. It is worth noting that the gradient of this family of constraints
can be applied to various policy-based algorithms. We demonstrate empirically
that it is possible to handle probabilistic constraints in a continuous
navigation problem.
",6
MSRL: Distributed Reinforcement Learning with Dataflow Fragments,"Huanzhou Zhu, Bo Zhao, Gang Chen, Weifeng Chen, Yijie Chen, Liang Shi, Yaodong Yang, Peter Pietzuch, Lei Chen",2022-10-03T12:34:58Z,"RAG, Reinforcement Learning","  Reinforcement learning (RL) trains many agents, which is resource-intensive
and must scale to large GPU clusters. Different RL training algorithms offer
different opportunities for distributing and parallelising the computation.
Yet, current distributed RL systems tie the definition of RL algorithms to
their distributed execution: they hard-code particular distribution strategies
and only accelerate specific parts of the computation (e.g. policy network
updates) on GPU workers. Fundamentally, current systems lack abstractions that
decouple RL algorithms from their execution.
  We describe MindSpore Reinforcement Learning (MSRL), a distributed RL
training system that supports distribution policies that govern how RL training
computation is parallelised and distributed on cluster resources, without
requiring changes to the algorithm implementation. MSRL introduces the new
abstraction of a fragmented dataflow graph, which maps Python functions from an
RL algorithm's training loop to parallel computational fragments. Fragments are
executed on different devices by translating them to low-level dataflow
representations, e.g. computational graphs as supported by deep learning
engines, CUDA implementations or multi-threaded CPU processes. We show that
MSRL subsumes the distribution strategies of existing systems, while scaling RL
training to 64 GPUs.
",3
Benchmarking Reinforcement Learning Techniques for Autonomous Navigation,"Zifan Xu, Bo Liu, Xuesu Xiao, Anirudh Nair, Peter Stone",2022-10-10T16:53:42Z,Reinforcement Learning,"  Deep reinforcement learning (RL) has brought many successes for autonomous
robot navigation. However, there still exists important limitations that
prevent real-world use of RL-based navigation systems. For example, most
learning approaches lack safety guarantees; and learned navigation systems may
not generalize well to unseen environments. Despite a variety of recent
learning techniques to tackle these challenges in general, a lack of an
open-source benchmark and reproducible learning methods specifically for
autonomous navigation makes it difficult for roboticists to choose what
learning methods to use for their mobile robots and for learning researchers to
identify current shortcomings of general learning methods for autonomous
navigation. In this paper, we identify four major desiderata of applying deep
RL approaches for autonomous navigation: (D1) reasoning under uncertainty, (D2)
safety, (D3) learning from limited trial-and-error data, and (D4)
generalization to diverse and novel environments. Then, we explore four major
classes of learning techniques with the purpose of achieving one or more of the
four desiderata: memory-based neural network architectures (D1), safe RL (D2),
model-based RL (D2, D3), and domain randomization (D4). By deploying these
learning techniques in a new open-source large-scale navigation benchmark and
real-world environments, we perform a comprehensive study aimed at establishing
to what extent can these techniques achieve these desiderata for RL-based
navigation systems.
",0
Regret Bounds for Risk-Sensitive Reinforcement Learning,"O. Bastani, Y. J. Ma, E. Shen, W. Xu",2022-10-11T17:49:01Z,Reinforcement Learning,"  In safety-critical applications of reinforcement learning such as healthcare
and robotics, it is often desirable to optimize risk-sensitive objectives that
account for tail outcomes rather than expected reward. We prove the first
regret bounds for reinforcement learning under a general class of
risk-sensitive objectives including the popular CVaR objective. Our theory is
based on a novel characterization of the CVaR objective as well as a novel
optimistic MDP construction.
",0
Reinforcement Learning with Automated Auxiliary Loss Search,"Tairan He, Yuge Zhang, Kan Ren, Minghuan Liu, Che Wang, Weinan Zhang, Yuqing Yang, Dongsheng Li",2022-10-12T09:24:53Z,Reinforcement Learning,"  A good state representation is crucial to solving complicated reinforcement
learning (RL) challenges. Many recent works focus on designing auxiliary losses
for learning informative representations. Unfortunately, these handcrafted
objectives rely heavily on expert knowledge and may be sub-optimal. In this
paper, we propose a principled and universal method for learning better
representations with auxiliary loss functions, named Automated Auxiliary Loss
Search (A2LS), which automatically searches for top-performing auxiliary loss
functions for RL. Specifically, based on the collected trajectory data, we
define a general auxiliary loss space of size $7.5 \times 10^{20}$ and explore
the space with an efficient evolutionary search strategy. Empirical results
show that the discovered auxiliary loss (namely, A2-winner) significantly
improves the performance on both high-dimensional (image) and low-dimensional
(vector) unseen tasks with much higher efficiency, showing promising
generalization ability to different settings and even different benchmark
domains. We conduct a statistical analysis to reveal the relations between
patterns of auxiliary losses and RL performance.
",0
Observed Adversaries in Deep Reinforcement Learning,"Eugene Lim, Harold Soh",2022-10-13T06:54:43Z,Reinforcement Learning,"  In this work, we point out the problem of observed adversaries for deep
policies. Specifically, recent work has shown that deep reinforcement learning
is susceptible to adversarial attacks where an observed adversary acts under
environmental constraints to invoke natural but adversarial observations. This
setting is particularly relevant for HRI since HRI-related robots are expected
to perform their tasks around and with other agents. In this work, we
demonstrate that this effect persists even with low-dimensional observations.
We further show that these adversarial attacks transfer across victims, which
potentially allows malicious attackers to train an adversary without access to
the target victim.
",0
Sustainable Online Reinforcement Learning for Auto-bidding,"Zhiyu Mou, Yusen Huo, Rongquan Bai, Mingzhou Xie, Chuan Yu, Jian Xu, Bo Zheng",2022-10-13T13:17:20Z,Reinforcement Learning,"  Recently, auto-bidding technique has become an essential tool to increase the
revenue of advertisers. Facing the complex and ever-changing bidding
environments in the real-world advertising system (RAS), state-of-the-art
auto-bidding policies usually leverage reinforcement learning (RL) algorithms
to generate real-time bids on behalf of the advertisers. Due to safety
concerns, it was believed that the RL training process can only be carried out
in an offline virtual advertising system (VAS) that is built based on the
historical data generated in the RAS. In this paper, we argue that there exists
significant gaps between the VAS and RAS, making the RL training process suffer
from the problem of inconsistency between online and offline (IBOO). Firstly,
we formally define the IBOO and systematically analyze its causes and
influences. Then, to avoid the IBOO, we propose a sustainable online RL (SORL)
framework that trains the auto-bidding policy by directly interacting with the
RAS, instead of learning in the VAS. Specifically, based on our proof of the
Lipschitz smooth property of the Q function, we design a safe and efficient
online exploration (SER) policy for continuously collecting data from the RAS.
Meanwhile, we derive the theoretical lower bound on the safety of the SER
policy. We also develop a variance-suppressed conservative Q-learning (V-CQL)
method to effectively and stably learn the auto-bidding policy with the
collected data. Finally, extensive simulated and real-world experiments
validate the superiority of our approach over the state-of-the-art auto-bidding
algorithm.
",0
Mutual Information Regularized Offline Reinforcement Learning,"Xiao Ma, Bingyi Kang, Zhongwen Xu, Min Lin, Shuicheng Yan",2022-10-14T03:22:43Z,Reinforcement Learning,"  The major challenge of offline RL is the distribution shift that appears when
out-of-distribution actions are queried, which makes the policy improvement
direction biased by extrapolation errors. Most existing methods address this
problem by penalizing the policy or value for deviating from the behavior
policy during policy improvement or evaluation. In this work, we propose a
novel MISA framework to approach offline RL from the perspective of Mutual
Information between States and Actions in the dataset by directly constraining
the policy improvement direction. MISA constructs lower bounds of mutual
information parameterized by the policy and Q-values. We show that optimizing
this lower bound is equivalent to maximizing the likelihood of a one-step
improved policy on the offline dataset. Hence, we constrain the policy
improvement direction to lie in the data manifold. The resulting algorithm
simultaneously augments the policy evaluation and improvement by adding mutual
information regularizations. MISA is a general framework that unifies
conservative Q-learning (CQL) and behavior regularization methods (e.g.,
TD3+BC) as special cases. We introduce 3 different variants of MISA, and
empirically demonstrate that tighter mutual information lower bound gives
better offline RL performance. In addition, our extensive experiments show MISA
significantly outperforms a wide range of baselines on various tasks of the
D4RL benchmark,e.g., achieving 742.9 total points on gym-locomotion tasks. Our
code is available at https://github.com/sail-sg/MISA.
",0
Multi-trainer Interactive Reinforcement Learning System,"Zhaori Guo, Timothy J. Norman, Enrico H. Gerding",2022-10-14T18:32:59Z,Reinforcement Learning,"  Interactive reinforcement learning can effectively facilitate the agent
training via human feedback. However, such methods often require the human
teacher to know what is the correct action that the agent should take. In other
words, if the human teacher is not always reliable, then it will not be
consistently able to guide the agent through its training. In this paper, we
propose a more effective interactive reinforcement learning system by
introducing multiple trainers, namely Multi-Trainer Interactive Reinforcement
Learning (MTIRL), which could aggregate the binary feedback from multiple
non-perfect trainers into a more reliable reward for an agent training in a
reward-sparse environment. In particular, our trainer feedback aggregation
experiments show that our aggregation method has the best accuracy when
compared with the majority voting, the weighted voting, and the Bayesian
method. Finally, we conduct a grid-world experiment to show that the policy
trained by the MTIRL with the review model is closer to the optimal policy than
that without a review model.
",0
Geometric Reinforcement Learning For Robotic Manipulation,"Naseem Alhousani, Matteo Saveriano, Ibrahim Sevinc, Talha Abdulkuddus, Hatice Kose, Fares J. Abu-Dakka",2022-10-14T21:46:20Z,Reinforcement Learning,"  Reinforcement learning (RL) is a popular technique that allows an agent to
learn by trial and error while interacting with a dynamic environment. The
traditional Reinforcement Learning (RL) approach has been successful in
learning and predicting Euclidean robotic manipulation skills such as
positions, velocities, and forces. However, in robotics, it is common to
encounter non-Euclidean data such as orientation or stiffness, and failing to
account for their geometric nature can negatively impact learning accuracy and
performance. In this paper, to address this challenge, we propose a novel
framework for RL that leverages Riemannian geometry, which we call Geometric
Reinforcement Learning (G-RL), to enable agents to learn robotic manipulation
skills with non-Euclidean data. Specifically, G-RL utilizes the tangent space
in two ways: a tangent space for parameterization and a local tangent space for
mapping to a nonEuclidean manifold. The policy is learned in the
parameterization tangent space, which remains constant throughout the training.
The policy is then transferred to the local tangent space via parallel
transport and projected onto the non-Euclidean manifold. The local tangent
space changes over time to remain within the neighborhood of the current
manifold point, reducing the approximation error. Therefore, by introducing a
geometrically grounded pre- and post-processing step into the traditional RL
pipeline, our G-RL framework enables several model-free algorithms designed for
Euclidean space to learn from non-Euclidean data without modifications.
Experimental results, obtained both in simulation and on a real robot, support
our hypothesis that G-RL is more accurate and converges to a better solution
than approximating non-Euclidean data.
",5
Entropy Regularized Reinforcement Learning with Cascading Networks,"Riccardo Della Vecchia, Alena Shilova, Philippe Preux, Riad Akrour",2022-10-16T10:28:59Z,Reinforcement Learning,"  Deep Reinforcement Learning (Deep RL) has had incredible achievements on high
dimensional problems, yet its learning process remains unstable even on the
simplest tasks. Deep RL uses neural networks as function approximators. These
neural models are largely inspired by developments in the (un)supervised
machine learning community. Compared to these learning frameworks, one of the
major difficulties of RL is the absence of i.i.d. data. One way to cope with
this difficulty is to control the rate of change of the policy at every
iteration. In this work, we challenge the common practices of the
(un)supervised learning community of using a fixed neural architecture, by
having a neural model that grows in size at each policy update. This allows a
closed form entropy regularized policy update, which leads to a better control
of the rate of change of the policy at each iteration and help cope with the
non i.i.d. nature of RL. Initial experiments on classical RL benchmarks show
promising results with remarkable convergence on some RL tasks when compared to
other deep RL baselines, while exhibiting limitations on others.
",1
Boosting Offline Reinforcement Learning via Data Rebalancing,"Yang Yue, Bingyi Kang, Xiao Ma, Zhongwen Xu, Gao Huang, Shuicheng Yan",2022-10-17T16:34:01Z,Reinforcement Learning,"  Offline reinforcement learning (RL) is challenged by the distributional shift
between learning policies and datasets. To address this problem, existing works
mainly focus on designing sophisticated algorithms to explicitly or implicitly
constrain the learned policy to be close to the behavior policy. The constraint
applies not only to well-performing actions but also to inferior ones, which
limits the performance upper bound of the learned policy. Instead of aligning
the densities of two distributions, aligning the supports gives a relaxed
constraint while still being able to avoid out-of-distribution actions.
Therefore, we propose a simple yet effective method to boost offline RL
algorithms based on the observation that resampling a dataset keeps the
distribution support unchanged. More specifically, we construct a better
behavior policy by resampling each transition in an old dataset according to
its episodic return. We dub our method ReD (Return-based Data Rebalance), which
can be implemented with less than 10 lines of code change and adds negligible
running time. Extensive experiments demonstrate that ReD is effective at
boosting offline RL performance and orthogonal to decoupling strategies in
long-tailed classification. New state-of-the-arts are achieved on the D4RL
benchmark.
",0
PaCo: Parameter-Compositional Multi-Task Reinforcement Learning,"Lingfeng Sun, Haichao Zhang, Wei Xu, Masayoshi Tomizuka",2022-10-21T01:00:10Z,Reinforcement Learning,"  The purpose of multi-task reinforcement learning (MTRL) is to train a single
policy that can be applied to a set of different tasks. Sharing parameters
allows us to take advantage of the similarities among tasks. However, the gaps
between contents and difficulties of different tasks bring us challenges on
both which tasks should share the parameters and what parameters should be
shared, as well as the optimization challenges due to parameter sharing. In
this work, we introduce a parameter-compositional approach (PaCo) as an attempt
to address these challenges. In this framework, a policy subspace represented
by a set of parameters is learned. Policies for all the single tasks lie in
this subspace and can be composed by interpolating with the learned set. It
allows not only flexible parameter sharing but also a natural way to improve
training. We demonstrate the state-of-the-art performance on Meta-World
benchmarks, verifying the effectiveness of the proposed approach.
",0
Implicit Offline Reinforcement Learning via Supervised Learning,"Alexandre Piche, Rafael Pardinas, David Vazquez, Igor Mordatch, Chris Pal",2022-10-21T21:59:42Z,Reinforcement Learning,"  Offline Reinforcement Learning (RL) via Supervised Learning is a simple and
effective way to learn robotic skills from a dataset collected by policies of
different expertise levels. It is as simple as supervised learning and Behavior
Cloning (BC), but takes advantage of return information. On datasets collected
by policies of similar expertise, implicit BC has been shown to match or
outperform explicit BC. Despite the benefits of using implicit models to learn
robotic skills via BC, offline RL via Supervised Learning algorithms have been
limited to explicit models. We show how implicit models can leverage return
information and match or outperform explicit algorithms to acquire robotic
skills from fixed datasets. Furthermore, we show the close relationship between
our implicit methods and other popular RL via Supervised Learning algorithms to
provide a unified framework. Finally, we demonstrate the effectiveness of our
method on high-dimension manipulation and locomotion tasks.
",0
Reachability-Aware Laplacian Representation in Reinforcement Learning,"Kaixin Wang, Kuangqi Zhou, Jiashi Feng, Bryan Hooi, Xinchao Wang",2022-10-24T12:13:40Z,Reinforcement Learning,"  In Reinforcement Learning (RL), Laplacian Representation (LapRep) is a
task-agnostic state representation that encodes the geometry of the
environment. A desirable property of LapRep stated in prior works is that the
Euclidean distance in the LapRep space roughly reflects the reachability
between states, which motivates the usage of this distance for reward shaping.
However, we find that LapRep does not necessarily have this property in
general: two states having small distance under LapRep can actually be far away
in the environment. Such mismatch would impede the learning process in reward
shaping. To fix this issue, we introduce a Reachability-Aware Laplacian
Representation (RA-LapRep), by properly scaling each dimension of LapRep.
Despite the simplicity, we demonstrate that RA-LapRep can better capture the
inter-state reachability as compared to LapRep, through both theoretical
explanations and experimental results. Additionally, we show that this
improvement yields a significant boost in reward shaping performance and also
benefits bottleneck state discovery.
",1
In-context Reinforcement Learning with Algorithm Distillation,"Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald, DJ Strouse, Steven Hansen, Angelos Filos, Ethan Brooks, Maxime Gazeau, Himanshu Sahni, Satinder Singh, Volodymyr Mnih",2022-10-25T17:57:49Z,Reinforcement Learning,"  We propose Algorithm Distillation (AD), a method for distilling reinforcement
learning (RL) algorithms into neural networks by modeling their training
histories with a causal sequence model. Algorithm Distillation treats learning
to reinforcement learn as an across-episode sequential prediction problem. A
dataset of learning histories is generated by a source RL algorithm, and then a
causal transformer is trained by autoregressively predicting actions given
their preceding learning histories as context. Unlike sequential policy
prediction architectures that distill post-learning or expert sequences, AD is
able to improve its policy entirely in-context without updating its network
parameters. We demonstrate that AD can reinforcement learn in-context in a
variety of environments with sparse rewards, combinatorial task structure, and
pixel-based observations, and find that AD learns a more data-efficient RL
algorithm than the one that generated the source data.
",0
Provable Safe Reinforcement Learning with Binary Feedback,"Andrew Bennett, Dipendra Misra, Nathan Kallus",2022-10-26T05:37:51Z,Reinforcement Learning,"  Safety is a crucial necessity in many applications of reinforcement learning
(RL), whether robotic, automotive, or medical. Many existing approaches to safe
RL rely on receiving numeric safety feedback, but in many cases this feedback
can only take binary values; that is, whether an action in a given state is
safe or unsafe. This is particularly true when feedback comes from human
experts. We therefore consider the problem of provable safe RL when given
access to an offline oracle providing binary feedback on the safety of state,
action pairs. We provide a novel meta algorithm, SABRE, which can be applied to
any MDP setting given access to a blackbox PAC RL algorithm for that setting.
SABRE applies concepts from active learning to reinforcement learning to
provably control the number of queries to the safety oracle. SABRE works by
iteratively exploring the state space to find regions where the agent is
currently uncertain about safety. Our main theoretical results shows that,
under appropriate technical assumptions, SABRE never takes unsafe actions
during training, and is guaranteed to return a near-optimal safe policy with
high probability. We provide a discussion of how our meta-algorithm may be
applied to various settings studied in both theoretical and empirical
frameworks.
",0
Environment Design for Inverse Reinforcement Learning,"Thomas Kleine Buening, Victor Villin, Christos Dimitrakakis",2022-10-26T18:31:17Z,Reinforcement Learning,"  Learning a reward function from demonstrations suffers from low
sample-efficiency. Even with abundant data, current inverse reinforcement
learning methods that focus on learning from a single environment can fail to
handle slight changes in the environment dynamics. We tackle these challenges
through adaptive environment design. In our framework, the learner repeatedly
interacts with the expert, with the former selecting environments to identify
the reward function as quickly as possible from the expert's demonstrations in
said environments. This results in improvements in both sample-efficiency and
robustness, as we show experimentally, for both exact and approximate
inference.
",1
Meta-Reinforcement Learning Using Model Parameters,"Gabriel Hartmann, Amos Azaria",2022-10-27T14:54:06Z,Reinforcement Learning,"  In meta-reinforcement learning, an agent is trained in multiple different
environments and attempts to learn a meta-policy that can efficiently adapt to
a new environment. This paper presents RAMP, a Reinforcement learning Agent
using Model Parameters that utilizes the idea that a neural network trained to
predict environment dynamics encapsulates the environment information. RAMP is
constructed in two phases: in the first phase, a multi-environment
parameterized dynamic model is learned. In the second phase, the model
parameters of the dynamic model are used as context for the multi-environment
policy of the model-free reinforcement learning agent.
",0
Knowledge-Guided Exploration in Deep Reinforcement Learning,"Sahisnu Mazumder, Bing Liu, Shuai Wang, Yingxuan Zhu, Xiaotian Yin, Lifeng Liu, Jian Li",2022-10-26T18:39:49Z,Reinforcement Learning,"  This paper proposes a new method to drastically speed up deep reinforcement
learning (deep RL) training for problems that have the property of state-action
permissibility (SAP). Two types of permissibility are defined under SAP. The
first type says that after an action $a_t$ is performed in a state $s_t$ and
the agent has reached the new state $s_{t+1}$, the agent can decide whether
$a_t$ is permissible or not permissible in $s_t$. The second type says that
even without performing $a_t$ in $s_t$, the agent can already decide whether
$a_t$ is permissible or not in $s_t$. An action is not permissible in a state
if the action can never lead to an optimal solution and thus should not be
tried (over and over again). We incorporate the proposed SAP property and
encode action permissibility knowledge into two state-of-the-art deep RL
algorithms to guide their state-action exploration together with a virtual
stopping strategy. Results show that the SAP-based guidance can markedly speed
up RL training.
",0
BIMRL: Brain Inspired Meta Reinforcement Learning,"Seyed Roozbeh Razavi Rohani, Saeed Hedayatian, Mahdieh Soleymani Baghshah",2022-10-29T08:34:47Z,Reinforcement Learning,"  Sample efficiency has been a key issue in reinforcement learning (RL). An
efficient agent must be able to leverage its prior experiences to quickly adapt
to similar, but new tasks and situations. Meta-RL is one attempt at formalizing
and addressing this issue. Inspired by recent progress in meta-RL, we introduce
BIMRL, a novel multi-layer architecture along with a novel brain-inspired
memory module that will help agents quickly adapt to new tasks within a few
episodes. We also utilize this memory module to design a novel intrinsic reward
that will guide the agent's exploration. Our architecture is inspired by
findings in cognitive neuroscience and is compatible with the knowledge on
connectivity and functionality of different regions in the brain. We
empirically validate the effectiveness of our proposed method by competing with
or surpassing the performance of some strong baselines on multiple MiniGrid
environments.
",0
DanZero: Mastering GuanDan Game with Reinforcement Learning,"Yudong Lu, Jian Zhao, Youpeng Zhao, Wengang Zhou, Houqiang Li",2022-10-31T06:29:08Z,Reinforcement Learning,"  Card game AI has always been a hot topic in the research of artificial
intelligence. In recent years, complex card games such as Mahjong, DouDizhu and
Texas Hold'em have been solved and the corresponding AI programs have reached
the level of human experts. In this paper, we are devoted to developing an AI
program for a more complex card game, GuanDan, whose rules are similar to
DouDizhu but much more complicated. To be specific, the characteristics of
large state and action space, long length of one episode and the unsure number
of players in the GuanDan pose great challenges for the development of the AI
program. To address these issues, we propose the first AI program DanZero for
GuanDan using reinforcement learning technique. Specifically, we utilize a
distributed framework to train our AI system. In the actor processes, we
carefully design the state features and agents generate samples by self-play.
In the learner process, the model is updated by Deep Monte-Carlo Method. After
training for 30 days using 160 CPUs and 1 GPU, we get our DanZero bot. We
compare it with 8 baseline AI programs which are based on heuristic rules and
the results reveal the outstanding performance of DanZero. We also test DanZero
with human players and demonstrate its human-level performance.
",0
Teacher-student curriculum learning for reinforcement learning,Yanick Schraner,2022-10-31T14:45:39Z,Reinforcement Learning,"  Reinforcement learning (rl) is a popular paradigm for sequential decision
making problems. The past decade's advances in rl have led to breakthroughs in
many challenging domains such as video games, board games, robotics, and chip
design. The sample inefficiency of deep reinforcement learning methods is a
significant obstacle when applying rl to real-world problems. Transfer learning
has been applied to reinforcement learning such that the knowledge gained in
one task can be applied when training in a new task. Curriculum learning is
concerned with sequencing tasks or data samples such that knowledge can be
transferred between those tasks to learn a target task that would otherwise be
too difficult to solve. Designing a curriculum that improves sample efficiency
is a complex problem. In this thesis, we propose a teacher-student curriculum
learning setting where we simultaneously train a teacher that selects tasks for
the student while the student learns how to solve the selected task. Our method
is independent of human domain knowledge and manual curriculum design. We
evaluated our methods on two reinforcement learning benchmarks: grid world and
the challenging Google Football environment. With our method, we can improve
the sample efficiency and generality of the student compared to tabula-rasa
reinforcement learning.
",1
Reinforcement Learning in Non-Markovian Environments,"Siddharth Chandak, Pratik Shah, Vivek S Borkar, Parth Dodhia",2022-11-03T05:41:23Z,Reinforcement Learning,"  Motivated by the novel paradigm developed by Van Roy and coauthors for
reinforcement learning in arbitrary non-Markovian environments, we propose a
related formulation and explicitly pin down the error caused by
non-Markovianity of observations when the Q-learning algorithm is applied on
this formulation. Based on this observation, we propose that the criterion for
agent design should be to seek good approximations for certain conditional
laws. Inspired by classical stochastic control, we show that our problem
reduces to that of recursive computation of approximate sufficient statistics.
This leads to an autoencoder-based scheme for agent design which is then
numerically tested on partially observed reinforcement learning environments.
",0
Connecting Stochastic Optimal Control and Reinforcement Learning,"Jannes Quer, Enric Ribera Borrell",2022-11-04T14:04:06Z,Reinforcement Learning,"  In this paper the connection between stochastic optimal control and
reinforcement learning is investigated. Our main motivation is to apply
importance sampling to sampling rare events which can be reformulated as an
optimal control problem. By using a parameterised approach the optimal control
problem becomes a stochastic optimization problem which still raises some open
questions regarding how to tackle the scalability to high-dimensional problems
and how to deal with the intrinsic metastability of the system. To explore new
methods we link the optimal control problem to reinforcement learning since
both share the same underlying framework, namely a Markov Decision Process
(MDP). For the optimal control problem we show how the MDP can be formulated.
In addition we discuss how the stochastic optimal control problem can be
interpreted in the framework of reinforcement learning. At the end of the
article we present the application of two different reinforcement learning
algorithms to the optimal control problem and a comparison of the advantages
and disadvantages of the two algorithms.
",0
Design Process is a Reinforcement Learning Problem,"Reza kakooee, Benjamin Dillunberger",2022-11-06T14:37:22Z,Reinforcement Learning,"  While reinforcement learning has been used widely in research during the past
few years, it found fewer real-world applications than supervised learning due
to some weaknesses that the RL algorithms suffer from, such as performance
degradation in transitioning from the simulator to the real world. Here, we
argue the design process is a reinforcement learning problem and can
potentially be a proper application for RL algorithms as it is an offline
process and conventionally is done in CAD software - a sort of simulator. This
creates opportunities for using RL methods and, at the same time, raises
challenges. While the design processes are so diverse, here we focus on the
space layout planning (SLP), frame it as an RL problem under the Markov
Decision Process, and use PPO to address the layout design problem. To do so,
we developed an environment named RLDesigner, to simulate the SLP. The
RLDesigner is an OpenAI Gym compatible environment that can be easily
customized to define a diverse range of design scenarios. We publicly share the
environment to encourage both RL and architecture communities to use it for
testing different RL algorithms or in their design practice. The codes are
available in the following GitHub repository https://github.com/
RezaKakooee/rldesigner/tree/Second_Paper
",0
Curriculum-based Asymmetric Multi-task Reinforcement Learning,"Hanchi Huang, Deheng Ye, Li Shen, Wei Liu",2022-11-07T08:05:13Z,Reinforcement Learning,"  We introduce CAMRL, the first curriculum-based asymmetric multi-task learning
(AMTL) algorithm for dealing with multiple reinforcement learning (RL) tasks
altogether. To mitigate the negative influence of customizing the one-off
training order in curriculum-based AMTL, CAMRL switches its training mode
between parallel single-task RL and asymmetric multi-task RL (MTRL), according
to an indicator regarding the training time, the overall performance, and the
performance gap among tasks. To leverage the multi-sourced prior knowledge
flexibly and to reduce negative transfer in AMTL, we customize a composite loss
with multiple differentiable ranking functions and optimize the loss through
alternating optimization and the Frank-Wolfe algorithm. The uncertainty-based
automatic adjustment of hyper-parameters is also applied to eliminate the need
of laborious hyper-parameter analysis during optimization. By optimizing the
composite loss, CAMRL predicts the next training task and continuously revisits
the transfer matrix and network weights. We have conducted experiments on a
wide range of benchmarks in multi-task RL, covering Gym-minigrid, Meta-world,
Atari video games, vision-based PyBullet tasks, and RLBench, to show the
improvements of CAMRL over the corresponding single-task RL algorithm and
state-of-the-art MTRL algorithms. The code is available at:
https://github.com/huanghanchi/CAMRL
",0
A Survey on Quantum Reinforcement Learning,"Nico Meyer, Christian Ufrecht, Maniraman Periyasamy, Daniel D. Scherer, Axel Plinge, Christopher Mutschler",2022-11-07T11:25:47Z,Reinforcement Learning,"  Quantum reinforcement learning is an emerging field at the intersection of
quantum computing and machine learning. While we intend to provide a broad
overview of the literature on quantum reinforcement learning - our
interpretation of this term will be clarified below - we put particular
emphasis on recent developments. With a focus on already available noisy
intermediate-scale quantum devices, these include variational quantum circuits
acting as function approximators in an otherwise classical reinforcement
learning setting. In addition, we survey quantum reinforcement learning
algorithms based on future fault-tolerant hardware, some of which come with a
provable quantum advantage. We provide both a birds-eye-view of the field, as
well as summaries and reviews for selected parts of the literature.
",0
Pretraining in Deep Reinforcement Learning: A Survey,"Zhihui Xie, Zichuan Lin, Junyou Li, Shuai Li, Deheng Ye",2022-11-08T02:17:54Z,Reinforcement Learning,"  The past few years have seen rapid progress in combining reinforcement
learning (RL) with deep learning. Various breakthroughs ranging from games to
robotics have spurred the interest in designing sophisticated RL algorithms and
systems. However, the prevailing workflow in RL is to learn tabula rasa, which
may incur computational inefficiency. This precludes continuous deployment of
RL algorithms and potentially excludes researchers without large-scale
computing resources. In many other areas of machine learning, the pretraining
paradigm has shown to be effective in acquiring transferable knowledge, which
can be utilized for a variety of downstream tasks. Recently, we saw a surge of
interest in Pretraining for Deep RL with promising results. However, much of
the research has been based on different experimental settings. Due to the
nature of RL, pretraining in this field is faced with unique challenges and
hence requires new design principles. In this survey, we seek to systematically
review existing works in pretraining for deep reinforcement learning, provide a
taxonomy of these methods, discuss each sub-field, and bring attention to open
problems and future directions.
",0
Reinforcement Learning with Stepwise Fairness Constraints,"Zhun Deng, He Sun, Zhiwei Steven Wu, Linjun Zhang, David C. Parkes",2022-11-08T04:06:23Z,Reinforcement Learning,"  AI methods are used in societally important settings, ranging from credit to
employment to housing, and it is crucial to provide fairness in regard to
algorithmic decision making. Moreover, many settings are dynamic, with
populations responding to sequential decision policies. We introduce the study
of reinforcement learning (RL) with stepwise fairness constraints, requiring
group fairness at each time step. Our focus is on tabular episodic RL, and we
provide learning algorithms with strong theoretical guarantees in regard to
policy optimality and fairness violation. Our framework provides useful tools
to study the impact of fairness constraints in sequential settings and brings
up new challenges in RL.
",0
Synthesis of separation processes with reinforcement learning,"Stephan C. P. A. van Kalmthout, Laurence I. Midgley, Meik B. Franke",2022-11-03T22:21:01Z,Reinforcement Learning,"  This paper shows the implementation of reinforcement learning (RL) in
commercial flowsheet simulator software (Aspen Plus V12) for designing and
optimising a distillation sequence. The aim of the SAC agent was to separate a
hydrocarbon mixture in its individual components by utilising distillation.
While doing so it tries to maximise the profit produced by the distillation
sequence. All actions of the agent were set by the SAC agent in Python and
communicated in Aspen Plus via an API. Here the distillation column was
simulated by use of the build-in RADFRAC column. With this a connection was
established for data transfer between Python and Aspen and the agent succeeded
to show learning behaviour, while increasing profit. Although results were
generated, the use of Aspen was slow (190 hours) and Aspen was found unsuitable
for parallelisation. This makes that Aspen is incompatible for solving RL
problems. Code and thesis are available at https://github.com/lollcat/Aspen-RL
",0
Leveraging Offline Data in Online Reinforcement Learning,"Andrew Wagenmaker, Aldo Pacchiano",2022-11-09T15:39:32Z,"RAG, Reinforcement Learning","  Two central paradigms have emerged in the reinforcement learning (RL)
community: online RL and offline RL. In the online RL setting, the agent has no
prior knowledge of the environment, and must interact with it in order to find
an $\epsilon$-optimal policy. In the offline RL setting, the learner instead
has access to a fixed dataset to learn from, but is unable to otherwise
interact with the environment, and must obtain the best policy it can from this
offline data. Practical scenarios often motivate an intermediate setting: if we
have some set of offline data and, in addition, may also interact with the
environment, how can we best use the offline data to minimize the number of
online interactions necessary to learn an $\epsilon$-optimal policy?
  In this work, we consider this setting, which we call the \textsf{FineTuneRL}
setting, for MDPs with linear structure. We characterize the necessary number
of online samples needed in this setting given access to some offline dataset,
and develop an algorithm, \textsc{FTPedel}, which is provably optimal, up to
$H$ factors. We show through an explicit example that combining offline data
with online interactions can lead to a provable improvement over either purely
offline or purely online RL. Finally, our results illustrate the distinction
between \emph{verifiable} learning, the typical setting considered in online
RL, and \emph{unverifiable} learning, the setting often considered in offline
RL, and show that there is a formal separation between these regimes.
",0
Autotelic Reinforcement Learning in Multi-Agent Environments,"Eleni Nisioti, Elías Masquil, Gautier Hamon, and Clément Moulin-Frier",2022-11-11T09:30:07Z,Reinforcement Learning,"  In the intrinsically motivated skills acquisition problem, the agent is set
in an environment without any pre-defined goals and needs to acquire an
open-ended repertoire of skills. To do so the agent needs to be autotelic
(deriving from the Greek auto (self) and telos (end goal)): it needs to
generate goals and learn to achieve them following its own intrinsic motivation
rather than external supervision. Autotelic agents have so far been considered
in isolation. But many applications of open-ended learning entail groups of
agents. Multi-agent environments pose an additional challenge for autotelic
agents: to discover and master goals that require cooperation agents must
pursue them simultaneously, but they have low chances of doing so if they
sample them independently. In this work, we propose a new learning paradigm for
modeling such settings, the Decentralized Intrinsically Motivated Skills
Acquisition Problem (Dec-IMSAP), and employ it to solve cooperative navigation
tasks. First, we show that agents setting their goals independently fail to
master the full diversity of goals. Then, we show that a sufficient condition
for achieving this is to ensure that a group aligns its goals, i.e., the agents
pursue the same cooperative goal. Our empirical analysis shows that alignment
enables specialization, an efficient strategy for cooperation. Finally, we
introduce the Goal-coordination game, a fully-decentralized emergent
communication algorithm, where goal alignment emerges from the maximization of
individual rewards in multi-goal cooperative environments and show that it is
able to reach equal performance to a centralized training baseline that
guarantees aligned goals. To our knowledge, this is the first contribution
addressing the problem of intrinsically motivated multi-agent goal exploration
in a decentralized training paradigm.
",0
Deep Reinforcement Learning with Vector Quantized Encoding,"Liang Zhang, Justin Lieffers, Adarsh Pyarelal",2022-11-12T19:51:19Z,Reinforcement Learning,"  Human decision-making often involves combining similar states into categories
and reasoning at the level of the categories rather than the actual states.
Guided by this intuition, we propose a novel method for clustering state
features in deep reinforcement learning (RL) methods to improve their
interpretability. Specifically, we propose a plug-and-play framework termed
\emph{vector quantized reinforcement learning} (VQ-RL) that extends classic RL
pipelines with an auxiliary classification task based on vector quantized (VQ)
encoding and aligns with policy training. The VQ encoding method categorizes
features with similar semantics into clusters and results in tighter clusters
with better separation compared to classic deep RL methods, thus enabling
neural models to learn similarities and differences between states better.
Furthermore, we introduce two regularization methods to help increase the
separation between clusters and avoid the risks associated with VQ training. In
simulations, we demonstrate that VQ-RL improves interpretability and
investigate its impact on robustness and generalization of deep RL.
",0
Controlling Commercial Cooling Systems Using Reinforcement Learning,"Jerry Luo, Cosmin Paduraru, Octavian Voicu, Yuri Chervonyi, Scott Munns, Jerry Li, Crystal Qian, Praneet Dutta, Jared Quincy Davis, Ningjia Wu, Xingwei Yang, Chu-Ming Chang, Ted Li, Rob Rose, Mingyan Fan, Hootan Nakhost, Tinglin Liu, Brian Kirkman, Frank Altamura, Lee Cline, Patrick Tonker, Joel Gouker, Dave Uden, Warren Buddy Bryan, Jason Law, Deeni Fatiha, Neil Satra, Juliet Rothenberg, Mandeep Waraich, Molly Carlin, Satish Tallapaka, Sims Witherspoon, David Parish, Peter Dolan, Chenyu Zhao, Daniel J. Mankowitz",2022-11-11T17:48:13Z,Reinforcement Learning,"  This paper is a technical overview of DeepMind and Google's recent work on
reinforcement learning for controlling commercial cooling systems. Building on
expertise that began with cooling Google's data centers more efficiently, we
recently conducted live experiments on two real-world facilities in partnership
with Trane Technologies, a building management system provider. These live
experiments had a variety of challenges in areas such as evaluation, learning
from offline data, and constraint satisfaction. Our paper describes these
challenges in the hope that awareness of them will benefit future applied RL
work. We also describe the way we adapted our RL system to deal with these
challenges, resulting in energy savings of approximately 9% and 13%
respectively at the two live experiment sites.
",0
Contextual Transformer for Offline Meta Reinforcement Learning,"Runji Lin, Ye Li, Xidong Feng, Zhaowei Zhang, Xian Hong Wu Fung, Haifeng Zhang, Jun Wang, Yali Du, Yaodong Yang",2022-11-15T10:00:14Z,Reinforcement Learning,"  The pretrain-finetuning paradigm in large-scale sequence models has made
significant progress in natural language processing and computer vision tasks.
However, such a paradigm is still hindered by several challenges in
Reinforcement Learning (RL), including the lack of self-supervised pretraining
algorithms based on offline data and efficient fine-tuning/prompt-tuning over
unseen downstream tasks. In this work, we explore how prompts can improve
sequence modeling-based offline reinforcement learning (offline-RL) algorithms.
Firstly, we propose prompt tuning for offline RL, where a context vector
sequence is concatenated with the input to guide the conditional policy
generation. As such, we can pretrain a model on the offline dataset with
self-supervised loss and learn a prompt to guide the policy towards desired
actions. Secondly, we extend our framework to Meta-RL settings and propose
Contextual Meta Transformer (CMT); CMT leverages the context among different
tasks as the prompt to improve generalization on unseen tasks. We conduct
extensive experiments across three different offline-RL settings: offline
single-agent RL on the D4RL dataset, offline Meta-RL on the MuJoCo benchmark,
and offline MARL on the SMAC benchmark. Superior results validate the strong
performance, and generality of our methods.
",5
Offline Reinforcement Learning with Adaptive Behavior Regularization,"Yunfan Zhou, Xijun Li, Qingyu Qu",2022-11-15T15:59:11Z,Reinforcement Learning,"  Offline reinforcement learning (RL) defines a sample-efficient learning
paradigm, where a policy is learned from static and previously collected
datasets without additional interaction with the environment. The major
obstacle to offline RL is the estimation error arising from evaluating the
value of out-of-distribution actions. To tackle this problem, most existing
offline RL methods attempt to acquire a policy both ``close"" to the behaviors
contained in the dataset and sufficiently improved over them, which requires a
trade-off between two possibly conflicting targets. In this paper, we propose a
novel approach, which we refer to as adaptive behavior regularization (ABR), to
balance this critical trade-off. By simply utilizing a sample-based
regularization, ABR enables the policy to adaptively adjust its optimization
objective between cloning and improving over the policy used to generate the
dataset. In the evaluation on D4RL datasets, a widely adopted benchmark for
offline reinforcement learning, ABR can achieve improved or competitive
performance compared to existing state-of-the-art algorithms.
",0
TEMPERA: Test-Time Prompting via Reinforcement Learning,"Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, Joseph E. Gonzalez",2022-11-21T22:38:20Z,Reinforcement Learning,"  Careful prompt design is critical to the use of large language models in
zero-shot or few-shot learning. As a consequence, there is a growing interest
in automated methods to design optimal prompts. In this work, we propose
Test-time Prompt Editing using Reinforcement learning (TEMPERA). In contrast to
prior prompt generation methods, TEMPERA can efficiently leverage prior
knowledge, is adaptive to different queries and provides an interpretable
prompt for every query. To achieve this, we design a novel action space that
allows flexible editing of the initial prompts covering a wide set of
commonly-used components like instructions, few-shot exemplars, and
verbalizers. The proposed method achieves significant gains compared with
recent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt, across a
variety of tasks including sentiment analysis, topic classification, natural
language inference, and reading comprehension. Our method achieves 5.33x on
average improvement in sample efficiency when compared to the traditional
fine-tuning methods.
",0
Causal Deep Reinforcement Learning Using Observational Data,"Wenxuan Zhu, Chao Yu, Qiang Zhang",2022-11-28T14:34:39Z,Reinforcement Learning,"  Deep reinforcement learning (DRL) requires the collection of interventional
data, which is sometimes expensive and even unethical in the real world, such
as in the autonomous driving and the medical field. Offline reinforcement
learning promises to alleviate this issue by exploiting the vast amount of
observational data available in the real world. However, observational data may
mislead the learning agent to undesirable outcomes if the behavior policy that
generates the data depends on unobserved random variables (i.e., confounders).
In this paper, we propose two deconfounding methods in DRL to address this
problem. The methods first calculate the importance degree of different samples
based on the causal inference technique, and then adjust the impact of
different samples on the loss function by reweighting or resampling the offline
dataset to ensure its unbiasedness. These deconfounding methods can be flexibly
combined with existing model-free DRL algorithms such as soft actor-critic and
deep Q-learning, provided that a weak condition can be satisfied by the loss
functions of these algorithms. We prove the effectiveness of our deconfounding
methods and validate them experimentally.
",0
Climate Change Policy Exploration using Reinforcement Learning,Theodore Wolf,2022-10-23T18:20:17Z,Reinforcement Learning,"  Climate Change is an incredibly complicated problem that humanity faces. When
many variables interact with each other, it can be difficult for humans to
grasp the causes and effects of the very large-scale problem of climate change.
The climate is a dynamical system, where small changes can have considerable
and unpredictable repercussions in the long term. Understanding how to nudge
this system in the right ways could help us find creative solutions to climate
change.
  In this research, we combine Deep Reinforcement Learning and a World-Earth
system model to find, and explain, creative strategies to a sustainable future.
This is an extension of the work from Strnad et al. where we extend on the
method and analysis, by taking multiple directions. We use four different
Reinforcement Learning agents varying in complexity to probe the environment in
different ways and to find various strategies. The environment is a
low-complexity World Earth system model where the goal is to reach a future
where all the energy for the economy is produced by renewables by enacting
different policies. We use a reward function based on planetary boundaries that
we modify to force the agents to find a wider range of strategies. To favour
applicability, we slightly modify the environment, by injecting noise and
making it fully observable, to understand the impacts of these factors on the
learning of the agents.
",0
Efficient Reinforcement Learning Through Trajectory Generation,"Wenqi Cui, Linbin Huang, Weiwei Yang, Baosen Zhang",2022-11-30T18:49:43Z,Reinforcement Learning,"  A key barrier to using reinforcement learning (RL) in many real-world
applications is the requirement of a large number of system interactions to
learn a good control policy. Off-policy and Offline RL methods have been
proposed to reduce the number of interactions with the physical environment by
learning control policies from historical data. However, their performances
suffer from the lack of exploration and the distributional shifts in
trajectories once controllers are updated. Moreover, most RL methods require
that all states are directly observed, which is difficult to be attained in
many settings.
  To overcome these challenges, we propose a trajectory generation algorithm,
which adaptively generates new trajectories as if the system is being operated
and explored under the updated control policies. Motivated by the fundamental
lemma for linear systems, assuming sufficient excitation, we generate
trajectories from linear combinations of historical trajectories. For linear
feedback control, we prove that the algorithm generates trajectories with the
exact distribution as if they are sampled from the real system using the
updated control policy. In particular, the algorithm extends to systems where
the states are not directly observed. Experiments show that the proposed method
significantly reduces the number of sampled data needed for RL algorithms.
",0
CT-DQN: Control-Tutored Deep Reinforcement Learning,"Francesco De Lellis, Marco Coraggio, Giovanni Russo, Mirco Musolesi, Mario di Bernardo",2022-12-02T17:59:43Z,Reinforcement Learning,"  One of the major challenges in Deep Reinforcement Learning for control is the
need for extensive training to learn the policy. Motivated by this, we present
the design of the Control-Tutored Deep Q-Networks (CT-DQN) algorithm, a Deep
Reinforcement Learning algorithm that leverages a control tutor, i.e., an
exogenous control law, to reduce learning time. The tutor can be designed using
an approximate model of the system, without any assumption about the knowledge
of the system's dynamics. There is no expectation that it will be able to
achieve the control objective if used stand-alone. During learning, the tutor
occasionally suggests an action, thus partially guiding exploration. We
validate our approach on three scenarios from OpenAI Gym: the inverted
pendulum, lunar lander, and car racing. We demonstrate that CT-DQN is able to
achieve better or equivalent data efficiency with respect to the classic
function approximation solutions.
",0
Multi-Agent Reinforcement Learning with Reward Delays,"Yuyang Zhang, Runyu Zhang, Yuantao Gu, Na Li",2022-12-02T20:50:48Z,Reinforcement Learning,"  This paper considers multi-agent reinforcement learning (MARL) where the
rewards are received after delays and the delay time varies across agents and
across time steps. Based on the V-learning framework, this paper proposes MARL
algorithms that efficiently deal with reward delays. When the delays are
finite, our algorithm reaches a coarse correlated equilibrium (CCE) with rate
$\tilde{\mathcal{O}}(\frac{H^3\sqrt{S\mathcal{T}_K}}{K}+\frac{H^3\sqrt{SA}}{\sqrt{K}})$
where $K$ is the number of episodes, $H$ is the planning horizon, $S$ is the
size of the state space, $A$ is the size of the largest action space, and
$\mathcal{T}_K$ is the measure of total delay formally defined in the paper.
Moreover, our algorithm is extended to cases with infinite delays through a
reward skipping scheme. It achieves convergence rate similar to the finite
delay case.
",0
Physics-Informed Model-Based Reinforcement Learning,"Adithya Ramesh, Balaraman Ravindran",2022-12-05T11:26:10Z,Reinforcement Learning,"  We apply reinforcement learning (RL) to robotics tasks. One of the drawbacks
of traditional RL algorithms has been their poor sample efficiency. One
approach to improve the sample efficiency is model-based RL. In our model-based
RL algorithm, we learn a model of the environment, essentially its transition
dynamics and reward function, use it to generate imaginary trajectories and
backpropagate through them to update the policy, exploiting the
differentiability of the model. Intuitively, learning more accurate models
should lead to better model-based RL performance. Recently, there has been
growing interest in developing better deep neural network based dynamics models
for physical systems, by utilizing the structure of the underlying physics. We
focus on robotic systems undergoing rigid body motion without contacts. We
compare two versions of our model-based RL algorithm, one which uses a standard
deep neural network based dynamics model and the other which uses a much more
accurate, physics-informed neural network based dynamics model. We show that,
in model-based RL, model accuracy mainly matters in environments that are
sensitive to initial conditions, where numerical errors accumulate fast. In
these environments, the physics-informed version of our algorithm achieves
significantly better average-return and sample efficiency. In environments that
are not sensitive to initial conditions, both versions of our algorithm achieve
similar average-return, while the physics-informed version achieves better
sample efficiency. We also show that, in challenging environments,
physics-informed model-based RL achieves better average-return than
state-of-the-art model-free RL algorithms such as Soft Actor-Critic, as it
computes the policy-gradient analytically, while the latter estimates it
through sampling.
",0
Understanding Self-Predictive Learning for Reinforcement Learning,"Yunhao Tang, Zhaohan Daniel Guo, Pierre Harvey Richemond, Bernardo Ávila Pires, Yash Chandak, Rémi Munos, Mark Rowland, Mohammad Gheshlaghi Azar, Charline Le Lan, Clare Lyle, András György, Shantanu Thakoor, Will Dabney, Bilal Piot, Daniele Calandriello, Michal Valko",2022-12-06T20:43:37Z,Reinforcement Learning,"  We study the learning dynamics of self-predictive learning for reinforcement
learning, a family of algorithms that learn representations by minimizing the
prediction error of their own future latent representations. Despite its recent
empirical success, such algorithms have an apparent defect: trivial
representations (such as constants) minimize the prediction error, yet it is
obviously undesirable to converge to such solutions. Our central insight is
that careful designs of the optimization dynamics are critical to learning
meaningful representations. We identify that a faster paced optimization of the
predictor and semi-gradient updates on the representation, are crucial to
preventing the representation collapse. Then in an idealized setup, we show
self-predictive learning dynamics carries out spectral decomposition on the
state transition matrix, effectively capturing information of the transition
dynamics. Building on the theoretical insights, we propose bidirectional
self-predictive learning, a novel self-predictive algorithm that learns two
representations simultaneously. We examine the robustness of our theoretical
insights with a number of small-scale experiments and showcase the promise of
the novel representation learning algorithm with large-scale experiments.
",24
Reinforcement Learning for Resilient Power Grids,"Zhenting Zhao, Po-Yen Chen, Yucheng Jin",2022-12-08T04:40:14Z,Reinforcement Learning,"  Traditional power grid systems have become obsolete under more frequent and
extreme natural disasters. Reinforcement learning (RL) has been a promising
solution for resilience given its successful history of power grid control.
However, most power grid simulators and RL interfaces do not support simulation
of power grid under large-scale blackouts or when the network is divided into
sub-networks. In this study, we proposed an updated power grid simulator built
on Grid2Op, an existing simulator and RL interface, and experimented on
limiting the action and observation spaces of Grid2Op. By testing with DDQN and
SliceRDQN algorithms, we found that reduced action spaces significantly improve
training performance and efficiency. In addition, we investigated a low-rank
neural network regularization method for deep Q-learning, one of the most
widely used RL algorithms, in this power grid control scenario. As a result,
the experiment demonstrated that in the power grid simulation environment,
adopting this method will significantly increase the performance of RL agents.
",3
Reinforcement Learning for Predicting Traffic Accidents,"Injoon Cho, Praveen Kumar Rajendran, Taeyoung Kim, Dongsoo Har",2022-12-09T05:53:30Z,Reinforcement Learning,"  As the demand for autonomous driving increases, it is paramount to ensure
safety. Early accident prediction using deep learning methods for driving
safety has recently gained much attention. In this task, early accident
prediction and a point prediction of where the drivers should look are
determined, with the dashcam video as input. We propose to exploit the double
actors and regularized critics (DARC) method, for the first time, on this
accident forecasting platform. We derive inspiration from DARC since it is
currently a state-of-the-art reinforcement learning (RL) model on continuous
action space suitable for accident anticipation. Results show that by utilizing
DARC, we can make predictions 5\% earlier on average while improving in
multiple metrics of precision compared to existing methods. The results imply
that using our RL-based problem formulation could significantly increase the
safety of autonomous driving.
",0
Near-Optimal Differentially Private Reinforcement Learning,"Dan Qiao, Yu-Xiang Wang",2022-12-09T06:03:02Z,Reinforcement Learning,"  Motivated by personalized healthcare and other applications involving
sensitive data, we study online exploration in reinforcement learning with
differential privacy (DP) constraints. Existing work on this problem
established that no-regret learning is possible under joint differential
privacy (JDP) and local differential privacy (LDP) but did not provide an
algorithm with optimal regret. We close this gap for the JDP case by designing
an $\epsilon$-JDP algorithm with a regret of
$\widetilde{O}(\sqrt{SAH^2T}+S^2AH^3/\epsilon)$ which matches the
information-theoretic lower bound of non-private learning for all choices of
$\epsilon> S^{1.5}A^{0.5} H^2/\sqrt{T}$. In the above, $S$, $A$ denote the
number of states and actions, $H$ denotes the planning horizon, and $T$ is the
number of steps. To the best of our knowledge, this is the first private RL
algorithm that achieves \emph{privacy for free} asymptotically as $T\rightarrow
\infty$. Our techniques -- which could be of independent interest -- include
privately releasing Bernstein-type exploration bonuses and an improved method
for releasing visitation statistics. The same techniques also imply a slightly
improved regret bound for the LDP case.
",8
Efficient Exploration in Resource-Restricted Reinforcement Learning,"Zhihai Wang, Taoxing Pan, Qi Zhou, Jie Wang",2022-12-14T02:50:26Z,Reinforcement Learning,"  In many real-world applications of reinforcement learning (RL), performing
actions requires consuming certain types of resources that are
non-replenishable in each episode. Typical applications include robotic control
with limited energy and video games with consumable items. In tasks with
non-replenishable resources, we observe that popular RL methods such as soft
actor critic suffer from poor sample efficiency. The major reason is that, they
tend to exhaust resources fast and thus the subsequent exploration is severely
restricted due to the absence of resources. To address this challenge, we first
formalize the aforementioned problem as a resource-restricted reinforcement
learning, and then propose a novel resource-aware exploration bonus (RAEB) to
make reasonable usage of resources. An appealing feature of RAEB is that, it
can significantly reduce unnecessary resource-consuming trials while
effectively encouraging the agent to explore unvisited states. Experiments
demonstrate that the proposed RAEB significantly outperforms state-of-the-art
exploration strategies in resource-restricted reinforcement learning
environments, improving the sample efficiency by up to an order of magnitude.
",0
Quantum Control based on Deep Reinforcement Learning,Zhikang Wang,2022-12-14T18:12:26Z,Reinforcement Learning,"  In this thesis, we consider two simple but typical control problems and apply
deep reinforcement learning to them, i.e., to cool and control a particle which
is subject to continuous position measurement in a one-dimensional quadratic
potential or in a quartic potential. We compare the performance of
reinforcement learning control and conventional control strategies on the two
problems, and show that the reinforcement learning achieves a performance
comparable to the optimal control for the quadratic case, and outperforms
conventional control strategies for the quartic case for which the optimal
control strategy is unknown. To our knowledge, this is the first time deep
reinforcement learning is applied to quantum control problems in continuous
real space. Our research demonstrates that deep reinforcement learning can be
used to control a stochastic quantum system in real space effectively as a
measurement-feedback closed-loop controller, and our research also shows the
ability of AI to discover new control strategies and properties of the quantum
systems that are not well understood, and we can gain insights into these
problems by learning from the AI, which opens up a new regime for scientific
research.
",0
Robust Policy Optimization in Deep Reinforcement Learning,"Md Masudur Rahman, Yexiang Xue",2022-12-14T22:43:56Z,Reinforcement Learning,"  The policy gradient method enjoys the simplicity of the objective where the
agent optimizes the cumulative reward directly. Moreover, in the continuous
action domain, parameterized distribution of action distribution allows easy
control of exploration, resulting from the variance of the representing
distribution. Entropy can play an essential role in policy optimization by
selecting the stochastic policy, which eventually helps better explore the
environment in reinforcement learning (RL). However, the stochasticity often
reduces as the training progresses; thus, the policy becomes less exploratory.
Additionally, certain parametric distributions might only work for some
environments and require extensive hyperparameter tuning. This paper aims to
mitigate these issues. In particular, we propose an algorithm called Robust
Policy Optimization (RPO), which leverages a perturbed distribution. We
hypothesize that our method encourages high-entropy actions and provides a way
to represent the action space better. We further provide empirical evidence to
verify our hypothesis. We evaluated our methods on various continuous control
tasks from DeepMind Control, OpenAI Gym, Pybullet, and IsaacGym. We observed
that in many settings, RPO increases the policy entropy early in training and
then maintains a certain level of entropy throughout the training period.
Eventually, our agent RPO shows consistently improved performance compared to
PPO and other techniques: entropy regularization, different distributions, and
data augmentation. Furthermore, in several settings, our method stays robust in
performance, while other baseline mechanisms fail to improve and even worsen
the performance.
",0
Reinforcement Learning in Credit Scoring and Underwriting,"Seksan Kiatsupaibul, Pakawan Chansiripas, Pojtanut Manopanjasiri, Kantapong Visantavarakul, Zheng Wen",2022-12-15T06:36:14Z,Reinforcement Learning,"  This paper proposes a novel reinforcement learning (RL) framework for credit
underwriting that tackles ungeneralizable contextual challenges. We adapt RL
principles for credit scoring, incorporating action space renewal and
multi-choice actions. Our work demonstrates that the traditional underwriting
approach aligns with the RL greedy strategy. We introduce two new RL-based
credit underwriting algorithms to enable more informed decision-making.
Simulations show these new approaches outperform the traditional method in
scenarios where the data aligns with the model. However, complex situations
highlight model limitations, emphasizing the importance of powerful machine
learning models for optimal performance. Future research directions include
exploring more sophisticated models alongside efficient exploration mechanisms.
",0
Offline Reinforcement Learning for Visual Navigation,"Dhruv Shah, Arjun Bhorkar, Hrish Leen, Ilya Kostrikov, Nick Rhinehart, Sergey Levine",2022-12-16T02:23:50Z,Reinforcement Learning,"  Reinforcement learning can enable robots to navigate to distant goals while
optimizing user-specified reward functions, including preferences for following
lanes, staying on paved paths, or avoiding freshly mowed grass. However, online
learning from trial-and-error for real-world robots is logistically
challenging, and methods that instead can utilize existing datasets of robotic
navigation data could be significantly more scalable and enable broader
generalization. In this paper, we present ReViND, the first offline RL system
for robotic navigation that can leverage previously collected data to optimize
user-specified reward functions in the real-world. We evaluate our system for
off-road navigation without any additional data collection or fine-tuning, and
show that it can navigate to distant goals using only offline training from
this dataset, and exhibit behaviors that qualitatively differ based on the
user-specified reward function.
",25
Latent Variable Representation for Reinforcement Learning,"Tongzheng Ren, Chenjun Xiao, Tianjun Zhang, Na Li, Zhaoran Wang, Sujay Sanghavi, Dale Schuurmans, Bo Dai",2022-12-17T00:26:31Z,Reinforcement Learning,"  Deep latent variable models have achieved significant empirical successes in
model-based reinforcement learning (RL) due to their expressiveness in modeling
complex transition dynamics. On the other hand, it remains unclear
theoretically and empirically how latent variable models may facilitate
learning, planning, and exploration to improve the sample efficiency of RL. In
this paper, we provide a representation view of the latent variable models for
state-action value functions, which allows both tractable variational learning
algorithm and effective implementation of the optimism/pessimism principle in
the face of uncertainty for exploration. In particular, we propose a
computationally efficient planning algorithm with UCB exploration by
incorporating kernel embeddings of latent variable models. Theoretically, we
establish the sample complexity of the proposed approach in the online and
offline settings. Empirically, we demonstrate superior performance over current
state-of-the-art algorithms across various benchmarks.
",0
Risk-Sensitive Reinforcement Learning with Exponential Criteria,"Erfaun Noorani, Christos Mavridis, John Baras",2022-12-18T04:44:38Z,Reinforcement Learning,"  While reinforcement learning has shown experimental success in a number of
applications, it is known to be sensitive to noise and perturbations in the
parameters of the system, leading to high variance in the total reward amongst
different episodes in slightly different environments. To introduce robustness,
as well as sample efficiency, risk-sensitive reinforcement learning methods are
being thoroughly studied. In this work, we provide a definition of robust
reinforcement learning policies and formulate a risk-sensitive reinforcement
learning problem to approximate them, by solving an optimization problem with
respect to a modified objective based on exponential criteria. In particular,
we study a model-free risk-sensitive variation of the widely-used Monte Carlo
Policy Gradient algorithm and introduce a novel risk-sensitive online
Actor-Critic algorithm based on solving a multiplicative Bellman equation using
stochastic approximation updates. Analytical results suggest that the use of
exponential criteria generalizes commonly used ad-hoc regularization
approaches, improves sample efficiency, and introduces robustness with respect
to perturbations in the model parameters and the environment. The
implementation, performance, and robustness properties of the proposed methods
are evaluated in simulated experiments.
",0
Neural Coreference Resolution based on Reinforcement Learning,"Yu Wang, Hongxia Jin",2022-12-18T07:36:35Z,Reinforcement Learning,"  The target of a coreference resolution system is to cluster all mentions that
refer to the same entity in a given context. All coreference resolution systems
need to solve two subtasks; one task is to detect all of the potential
mentions, and the other is to learn the linking of an antecedent for each
possible mention. In this paper, we propose a reinforcement learning
actor-critic-based neural coreference resolution system, which can achieve both
mention detection and mention clustering by leveraging an actor-critic deep
reinforcement learning technique and a joint training algorithm. We experiment
on the BERT model to generate different input span representations. Our model
with the BERT span representation achieves the state-of-the-art performance
among the models on the CoNLL-2012 Shared Task English Test Set.
",0
Inverse Reinforcement Learning for Text Summarization,"Yu Fu, Deyi Xiong, Yue Dong",2022-12-19T23:45:05Z,Reinforcement Learning,"  We introduce inverse reinforcement learning (IRL) as an effective paradigm
for training abstractive summarization models, imitating human summarization
behaviors. Our IRL model estimates the reward function using a suite of
important sub-rewards for summarization and concurrently optimizes the policy
network. Experimental results across datasets in different domains
(CNN/DailyMail and WikiHow) and various model sizes (BART-base and BART-large)
demonstrate the superiority of our proposed IRL model for summarization over
MLE and RL baselines. The resulting summaries exhibit greater similarity to
human-crafted gold references, outperforming MLE and RL baselines on metrics
such as ROUGE, coverage, novelty, compression ratio, factuality, and human
evaluations.
",0
Lifelong Reinforcement Learning with Modulating Masks,"Eseoghene Ben-Iwhiwhu, Saptarshi Nath, Praveen K. Pilly, Soheil Kolouri, Andrea Soltoggio",2022-12-21T15:49:20Z,Reinforcement Learning,"  Lifelong learning aims to create AI systems that continuously and
incrementally learn during a lifetime, similar to biological learning. Attempts
so far have met problems, including catastrophic forgetting, interference among
tasks, and the inability to exploit previous knowledge. While considerable
research has focused on learning multiple supervised classification tasks that
involve changes in the input distribution, lifelong reinforcement learning
(LRL) must deal with variations in the state and transition distributions, and
in the reward functions. Modulating masks with a fixed backbone network,
recently developed for classification, are particularly suitable to deal with
such a large spectrum of task variations. In this paper, we adapted modulating
masks to work with deep LRL, specifically PPO and IMPALA agents. The comparison
with LRL baselines in both discrete and continuous RL tasks shows superior
performance. We further investigated the use of a linear combination of
previously learned masks to exploit previous knowledge when learning new tasks:
not only is learning faster, the algorithm solves tasks that we could not
otherwise solve from scratch due to extremely sparse rewards. The results
suggest that RL with modulating masks is a promising approach to lifelong
learning, to the composition of knowledge to learn increasingly complex tasks,
and to knowledge reuse for efficient and faster learning.
",0
NARS vs. Reinforcement learning: ONA vs. Q-Learning,Ali Beikmohammadi,2022-12-23T18:27:45Z,Reinforcement Learning,"  One of the realistic scenarios is taking a sequence of optimal actions to do
a task. Reinforcement learning is the most well-known approach to deal with
this kind of task in the machine learning community. Finding a suitable
alternative could always be an interesting and out-of-the-box matter.
Therefore, in this project, we are looking to investigate the capability of
NARS and answer the question of whether NARS has the potential to be a
substitute for RL or not. Particularly, we are making a comparison between
$Q$-Learning and ONA on some environments developed by an Open AI gym. The
source code for the experiments is publicly available in the following link:
\url{https://github.com/AliBeikmohammadi/OpenNARS-for-Applications/tree/master/misc/Python}.
",0
Deep Reinforcement Learning for Heat Pump Control,"Tobias Rohrer, Lilli Frison, Lukas Kaupenjohann, Katrin Scharf, Elke Hergenrother",2022-12-24T11:24:08Z,Reinforcement Learning,"  Heating in private households is a major contributor to the emissions
generated today. Heat pumps are a promising alternative for heat generation and
are a key technology in achieving our goals of the German energy transformation
and to become less dependent on fossil fuels. Today, the majority of heat pumps
in the field are controlled by a simple heating curve, which is a naive mapping
of the current outdoor temperature to a control action. A more advanced control
approach is model predictive control (MPC) which was applied in multiple
research works to heat pump control. However, MPC is heavily dependent on the
building model, which has several disadvantages. Motivated by this and by
recent breakthroughs in the field, this work applies deep reinforcement
learning (DRL) to heat pump control in a simulated environment. Through a
comparison to MPC, it could be shown that it is possible to apply DRL in a
model-free manner to achieve MPC-like performance. This work extends other
works which have already applied DRL to building heating operation by
performing an in-depth analysis of the learned control strategies and by giving
a detailed comparison of the two state-of-the-art control methods.
",0
Towards Learning Abstractions via Reinforcement Learning,"Erik Jergéus, Leo Karlsson Oinonen, Emil Carlsson, Moa Johansson",2022-12-28T17:54:32Z,Reinforcement Learning,"  In this paper we take the first steps in studying a new approach to synthesis
of efficient communication schemes in multi-agent systems, trained via
reinforcement learning. We combine symbolic methods with machine learning, in
what is referred to as a neuro-symbolic system. The agents are not restricted
to only use initial primitives: reinforcement learning is interleaved with
steps to extend the current language with novel higher-level concepts, allowing
generalisation and more informative communication via shorter messages. We
demonstrate that this approach allow agents to converge more quickly on a small
collaborative construction task.
",0
Risk-Sensitive Policy with Distributional Reinforcement Learning,"Thibaut Théate, Damien Ernst",2022-12-30T14:37:28Z,Reinforcement Learning,"  Classical reinforcement learning (RL) techniques are generally concerned with
the design of decision-making policies driven by the maximisation of the
expected outcome. Nevertheless, this approach does not take into consideration
the potential risk associated with the actions taken, which may be critical in
certain applications. To address that issue, the present research work
introduces a novel methodology based on distributional RL to derive sequential
decision-making policies that are sensitive to the risk, the latter being
modelled by the tail of the return probability distribution. The core idea is
to replace the $Q$ function generally standing at the core of learning schemes
in RL by another function taking into account both the expected return and the
risk. Named the risk-based utility function $U$, it can be extracted from the
random return distribution $Z$ naturally learnt by any distributional RL
algorithm. This enables to span the complete potential trade-off between risk
minimisation and expected return maximisation, in contrast to fully risk-averse
methodologies. Fundamentally, this research yields a truly practical and
accessible solution for learning risk-sensitive policies with minimal
modification to the distributional RL algorithm, and with an emphasis on the
interpretability of the resulting decision-making process.
",4
Reinforcement Learning with Success Induced Task Prioritization,"Maria Nesterova, Alexey Skrynnik, Aleksandr Panov",2022-12-30T12:32:43Z,Reinforcement Learning,"  Many challenging reinforcement learning (RL) problems require designing a
distribution of tasks that can be applied to train effective policies. This
distribution of tasks can be specified by the curriculum. A curriculum is meant
to improve the results of learning and accelerate it. We introduce Success
Induced Task Prioritization (SITP), a framework for automatic curriculum
learning, where a task sequence is created based on the success rate of each
task. In this setting, each task is an algorithmically created environment
instance with a unique configuration. The algorithm selects the order of tasks
that provide the fastest learning for agents. The probability of selecting any
of the tasks for the next stage of learning is determined by evaluating its
performance score in previous stages. Experiments were carried out in the
Partially Observable Grid Environment for Multiple Agents (POGEMA) and Procgen
benchmark. We demonstrate that SITP matches or surpasses the results of other
curriculum design methods. Our method can be implemented with handful of minor
modifications to any standard RL framework and provides useful prioritization
with minimal computational overhead.
",2
Contextual Conservative Q-Learning for Offline Reinforcement Learning,"Ke Jiang, Jiayu Yao, Xiaoyang Tan",2023-01-03T13:33:54Z,Reinforcement Learning,"  Offline reinforcement learning learns an effective policy on offline datasets
without online interaction, and it attracts persistent research attention due
to its potential of practical application. However, extrapolation error
generated by distribution shift will still lead to the overestimation for those
actions that transit to out-of-distribution(OOD) states, which degrades the
reliability and robustness of the offline policy. In this paper, we propose
Contextual Conservative Q-Learning(C-CQL) to learn a robustly reliable policy
through the contextual information captured via an inverse dynamics model. With
the supervision of the inverse dynamics model, it tends to learn a policy that
generates stable transition at perturbed states, for the fact that pertuebed
states are a common kind of OOD states. In this manner, we enable the learnt
policy more likely to generate transition that destines to the empirical next
state distributions of the offline dataset, i.e., robustly reliable transition.
Besides, we theoretically reveal that C-CQL is the generalization of the
Conservative Q-Learning(CQL) and aggressive State Deviation Correction(SDC).
Finally, experimental results demonstrate the proposed C-CQL achieves the
state-of-the-art performance in most environments of offline Mujoco suite and a
noisy Mujoco setting.
",0
A Succinct Summary of Reinforcement Learning,Sanjeevan Ahilan,2023-01-03T22:17:55Z,Reinforcement Learning,"  This document is a concise summary of many key results in single-agent
reinforcement learning (RL). The intended audience are those who already have
some familiarity with RL and are looking to review, reference and/or remind
themselves of important ideas in the field.
",0
Reinforcement Learning-Based Air Traffic Deconfliction,"Denis Osipychev, Dragos Margineantu, Girish Chowdhary",2023-01-05T00:37:20Z,Reinforcement Learning,"  Remain Well Clear, keeping the aircraft away from hazards by the appropriate
separation distance, is an essential technology for the safe operation of
uncrewed aerial vehicles in congested airspace. This work focuses on automating
the horizontal separation of two aircraft and presents the obstacle avoidance
problem as a 2D surrogate optimization task. By our design, the surrogate task
is made more conservative to guarantee the execution of the solution in the
primary domain. Using Reinforcement Learning (RL), we optimize the avoidance
policy and model the dynamics, interactions, and decision-making. By
recursively sampling the resulting policy and the surrogate transitions, the
system translates the avoidance policy into a complete avoidance trajectory.
Then, the solver publishes the trajectory as a set of waypoints for the
airplane to follow using the Robot Operating System (ROS) interface. The
proposed system generates a quick and achievable avoidance trajectory that
satisfies the safety requirements. Evaluation of our system is completed in a
high-fidelity simulation and full-scale airplane demonstration. Moreover, the
paper concludes an enormous integration effort that has enabled a real-life
demonstration of the RL-based system.
",0
A Survey on Transformers in Reinforcement Learning,"Wenzhe Li, Hao Luo, Zichuan Lin, Chongjie Zhang, Zongqing Lu, Deheng Ye",2023-01-08T14:04:26Z,Reinforcement Learning,"  Transformer has been considered the dominating neural architecture in NLP and
CV, mostly under supervised settings. Recently, a similar surge of using
Transformers has appeared in the domain of reinforcement learning (RL), but it
is faced with unique design choices and challenges brought by the nature of RL.
However, the evolution of Transformers in RL has not yet been well unraveled.
In this paper, we seek to systematically review motivations and progress on
using Transformers in RL, provide a taxonomy on existing works, discuss each
sub-field, and summarize future prospects.
",0
Adversarial Online Multi-Task Reinforcement Learning,"Quan Nguyen, Nishant A. Mehta",2023-01-11T02:18:26Z,Reinforcement Learning,"  We consider the adversarial online multi-task reinforcement learning setting,
where in each of $K$ episodes the learner is given an unknown task taken from a
finite set of $M$ unknown finite-horizon MDP models. The learner's objective is
to minimize its regret with respect to the optimal policy for each task. We
assume the MDPs in $\mathcal{M}$ are well-separated under a notion of
$\lambda$-separability, and show that this notion generalizes many
task-separability notions from previous works. We prove a minimax lower bound
of $\Omega(K\sqrt{DSAH})$ on the regret of any learning algorithm and an
instance-specific lower bound of $\Omega(\frac{K}{\lambda^2})$ in sample
complexity for a class of uniformly-good cluster-then-learn algorithms. We use
a novel construction called 2-JAO MDP for proving the instance-specific lower
bound. The lower bounds are complemented with a polynomial time algorithm that
obtains $\tilde{O}(\frac{K}{\lambda^2})$ sample complexity guarantee for the
clustering phase and $\tilde{O}(\sqrt{MK})$ regret guarantee for the learning
phase, indicating that the dependency on $K$ and $\frac{1}{\lambda^2}$ is
tight.
",0
Asynchronous training of quantum reinforcement learning,Samuel Yen-Chi Chen,2023-01-12T15:54:44Z,Reinforcement Learning,"  The development of quantum machine learning (QML) has received a lot of
interest recently thanks to developments in both quantum computing (QC) and
machine learning (ML). One of the ML paradigms that can be utilized to address
challenging sequential decision-making issues is reinforcement learning (RL).
It has been demonstrated that classical RL can successfully complete many
difficult tasks. A leading method of building quantum RL agents relies on the
variational quantum circuits (VQC). However, training QRL algorithms with VQCs
requires significant amount of computational resources. This issue hurdles the
exploration of various QRL applications. In this paper, we approach this
challenge through asynchronous training QRL agents. Specifically, we choose the
asynchronous training of advantage actor-critic variational quantum policies.
We demonstrate the results via numerical simulations that within the tasks
considered, the asynchronous training of QRL agents can reach performance
comparable to or superior than classical agents with similar model sizes and
architectures.
",13
DQNAS: Neural Architecture Search using Reinforcement Learning,"Anshumaan Chauhan, Siddhartha Bhattacharyya, S. Vadivel",2023-01-17T04:01:47Z,Reinforcement Learning,"  Convolutional Neural Networks have been used in a variety of image related
applications after their rise in popularity due to ImageNet competition.
Convolutional Neural Networks have shown remarkable results in applications
including face recognition, moving target detection and tracking,
classification of food based on the calorie content and many more. Designing of
Convolutional Neural Networks requires experts having a cross domain knowledge
and it is laborious, which requires a lot of time for testing different values
for different hyperparameter along with the consideration of different
configurations of existing architectures. Neural Architecture Search is an
automated way of generating Neural Network architectures which saves
researchers from all the brute-force testing trouble, but with the drawback of
consuming a lot of computational resources for a prolonged period. In this
paper, we propose an automated Neural Architecture Search framework DQNAS,
guided by the principles of Reinforcement Learning along with One-shot Training
which aims to generate neural network architectures that show superior
performance and have minimum scalability problem.
",0
A Survey of Meta-Reinforcement Learning,"Jacob Beck, Risto Vuorio, Evan Zheran Liu, Zheng Xiong, Luisa Zintgraf, Chelsea Finn, Shimon Whiteson",2023-01-19T12:01:41Z,Reinforcement Learning,"  While deep reinforcement learning (RL) has fueled multiple high-profile
successes in machine learning, it is held back from more widespread adoption by
its often poor data efficiency and the limited generality of the policies it
produces. A promising approach for alleviating these limitations is to cast the
development of better RL algorithms as a machine learning problem itself in a
process called meta-RL. Meta-RL is most commonly studied in a problem setting
where, given a distribution of tasks, the goal is to learn a policy that is
capable of adapting to any new task from the task distribution with as little
data as possible. In this survey, we describe the meta-RL problem setting in
detail as well as its major variations. We discuss how, at a high level,
meta-RL research can be clustered based on the presence of a task distribution
and the learning budget available for each individual task. Using these
clusters, we then survey meta-RL algorithms and applications. We conclude by
presenting the open problems on the path to making meta-RL part of the standard
toolbox for a deep RL practitioner.
",0
Generative Slate Recommendation with Reinforcement Learning,"Romain Deffayet, Thibaut Thonet, Jean-Michel Renders, Maarten de Rijke",2023-01-20T15:28:09Z,Reinforcement Learning,"  Recent research has employed reinforcement learning (RL) algorithms to
optimize long-term user engagement in recommender systems, thereby avoiding
common pitfalls such as user boredom and filter bubbles. They capture the
sequential and interactive nature of recommendations, and thus offer a
principled way to deal with long-term rewards and avoid myopic behaviors.
However, RL approaches are intractable in the slate recommendation scenario -
where a list of items is recommended at each interaction turn - due to the
combinatorial action space. In that setting, an action corresponds to a slate
that may contain any combination of items.
  While previous work has proposed well-chosen decompositions of actions so as
to ensure tractability, these rely on restrictive and sometimes unrealistic
assumptions. Instead, in this work we propose to encode slates in a continuous,
low-dimensional latent space learned by a variational auto-encoder. Then, the
RL agent selects continuous actions in this latent space, which are ultimately
decoded into the corresponding slates. By doing so, we are able to (i) relax
assumptions required by previous work, and (ii) improve the quality of the
action selection by modeling full slates instead of independent items, in
particular by enabling diversity. Our experiments performed on a wide array of
simulated environments confirm the effectiveness of our generative modeling of
slates over baselines in practical scenarios where the restrictive assumptions
underlying the baselines are lifted. Our findings suggest that representation
learning using generative models is a promising direction towards generalizable
RL-based slate recommendation.
",0
Quasi-optimal Reinforcement Learning with Continuous Actions,"Yuhan Li, Wenzhuo Zhou, Ruoqing Zhu",2023-01-21T11:30:13Z,Reinforcement Learning,"  Many real-world applications of reinforcement learning (RL) require making
decisions in continuous action environments. In particular, determining the
optimal dose level plays a vital role in developing medical treatment regimes.
One challenge in adapting existing RL algorithms to medical applications,
however, is that the popular infinite support stochastic policies, e.g.,
Gaussian policy, may assign riskily high dosages and harm patients seriously.
Hence, it is important to induce a policy class whose support only contains
near-optimal actions, and shrink the action-searching area for effectiveness
and reliability. To achieve this, we develop a novel \emph{quasi-optimal
learning algorithm}, which can be easily optimized in off-policy settings with
guaranteed convergence under general function approximations. Theoretically, we
analyze the consistency, sample complexity, adaptability, and convergence of
the proposed algorithm. We evaluate our algorithm with comprehensive simulated
experiments and a dose suggestion real application to Ohio Type 1 diabetes
dataset.
",0
Constrained Reinforcement Learning for Dexterous Manipulation,"Abhineet Jain, Jack Kolb, Harish Ravichandar",2023-01-24T00:31:28Z,Reinforcement Learning,"  Existing learning approaches to dexterous manipulation use demonstrations or
interactions with the environment to train black-box neural networks that
provide little control over how the robot learns the skills or how it would
perform post training. These approaches pose significant challenges when
implemented on physical platforms given that, during initial stages of
training, the robot's behavior could be erratic and potentially harmful to its
own hardware, the environment, or any humans in the vicinity. A potential way
to address these limitations is to add constraints during learning that
restrict and guide the robot's behavior during training as well as roll outs.
Inspired by the success of constrained approaches in other domains, we
investigate the effects of adding position-based constraints to a 24-DOF robot
hand learning to perform object relocation using Constrained Policy
Optimization. We find that a simple geometric constraint can ensure the robot
learns to move towards the object sooner than without constraints. Further,
training with this constraint requires a similar number of samples as its
unconstrained counterpart to master the skill. These findings shed light on how
simple constraints can help robots achieve sensible and safe behavior quickly
and ease concerns surrounding hardware deployment. We also investigate the
effects of the strictness of these constraints and report findings that provide
insights into how different degrees of strictness affect learning outcomes. Our
code is available at
https://github.com/GT-STAR-Lab/constrained-rl-dexterous-manipulation.
",1
ASQ-IT: Interactive Explanations for Reinforcement-Learning Agents,"Yotam Amitai, Guy Avni, Ofra Amir",2023-01-24T11:57:37Z,Other,"  As reinforcement learning methods increasingly amass accomplishments, the
need for comprehending their solutions becomes more crucial. Most explainable
reinforcement learning (XRL) methods generate a static explanation depicting
their developers' intuition of what should be explained and how. In contrast,
literature from the social sciences proposes that meaningful explanations are
structured as a dialog between the explainer and the explainee, suggesting a
more active role for the user and her communication with the agent. In this
paper, we present ASQ-IT -- an interactive tool that presents video clips of
the agent acting in its environment based on queries given by the user that
describe temporal properties of behaviors of interest. Our approach is based on
formal methods: queries in ASQ-IT's user interface map to a fragment of Linear
Temporal Logic over finite traces (LTLf), which we developed, and our algorithm
for query processing is based on automata theory. User studies show that
end-users can understand and formulate queries in ASQ-IT, and that using ASQ-IT
assists users in identifying faulty agent behaviors.
",0
Single-Trajectory Distributionally Robust Reinforcement Learning,"Zhipeng Liang, Xiaoteng Ma, Jose Blanchet, Jiheng Zhang, Zhengyuan Zhou",2023-01-27T14:08:09Z,Reinforcement Learning,"  To mitigate the limitation that the classical reinforcement learning (RL)
framework heavily relies on identical training and test environments,
Distributionally Robust RL (DRRL) has been proposed to enhance performance
across a range of environments, possibly including unknown test environments.
As a price for robustness gain, DRRL involves optimizing over a set of
distributions, which is inherently more challenging than optimizing over a
fixed distribution in the non-robust case. Existing DRRL algorithms are either
model-based or fail to learn from a single sample trajectory. In this paper, we
design a first fully model-free DRRL algorithm, called distributionally robust
Q-learning with single trajectory (DRQ). We delicately design a multi-timescale
framework to fully utilize each incrementally arriving sample and directly
learn the optimal distributionally robust policy without modelling the
environment, thus the algorithm can be trained along a single trajectory in a
model-free fashion. Despite the algorithm's complexity, we provide asymptotic
convergence guarantees by generalizing classical stochastic approximation
tools. Comprehensive experimental results demonstrate the superior robustness
and sample complexity of our proposed algorithm, compared to non-robust methods
and other robust RL algorithms.
",0
Reinforcement Learning from Diverse Human Preferences,"Wanqi Xue, Bo An, Shuicheng Yan, Zhongwen Xu",2023-01-27T15:18:54Z,Reinforcement Learning,"  The complexity of designing reward functions has been a major obstacle to the
wide application of deep reinforcement learning (RL) techniques. Describing an
agent's desired behaviors and properties can be difficult, even for experts. A
new paradigm called reinforcement learning from human preferences (or
preference-based RL) has emerged as a promising solution, in which reward
functions are learned from human preference labels among behavior trajectories.
However, existing methods for preference-based RL are limited by the need for
accurate oracle preference labels. This paper addresses this limitation by
developing a method for crowd-sourcing preference labels and learning from
diverse human preferences. The key idea is to stabilize reward learning through
regularization and correction in a latent space. To ensure temporal
consistency, a strong constraint is imposed on the reward model that forces its
latent space to be close to the prior distribution. Additionally, a
confidence-based reward model ensembling method is designed to generate more
stable and reliable predictions. The proposed method is tested on a variety of
tasks in DMcontrol and Meta-world and has shown consistent and significant
improvements over existing preference-based RL algorithms when learning from
diverse feedback, paving the way for real-world applications of RL methods.
",0
Planning Multiple Epidemic Interventions with Reinforcement Learning,"Anh Mai, Nikunj Gupta, Azza Abouzied, Dennis Shasha",2023-01-30T11:51:24Z,Reinforcement Learning,"  Combating an epidemic entails finding a plan that describes when and how to
apply different interventions, such as mask-wearing mandates, vaccinations,
school or workplace closures. An optimal plan will curb an epidemic with
minimal loss of life, disease burden, and economic cost. Finding an optimal
plan is an intractable computational problem in realistic settings.
Policy-makers, however, would greatly benefit from tools that can efficiently
search for plans that minimize disease and economic costs especially when
considering multiple possible interventions over a continuous and complex
action space given a continuous and equally complex state space. We formulate
this problem as a Markov decision process. Our formulation is unique in its
ability to represent multiple continuous interventions over any disease model
defined by ordinary differential equations. We illustrate how to effectively
apply state-of-the-art actor-critic reinforcement learning algorithms (PPO and
SAC) to search for plans that minimize overall costs. We empirically evaluate
the learning performance of these algorithms and compare their performance to
hand-crafted baselines that mimic plans constructed by policy-makers. Our
method outperforms baselines. Our work confirms the viability of a
computational approach to support policy-makers
",0
V2N Service Scaling with Deep Reinforcement Learning,"Cyril Shih-Huan Hsu, Jorge Martín-Pérez, Chrysa Papagianni, Paola Grosso",2023-01-30T23:13:18Z,Reinforcement Learning,"  The fifth generation (5G) of wireless networks is set out to meet the
stringent requirements of vehicular use cases. Edge computing resources can aid
in this direction by moving processing closer to end-users, reducing latency.
However, given the stochastic nature of traffic loads and availability of
physical resources, appropriate auto-scaling mechanisms need to be employed to
support cost-efficient and performant services. To this end, we employ Deep
Reinforcement Learning (DRL) for vertical scaling in Edge computing to support
vehicular-to-network communications. We address the problem using Deep
Deterministic Policy Gradient (DDPG). As DDPG is a model-free off-policy
algorithm for learning continuous actions, we introduce a discretization
approach to support discrete scaling actions. Thus we address scalability
problems inherent to high-dimensional discrete action spaces. Employing a
real-world vehicular trace data set, we show that DDPG outperforms existing
solutions, reducing (at minimum) the average number of active CPUs by 23% while
increasing the long-term reward by 24%.
",0
Scaling laws for single-agent reinforcement learning,"Jacob Hilton, Jie Tang, John Schulman",2023-01-31T06:38:53Z,Reinforcement Learning,"  Recent work has shown that, in generative modeling, cross-entropy loss
improves smoothly with model size and training compute, following a power law
plus constant scaling law. One challenge in extending these results to
reinforcement learning is that the main performance objective of interest, mean
episode return, need not vary smoothly. To overcome this, we introduce
*intrinsic performance*, a monotonic function of the return defined as the
minimum compute required to achieve the given return across a family of models
of different sizes. We find that, across a range of environments, intrinsic
performance scales as a power law in model size and environment interactions.
Consequently, as in generative modeling, the optimal model size scales as a
power law in the training compute budget. Furthermore, we study how this
relationship varies with the environment and with other properties of the
training setup. In particular, using a toy MNIST-based environment, we show
that varying the ""horizon length"" of the task mostly changes the coefficient
but not the exponent of this relationship.
",0
Learning to Optimize for Reinforcement Learning,"Qingfeng Lan, A. Rupam Mahmood, Shuicheng Yan, Zhongwen Xu",2023-02-03T00:11:02Z,Reinforcement Learning,"  In recent years, by leveraging more data, computation, and diverse tasks,
learned optimizers have achieved remarkable success in supervised learning,
outperforming classical hand-designed optimizers. Reinforcement learning (RL)
is essentially different from supervised learning, and in practice, these
learned optimizers do not work well even in simple RL tasks. We investigate
this phenomenon and identify two issues. First, the agent-gradient distribution
is non-independent and identically distributed, leading to inefficient
meta-training. Moreover, due to highly stochastic agent-environment
interactions, the agent-gradients have high bias and variance, which increases
the difficulty of learning an optimizer for RL. We propose pipeline training
and a novel optimizer structure with a good inductive bias to address these
issues, making it possible to learn an optimizer for reinforcement learning
from scratch. We show that, although only trained in toy tasks, our learned
optimizer can generalize to unseen complex tasks in Brax.
",4
Reinforcement Learning with History-Dependent Dynamic Contexts,"Guy Tennenholtz, Nadav Merlis, Lior Shani, Martin Mladenov, Craig Boutilier",2023-02-04T01:58:21Z,Reinforcement Learning,"  We introduce Dynamic Contextual Markov Decision Processes (DCMDPs), a novel
reinforcement learning framework for history-dependent environments that
generalizes the contextual MDP framework to handle non-Markov environments,
where contexts change over time. We consider special cases of the model, with a
focus on logistic DCMDPs, which break the exponential dependence on history
length by leveraging aggregation functions to determine context transitions.
This special structure allows us to derive an upper-confidence-bound style
algorithm for which we establish regret bounds. Motivated by our theoretical
results, we introduce a practical model-based algorithm for logistic DCMDPs
that plans in a latent space and uses optimism over history-dependent features.
We demonstrate the efficacy of our approach on a recommendation task (using
MovieLens data) where user behavior dynamics evolve in response to
recommendations.
",0
Efficient Online Reinforcement Learning with Offline Data,"Philip J. Ball, Laura Smith, Ilya Kostrikov, Sergey Levine",2023-02-06T17:30:22Z,Reinforcement Learning,"  Sample efficiency and exploration remain major challenges in online
reinforcement learning (RL). A powerful approach that can be applied to address
these issues is the inclusion of offline data, such as prior trajectories from
a human expert or a sub-optimal exploration policy. Previous methods have
relied on extensive modifications and additional complexity to ensure the
effective use of this data. Instead, we ask: can we simply apply existing
off-policy methods to leverage offline data when learning online? In this work,
we demonstrate that the answer is yes; however, a set of minimal but important
changes to existing off-policy RL algorithms are required to achieve reliable
performance. We extensively ablate these design choices, demonstrating the key
factors that most affect performance, and arrive at a set of recommendations
that practitioners can readily apply, whether their data comprise a small
number of expert demonstrations or large volumes of sub-optimal trajectories.
We see that correct application of these simple recommendations can provide a
$\mathbf{2.5\times}$ improvement over existing approaches across a diverse set
of competitive benchmarks, with no additional computational overhead. We have
released our code at https://github.com/ikostrikov/rlpd.
",0
State-wise Safe Reinforcement Learning: A Survey,"Weiye Zhao, Tairan He, Rui Chen, Tianhao Wei, Changliu Liu",2023-02-06T21:11:29Z,Reinforcement Learning,"  Despite the tremendous success of Reinforcement Learning (RL) algorithms in
simulation environments, applying RL to real-world applications still faces
many challenges. A major concern is safety, in another word, constraint
satisfaction. State-wise constraints are one of the most common constraints in
real-world applications and one of the most challenging constraints in Safe RL.
Enforcing state-wise constraints is necessary and essential to many challenging
tasks such as autonomous driving, robot manipulation. This paper provides a
comprehensive review of existing approaches that address state-wise constraints
in RL. Under the framework of State-wise Constrained Markov Decision Process
(SCMDP), we will discuss the connections, differences, and trade-offs of
existing approaches in terms of (i) safety guarantee and scalability, (ii)
safety and reward performance, and (iii) safety after convergence and during
training. We also summarize limitations of current methods and discuss
potential future directions.
",0
Multi-Task Recommendations with Reinforcement Learning,"Ziru Liu, Jiejie Tian, Qingpeng Cai, Xiangyu Zhao, Jingtong Gao, Shuchang Liu, Dayou Chen, Tonghao He, Dong Zheng, Peng Jiang, Kun Gai",2023-02-07T09:11:17Z,Reinforcement Learning,"  In recent years, Multi-task Learning (MTL) has yielded immense success in
Recommender System (RS) applications. However, current MTL-based recommendation
models tend to disregard the session-wise patterns of user-item interactions
because they are predominantly constructed based on item-wise datasets.
Moreover, balancing multiple objectives has always been a challenge in this
field, which is typically avoided via linear estimations in existing works. To
address these issues, in this paper, we propose a Reinforcement Learning (RL)
enhanced MTL framework, namely RMTL, to combine the losses of different
recommendation tasks using dynamic weights. To be specific, the RMTL structure
can address the two aforementioned issues by (i) constructing an MTL
environment from session-wise interactions and (ii) training multi-task
actor-critic network structure, which is compatible with most existing
MTL-based recommendation models, and (iii) optimizing and fine-tuning the MTL
loss function using the weights generated by critic networks. Experiments on
two real-world public datasets demonstrate the effectiveness of RMTL with a
higher AUC against state-of-the-art MTL-based recommendation models.
Additionally, we evaluate and validate RMTL's compatibility and transferability
across various MTL models.
",0
Online Reinforcement Learning with Uncertain Episode Lengths,"Debmalya Mandal, Goran Radanovic, Jiarui Gan, Adish Singla, Rupak Majumdar",2023-02-07T17:12:49Z,Reinforcement Learning,"  Existing episodic reinforcement algorithms assume that the length of an
episode is fixed across time and known a priori. In this paper, we consider a
general framework of episodic reinforcement learning when the length of each
episode is drawn from a distribution. We first establish that this problem is
equivalent to online reinforcement learning with general discounting where the
learner is trying to optimize the expected discounted sum of rewards over an
infinite horizon, but where the discounting function is not necessarily
geometric. We show that minimizing regret with this new general discounting is
equivalent to minimizing regret with uncertain episode lengths. We then design
a reinforcement learning algorithm that minimizes regret with general
discounting but acts for the setting with uncertain episode lengths. We
instantiate our general bound for different types of discounting, including
geometric and polynomial discounting. We also show that we can obtain similar
regret bounds even when the uncertainty over the episode lengths is unknown, by
estimating the unknown distribution over time. Finally, we compare our learning
algorithms with existing value-iteration based episodic RL algorithms in a
grid-world environment.
",0
A Survey on Causal Reinforcement Learning,"Yan Zeng, Ruichu Cai, Fuchun Sun, Libo Huang, Zhifeng Hao",2023-02-10T12:25:08Z,Reinforcement Learning,"  While Reinforcement Learning (RL) achieves tremendous success in sequential
decision-making problems of many domains, it still faces key challenges of data
inefficiency and the lack of interpretability. Interestingly, many researchers
have leveraged insights from the causality literature recently, bringing forth
flourishing works to unify the merits of causality and address well the
challenges from RL. As such, it is of great necessity and significance to
collate these Causal Reinforcement Learning (CRL) works, offer a review of CRL
methods, and investigate the potential functionality from causality toward RL.
In particular, we divide existing CRL approaches into two categories according
to whether their causality-based information is given in advance or not. We
further analyze each category in terms of the formalization of different
models, ranging from the Markov Decision Process (MDP), Partially Observed
Markov Decision Process (POMDP), Multi-Arm Bandits (MAB), and Dynamic Treatment
Regime (DTR). Moreover, we summarize the evaluation matrices and open sources
while we discuss emerging applications, along with promising prospects for the
future development of CRL.
",0
Robust Knowledge Transfer in Tiered Reinforcement Learning,"Jiawei Huang, Niao He",2023-02-10T22:25:42Z,Reinforcement Learning,"  In this paper, we study the Tiered Reinforcement Learning setting, a parallel
transfer learning framework, where the goal is to transfer knowledge from the
low-tier (source) task to the high-tier (target) task to reduce the exploration
risk of the latter while solving the two tasks in parallel. Unlike previous
work, we do not assume the low-tier and high-tier tasks share the same dynamics
or reward functions, and focus on robust knowledge transfer without prior
knowledge on the task similarity. We identify a natural and necessary condition
called the ``Optimal Value Dominance'' for our objective. Under this condition,
we propose novel online learning algorithms such that, for the high-tier task,
it can achieve constant regret on partial states depending on the task
similarity and retain near-optimal regret when the two tasks are dissimilar,
while for the low-tier task, it can keep near-optimal without making sacrifice.
Moreover, we further study the setting with multiple low-tier tasks, and
propose a novel transfer source selection mechanism, which can ensemble the
information from all low-tier tasks and allow provable benefits on a much
larger state-action space.
",0
Procedural generation of meta-reinforcement learning tasks,Thomas Miconi,2023-02-11T02:58:41Z,Reinforcement Learning,"  Open-endedness stands to benefit from the ability to generate an infinite
variety of diverse, challenging environments. One particularly interesting type
of challenge is meta-learning (""learning-to-learn""), a hallmark of intelligent
behavior. However, the number of meta-learning environments in the literature
is limited. Here we describe a parametrized space for simple meta-reinforcement
learning (meta-RL) tasks with arbitrary stimuli. The parametrization allows us
to randomly generate an arbitrary number of novel simple meta-learning tasks.
The parametrization is expressive enough to include many well-known meta-RL
tasks, such as bandit problems, the Harlow task, T-mazes, the Daw two-step task
and others. Simple extensions allow it to capture tasks based on
two-dimensional topological spaces, such as full mazes or find-the-spot
domains. We describe a number of randomly generated meta-RL domains of varying
complexity and discuss potential issues arising from random generation.
",0
Regret-Based Defense in Adversarial Reinforcement Learning,"Roman Belaire, Pradeep Varakantham, Thanh Nguyen, David Lo",2023-02-14T08:56:50Z,Reinforcement Learning,"  Deep Reinforcement Learning (DRL) policies have been shown to be vulnerable
to small adversarial noise in observations. Such adversarial noise can have
disastrous consequences in safety-critical environments. For instance, a
self-driving car receiving adversarially perturbed sensory observations about
nearby signs (e.g., a stop sign physically altered to be perceived as a speed
limit sign) or objects (e.g., cars altered to be recognized as trees) can be
fatal. Existing approaches for making RL algorithms robust to an
observation-perturbing adversary have focused on reactive approaches that
iteratively improve against adversarial examples generated at each iteration.
While such approaches have been shown to provide improvements over regular RL
methods, they are reactive and can fare significantly worse if certain
categories of adversarial examples are not generated during training. To that
end, we pursue a more proactive approach that relies on directly optimizing a
well-studied robustness measure, regret instead of expected value. We provide a
principled approach that minimizes maximum regret over a ""neighborhood"" of
observations to the received ""observation"". Our regret criterion can be used to
modify existing value- and policy-based Deep RL methods. We demonstrate that
our approaches provide a significant improvement in performance across a wide
variety of benchmarks against leading approaches for robust Deep RL.
",0
CERiL: Continuous Event-based Reinforcement Learning,"Celyn Walters, Simon Hadfield",2023-02-15T13:58:02Z,Reinforcement Learning,"  This paper explores the potential of event cameras to enable continuous time
reinforcement learning. We formalise this problem where a continuous stream of
unsynchronised observations is used to produce a corresponding stream of output
actions for the environment. This lack of synchronisation enables greatly
enhanced reactivity. We present a method to train on event streams derived from
standard RL environments, thereby solving the proposed continuous time RL
problem. The CERiL algorithm uses specialised network layers which operate
directly on an event stream, rather than aggregating events into quantised
image frames. We show the advantages of event streams over less-frequent RGB
images. The proposed system outperforms networks typically used in RL, even
succeeding at tasks which cannot be solved traditionally. We also demonstrate
the value of our CERiL approach over a standard SNN baseline using event
streams.
",0
Meta-Reinforcement Learning via Exploratory Task Clustering,"Zhendong Chu, Hongning Wang",2023-02-15T21:42:38Z,Reinforcement Learning,"  Meta-reinforcement learning (meta-RL) aims to quickly solve new tasks by
leveraging knowledge from prior tasks. However, previous studies often assume a
single mode homogeneous task distribution, ignoring possible structured
heterogeneity among tasks. Leveraging such structures can better facilitate
knowledge sharing among related tasks and thus improve sample efficiency. In
this paper, we explore the structured heterogeneity among tasks via clustering
to improve meta-RL. We develop a dedicated exploratory policy to discover task
structures via divide-and-conquer. The knowledge of the identified clusters
helps to narrow the search space of task-specific information, leading to more
sample efficient policy adaptation. Experiments on various MuJoCo tasks showed
the proposed method can unravel cluster structures effectively in both rewards
and state dynamics, proving strong advantages against a set of state-of-the-art
baselines.
",0
Swapped goal-conditioned offline reinforcement learning,"Wenyan Yang, Huiling Wang, Dingding Cai, Joni Pajarinen, Joni-Kristen Kämäräinen",2023-02-17T13:22:40Z,Reinforcement Learning,"  Offline goal-conditioned reinforcement learning (GCRL) can be challenging due
to overfitting to the given dataset. To generalize agents' skills outside the
given dataset, we propose a goal-swapping procedure that generates additional
trajectories. To alleviate the problem of noise and extrapolation errors, we
present a general offline reinforcement learning method called deterministic
Q-advantage policy gradient (DQAPG). In the experiments, DQAPG outperforms
state-of-the-art goal-conditioned offline RL methods in a wide range of
benchmark tasks, and goal-swapping further improves the test results. It is
noteworthy, that the proposed method obtains good performance on the
challenging dexterous in-hand manipulation tasks for which the prior methods
failed.
",0
Neuro-symbolic Meta Reinforcement Learning for Trading,"S I Harini, Gautam Shroff, Ashwin Srinivasan, Prayushi Faldu, Lovekesh Vig",2023-01-15T16:38:43Z,Reinforcement Learning,"  We model short-duration (e.g. day) trading in financial markets as a
sequential decision-making problem under uncertainty, with the added
complication of continual concept-drift. We, therefore, employ meta
reinforcement learning via the RL2 algorithm. It is also known that human
traders often rely on frequently occurring symbolic patterns in price series.
We employ logical program induction to discover symbolic patterns that occur
frequently as well as recently, and explore whether using such features
improves the performance of our meta reinforcement learning algorithm. We
report experiments on real data indicating that meta-RL is better than vanilla
RL and also benefits from learned symbolic features.
",1
Robot path planning using deep reinforcement learning,"Miguel Quinones-Ramirez, Jorge Rios-Martinez, Victor Uc-Cetina",2023-02-17T20:08:59Z,Reinforcement Learning,"  Autonomous navigation is challenging for mobile robots, especially in an
unknown environment. Commonly, the robot requires multiple sensors to map the
environment, locate itself, and make a plan to reach the target. However,
reinforcement learning methods offer an alternative to map-free navigation
tasks by learning the optimal actions to take. In this article, deep
reinforcement learning agents are implemented using variants of the deep Q
networks method, the D3QN and rainbow algorithms, for both the obstacle
avoidance and the goal-oriented navigation task. The agents are trained and
evaluated in a simulated environment. Furthermore, an analysis of the changes
in the behaviour and performance of the agents caused by modifications in the
reward function is conducted.
",0
Assessment of Reinforcement Learning for Macro Placement,"Chung-Kuan Cheng, Andrew B. Kahng, Sayak Kundu, Yucheng Wang, Zhiang Wang",2023-02-21T21:26:28Z,Reinforcement Learning,"  We provide open, transparent implementation and assessment of Google Brain's
deep reinforcement learning approach to macro placement and its Circuit
Training (CT) implementation in GitHub. We implement in open source key
""blackbox"" elements of CT, and clarify discrepancies between CT and Nature
paper. New testcases on open enablements are developed and released. We assess
CT alongside multiple alternative macro placers, with all evaluation flows and
related scripts public in GitHub. Our experiments also encompass academic
mixed-size placement benchmarks, as well as ablation and stability studies. We
comment on the impact of Nature and CT, as well as directions for future
research.
",0
Adversarial Model for Offline Reinforcement Learning,"Mohak Bhardwaj, Tengyang Xie, Byron Boots, Nan Jiang, Ching-An Cheng",2023-02-21T23:08:09Z,Reinforcement Learning,"  We propose a novel model-based offline Reinforcement Learning (RL) framework,
called Adversarial Model for Offline Reinforcement Learning (ARMOR), which can
robustly learn policies to improve upon an arbitrary reference policy
regardless of data coverage. ARMOR is designed to optimize policies for the
worst-case performance relative to the reference policy through adversarially
training a Markov decision process model. In theory, we prove that ARMOR, with
a well-tuned hyperparameter, can compete with the best policy within data
coverage when the reference policy is supported by the data. At the same time,
ARMOR is robust to hyperparameter choices: the policy learned by ARMOR, with
""any"" admissible hyperparameter, would never degrade the performance of the
reference policy, even when the reference policy is not covered by the dataset.
To validate these properties in practice, we design a scalable implementation
of ARMOR, which by adversarial training, can optimize policies without using
model ensembles in contrast to typical model-based methods. We show that ARMOR
achieves competent performance with both state-of-the-art offline model-free
and model-based RL algorithms and can robustly improve the reference policy
over various hyperparameter choices.
",0
Provably Efficient Reinforcement Learning via Surprise Bound,"Hanlin Zhu, Ruosong Wang, Jason D. Lee",2023-02-22T20:21:25Z,Reinforcement Learning,"  Value function approximation is important in modern reinforcement learning
(RL) problems especially when the state space is (infinitely) large. Despite
the importance and wide applicability of value function approximation, its
theoretical understanding is still not as sophisticated as its empirical
success, especially in the context of general function approximation. In this
paper, we propose a provably efficient RL algorithm (both computationally and
statistically) with general value function approximations. We show that if the
value functions can be approximated by a function class that satisfies the
Bellman-completeness assumption, our algorithm achieves an
$\widetilde{O}(\text{poly}(\iota H)\sqrt{T})$ regret bound where $\iota$ is the
product of the surprise bound and log-covering numbers, $H$ is the planning
horizon, $K$ is the number of episodes and $T = HK$ is the total number of
steps the agent interacts with the environment. Our algorithm achieves
reasonable regret bounds when applied to both the linear setting and the sparse
high-dimensional linear setting. Moreover, our algorithm only needs to solve
$O(H\log K)$ empirical risk minimization (ERM) problems, which is far more
efficient than previous algorithms that need to solve ERM problems for
$\Omega(HK)$ times.
",0
Q-Cogni: An Integrated Causal Reinforcement Learning Framework,"Cris Cunha, Wei Liu, Tim French, Ajmal Mian",2023-02-26T05:50:26Z,Reinforcement Learning,"  We present Q-Cogni, an algorithmically integrated causal reinforcement
learning framework that redesigns Q-Learning with an autonomous causal
structure discovery method to improve the learning process with causal
inference. Q-Cogni achieves optimal learning with a pre-learned structural
causal model of the environment that can be queried during the learning process
to infer cause-and-effect relationships embedded in a state-action space. We
leverage on the sample efficient techniques of reinforcement learning, enable
reasoning about a broader set of policies and bring higher degrees of
interpretability to decisions made by the reinforcement learning agent. We
apply Q-Cogni on the Vehicle Routing Problem (VRP) and compare against
state-of-the-art reinforcement learning algorithms. We report results that
demonstrate better policies, improved learning efficiency and superior
interpretability of the agent's decision making. We also compare this approach
with traditional shortest-path search algorithms and demonstrate the benefits
of our causal reinforcement learning framework to high dimensional problems.
Finally, we apply Q-Cogni to derive optimal routing decisions for taxis in New
York City using the Taxi & Limousine Commission trip record data and compare
with shortest-path search, reporting results that show 85% of the cases with an
equal or better policy derived from Q-Cogni in a real-world domain.
",0
Distributional Method for Risk Averse Reinforcement Learning,"Ziteng Cheng, Sebastian Jaimungal, Nick Martin",2023-02-27T19:48:42Z,Reinforcement Learning,"  We introduce a distributional method for learning the optimal policy in risk
averse Markov decision process with finite state action spaces, latent costs,
and stationary dynamics. We assume sequential observations of states, actions,
and costs and assess the performance of a policy using dynamic risk measures
constructed from nested Kusuoka-type conditional risk mappings. For such
performance criteria, randomized policies may outperform deterministic
policies, therefore, the candidate policies lie in the d-dimensional simplex
where d is the cardinality of the action space. Existing risk averse
reinforcement learning methods seldom concern randomized policies, na\""ive
extensions to current setting suffer from the curse of dimensionality. By
exploiting certain structures embedded in the corresponding dynamic programming
principle, we propose a distributional learning method for seeking the optimal
policy. The conditional distribution of the value function is casted into a
specific type of function, which is chosen with in mind the ease of risk averse
optimization. We use a deep neural network to approximate said function,
illustrate that the proposed method avoids the curse of dimensionality in the
exploration phase, and explore the method's performance with a wide range of
model parameters that are picked randomly.
",0
Hierarchical Reinforcement Learning in Complex 3D Environments,"Bernardo Avila Pires, Feryal Behbahani, Hubert Soyer, Kyriacos Nikiforou, Thomas Keck, Satinder Singh",2023-02-28T09:56:36Z,Reinforcement Learning,"  Hierarchical Reinforcement Learning (HRL) agents have the potential to
demonstrate appealing capabilities such as planning and exploration with
abstraction, transfer, and skill reuse. Recent successes with HRL across
different domains provide evidence that practical, effective HRL agents are
possible, even if existing agents do not yet fully realize the potential of
HRL. Despite these successes, visually complex partially observable 3D
environments remained a challenge for HRL agents. We address this issue with
Hierarchical Hybrid Offline-Online (H2O2), a hierarchical deep reinforcement
learning agent that discovers and learns to use options from scratch using its
own experience. We show that H2O2 is competitive with a strong non-hierarchical
Muesli baseline in the DeepMind Hard Eight tasks and we shed new light on the
problem of learning hierarchical agents in complex environments. Our empirical
study of H2O2 reveals previously unnoticed practical challenges and brings new
perspective to the current understanding of hierarchical agents in complex
domains.
",0
POPGym: Benchmarking Partially Observable Reinforcement Learning,"Steven Morad, Ryan Kortvelesy, Matteo Bettini, Stephan Liwicki, Amanda Prorok",2023-03-03T11:25:33Z,Reinforcement Learning,"  Real world applications of Reinforcement Learning (RL) are often partially
observable, thus requiring memory. Despite this, partial observability is still
largely ignored by contemporary RL benchmarks and libraries. We introduce
Partially Observable Process Gym (POPGym), a two-part library containing (1) a
diverse collection of 15 partially observable environments, each with multiple
difficulties and (2) implementations of 13 memory model baselines -- the most
in a single RL library. Existing partially observable benchmarks tend to fixate
on 3D visual navigation, which is computationally expensive and only one type
of POMDP. In contrast, POPGym environments are diverse, produce smaller
observations, use less memory, and often converge within two hours of training
on a consumer-grade GPU. We implement our high-level memory API and memory
baselines on top of the popular RLlib framework, providing plug-and-play
compatibility with various training algorithms, exploration strategies, and
distributed training paradigms. Using POPGym, we execute the largest comparison
across RL memory models to date. POPGym is available at
https://github.com/proroklab/popgym.
",0
Safe Reinforcement Learning via Probabilistic Logic Shields,"Wen-Chi Yang, Giuseppe Marra, Gavin Rens, Luc De Raedt",2023-03-06T15:43:41Z,Reinforcement Learning,"  Safe Reinforcement learning (Safe RL) aims at learning optimal policies while
staying safe. A popular solution to Safe RL is shielding, which uses a logical
safety specification to prevent an RL agent from taking unsafe actions.
However, traditional shielding techniques are difficult to integrate with
continuous, end-to-end deep RL methods. To this end, we introduce Probabilistic
Logic Policy Gradient (PLPG). PLPG is a model-based Safe RL technique that uses
probabilistic logic programming to model logical safety constraints as
differentiable functions. Therefore, PLPG can be seamlessly applied to any
policy gradient algorithm while still providing the same convergence
guarantees. In our experiments, we show that PLPG learns safer and more
rewarding policies compared to other state-of-the-art shielding techniques.
",0
Real-World Humanoid Locomotion with Reinforcement Learning,"Ilija Radosavovic, Tete Xiao, Bike Zhang, Trevor Darrell, Jitendra Malik, Koushil Sreenath",2023-03-06T18:59:09Z,Reinforcement Learning,"  Humanoid robots that can autonomously operate in diverse environments have
the potential to help address labour shortages in factories, assist elderly at
homes, and colonize new planets. While classical controllers for humanoid
robots have shown impressive results in a number of settings, they are
challenging to generalize and adapt to new environments. Here, we present a
fully learning-based approach for real-world humanoid locomotion. Our
controller is a causal transformer that takes the history of proprioceptive
observations and actions as input and predicts the next action. We hypothesize
that the observation-action history contains useful information about the world
that a powerful transformer model can use to adapt its behavior in-context,
without updating its weights. We train our model with large-scale model-free
reinforcement learning on an ensemble of randomized environments in simulation
and deploy it to the real world zero-shot. Our controller can walk over various
outdoor terrains, is robust to external disturbances, and can adapt in context.
",0
Conceptual Reinforcement Learning for Language-Conditioned Tasks,"Shaohui Peng, Xing Hu, Rui Zhang, Jiaming Guo, Qi Yi, Ruizhi Chen, Zidong Du, Ling Li, Qi Guo, Yunji Chen",2023-03-09T07:01:06Z,Reinforcement Learning,"  Despite the broad application of deep reinforcement learning (RL),
transferring and adapting the policy to unseen but similar environments is
still a significant challenge. Recently, the language-conditioned policy is
proposed to facilitate policy transfer through learning the joint
representation of observation and text that catches the compact and invariant
information across environments. Existing studies of language-conditioned RL
methods often learn the joint representation as a simple latent layer for the
given instances (episode-specific observation and text), which inevitably
includes noisy or irrelevant information and cause spurious correlations that
are dependent on instances, thus hurting generalization performance and
training efficiency. To address this issue, we propose a conceptual
reinforcement learning (CRL) framework to learn the concept-like joint
representation for language-conditioned policy. The key insight is that
concepts are compact and invariant representations in human cognition through
extracting similarities from numerous instances in real-world. In CRL, we
propose a multi-level attention encoder and two mutual information constraints
for learning compact and invariant concepts. Verified in two challenging
environments, RTFM and Messenger, CRL significantly improves the training
efficiency (up to 70%) and generalization ability (up to 30%) to the new
environment dynamics.
",0
Beware of Instantaneous Dependence in Reinforcement Learning,"Zhengmao Zhu, Yuren Liu, Honglong Tian, Yang Yu, Kun Zhang",2023-03-09T17:57:36Z,Reinforcement Learning,"  Playing an important role in Model-Based Reinforcement Learning (MBRL),
environment models aim to predict future states based on the past. Existing
works usually ignore instantaneous dependence in the state, that is, assuming
that the future state variables are conditionally independent given the past
states. However, instantaneous dependence is prevalent in many RL environments.
For instance, in the stock market, instantaneous dependence can exist between
two stocks because the fluctuation of one stock can quickly affect the other
and the resolution of price change is lower than that of the effect. In this
paper, we prove that with few exceptions, ignoring instantaneous dependence can
result in suboptimal policy learning in MBRL. To address the suboptimality
problem, we propose a simple plug-and-play method to enable existing MBRL
algorithms to take instantaneous dependence into account. Through experiments
on two benchmarks, we (1) confirm the existence of instantaneous dependence
with visualization; (2) validate our theoretical findings that ignoring
instantaneous dependence leads to suboptimal policy; (3) verify that our method
effectively enables reinforcement learning with instantaneous dependence and
improves policy performance.
",0
Kernel Density Bayesian Inverse Reinforcement Learning,"Aishwarya Mandyam, Didong Li, Diana Cai, Andrew Jones, Barbara E. Engelhardt",2023-03-13T03:00:03Z,Reinforcement Learning,"  Inverse reinforcement learning~(IRL) is a powerful framework to infer an
agent's reward function by observing its behavior, but IRL algorithms that
learn point estimates of the reward function can be misleading because there
may be several functions that describe an agent's behavior equally well. A
Bayesian approach to IRL models a distribution over candidate reward functions,
alleviating the shortcomings of learning a point estimate. However, several
Bayesian IRL algorithms use a $Q$-value function in place of the likelihood
function. The resulting posterior is computationally intensive to calculate,
has few theoretical guarantees, and the $Q$-value function is often a poor
approximation for the likelihood. We introduce kernel density Bayesian IRL
(KD-BIRL), which uses conditional kernel density estimation to directly
approximate the likelihood, providing an efficient framework that, with a
modified reward function parameterization, is applicable to environments with
complex and infinite state spaces. We demonstrate KD-BIRL's benefits through a
series of experiments in Gridworld environments and a simulated sepsis
treatment task.
",0
Deploying Offline Reinforcement Learning with Human Feedback,"Ziniu Li, Ke Xu, Liu Liu, Lanqing Li, Deheng Ye, Peilin Zhao",2023-03-13T12:13:16Z,Reinforcement Learning,"  Reinforcement learning (RL) has shown promise for decision-making tasks in
real-world applications. One practical framework involves training
parameterized policy models from an offline dataset and subsequently deploying
them in an online environment. However, this approach can be risky since the
offline training may not be perfect, leading to poor performance of the RL
models that may take dangerous actions. To address this issue, we propose an
alternative framework that involves a human supervising the RL models and
providing additional feedback in the online deployment phase. We formalize this
online deployment problem and develop two approaches. The first approach uses
model selection and the upper confidence bound algorithm to adaptively select a
model to deploy from a candidate set of trained offline RL models. The second
approach involves fine-tuning the model in the online deployment phase when a
supervision signal arrives. We demonstrate the effectiveness of these
approaches for robot locomotion control and traffic light control tasks through
empirical validation.
",1
Human-Inspired Framework to Accelerate Reinforcement Learning,"Ali Beikmohammadi, Sindri Magnússon",2023-02-28T13:15:04Z,Reinforcement Learning,"  Reinforcement learning (RL) is crucial for data science decision-making but
suffers from sample inefficiency, particularly in real-world scenarios with
costly physical interactions. This paper introduces a novel human-inspired
framework to enhance RL algorithm sample efficiency. It achieves this by
initially exposing the learning agent to simpler tasks that progressively
increase in complexity, ultimately leading to the main task. This method
requires no pre-training and involves learning simpler tasks for just one
iteration. The resulting knowledge can facilitate various transfer learning
approaches, such as value and policy transfer, without increasing computational
complexity. It can be applied across different goals, environments, and RL
algorithms, including value-based, policy-based, tabular, and deep RL methods.
Experimental evaluations demonstrate the framework's effectiveness in enhancing
sample efficiency, especially in challenging main tasks, demonstrated through
both a simple Random Walk and more complex optimal control problems with
constraints.
",0
Online Reinforcement Learning in Periodic MDP,"Ayush Aniket, Arpan Chattopadhyay",2023-03-16T20:16:45Z,Reinforcement Learning,"  We study learning in periodic Markov Decision Process (MDP), a special type
of non-stationary MDP where both the state transition probabilities and reward
functions vary periodically, under the average reward maximization setting. We
formulate the problem as a stationary MDP by augmenting the state space with
the period index, and propose a periodic upper confidence bound reinforcement
learning-2 (PUCRL2) algorithm. We show that the regret of PUCRL2 varies
linearly with the period $N$ and as $\mathcal{O}(\sqrt{Tlog T})$ with the
horizon length $T$. Utilizing the information about the sparsity of transition
matrix of augmented MDP, we propose another algorithm PUCRLB which enhances
upon PUCRL2, both in terms of regret ($O(\sqrt{N})$ dependency on period) and
empirical performance. Finally, we propose two other algorithms U-PUCRL2 and
U-PUCRLB for extended uncertainty in the environment in which the period is
unknown but a set of candidate periods are known. Numerical results demonstrate
the efficacy of all the algorithms.
",0
Deceptive Reinforcement Learning in Model-Free Domains,"Alan Lewis, Tim Miller",2023-03-20T02:47:40Z,Reinforcement Learning,"  This paper investigates deceptive reinforcement learning for privacy
preservation in model-free and continuous action space domains. In
reinforcement learning, the reward function defines the agent's objective. In
adversarial scenarios, an agent may need to both maximise rewards and keep its
reward function private from observers. Recent research presented the ambiguity
model (AM), which selects actions that are ambiguous over a set of possible
reward functions, via pre-trained $Q$-functions. Despite promising results in
model-based domains, our investigation shows that AM is ineffective in
model-free domains due to misdirected state space exploration. It is also
inefficient to train and inapplicable in continuous action space domains. We
propose the deceptive exploration ambiguity model (DEAM), which learns using
the deceptive policy during training, leading to targeted exploration of the
state space. DEAM is also applicable in continuous action spaces. We evaluate
DEAM in discrete and continuous action space path planning environments. DEAM
achieves similar performance to an optimal model-based version of AM and
outperforms a model-free version of AM in terms of path cost, deceptiveness and
training efficiency. These results extend to the continuous domain.
",0
Reflexion: Language Agents with Verbal Reinforcement Learning,"Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao",2023-03-20T18:08:50Z,Reinforcement Learning,"  Large language models (LLMs) have been increasingly used to interact with
external environments (e.g., games, compilers, APIs) as goal-driven agents.
However, it remains challenging for these language agents to quickly and
efficiently learn from trial-and-error as traditional reinforcement learning
methods require extensive training samples and expensive model fine-tuning. We
propose Reflexion, a novel framework to reinforce language agents not by
updating weights, but instead through linguistic feedback. Concretely,
Reflexion agents verbally reflect on task feedback signals, then maintain their
own reflective text in an episodic memory buffer to induce better
decision-making in subsequent trials. Reflexion is flexible enough to
incorporate various types (scalar values or free-form language) and sources
(external or internally simulated) of feedback signals, and obtains significant
improvements over a baseline agent across diverse tasks (sequential
decision-making, coding, language reasoning). For example, Reflexion achieves a
91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous
state-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis
studies using different feedback signals, feedback incorporation methods, and
agent types, and provide insights into how they affect performance.
",0
Reinforcement Learning with Exogenous States and Rewards,"George Trimponias, Thomas G. Dietterich",2023-03-22T23:37:28Z,Reinforcement Learning,"  Exogenous state variables and rewards can slow reinforcement learning by
injecting uncontrolled variation into the reward signal. This paper formalizes
exogenous state variables and rewards and shows that if the reward function
decomposes additively into endogenous and exogenous components, the MDP can be
decomposed into an exogenous Markov Reward Process (based on the exogenous
reward) and an endogenous Markov Decision Process (optimizing the endogenous
reward). Any optimal policy for the endogenous MDP is also an optimal policy
for the original MDP, but because the endogenous reward typically has reduced
variance, the endogenous MDP is easier to solve. We study settings where the
decomposition of the state space into exogenous and endogenous state spaces is
not given but must be discovered. The paper introduces and proves correctness
of algorithms for discovering the exogenous and endogenous subspaces of the
state space when they are mixed through linear combination. These algorithms
can be applied during reinforcement learning to discover the exogenous space,
remove the exogenous reward, and focus reinforcement learning on the endogenous
MDP. Experiments on a variety of challenging synthetic MDPs show that these
methods, applied online, discover large exogenous state spaces and produce
substantial speedups in reinforcement learning.
",0
Robotic Packaging Optimization with Reinforcement Learning,"Eveline Drijver, Rodrigo Pérez-Dattari, Jens Kober, Cosimo Della Santina, Zlatan Ajanović",2023-03-26T11:42:49Z,Reinforcement Learning,"  Intelligent manufacturing is becoming increasingly important due to the
growing demand for maximizing productivity and flexibility while minimizing
waste and lead times. This work investigates automated secondary robotic food
packaging solutions that transfer food products from the conveyor belt into
containers. A major problem in these solutions is varying product supply which
can cause drastic productivity drops. Conventional rule-based approaches, used
to address this issue, are often inadequate, leading to violation of the
industry's requirements. Reinforcement learning, on the other hand, has the
potential of solving this problem by learning responsive and predictive policy,
based on experience. However, it is challenging to utilize it in highly complex
control schemes. In this paper, we propose a reinforcement learning framework,
designed to optimize the conveyor belt speed while minimizing interference with
the rest of the control system. When tested on real-world data, the framework
exceeds the performance requirements (99.8% packed products) and maintains
quality (100% filled boxes). Compared to the existing solution, our proposed
framework improves productivity, has smoother control, and reduces computation
time.
",0
Model-Based Reinforcement Learning with Isolated Imaginations,"Minting Pan, Xiangming Zhu, Yitao Zheng, Yunbo Wang, Xiaokang Yang",2023-03-27T02:55:56Z,Reinforcement Learning,"  World models learn the consequences of actions in vision-based interactive
systems. However, in practical scenarios like autonomous driving,
noncontrollable dynamics that are independent or sparsely dependent on action
signals often exist, making it challenging to learn effective world models. To
address this issue, we propose Iso-Dream++, a model-based reinforcement
learning approach that has two main contributions. First, we optimize the
inverse dynamics to encourage the world model to isolate controllable state
transitions from the mixed spatiotemporal variations of the environment.
Second, we perform policy optimization based on the decoupled latent
imaginations, where we roll out noncontrollable states into the future and
adaptively associate them with the current controllable state. This enables
long-horizon visuomotor control tasks to benefit from isolating mixed dynamics
sources in the wild, such as self-driving cars that can anticipate the movement
of other vehicles, thereby avoiding potential risks. On top of our previous
work, we further consider the sparse dependencies between controllable and
noncontrollable states, address the training collapse problem of state
decoupling, and validate our approach in transfer learning setups. Our
empirical study demonstrates that Iso-Dream++ outperforms existing
reinforcement learning models significantly on CARLA and DeepMind Control.
",0
Physical Deep Reinforcement Learning Towards Safety Guarantee,"Hongpeng Cao, Yanbing Mao, Lui Sha, Marco Caccamo",2023-03-29T17:17:59Z,Reinforcement Learning,"  Deep reinforcement learning (DRL) has achieved tremendous success in many
complex decision-making tasks of autonomous systems with high-dimensional state
and/or action spaces. However, the safety and stability still remain major
concerns that hinder the applications of DRL to safety-critical autonomous
systems. To address the concerns, we proposed the Phy-DRL: a physical deep
reinforcement learning framework. The Phy-DRL is novel in two architectural
designs: i) Lyapunov-like reward, and ii) residual control (i.e., integration
of physics-model-based control and data-driven control). The concurrent
physical reward and residual control empower the Phy-DRL the (mathematically)
provable safety and stability guarantees. Through experiments on the inverted
pendulum, we show that the Phy-DRL features guaranteed safety and stability and
enhanced robustness, while offering remarkably accelerated training and
enlarged reward.
",0
A Tutorial Introduction to Reinforcement Learning,Mathukumalli Vidyasagar,2023-04-03T08:50:58Z,Reinforcement Learning,"  In this paper, we present a brief survey of Reinforcement Learning (RL), with
particular emphasis on Stochastic Approximation (SA) as a unifying theme. The
scope of the paper includes Markov Reward Processes, Markov Decision Processes,
Stochastic Approximation algorithms, and widely used algorithms such as
Temporal Difference Learning and $Q$-learning.
",0
Swarm Reinforcement Learning For Adaptive Mesh Refinement,"Niklas Freymuth, Philipp Dahlinger, Tobias Würth, Simon Reisch, Luise Kärger, Gerhard Neumann",2023-04-03T09:07:17Z,Reinforcement Learning,"  Adaptive Mesh Refinement (AMR) enhances the Finite Element Method, an
important technique for simulating complex problems in engineering, by
dynamically refining mesh regions, enabling a favorable trade-off between
computational speed and simulation accuracy. Classical methods for AMR depend
on heuristics or expensive error estimators, hindering their use for complex
simulations. Recent learning-based AMR methods tackle these issues, but so far
scale only to simple toy examples. We formulate AMR as a novel Adaptive Swarm
Markov Decision Process in which a mesh is modeled as a system of simple
collaborating agents that may split into multiple new agents. This framework
allows for a spatial reward formulation that simplifies the credit assignment
problem, which we combine with Message Passing Networks to propagate
information between neighboring mesh elements. We experimentally validate our
approach, Adaptive Swarm Mesh Refinement (ASMR), on challenging refinement
tasks. Our approach learns reliable and efficient refinement strategies that
can robustly generalize to different domains during inference. Additionally, it
achieves a speedup of up to $2$ orders of magnitude compared to uniform
refinements in more demanding simulations. We outperform learned baselines and
heuristics, achieving a refinement quality that is on par with costly
error-based oracle AMR strategies.
",0
Risk-Aware Distributed Multi-Agent Reinforcement Learning,"Abdullah Al Maruf, Luyao Niu, Bhaskar Ramasubramanian, Andrew Clark, Radha Poovendran",2023-04-04T17:56:44Z,Reinforcement Learning,"  Autonomous cyber and cyber-physical systems need to perform decision-making,
learning, and control in unknown environments. Such decision-making can be
sensitive to multiple factors, including modeling errors, changes in costs, and
impacts of events in the tails of probability distributions. Although
multi-agent reinforcement learning (MARL) provides a framework for learning
behaviors through repeated interactions with the environment by minimizing an
average cost, it will not be adequate to overcome the above challenges. In this
paper, we develop a distributed MARL approach to solve decision-making problems
in unknown environments by learning risk-aware actions. We use the conditional
value-at-risk (CVaR) to characterize the cost function that is being minimized,
and define a Bellman operator to characterize the value function associated to
a given state-action pair. We prove that this operator satisfies a contraction
property, and that it converges to the optimal value function. We then propose
a distributed MARL algorithm called the CVaR QD-Learning algorithm, and
establish that value functions of individual agents reaches consensus. We
identify several challenges that arise in the implementation of the CVaR
QD-Learning algorithm, and present solutions to overcome these. We evaluate the
CVaR QD-Learning algorithm through simulations, and demonstrate the effect of a
risk parameter on value functions at consensus.
",0
Aiding reinforcement learning for set point control,"Ruoqi Zhang, Per Mattsson, Torbjörn Wigren",2023-04-20T13:12:00Z,Reinforcement Learning,"  While reinforcement learning has made great improvements, state-of-the-art
algorithms can still struggle with seemingly simple set-point feedback control
problems. One reason for this is that the learned controller may not be able to
excite the system dynamics well enough initially, and therefore it can take a
long time to get data that is informative enough to learn for good control. The
paper contributes by augmentation of reinforcement learning with a simple
guiding feedback controller, for example, a proportional controller. The key
advantage in set point control is a much improved excitation that improves the
convergence properties of the reinforcement learning controller significantly.
This can be very important in real-world control where quick and accurate
convergence is needed. The proposed method is evaluated with simulation and on
a real-world double tank process with promising results.
",3
Efficient Deep Reinforcement Learning Requires Regulating Overfitting,"Qiyang Li, Aviral Kumar, Ilya Kostrikov, Sergey Levine",2023-04-20T17:11:05Z,Reinforcement Learning,"  Deep reinforcement learning algorithms that learn policies by trial-and-error
must learn from limited amounts of data collected by actively interacting with
the environment. While many prior works have shown that proper regularization
techniques are crucial for enabling data-efficient RL, a general understanding
of the bottlenecks in data-efficient RL has remained unclear. Consequently, it
has been difficult to devise a universal technique that works well across all
domains. In this paper, we attempt to understand the primary bottleneck in
sample-efficient deep RL by examining several potential hypotheses such as
non-stationarity, excessive action distribution shift, and overfitting. We
perform thorough empirical analysis on state-based DeepMind control suite (DMC)
tasks in a controlled and systematic way to show that high temporal-difference
(TD) error on the validation set of transitions is the main culprit that
severely affects the performance of deep RL algorithms, and prior methods that
lead to good performance do in fact, control the validation TD error to be low.
This observation gives us a robust principle for making deep RL efficient: we
can hill-climb on the validation TD error by utilizing any form of
regularization techniques from supervised learning. We show that a simple
online model selection method that targets the validation TD error is effective
across state-based DMC and Gym tasks.
",0
Reinforcement Learning with an Abrupt Model Change,"Wuxia Chen, Taposh Banerjee, Jemin George, Carl Busart",2023-04-22T18:16:01Z,Reinforcement Learning,"  The problem of reinforcement learning is considered where the environment or
the model undergoes a change. An algorithm is proposed that an agent can apply
in such a problem to achieve the optimal long-time discounted reward. The
algorithm is model-free and learns the optimal policy by interacting with the
environment. It is shown that the proposed algorithm has strong optimality
properties. The effectiveness of the algorithm is also demonstrated using
simulation results. The proposed algorithm exploits a fundamental
reward-detection trade-off present in these problems and uses a quickest change
detection algorithm to detect the model change. Recommendations are provided
for faster detection of model changes and for smart initialization strategies.
",0
Efficient Halftoning via Deep Reinforcement Learning,"Haitian Jiang, Dongliang Xiong, Xiaowen Jiang, Li Ding, Liang Chen, Kai Huang",2023-04-24T15:03:37Z,Reinforcement Learning,"  Halftoning aims to reproduce a continuous-tone image with pixels whose
intensities are constrained to two discrete levels. This technique has been
deployed on every printer, and the majority of them adopt fast methods (e.g.,
ordered dithering, error diffusion) that fail to render structural details,
which determine halftone's quality. Other prior methods of pursuing visual
pleasure by searching for the optimal halftone solution, on the contrary,
suffer from their high computational cost. In this paper, we propose a fast and
structure-aware halftoning method via a data-driven approach. Specifically, we
formulate halftoning as a reinforcement learning problem, in which each binary
pixel's value is regarded as an action chosen by a virtual agent with a shared
fully convolutional neural network (CNN) policy. In the offline phase, an
effective gradient estimator is utilized to train the agents in producing
high-quality halftones in one action step. Then, halftones can be generated
online by one fast CNN inference. Besides, we propose a novel anisotropy
suppressing loss function, which brings the desirable blue-noise property.
Finally, we find that optimizing SSIM could result in holes in flat areas,
which can be avoided by weighting the metric with the contone's contrast map.
Experiments show that our framework can effectively train a light-weight CNN,
which is 15x faster than previous structure-aware methods, to generate
blue-noise halftones with satisfactory visual quality. We also present a
prototype of deep multitoning to demonstrate the extensibility of our method.
",0
Proximal Curriculum for Reinforcement Learning Agents,"Georgios Tzannetos, Bárbara Gomes Ribeiro, Parameswaran Kamalaruban, Adish Singla",2023-04-25T14:49:34Z,Reinforcement Learning,"  We consider the problem of curriculum design for reinforcement learning (RL)
agents in contextual multi-task settings. Existing techniques on automatic
curriculum design typically require domain-specific hyperparameter tuning or
have limited theoretical underpinnings. To tackle these limitations, we design
our curriculum strategy, ProCuRL, inspired by the pedagogical concept of Zone
of Proximal Development (ZPD). ProCuRL captures the intuition that learning
progress is maximized when picking tasks that are neither too hard nor too easy
for the learner. We mathematically derive ProCuRL by analyzing two simple
learning settings. We also present a practical variant of ProCuRL that can be
directly integrated with deep RL frameworks with minimal hyperparameter tuning.
Experimental results on a variety of domains demonstrate the effectiveness of
our curriculum strategy over state-of-the-art baselines in accelerating the
training process of deep RL agents.
",0
Towards Theoretical Understanding of Inverse Reinforcement Learning,"Alberto Maria Metelli, Filippo Lazzati, Marcello Restelli",2023-04-25T16:21:10Z,Reinforcement Learning,"  Inverse reinforcement learning (IRL) denotes a powerful family of algorithms
for recovering a reward function justifying the behavior demonstrated by an
expert agent. A well-known limitation of IRL is the ambiguity in the choice of
the reward function, due to the existence of multiple rewards that explain the
observed behavior. This limitation has been recently circumvented by
formulating IRL as the problem of estimating the feasible reward set, i.e., the
region of the rewards compatible with the expert's behavior. In this paper, we
make a step towards closing the theory gap of IRL in the case of finite-horizon
problems with a generative model. We start by formally introducing the problem
of estimating the feasible reward set, the corresponding PAC requirement, and
discussing the properties of particular classes of rewards. Then, we provide
the first minimax lower bound on the sample complexity for the problem of
estimating the feasible reward set of order ${\Omega}\Bigl(
\frac{H^3SA}{\epsilon^2} \bigl( \log \bigl(\frac{1}{\delta}\bigl) + S
\bigl)\Bigl)$, being $S$ and $A$ the number of states and actions respectively,
$H$ the horizon, $\epsilon$ the desired accuracy, and $\delta$ the confidence.
We analyze the sample complexity of a uniform sampling strategy (US-IRL),
proving a matching upper bound up to logarithmic factors. Finally, we outline
several open questions in IRL and propose future research directions.
",0
Reinforcement Learning with Partial Parametric Model Knowledge,"Shuyuan Wang, Philip D. Loewen, Nathan P. Lawrence, Michael G. Forbes, R. Bhushan Gopaluni",2023-04-26T01:04:35Z,Reinforcement Learning,"  We adapt reinforcement learning (RL) methods for continuous control to bridge
the gap between complete ignorance and perfect knowledge of the environment.
Our method, Partial Knowledge Least Squares Policy Iteration (PLSPI), takes
inspiration from both model-free RL and model-based control. It uses incomplete
information from a partial model and retains RL's data-driven adaption towards
optimal performance. The linear quadratic regulator provides a case study;
numerical experiments demonstrate the effectiveness and resulting benefits of
the proposed method.
",0
Adversarial Policy Optimization in Deep Reinforcement Learning,"Md Masudur Rahman, Yexiang Xue",2023-04-27T21:01:08Z,Reinforcement Learning,"  The policy represented by the deep neural network can overfit the spurious
features in observations, which hamper a reinforcement learning agent from
learning effective policy. This issue becomes severe in high-dimensional state,
where the agent struggles to learn a useful policy. Data augmentation can
provide a performance boost to RL agents by mitigating the effect of
overfitting. However, such data augmentation is a form of prior knowledge, and
naively applying them in environments might worsen an agent's performance. In
this paper, we propose a novel RL algorithm to mitigate the above issue and
improve the efficiency of the learned policy. Our approach consists of a
max-min game theoretic objective where a perturber network modifies the state
to maximize the agent's probability of taking a different action while
minimizing the distortion in the state. In contrast, the policy network updates
its parameters to minimize the effect of perturbation while maximizing the
expected future reward. Based on this objective, we propose a practical deep
reinforcement learning algorithm, Adversarial Policy Optimization (APO). Our
method is agnostic to the type of policy optimization, and thus data
augmentation can be incorporated to harness the benefit. We evaluated our
approaches on several DeepMind Control robotic environments with
high-dimensional and noisy state settings. Empirical results demonstrate that
our method APO consistently outperforms the state-of-the-art on-policy PPO
agent. We further compare our method with state-of-the-art data augmentation,
RAD, and regularization-based approach DRAC. Our agent APO shows better
performance compared to these baselines.
",0
Posterior Sampling for Deep Reinforcement Learning,"Remo Sasso, Michelangelo Conserva, Paulo Rauber",2023-04-30T13:23:50Z,Reinforcement Learning,"  Despite remarkable successes, deep reinforcement learning algorithms remain
sample inefficient: they require an enormous amount of trial and error to find
good policies. Model-based algorithms promise sample efficiency by building an
environment model that can be used for planning. Posterior Sampling for
Reinforcement Learning is such a model-based algorithm that has attracted
significant interest due to its performance in the tabular setting. This paper
introduces Posterior Sampling for Deep Reinforcement Learning (PSDRL), the
first truly scalable approximation of Posterior Sampling for Reinforcement
Learning that retains its model-based essence. PSDRL combines efficient
uncertainty quantification over latent state space models with a specially
tailored continual planning algorithm based on value-function approximation.
Extensive experiments on the Atari benchmark show that PSDRL significantly
outperforms previous state-of-the-art attempts at scaling up posterior sampling
while being competitive with a state-of-the-art (model-based) reinforcement
learning method, both in sample efficiency and computational efficiency.
",0
Maximum Causal Entropy Inverse Constrained Reinforcement Learning,"Mattijs Baert, Pietro Mazzaglia, Sam Leroux, Pieter Simoens",2023-05-04T14:18:19Z,Reinforcement Learning,"  When deploying artificial agents in real-world environments where they
interact with humans, it is crucial that their behavior is aligned with the
values, social norms or other requirements of that environment. However, many
environments have implicit constraints that are difficult to specify and
transfer to a learning agent. To address this challenge, we propose a novel
method that utilizes the principle of maximum causal entropy to learn
constraints and an optimal policy that adheres to these constraints, using
demonstrations of agents that abide by the constraints. We prove convergence in
a tabular setting and provide an approximation which scales to complex
environments. We evaluate the effectiveness of the learned policy by assessing
the reward received and the number of constraint violations, and we evaluate
the learned cost function based on its transferability to other agents. Our
method has been shown to outperform state-of-the-art approaches across a
variety of tasks and environments, and it is able to handle problems with
stochastic dynamics and a continuous state-action space.
",0
Simple Noisy Environment Augmentation for Reinforcement Learning,"Raad Khraishi, Ramin Okhrati",2023-05-04T14:45:09Z,Reinforcement Learning,"  Data augmentation is a widely used technique for improving model performance
in machine learning, particularly in computer vision and natural language
processing. Recently, there has been increasing interest in applying
augmentation techniques to reinforcement learning (RL) problems, with a focus
on image-based augmentation. In this paper, we explore a set of generic
wrappers designed to augment RL environments with noise and encourage agent
exploration and improve training data diversity which are applicable to a broad
spectrum of RL algorithms and environments. Specifically, we concentrate on
augmentations concerning states, rewards, and transition dynamics and introduce
two novel augmentation techniques. In addition, we introduce a noise rate
hyperparameter for control over the frequency of noise injection. We present
experimental results on the impact of these wrappers on return using three
popular RL algorithms, Soft Actor-Critic (SAC), Twin Delayed DDPG (TD3), and
Proximal Policy Optimization (PPO), across five MuJoCo environments. To support
the choice of augmentation technique in practice, we also present analysis that
explores the performance these techniques across environments. Lastly, we
publish the wrappers in our noisyenv repository for use with gym environments.
",0
Rethinking Population-assisted Off-policy Reinforcement Learning,"Bowen Zheng, Ran Cheng",2023-05-04T15:53:00Z,Reinforcement Learning,"  While off-policy reinforcement learning (RL) algorithms are sample efficient
due to gradient-based updates and data reuse in the replay buffer, they
struggle with convergence to local optima due to limited exploration. On the
other hand, population-based algorithms offer a natural exploration strategy,
but their heuristic black-box operators are inefficient. Recent algorithms have
integrated these two methods, connecting them through a shared replay buffer.
However, the effect of using diverse data from population optimization
iterations on off-policy RL algorithms has not been thoroughly investigated. In
this paper, we first analyze the use of off-policy RL algorithms in combination
with population-based algorithms, showing that the use of population data could
introduce an overlooked error and harm performance. To test this, we propose a
uniform and scalable training design and conduct experiments on our tailored
framework in robot locomotion tasks from the OpenAI gym. Our results
substantiate that using population data in off-policy RL can cause instability
during training and even degrade performance. To remedy this issue, we further
propose a double replay buffer design that provides more on-policy data and
show its effectiveness through experiments. Our results offer practical
insights for training these hybrid methods.
",0
Federated Ensemble-Directed Offline Reinforcement Learning,"Desik Rengarajan, Nitin Ragothaman, Dileep Kalathil, Srinivas Shakkottai",2023-05-04T18:25:34Z,Reinforcement Learning,"  We consider the problem of federated offline reinforcement learning (RL), a
scenario under which distributed learning agents must collaboratively learn a
high-quality control policy only using small pre-collected datasets generated
according to different unknown behavior policies. Na\""{i}vely combining a
standard offline RL approach with a standard federated learning approach to
solve this problem can lead to poorly performing policies. In response, we
develop the Federated Ensemble-Directed Offline Reinforcement Learning
Algorithm (FEDORA), which distills the collective wisdom of the clients using
an ensemble learning approach. We develop the FEDORA codebase to utilize
distributed compute resources on a federated learning platform. We show that
FEDORA significantly outperforms other approaches, including offline RL over
the combined data pool, in various complex continuous control environments and
real-world datasets. Finally, we demonstrate the performance of FEDORA in the
real-world on a mobile robot. We provide our code and a video of our
experiments at \url{https://github.com/DesikRengarajan/FEDORA}.
",0
Bayesian Reinforcement Learning with Limited Cognitive Load,"Dilip Arumugam, Mark K. Ho, Noah D. Goodman, Benjamin Van Roy",2023-05-05T03:29:34Z,Reinforcement Learning,"  All biological and artificial agents must learn and make decisions given
limits on their ability to process information. As such, a general theory of
adaptive behavior should be able to account for the complex interactions
between an agent's learning history, decisions, and capacity constraints.
Recent work in computer science has begun to clarify the principles that shape
these dynamics by bridging ideas from reinforcement learning, Bayesian
decision-making, and rate-distortion theory. This body of work provides an
account of capacity-limited Bayesian reinforcement learning, a unifying
normative framework for modeling the effect of processing constraints on
learning and action selection. Here, we provide an accessible review of recent
algorithms and theoretical results in this setting, paying special attention to
how these ideas can be applied to studying questions in the cognitive and
behavioral sciences.
",7
Truncating Trajectories in Monte Carlo Reinforcement Learning,"Riccardo Poiani, Alberto Maria Metelli, Marcello Restelli",2023-05-07T19:41:57Z,Reinforcement Learning,"  In Reinforcement Learning (RL), an agent acts in an unknown environment to
maximize the expected cumulative discounted sum of an external reward signal,
i.e., the expected return. In practice, in many tasks of interest, such as
policy optimization, the agent usually spends its interaction budget by
collecting episodes of fixed length within a simulator (i.e., Monte Carlo
simulation). However, given the discounted nature of the RL objective, this
data collection strategy might not be the best option. Indeed, the rewards
taken in early simulation steps weigh exponentially more than future rewards.
Taking a cue from this intuition, in this paper, we design an a-priori budget
allocation strategy that leads to the collection of trajectories of different
lengths, i.e., truncated. The proposed approach provably minimizes the width of
the confidence intervals around the empirical estimates of the expected return
of a policy. After discussing the theoretical properties of our method, we make
use of our trajectory truncation mechanism to extend Policy Optimization via
Importance Sampling (POIS, Metelli et al., 2018) algorithm. Finally, we conduct
a numerical comparison between our algorithm and POIS: the results are
consistent with our theory and show that an appropriate truncation of the
trajectories can succeed in improving performance.
",1
RLocator: Reinforcement Learning for Bug Localization,"Partha Chakraborty, Mahmoud Alfadel, Meiyappan Nagappan",2023-05-09T16:19:33Z,Reinforcement Learning,"  Software developers spend a significant portion of time fixing bugs in their
projects. To streamline this process, bug localization approaches have been
proposed to identify the source code files that are likely responsible for a
particular bug. Prior work proposed several similarity-based machine-learning
techniques for bug localization. Despite significant advances in these
techniques, they do not directly optimize the evaluation measures. We argue
that directly optimizing evaluation measures can positively contribute to the
performance of bug localization approaches. Therefore, In this paper, we
utilize Reinforcement Learning (RL) techniques to directly optimize the ranking
metrics. We propose RLocator, a Reinforcement Learning-based bug localization
approach. We formulate RLocator using a Markov Decision Process (MDP) to
optimize the evaluation measures directly. We present the technique and
experimentally evaluate it based on a benchmark dataset of 8,316 bug reports
from six highly popular Apache projects. The results of our evaluation reveal
that RLocator achieves a Mean Reciprocal Rank (MRR) of 0.62, a Mean Average
Precision (MAP) of 0.59, and a Top 1 score of 0.46. We compare RLocator with
two state-of-the-art bug localization tools, FLIM and BugLocator. Our
evaluation reveals that RLocator outperforms both approaches by a substantial
margin, with improvements of 38.3% in MAP, 36.73% in MRR, and 23.68% in the Top
K metric. These findings highlight that directly optimizing evaluation measures
considerably contributes to performance improvement of the bug localization
problem.
",0
Information Design in Multi-Agent Reinforcement Learning,"Yue Lin, Wenhao Li, Hongyuan Zha, Baoxiang Wang",2023-05-08T07:52:15Z,Reinforcement Learning,"  Reinforcement learning (RL) is inspired by the way human infants and animals
learn from the environment. The setting is somewhat idealized because, in
actual tasks, other agents in the environment have their own goals and behave
adaptively to the ego agent. To thrive in those environments, the agent needs
to influence other agents so their actions become more helpful and less
harmful. Research in computational economics distills two ways to influence
others directly: by providing tangible goods (mechanism design) and by
providing information (information design). This work investigates information
design problems for a group of RL agents. The main challenges are two-fold. One
is the information provided will immediately affect the transition of the agent
trajectories, which introduces additional non-stationarity. The other is the
information can be ignored, so the sender must provide information that the
receiver is willing to respect. We formulate the Markov signaling game, and
develop the notions of signaling gradient and the extended obedience
constraints that address these challenges. Our algorithm is efficient on
various mixed-motive tasks and provides further insights into computational
economics. Our code is publicly available at
https://github.com/YueLin301/InformationDesignMARL.
",0
Optimizing Memory Mapping Using Deep Reinforcement Learning,"Pengming Wang, Mikita Sazanovich, Berkin Ilbeyi, Phitchaya Mangpo Phothilimthana, Manish Purohit, Han Yang Tay, Ngân Vũ, Miaosen Wang, Cosmin Paduraru, Edouard Leurent, Anton Zhernov, Po-Sen Huang, Julian Schrittwieser, Thomas Hubert, Robert Tung, Paula Kurylowicz, Kieran Milan, Oriol Vinyals, Daniel J. Mankowitz",2023-05-11T11:55:16Z,Reinforcement Learning,"  Resource scheduling and allocation is a critical component of many high
impact systems ranging from congestion control to cloud computing. Finding more
optimal solutions to these problems often has significant impact on resource
and time savings, reducing device wear-and-tear, and even potentially improving
carbon emissions. In this paper, we focus on a specific instance of a
scheduling problem, namely the memory mapping problem that occurs during
compilation of machine learning programs: That is, mapping tensors to different
memory layers to optimize execution time.
  We introduce an approach for solving the memory mapping problem using
Reinforcement Learning. RL is a solution paradigm well-suited for sequential
decision making problems that are amenable to planning, and combinatorial
search spaces with high-dimensional data inputs. We formulate the problem as a
single-player game, which we call the mallocGame, such that high-reward
trajectories of the game correspond to efficient memory mappings on the target
hardware. We also introduce a Reinforcement Learning agent, mallocMuZero, and
show that it is capable of playing this game to discover new and improved
memory mapping solutions that lead to faster execution times on real ML
workloads on ML accelerators. We compare the performance of mallocMuZero to the
default solver used by the Accelerated Linear Algebra (XLA) compiler on a
benchmark of realistic ML workloads. In addition, we show that mallocMuZero is
capable of improving the execution time of the recently published AlphaTensor
matrix multiplication model.
",0
Inverse Reinforcement Learning With Constraint Recovery,"Nirjhar Das, Arpan Chattopadhyay",2023-05-14T11:49:37Z,Reinforcement Learning,"  In this work, we propose a novel inverse reinforcement learning (IRL)
algorithm for constrained Markov decision process (CMDP) problems. In standard
IRL problems, the inverse learner or agent seeks to recover the reward function
of the MDP, given a set of trajectory demonstrations for the optimal policy. In
this work, we seek to infer not only the reward functions of the CMDP, but also
the constraints. Using the principle of maximum entropy, we show that the IRL
with constraint recovery (IRL-CR) problem can be cast as a constrained
non-convex optimization problem. We reduce it to an alternating constrained
optimization problem whose sub-problems are convex. We use exponentiated
gradient descent algorithm to solve it. Finally, we demonstrate the efficacy of
our algorithm for the grid world environment.
",0
What Matters in Reinforcement Learning for Tractography,"Antoine Théberge, Christian Desrosiers, Maxime Descoteaux, Pierre-Marc Jodoin",2023-05-15T22:01:48Z,Reinforcement Learning,"  Recently, deep reinforcement learning (RL) has been proposed to learn the
tractography procedure and train agents to reconstruct the structure of the
white matter without manually curated reference streamlines. While the
performances reported were competitive, the proposed framework is complex, and
little is still known about the role and impact of its multiple parts. In this
work, we thoroughly explore the different components of the proposed framework,
such as the choice of the RL algorithm, seeding strategy, the input signal and
reward function, and shed light on their impact. Approximately 7,400 models
were trained for this work, totalling nearly 41,000 hours of GPU time. Our goal
is to guide researchers eager to explore the possibilities of deep RL for
tractography by exposing what works and what does not work with the category of
approach. As such, we ultimately propose a series of recommendations concerning
the choice of RL algorithm, the input to the agents, the reward function and
more to help future work using reinforcement learning for tractography. We also
release the open source codebase, trained models, and datasets for users and
researchers wanting to explore reinforcement learning for tractography.
",0
Model-Free Robust Average-Reward Reinforcement Learning,"Yue Wang, Alvaro Velasquez, George Atia, Ashley Prater-Bennette, Shaofeng Zou",2023-05-17T18:19:23Z,"RAG, Reinforcement Learning","  Robust Markov decision processes (MDPs) address the challenge of model
uncertainty by optimizing the worst-case performance over an uncertainty set of
MDPs. In this paper, we focus on the robust average-reward MDPs under the
model-free setting. We first theoretically characterize the structure of
solutions to the robust average-reward Bellman equation, which is essential for
our later convergence analysis. We then design two model-free algorithms,
robust relative value iteration (RVI) TD and robust RVI Q-learning, and
theoretically prove their convergence to the optimal solution. We provide
several widely used uncertainty sets as examples, including those defined by
the contamination model, total variation, Chi-squared divergence,
Kullback-Leibler (KL) divergence and Wasserstein distance.
",0
Multi-task Hierarchical Adversarial Inverse Reinforcement Learning,"Jiayu Chen, Dipesh Tamboli, Tian Lan, Vaneet Aggarwal",2023-05-22T01:58:40Z,Reinforcement Learning,"  Multi-task Imitation Learning (MIL) aims to train a policy capable of
performing a distribution of tasks based on multi-task expert demonstrations,
which is essential for general-purpose robots. Existing MIL algorithms suffer
from low data efficiency and poor performance on complex long-horizontal tasks.
We develop Multi-task Hierarchical Adversarial Inverse Reinforcement Learning
(MH-AIRL) to learn hierarchically-structured multi-task policies, which is more
beneficial for compositional tasks with long horizons and has higher expert
data efficiency through identifying and transferring reusable basic skills
across tasks. To realize this, MH-AIRL effectively synthesizes context-based
multi-task learning, AIRL (an IL approach), and hierarchical policy learning.
Further, MH-AIRL can be adopted to demonstrations without the task or skill
annotations (i.e., state-action pairs only) which are more accessible in
practice. Theoretical justifications are provided for each module of MH-AIRL,
and evaluations on challenging multi-task settings demonstrate superior
performance and transferability of the multi-task policies learned with MH-AIRL
as compared to SOTA MIL baselines.
",0
Offline Reinforcement Learning with Additional Covering Distributions,Chenjie Mao,2023-05-22T03:31:03Z,Reinforcement Learning,"  We study learning optimal policies from a logged dataset, i.e., offline RL,
with function approximation. Despite the efforts devoted, existing algorithms
with theoretic finite-sample guarantees typically assume exploratory data
coverage or strong realizable function classes, which is hard to be satisfied
in reality. While there are recent works that successfully tackle these strong
assumptions, they either require the gap assumptions that only could be
satisfied by part of MDPs or use the behavior regularization that makes the
optimality of learned policy even intractable. To solve this challenge, we
provide finite-sample guarantees for a simple algorithm based on marginalized
importance sampling (MIS), showing that sample-efficient offline RL for general
MDPs is possible with only a partial coverage dataset and weak realizable
function classes given additional side information of a covering distribution.
Furthermore, we demonstrate that the covering distribution trades off prior
knowledge of the optimal trajectories against the coverage requirement of the
dataset, revealing the effect of this inductive bias in the learning processes.
",0
Training Diffusion Models with Reinforcement Learning,"Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, Sergey Levine",2023-05-22T17:57:41Z,Reinforcement Learning,"  Diffusion models are a class of flexible generative models trained with an
approximation to the log-likelihood objective. However, most use cases of
diffusion models are not concerned with likelihoods, but instead with
downstream objectives such as human-perceived image quality or drug
effectiveness. In this paper, we investigate reinforcement learning methods for
directly optimizing diffusion models for such objectives. We describe how
posing denoising as a multi-step decision-making problem enables a class of
policy gradient algorithms, which we refer to as denoising diffusion policy
optimization (DDPO), that are more effective than alternative reward-weighted
likelihood approaches. Empirically, DDPO is able to adapt text-to-image
diffusion models to objectives that are difficult to express via prompting,
such as image compressibility, and those derived from human feedback, such as
aesthetic quality. Finally, we show that DDPO can improve prompt-image
alignment using feedback from a vision-language model without the need for
additional data collection or human annotation. The project's website can be
found at http://rl-diffusion.github.io .
",0
GUARD: A Safe Reinforcement Learning Benchmark,"Weiye Zhao, Yifan Sun, Feihan Li, Rui Chen, Ruixuan Liu, Tianhao Wei, Changliu Liu",2023-05-23T04:40:29Z,Reinforcement Learning,"  Due to the trial-and-error nature, it is typically challenging to apply RL
algorithms to safety-critical real-world applications, such as autonomous
driving, human-robot interaction, robot manipulation, etc, where such errors
are not tolerable. Recently, safe RL (i.e. constrained RL) has emerged rapidly
in the literature, in which the agents explore the environment while satisfying
constraints. Due to the diversity of algorithms and tasks, it remains difficult
to compare existing safe RL algorithms. To fill that gap, we introduce GUARD, a
Generalized Unified SAfe Reinforcement Learning Development Benchmark. GUARD
has several advantages compared to existing benchmarks. First, GUARD is a
generalized benchmark with a wide variety of RL agents, tasks, and safety
constraint specifications. Second, GUARD comprehensively covers
state-of-the-art safe RL algorithms with self-contained implementations. Third,
GUARD is highly customizable in tasks and algorithms. We present a comparison
of state-of-the-art safe RL algorithms in various task settings using GUARD and
establish baselines that future work can build on.
",0
Constrained Reinforcement Learning for Dynamic Material Handling,"Chengpeng Hu, Ziming Wang, Jialin Liu, Junyi Wen, Bifei Mao, Xin Yao",2023-05-23T08:48:54Z,Reinforcement Learning,"  As one of the core parts of flexible manufacturing systems, material handling
involves storage and transportation of materials between workstations with
automated vehicles. The improvement in material handling can impulse the
overall efficiency of the manufacturing system. However, the occurrence of
dynamic events during the optimisation of task arrangements poses a challenge
that requires adaptability and effectiveness. In this paper, we aim at the
scheduling of automated guided vehicles for dynamic material handling.
Motivated by some real-world scenarios, unknown new tasks and unexpected
vehicle breakdowns are regarded as dynamic events in our problem. We formulate
the problem as a constrained Markov decision process which takes into account
tardiness and available vehicles as cumulative and instantaneous constraints,
respectively. An adaptive constrained reinforcement learning algorithm that
combines Lagrangian relaxation and invalid action masking, named RCPOM, is
proposed to address the problem with two hybrid constraints. Moreover, a
gym-like dynamic material handling simulator, named DMH-GYM, is developed and
equipped with diverse problem instances, which can be used as benchmarks for
dynamic material handling. Experimental results on the problem instances
demonstrate the outstanding performance of our proposed approach compared with
eight state-of-the-art constrained and non-constrained reinforcement learning
algorithms, and widely used dispatching rules for material handling.
",0
Provable Offline Preference-Based Reinforcement Learning,"Wenhao Zhan, Masatoshi Uehara, Nathan Kallus, Jason D. Lee, Wen Sun",2023-05-24T07:11:26Z,Reinforcement Learning,"  In this paper, we investigate the problem of offline Preference-based
Reinforcement Learning (PbRL) with human feedback where feedback is available
in the form of preference between trajectory pairs rather than explicit
rewards. Our proposed algorithm consists of two main steps: (1) estimate the
implicit reward using Maximum Likelihood Estimation (MLE) with general function
approximation from offline data and (2) solve a distributionally robust
planning problem over a confidence set around the MLE. We consider the general
reward setting where the reward can be defined over the whole trajectory and
provide a novel guarantee that allows us to learn any target policy with a
polynomial number of samples, as long as the target policy is covered by the
offline data. This guarantee is the first of its kind with general function
approximation. To measure the coverage of the target policy, we introduce a new
single-policy concentrability coefficient, which can be upper bounded by the
per-trajectory concentrability coefficient. We also establish lower bounds that
highlight the necessity of such concentrability and the difference from
standard RL, where state-action-wise rewards are directly observed. We further
extend and analyze our algorithm when the feedback is given over action pairs.
",0
Deep Reinforcement Learning with Plasticity Injection,"Evgenii Nikishin, Junhyuk Oh, Georg Ostrovski, Clare Lyle, Razvan Pascanu, Will Dabney, André Barreto",2023-05-24T20:41:35Z,Reinforcement Learning,"  A growing body of evidence suggests that neural networks employed in deep
reinforcement learning (RL) gradually lose their plasticity, the ability to
learn from new data; however, the analysis and mitigation of this phenomenon is
hampered by the complex relationship between plasticity, exploration, and
performance in RL. This paper introduces plasticity injection, a minimalistic
intervention that increases the network plasticity without changing the number
of trainable parameters or biasing the predictions. The applications of this
intervention are two-fold: first, as a diagnostic tool $\unicode{x2014}$ if
injection increases the performance, we may conclude that an agent's network
was losing its plasticity. This tool allows us to identify a subset of Atari
environments where the lack of plasticity causes performance plateaus,
motivating future studies on understanding and combating plasticity loss.
Second, plasticity injection can be used to improve the computational
efficiency of RL training if the agent has to re-learn from scratch due to
exhausted plasticity or by growing the agent's network dynamically without
compromising performance. The results on Atari show that plasticity injection
attains stronger performance compared to alternative methods while being
computationally efficient.
",0
"Reward-Machine-Guided, Self-Paced Reinforcement Learning","Cevahir Koprulu, Ufuk Topcu",2023-05-25T22:13:37Z,Reinforcement Learning,"  Self-paced reinforcement learning (RL) aims to improve the data efficiency of
learning by automatically creating sequences, namely curricula, of probability
distributions over contexts. However, existing techniques for self-paced RL
fail in long-horizon planning tasks that involve temporally extended behaviors.
We hypothesize that taking advantage of prior knowledge about the underlying
task structure can improve the effectiveness of self-paced RL. We develop a
self-paced RL algorithm guided by reward machines, i.e., a type of finite-state
machine that encodes the underlying task structure. The algorithm integrates
reward machines in 1) the update of the policy and value functions obtained by
any RL algorithm of choice, and 2) the update of the automated curriculum that
generates context distributions. Our empirical results evidence that the
proposed algorithm achieves optimal behavior reliably even in cases in which
existing baselines cannot make any meaningful progress. It also decreases the
curriculum length and reduces the variance in the curriculum generation process
by up to one-fourth and four orders of magnitude, respectively.
",0
Physics-Regulated Deep Reinforcement Learning: Invariant Embeddings,"Hongpeng Cao, Yanbing Mao, Lui Sha, Marco Caccamo",2023-05-26T04:20:02Z,Reinforcement Learning,"  This paper proposes the Phy-DRL: a physics-regulated deep reinforcement
learning (DRL) framework for safety-critical autonomous systems. The Phy-DRL
has three distinguished invariant-embedding designs: i) residual action policy
(i.e., integrating data-driven-DRL action policy and physics-model-based action
policy), ii) automatically constructed safety-embedded reward, and iii)
physics-model-guided neural network (NN) editing, including link editing and
activation editing. Theoretically, the Phy-DRL exhibits 1) a mathematically
provable safety guarantee and 2) strict compliance of critic and actor networks
with physics knowledge about the action-value function and action policy.
Finally, we evaluate the Phy-DRL on a cart-pole system and a quadruped robot.
The experiments validate our theoretical results and demonstrate that Phy-DRL
features guaranteed safety compared to purely data-driven DRL and solely
model-based design while offering remarkably fewer learning parameters and fast
training towards safety guarantee.
",4
Reinforcement Learning with Simple Sequence Priors,"Tankred Saanum, Noémi Éltető, Peter Dayan, Marcel Binz, Eric Schulz",2023-05-26T17:18:14Z,Reinforcement Learning,"  Everything else being equal, simpler models should be preferred over more
complex ones. In reinforcement learning (RL), simplicity is typically
quantified on an action-by-action basis -- but this timescale ignores temporal
regularities, like repetitions, often present in sequential strategies. We
therefore propose an RL algorithm that learns to solve tasks with sequences of
actions that are compressible. We explore two possible sources of simple action
sequences: Sequences that can be learned by autoregressive models, and
sequences that are compressible with off-the-shelf data compression algorithms.
Distilling these preferences into sequence priors, we derive a novel
information-theoretic objective that incentivizes agents to learn policies that
maximize rewards while conforming to these priors. We show that the resulting
RL algorithm leads to faster learning, and attains higher returns than
state-of-the-art model-free approaches in a series of continuous control tasks
from the DeepMind Control Suite. These priors also produce a powerful
information-regularized agent that is robust to noisy observations and can
perform open-loop control.
",0
Probing reaction channels via reinforcement learning,"Senwei Liang, Aditya N. Singh, Yuanran Zhu, David T. Limmer, Chao Yang",2023-05-27T17:22:32Z,Reinforcement Learning,"  We propose a reinforcement learning based method to identify important
configurations that connect reactant and product states along chemical reaction
paths. By shooting multiple trajectories from these configurations, we can
generate an ensemble of configurations that concentrate on the transition path
ensemble. This configuration ensemble can be effectively employed in a neural
network-based partial differential equation solver to obtain an approximation
solution of a restricted Backward Kolmogorov equation, even when the dimension
of the problem is very high. The resulting solution, known as the committor
function, encodes mechanistic information for the reaction and can in turn be
used to evaluate reaction rates.
",0
Online Nonstochastic Model-Free Reinforcement Learning,"Udaya Ghai, Arushi Gupta, Wenhan Xia, Karan Singh, Elad Hazan",2023-05-27T19:02:55Z,Reinforcement Learning,"  We investigate robust model-free reinforcement learning algorithms designed
for environments that may be dynamic or even adversarial. Traditional
state-based policies often struggle to accommodate the challenges imposed by
the presence of unmodeled disturbances in such settings. Moreover, optimizing
linear state-based policies pose an obstacle for efficient optimization,
leading to nonconvex objectives, even in benign environments like linear
dynamical systems.
  Drawing inspiration from recent advancements in model-based control, we
introduce a novel class of policies centered on disturbance signals. We define
several categories of these signals, which we term pseudo-disturbances, and
develop corresponding policy classes based on them. We provide efficient and
practical algorithms for optimizing these policies.
  Next, we examine the task of online adaptation of reinforcement learning
agents in the face of adversarial disturbances. Our methods seamlessly
integrate with any black-box model-free approach, yielding provable regret
guarantees when dealing with linear dynamics. These regret guarantees
unconditionally improve the best-known results for bandit linear control in
having no dependence on the state-space dimension. We evaluate our method over
various standard RL benchmarks and demonstrate improved robustness.
",0
Provable Reward-Agnostic Preference-Based Reinforcement Learning,"Wenhao Zhan, Masatoshi Uehara, Wen Sun, Jason D. Lee",2023-05-29T15:00:09Z,Reinforcement Learning,"  Preference-based Reinforcement Learning (PbRL) is a paradigm in which an RL
agent learns to optimize a task using pair-wise preference-based feedback over
trajectories, rather than explicit reward signals. While PbRL has demonstrated
practical success in fine-tuning language models, existing theoretical work
focuses on regret minimization and fails to capture most of the practical
frameworks. In this study, we fill in such a gap between theoretical PbRL and
practical algorithms by proposing a theoretical reward-agnostic PbRL framework
where exploratory trajectories that enable accurate learning of hidden reward
functions are acquired before collecting any human feedback. Theoretical
analysis demonstrates that our algorithm requires less human feedback for
learning the optimal policy under preference-based models with linear
parameterization and unknown transitions, compared to the existing theoretical
literature. Specifically, our framework can incorporate linear and low-rank
MDPs with efficient sample complexity. Additionally, we investigate
reward-agnostic RL with action-based comparison feedback and introduce an
efficient querying algorithm tailored to this scenario.
",0
Policy Optimization for Continuous Reinforcement Learning,"Hanyang Zhao, Wenpin Tang, David D. Yao",2023-05-30T09:59:04Z,Reinforcement Learning,"  We study reinforcement learning (RL) in the setting of continuous time and
space, for an infinite horizon with a discounted objective and the underlying
dynamics driven by a stochastic differential equation. Built upon recent
advances in the continuous approach to RL, we develop a notion of occupation
time (specifically for a discounted objective), and show how it can be
effectively used to derive performance-difference and local-approximation
formulas. We further extend these results to illustrate their applications in
the PG (policy gradient) and TRPO/PPO (trust region policy optimization/
proximal policy optimization) methods, which have been familiar and powerful
tools in the discrete RL setting but under-developed in continuous RL. Through
numerical experiments, we demonstrate the effectiveness and advantages of our
approach.
",0
Subequivariant Graph Reinforcement Learning in 3D Environments,"Runfa Chen, Jiaqi Han, Fuchun Sun, Wenbing Huang",2023-05-30T11:34:57Z,Reinforcement Learning,"  Learning a shared policy that guides the locomotion of different agents is of
core interest in Reinforcement Learning (RL), which leads to the study of
morphology-agnostic RL. However, existing benchmarks are highly restrictive in
the choice of starting point and target point, constraining the movement of the
agents within 2D space. In this work, we propose a novel setup for
morphology-agnostic RL, dubbed Subequivariant Graph RL in 3D environments
(3D-SGRL). Specifically, we first introduce a new set of more practical yet
challenging benchmarks in 3D space that allows the agent to have full
Degree-of-Freedoms to explore in arbitrary directions starting from arbitrary
configurations. Moreover, to optimize the policy over the enlarged state-action
space, we propose to inject geometric symmetry, i.e., subequivariance, into the
modeling of the policy and Q-function such that the policy can generalize to
all directions, improving exploration efficiency. This goal is achieved by a
novel SubEquivariant Transformer (SET) that permits expressive message
exchange. Finally, we evaluate the proposed method on the proposed benchmarks,
where our method consistently and significantly outperforms existing approaches
on single-task, multi-task, and zero-shot generalization scenarios. Extensive
ablations are also conducted to verify our design. Code and videos are
available on our project page: https://alpc91.github.io/SGRL/.
",0
Efficient Diffusion Policies for Offline Reinforcement Learning,"Bingyi Kang, Xiao Ma, Chao Du, Tianyu Pang, Shuicheng Yan",2023-05-31T17:55:21Z,Reinforcement Learning,"  Offline reinforcement learning (RL) aims to learn optimal policies from
offline datasets, where the parameterization of policies is crucial but often
overlooked. Recently, Diffsuion-QL significantly boosts the performance of
offline RL by representing a policy with a diffusion model, whose success
relies on a parametrized Markov Chain with hundreds of steps for sampling.
However, Diffusion-QL suffers from two critical limitations. 1) It is
computationally inefficient to forward and backward through the whole Markov
chain during training. 2) It is incompatible with maximum likelihood-based RL
algorithms (e.g., policy gradient methods) as the likelihood of diffusion
models is intractable. Therefore, we propose efficient diffusion policy (EDP)
to overcome these two challenges. EDP approximately constructs actions from
corrupted ones at training to avoid running the sampling chain. We conduct
extensive experiments on the D4RL benchmark. The results show that EDP can
reduce the diffusion policy training time from 5 days to 5 hours on
gym-locomotion tasks. Moreover, we show that EDP is compatible with various
offline RL algorithms (TD3, CRR, and IQL) and achieves new state-of-the-art on
D4RL by large margins over previous methods. Our code is available at
https://github.com/sail-sg/edp.
",0
ROSARL: Reward-Only Safe Reinforcement Learning,"Geraud Nangue Tasse, Tamlin Love, Mark Nemecek, Steven James, Benjamin Rosman",2023-05-31T08:33:23Z,Reinforcement Learning,"  An important problem in reinforcement learning is designing agents that learn
to solve tasks safely in an environment. A common solution is for a human
expert to define either a penalty in the reward function or a cost to be
minimised when reaching unsafe states. However, this is non-trivial, since too
small a penalty may lead to agents that reach unsafe states, while too large a
penalty increases the time to convergence. Additionally, the difficulty in
designing reward or cost functions can increase with the complexity of the
problem. Hence, for a given environment with a given set of unsafe states, we
are interested in finding the upper bound of rewards at unsafe states whose
optimal policies minimise the probability of reaching those unsafe states,
irrespective of task rewards. We refer to this exact upper bound as the ""Minmax
penalty"", and show that it can be obtained by taking into account both the
controllability and diameter of an environment. We provide a simple practical
model-free algorithm for an agent to learn this Minmax penalty while learning
the task policy, and demonstrate that using it leads to agents that learn safe
policies in high-dimensional continuous control environments.
",0
Normalization Enhances Generalization in Visual Reinforcement Learning,"Lu Li, Jiafei Lyu, Guozheng Ma, Zilin Wang, Zhenjie Yang, Xiu Li, Zhiheng Li",2023-06-01T13:24:56Z,Reinforcement Learning,"  Recent advances in visual reinforcement learning (RL) have led to impressive
success in handling complex tasks. However, these methods have demonstrated
limited generalization capability to visual disturbances, which poses a
significant challenge for their real-world application and adaptability. Though
normalization techniques have demonstrated huge success in supervised and
unsupervised learning, their applications in visual RL are still scarce. In
this paper, we explore the potential benefits of integrating normalization into
visual RL methods with respect to generalization performance. We find that,
perhaps surprisingly, incorporating suitable normalization techniques is
sufficient to enhance the generalization capabilities, without any additional
special design. We utilize the combination of two normalization techniques,
CrossNorm and SelfNorm, for generalizable visual RL. Extensive experiments are
conducted on DMControl Generalization Benchmark and CARLA to validate the
effectiveness of our method. We show that our method significantly improves
generalization capability while only marginally affecting sample efficiency. In
particular, when integrated with DrQ-v2, our method enhances the test
performance of DrQ-v2 on CARLA across various scenarios, from 14% of the
training performance to 97%.
",0
Improving and Benchmarking Offline Reinforcement Learning Algorithms,"Bingyi Kang, Xiao Ma, Yirui Wang, Yang Yue, Shuicheng Yan",2023-06-01T17:58:46Z,Reinforcement Learning,"  Recently, Offline Reinforcement Learning (RL) has achieved remarkable
progress with the emergence of various algorithms and datasets. However, these
methods usually focus on algorithmic advancements, ignoring that many low-level
implementation choices considerably influence or even drive the final
performance. As a result, it becomes hard to attribute the progress in Offline
RL as these choices are not sufficiently discussed and aligned in the
literature. In addition, papers focusing on a dataset (e.g., D4RL) often ignore
algorithms proposed on another dataset (e.g., RL Unplugged), causing isolation
among the algorithms, which might slow down the overall progress. Therefore,
this work aims to bridge the gaps caused by low-level choices and datasets. To
this end, we empirically investigate 20 implementation choices using three
representative algorithms (i.e., CQL, CRR, and IQL) and present a guidebook for
choosing implementations. Following the guidebook, we find two variants CRR+
and CQL+ , achieving new state-of-the-art on D4RL. Moreover, we benchmark eight
popular offline RL algorithms across datasets under unified training and
evaluation framework. The findings are inspiring: the success of a learning
paradigm severely depends on the data distribution, and some previous
conclusions are biased by the dataset used. Our code is available at
https://github.com/sail-sg/offbench.
",0
Heterogeneous Knowledge for Augmented Modular Reinforcement Learning,"Lorenz Wolf, Mirco Musolesi",2023-06-01T21:31:59Z,Reinforcement Learning,"  Existing modular Reinforcement Learning (RL) architectures are generally
based on reusable components, also allowing for ``plug-and-play'' integration.
However, these modules are homogeneous in nature - in fact, they essentially
provide policies obtained via RL through the maximization of individual reward
functions. Consequently, such solutions still lack the ability to integrate and
process multiple types of information (i.e., heterogeneous knowledge
representations), such as rules, sub-goals, and skills from various sources. In
this paper, we discuss several practical examples of heterogeneous knowledge
and propose Augmented Modular Reinforcement Learning (AMRL) to address these
limitations. Our framework uses a selector to combine heterogeneous modules and
seamlessly incorporate different types of knowledge representations and
processing mechanisms. Our results demonstrate the performance and efficiency
improvements, also in terms of generalization, that can be achieved by
augmenting traditional modular RL with heterogeneous knowledge sources and
processing mechanisms.
",0
Survival Instinct in Offline Reinforcement Learning,"Anqi Li, Dipendra Misra, Andrey Kolobov, Ching-An Cheng",2023-06-05T22:15:39Z,Reinforcement Learning,"  We present a novel observation about the behavior of offline reinforcement
learning (RL) algorithms: on many benchmark datasets, offline RL can produce
well-performing and safe policies even when trained with ""wrong"" reward labels,
such as those that are zero everywhere or are negatives of the true rewards.
This phenomenon cannot be easily explained by offline RL's return maximization
objective. Moreover, it gives offline RL a degree of robustness that is
uncharacteristic of its online RL counterparts, which are known to be sensitive
to reward design. We demonstrate that this surprising robustness property is
attributable to an interplay between the notion of pessimism in offline RL
algorithms and certain implicit biases in common data collection practices. As
we prove in this work, pessimism endows the agent with a ""survival instinct"",
i.e., an incentive to stay within the data support in the long term, while the
limited and biased data coverage further constrains the set of survival
policies. Formally, given a reward class -- which may not even contain the true
reward -- we identify conditions on the training data distribution that enable
offline RL to learn a near-optimal and safe policy from any reward within the
class. We argue that the survival instinct should be taken into account when
interpreting results from existing offline RL benchmarks and when creating
future ones. Our empirical and theoretical results suggest a new paradigm for
RL, whereby an agent is nudged to learn a desirable behavior with imperfect
reward but purposely biased data coverage.
",0
Generalization Across Observation Shifts in Reinforcement Learning,"Anuj Mahajan, Amy Zhang",2023-06-07T16:49:03Z,Reinforcement Learning,"  Learning policies which are robust to changes in the environment are critical
for real world deployment of Reinforcement Learning agents. They are also
necessary for achieving good generalization across environment shifts. We focus
on bisimulation metrics, which provide a powerful means for abstracting task
relevant components of the observation and learning a succinct representation
space for training the agent using reinforcement learning. In this work, we
extend the bisimulation framework to also account for context dependent
observation shifts. Specifically, we focus on the simulator based learning
setting and use alternate observations to learn a representation space which is
invariant to observation shifts using a novel bisimulation based objective.
This allows us to deploy the agent to varying observation settings during test
time and generalize to unseen scenarios. We further provide novel theoretical
bounds for simulator fidelity and performance transfer guarantees for using a
learnt policy to unseen shifts. Empirical analysis on the high-dimensional
image based control domains demonstrates the efficacy of our method.
",0
Explaining Reinforcement Learning with Shapley Values,"Daniel Beechey, Thomas M. S. Smith, Özgür Şimşek",2023-06-09T10:52:39Z,Reinforcement Learning,"  For reinforcement learning systems to be widely adopted, their users must
understand and trust them. We present a theoretical analysis of explaining
reinforcement learning using Shapley values, following a principled approach
from game theory for identifying the contribution of individual players to the
outcome of a cooperative game. We call this general framework Shapley Values
for Explaining Reinforcement Learning (SVERL). Our analysis exposes the
limitations of earlier uses of Shapley values in reinforcement learning. We
then develop an approach that uses Shapley values to explain agent performance.
In a variety of domains, SVERL produces meaningful explanations that match and
supplement human intuition.
",0
Diverse Projection Ensembles for Distributional Reinforcement Learning,"Moritz A. Zanger, Wendelin Böhmer, Matthijs T. J. Spaan",2023-06-12T13:59:48Z,Reinforcement Learning,"  In contrast to classical reinforcement learning, distributional reinforcement
learning algorithms aim to learn the distribution of returns rather than their
expected value. Since the nature of the return distribution is generally
unknown a priori or arbitrarily complex, a common approach finds approximations
within a set of representable, parametric distributions. Typically, this
involves a projection of the unconstrained distribution onto the set of
simplified distributions. We argue that this projection step entails a strong
inductive bias when coupled with neural networks and gradient descent, thereby
profoundly impacting the generalization behavior of learned models. In order to
facilitate reliable uncertainty estimation through diversity, this work studies
the combination of several different projections and representations in a
distributional ensemble. We establish theoretical properties of such projection
ensembles and derive an algorithm that uses ensemble disagreement, measured by
the average $1$-Wasserstein distance, as a bonus for deep exploration. We
evaluate our algorithm on the behavior suite benchmark and find that diverse
projection ensembles lead to significant performance improvements over existing
methods on a wide variety of tasks with the most pronounced gains in directed
exploration problems.
",0
Robust Reinforcement Learning through Efficient Adversarial Herding,"Juncheng Dong, Hao-Lun Hsu, Qitong Gao, Vahid Tarokh, Miroslav Pajic",2023-06-12T20:21:40Z,Reinforcement Learning,"  Although reinforcement learning (RL) is considered the gold standard for
policy design, it may not always provide a robust solution in various
scenarios. This can result in severe performance degradation when the
environment is exposed to potential disturbances. Adversarial training using a
two-player max-min game has been proven effective in enhancing the robustness
of RL agents. In this work, we extend the two-player game by introducing an
adversarial herd, which involves a group of adversaries, in order to address
($\textit{i}$) the difficulty of the inner optimization problem, and
($\textit{ii}$) the potential over pessimism caused by the selection of a
candidate adversary set that may include unlikely scenarios. We first prove
that adversarial herds can efficiently approximate the inner optimization
problem. Then we address the second issue by replacing the worst-case
performance in the inner optimization with the average performance over the
worst-$k$ adversaries. We evaluate the proposed method on multiple MuJoCo
environments. Experimental results demonstrate that our approach consistently
generates more robust policies.
",0
Curricular Subgoals for Inverse Reinforcement Learning,"Shunyu Liu, Yunpeng Qing, Shuqi Xu, Hongyan Wu, Jiangtao Zhang, Jingyuan Cong, Tianhao Chen, Yunfu Liu, Mingli Song",2023-06-14T04:06:41Z,Reinforcement Learning,"  Inverse Reinforcement Learning (IRL) aims to reconstruct the reward function
from expert demonstrations to facilitate policy learning, and has demonstrated
its remarkable success in imitation learning. To promote expert-like behavior,
existing IRL methods mainly focus on learning global reward functions to
minimize the trajectory difference between the imitator and the expert.
However, these global designs are still limited by the redundant noise and
error propagation problems, leading to the unsuitable reward assignment and
thus downgrading the agent capability in complex multi-stage tasks. In this
paper, we propose a novel Curricular Subgoal-based Inverse Reinforcement
Learning (CSIRL) framework, that explicitly disentangles one task with several
local subgoals to guide agent imitation. Specifically, CSIRL firstly introduces
decision uncertainty of the trained agent over expert trajectories to
dynamically select subgoals, which directly determines the exploration boundary
of different task stages. To further acquire local reward functions for each
stage, we customize a meta-imitation objective based on these curricular
subgoals to train an intrinsic reward generator. Experiments on the D4RL and
autonomous driving benchmarks demonstrate that the proposed methods yields
results superior to the state-of-the-art counterparts, as well as better
interpretability. Our code is available at https://github.com/Plankson/CSIRL.
",0
Fairness in Preference-based Reinforcement Learning,"Umer Siddique, Abhinav Sinha, Yongcan Cao",2023-06-16T17:47:36Z,Reinforcement Learning,"  In this paper, we address the issue of fairness in preference-based
reinforcement learning (PbRL) in the presence of multiple objectives. The main
objective is to design control policies that can optimize multiple objectives
while treating each objective fairly. Toward this objective, we design a new
fairness-induced preference-based reinforcement learning or FPbRL. The main
idea of FPbRL is to learn vector reward functions associated with multiple
objectives via new welfare-based preferences rather than reward-based
preference in PbRL, coupled with policy learning via maximizing a generalized
Gini welfare function. Finally, we provide experiment studies on three
different environments to show that the proposed FPbRL approach can achieve
both efficiency and equity for learning effective and fair policies.
",0
Vanishing Bias Heuristic-guided Reinforcement Learning Algorithm,"Qinru Li, Hao Xiang",2023-06-17T00:25:58Z,Reinforcement Learning,"  Reinforcement Learning has achieved tremendous success in the many Atari
games. In this paper we explored with the lunar lander environment and
implemented classical methods including Q-Learning, SARSA, MC as well as tiling
coding. We also implemented Neural Network based methods including DQN, Double
DQN, Clipped DQN. On top of these, we proposed a new algorithm called Heuristic
RL which utilizes heuristic to guide the early stage training while alleviating
the introduced human bias. Our experiments showed promising results for our
proposed methods in the lunar lander environment.
",0
On the Model-Misspecification in Reinforcement Learning,"Yunfan Li, Lin Yang",2023-06-19T04:31:59Z,Reinforcement Learning,"  The success of reinforcement learning (RL) crucially depends on effective
function approximation when dealing with complex ground-truth models. Existing
sample-efficient RL algorithms primarily employ three approaches to function
approximation: policy-based, value-based, and model-based methods. However, in
the face of model misspecification (a disparity between the ground-truth and
optimal function approximators), it is shown that policy-based approaches can
be robust even when the policy function approximation is under a large
locally-bounded misspecification error, with which the function class may
exhibit a $\Omega(1)$ approximation error in specific states and actions, but
remains small on average within a policy-induced state distribution. Yet it
remains an open question whether similar robustness can be achieved with
value-based and model-based approaches, especially with general function
approximation.
  To bridge this gap, in this paper we present a unified theoretical framework
for addressing model misspecification in RL. We demonstrate that, through
meticulous algorithm design and sophisticated analysis, value-based and
model-based methods employing general function approximation can achieve
robustness under local misspecification error bounds. In particular, they can
attain a regret bound of $\widetilde{O}\left(\text{poly}(d H)(\sqrt{K} +
K\zeta) \right)$, where $d$ represents the complexity of the function class,
$H$ is the episode length, $K$ is the total number of episodes, and $\zeta$
denotes the local bound for misspecification error. Furthermore, we propose an
algorithmic framework that can achieve the same order of regret bound without
prior knowledge of $\zeta$, thereby enhancing its practical applicability.
",0
Maximum Entropy Heterogeneous-Agent Reinforcement Learning,"Jiarong Liu, Yifan Zhong, Siyi Hu, Haobo Fu, Qiang Fu, Xiaojun Chang, Yaodong Yang",2023-06-19T06:22:02Z,Reinforcement Learning,"  Multi-agent reinforcement learning (MARL) has been shown effective for
cooperative games in recent years. However, existing state-of-the-art methods
face challenges related to sample complexity, training instability, and the
risk of converging to a suboptimal Nash Equilibrium. In this paper, we propose
a unified framework for learning \emph{stochastic} policies to resolve these
issues. We embed cooperative MARL problems into probabilistic graphical models,
from which we derive the maximum entropy (MaxEnt) objective for MARL. Based on
the MaxEnt framework, we propose Heterogeneous-Agent Soft Actor-Critic (HASAC)
algorithm. Theoretically, we prove the monotonic improvement and convergence to
quantal response equilibrium (QRE) properties of HASAC. Furthermore, we
generalize a unified template for MaxEnt algorithmic design named Maximum
Entropy Heterogeneous-Agent Mirror Learning (MEHAML), which provides any
induced method with the same guarantees as HASAC. We evaluate HASAC on six
benchmarks: Bi-DexHands, Multi-Agent MuJoCo, StarCraft Multi-Agent Challenge,
Google Research Football, Multi-Agent Particle Environment, and Light Aircraft
Game. Results show that HASAC consistently outperforms strong baselines,
exhibiting better sample efficiency, robustness, and sufficient exploration.
",0
Option Dynamic Hedging Using Reinforcement Learning,"Cong Zheng, Jiafa He, Can Yang",2023-06-19T07:20:34Z,Reinforcement Learning,"  This work focuses on the dynamic hedging of financial derivatives, where a
reinforcement learning algorithm is designed to minimize the variance of the
delta hedging process. In contrast to previous research in this area, we apply
uncertainty estimation technology to measure the uncertainty of the agent's
decision, which can further reduce unnecessary wear and tear in the hedging
process and control model overconfidence that may lead to significant losses.
Numerical experiments show the superiority of our strategy in Monte Carlo
simulations and SP 500 option data.
",0
PTDRL: Parameter Tuning using Deep Reinforcement Learning,"Elias Goldsztejn, Tal Feiner, Ronen Brafman",2023-06-19T10:36:53Z,Reinforcement Learning,"  A variety of autonomous navigation algorithms exist that allow robots to move
around in a safe and fast manner. However, many of these algorithms require
parameter re-tuning when facing new environments. In this paper, we propose
PTDRL, a parameter-tuning strategy that adaptively selects from a fixed set of
parameters those that maximize the expected reward for a given navigation
system. Our learning strategy can be used for different environments, different
platforms, and different user preferences. Specifically, we attend to the
problem of social navigation in indoor spaces, using a classical motion
planning algorithm as our navigation system and training its parameters to
optimize its behavior. Experimental results show that PTDRL can outperform
other online parameter-tuning strategies.
",0
Int-HRL: Towards Intention-based Hierarchical Reinforcement Learning,"Anna Penzkofer, Simon Schaefer, Florian Strohm, Mihai Bâce, Stefan Leutenegger, Andreas Bulling",2023-06-20T12:12:16Z,Reinforcement Learning,"  While deep reinforcement learning (RL) agents outperform humans on an
increasing number of tasks, training them requires data equivalent to decades
of human gameplay. Recent hierarchical RL methods have increased sample
efficiency by incorporating information inherent to the structure of the
decision problem but at the cost of having to discover or use human-annotated
sub-goals that guide the learning process. We show that intentions of human
players, i.e. the precursor of goal-oriented decisions, can be robustly
predicted from eye gaze even for the long-horizon sparse rewards task of
Montezuma's Revenge - one of the most challenging RL tasks in the Atari2600
game suite. We propose Int-HRL: Hierarchical RL with intention-based sub-goals
that are inferred from human eye gaze. Our novel sub-goal extraction pipeline
is fully automatic and replaces the need for manual sub-goal annotation by
human experts. Our evaluations show that replacing hand-crafted sub-goals with
automatically extracted intentions leads to a HRL agent that is significantly
more sample efficient than previous methods.
",0
Active Coverage for PAC Reinforcement Learning,"Aymen Al-Marjani, Andrea Tirinzoni, Emilie Kaufmann",2023-06-23T16:39:37Z,"RAG, Reinforcement Learning","  Collecting and leveraging data with good coverage properties plays a crucial
role in different aspects of reinforcement learning (RL), including reward-free
exploration and offline learning. However, the notion of ""good coverage"" really
depends on the application at hand, as data suitable for one context may not be
so for another. In this paper, we formalize the problem of active coverage in
episodic Markov decision processes (MDPs), where the goal is to interact with
the environment so as to fulfill given sampling requirements. This framework is
sufficiently flexible to specify any desired coverage property, making it
applicable to any problem that involves online exploration. Our main
contribution is an instance-dependent lower bound on the sample complexity of
active coverage and a simple game-theoretic algorithm, CovGame, that nearly
matches it. We then show that CovGame can be used as a building block to solve
different PAC RL tasks. In particular, we obtain a simple algorithm for PAC
reward-free exploration with an instance-dependent sample complexity that, in
certain MDPs which are ""easy to explore"", is lower than the minimax one. By
further coupling this exploration algorithm with a new technique to do implicit
eliminations in policy space, we obtain a computationally-efficient algorithm
for best-policy identification whose instance-dependent sample complexity
scales with gaps between policy values.
",0
Automatic Truss Design with Reinforcement Learning,"Weihua Du, Jinglun Zhao, Chao Yu, Xingcheng Yao, Zimeng Song, Siyang Wu, Ruifeng Luo, Zhiyuan Liu, Xianzhong Zhao, Yi Wu",2023-06-27T03:42:31Z,Reinforcement Learning,"  Truss layout design, namely finding a lightweight truss layout satisfying all
the physical constraints, is a fundamental problem in the building industry.
Generating the optimal layout is a challenging combinatorial optimization
problem, which can be extremely expensive to solve by exhaustive search.
Directly applying end-to-end reinforcement learning (RL) methods to truss
layout design is infeasible either, since only a tiny portion of the entire
layout space is valid under the physical constraints, leading to particularly
sparse rewards for RL training. In this paper, we develop AutoTruss, a
two-stage framework to efficiently generate both lightweight and valid truss
layouts. AutoTruss first adopts Monte Carlo tree search to discover a diverse
collection of valid layouts. Then RL is applied to iteratively refine the valid
solutions. We conduct experiments and ablation studies in popular truss layout
design test cases in both 2D and 3D settings. AutoTruss outperforms the
best-reported layouts by 25.1% in the most challenging 3D test cases, resulting
in the first effective deep-RL-based approach in the truss layout design
literature.
",0
Reinforcement Learning in Ultracold Atom Experiments,"Malte Reinschmidt, József Fortágh, Andreas Günther, Valentin Volchkov",2023-06-29T08:07:41Z,Reinforcement Learning,"  Cold atom traps are at the heart of many quantum applications in science and
technology. The preparation and control of atomic clouds involves complex
optimization processes, that could be supported and accelerated by machine
learning. In this work, we introduce reinforcement learning to cold atom
experiments and demonstrate a flexible and adaptive approach to control a
magneto-optical trap. Instead of following a set of predetermined rules to
accomplish a specific task, the objectives are defined by a reward function.
This approach not only optimizes the cooling of atoms just as an
experimentalist would do, but also enables new operational modes such as the
preparation of pre-defined numbers of atoms in a cloud. The machine control is
trained to be robust against external perturbations and able to react to
situations not seen during the training. Finally, we show that the time
consuming training can be performed in-silico using a generic simulation and
demonstrate successful transfer to the real world experiment.
",0
Probabilistic Constraint for Safety-Critical Reinforcement Learning,"Weiqin Chen, Dharmashankar Subramanian, Santiago Paternain",2023-06-29T19:41:56Z,Reinforcement Learning,"  In this paper, we consider the problem of learning safe policies for
probabilistic-constrained reinforcement learning (RL). Specifically, a safe
policy or controller is one that, with high probability, maintains the
trajectory of the agent in a given safe set. We establish a connection between
this probabilistic-constrained setting and the cumulative-constrained
formulation that is frequently explored in the existing literature. We provide
theoretical bounds elucidating that the probabilistic-constrained setting
offers a better trade-off in terms of optimality and safety (constraint
satisfaction). The challenge encountered when dealing with the probabilistic
constraints, as explored in this work, arises from the absence of explicit
expressions for their gradients. Our prior work provides such an explicit
gradient expression for probabilistic constraints which we term Safe Policy
Gradient-REINFORCE (SPG-REINFORCE). In this work, we provide an improved
gradient SPG-Actor-Critic that leads to a lower variance than SPG-REINFORCE,
which is substantiated by our theoretical results. A noteworthy aspect of both
SPGs is their inherent algorithm independence, rendering them versatile for
application across a range of policy-based algorithms. Furthermore, we propose
a Safe Primal-Dual algorithm that can leverage both SPGs to learn safe
policies. It is subsequently followed by theoretical analyses that encompass
the convergence of the algorithm, as well as the near-optimality and
feasibility on average. In addition, we test the proposed approaches by a
series of empirical experiments. These experiments aim to examine and analyze
the inherent trade-offs between the optimality and safety, and serve to
substantiate the efficacy of two SPGs, as well as our theoretical
contributions.
",0
Design of Induction Machines using Reinforcement Learning,"Yasmin SarcheshmehPour, Tommi Ryyppo, Victor Mukherjee, Alex Jung",2023-06-30T12:56:31Z,Reinforcement Learning,"  The design of induction machine is a challenging task due to different
electromagnetic and thermal constraints. Quick estimation of machine's
dimensions is important in the sales tool to provide quick quotations to
customers based on specific requirements. The key part of this process is to
select different design parameters like length, diameter, tooth tip height and
winding turns to achieve certain torque, current and temperature of the
machine. Electrical machine designers, with their experience know how to alter
different machine design parameters to achieve a customer specific operation
requirements. We propose a reinforcement learning algorithm to design a
customised induction motor. The neural network model is trained off-line by
simulating different instances of of electrical machine design game with a
reward or penalty function when a good or bad design choice is made. The
results demonstrate that the suggested method automates electrical machine
design without applying any human engineering knowledge.
",0
Is Risk-Sensitive Reinforcement Learning Properly Resolved?,"Ruiwen Zhou, Minghuan Liu, Kan Ren, Xufang Luo, Weinan Zhang, Dongsheng Li",2023-07-02T11:47:21Z,Reinforcement Learning,"  Due to the nature of risk management in learning applicable policies,
risk-sensitive reinforcement learning (RSRL) has been realized as an important
direction. RSRL is usually achieved by learning risk-sensitive objectives
characterized by various risk measures, under the framework of distributional
reinforcement learning. However, it remains unclear if the distributional
Bellman operator properly optimizes the RSRL objective in the sense of risk
measures. In this paper, we prove that the existing RSRL methods do not achieve
unbiased optimization and can not guarantee optimality or even improvements
regarding risk measures over accumulated return distributions. To remedy this
issue, we further propose a novel algorithm, namely Trajectory Q-Learning
(TQL), for RSRL problems with provable convergence to the optimal policy. Based
on our new learning architecture, we are free to introduce a general and
practical implementation for different risk measures to learn disparate
risk-sensitive policies. In the experiments, we verify the learnability of our
algorithm and show how our method effectively achieves better performances
toward risk-sensitive objectives.
",2
Market Making of Options via Reinforcement Learning,"Zhou Fang, Haiqing Xu",2023-07-04T16:41:07Z,Reinforcement Learning,"  Market making of options with different maturities and strikes is a
challenging problem due to its high dimensional nature. In this paper, we
propose a novel approach that combines a stochastic policy and reinforcement
learning-inspired techniques to determine the optimal policy for posting
bid-ask spreads for an options market maker who trades options with different
maturities and strikes. When the arrival of market orders is linearly inverse
to the spreads, the optimal policy is normally distributed.
",0
LLQL: Logistic Likelihood Q-Learning for Reinforcement Learning,"Outongyi Lv, Bingxin Zhou",2023-07-05T15:00:29Z,Reinforcement Learning,"  Modern reinforcement learning (RL) can be categorized into online and offline
variants. As a pivotal aspect of both online and offline RL, current research
on the Bellman equation revolves primarily around optimization techniques and
performance enhancement rather than exploring the inherent structural
properties of the Bellman error, such as its distribution characteristics. This
study investigates the distribution of the Bellman approximation error through
iterative exploration of the Bellman equation with the observation that the
Bellman error approximately follows the Logistic distribution. Based on this,
we proposed the utilization of the Logistic maximum likelihood function (LLoss)
as an alternative to the commonly used mean squared error (MSELoss) that
assumes a Normal distribution for Bellman errors. We validated the hypotheses
through extensive numerical experiments across diverse online and offline
environments. In particular, we applied the Logistic correction to loss
functions in various RL baseline methods and observed that the results with
LLoss consistently outperformed the MSE counterparts. We also conducted the
Kolmogorov-Smirnov tests to confirm the reliability of the Logistic
distribution. Moreover, our theory connects the Bellman error to the
proportional reward scaling phenomenon by providing a distribution-based
analysis. Furthermore, we applied the bias-variance decomposition for sampling
from the Logistic distribution. The theoretical and empirical insights of this
study lay a valuable foundation for future investigations and enhancements
centered on the distribution of Bellman error.
",0
Offline Reinforcement Learning with Imbalanced Datasets,"Li Jiang, Sijie Cheng, Jielin Qiu, Haoran Xu, Wai Kin Chan, Zhao Ding",2023-07-06T03:22:19Z,Reinforcement Learning,"  The prevalent use of benchmarks in current offline reinforcement learning
(RL) research has led to a neglect of the imbalance of real-world dataset
distributions in the development of models. The real-world offline RL dataset
is often imbalanced over the state space due to the challenge of exploration or
safety considerations. In this paper, we specify properties of imbalanced
datasets in offline RL, where the state coverage follows a power law
distribution characterized by skewed policies. Theoretically and empirically,
we show that typically offline RL methods based on distributional constraints,
such as conservative Q-learning (CQL), are ineffective in extracting policies
under the imbalanced dataset. Inspired by natural intelligence, we propose a
novel offline RL method that utilizes the augmentation of CQL with a retrieval
process to recall past related experiences, effectively alleviating the
challenges posed by imbalanced datasets. We evaluate our method on several
tasks in the context of imbalanced datasets with varying levels of imbalance,
utilizing the variant of D4RL. Empirical results demonstrate the superiority of
our method over other baselines.
",0
Continual Learning as Computationally Constrained Reinforcement Learning,"Saurabh Kumar, Henrik Marklund, Ashish Rao, Yifan Zhu, Hong Jun Jeon, Yueyang Liu, Benjamin Van Roy",2023-07-10T05:06:41Z,Reinforcement Learning,"  An agent that efficiently accumulates knowledge to develop increasingly
sophisticated skills over a long lifetime could advance the frontier of
artificial intelligence capabilities. The design of such agents, which remains
a long-standing challenge of artificial intelligence, is addressed by the
subject of continual learning. This monograph clarifies and formalizes concepts
of continual learning, introducing a framework and set of tools to stimulate
further research.
",0
RLTF: Reinforcement Learning from Unit Test Feedback,"Jiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao Han, Wei Yang, Deheng Ye",2023-07-10T05:18:18Z,Reinforcement Learning,"  The goal of program synthesis, or code generation, is to generate executable
code based on given descriptions. Recently, there has been an increasing number
of studies employing reinforcement learning (RL) to improve the performance of
large language models (LLMs) for code. However, current representative works
either rely solely on offline frameworks, limiting the exploration of new
sample spaces, or fall short in the utilization of unit test signals, not
accounting for specific error locations within the code. To address these
issues, we propose RLTF, i.e., Reinforcement Learning from Unit Test Feedback,
a novel online RL framework with unit test feedback of multi-granularity for
refining code LLMs. Our approach generates data in real-time during training
and simultaneously utilizes fine-grained feedback signals to guide the model
towards producing higher-quality code. Extensive experiments show that RLTF
achieves state-of-the-art performance on the APPS and the MBPP benchmarks. Our
code is available at: https://github.com/Zyq-scut/RLTF.
",0
Loss Dynamics of Temporal Difference Reinforcement Learning,"Blake Bordelon, Paul Masset, Henry Kuo, Cengiz Pehlevan",2023-07-10T18:17:50Z,Reinforcement Learning,"  Reinforcement learning has been successful across several applications in
which agents have to learn to act in environments with sparse feedback.
However, despite this empirical success there is still a lack of theoretical
understanding of how the parameters of reinforcement learning models and the
features used to represent states interact to control the dynamics of learning.
In this work, we use concepts from statistical physics, to study the typical
case learning curves for temporal difference learning of a value function with
linear function approximators. Our theory is derived under a Gaussian
equivalence hypothesis where averages over the random trajectories are replaced
with temporally correlated Gaussian feature averages and we validate our
assumptions on small scale Markov Decision Processes. We find that the
stochastic semi-gradient noise due to subsampling the space of possible
episodes leads to significant plateaus in the value error, unlike in
traditional gradient descent dynamics. We study how learning dynamics and
plateaus depend on feature structure, learning rate, discount factor, and
reward function. We then analyze how strategies like learning rate annealing
and reward shaping can favorably alter learning dynamics and plateaus. To
conclude, our work introduces new tools to open a new direction towards
developing a theory of learning dynamics in reinforcement learning.
",0
Measuring and Mitigating Interference in Reinforcement Learning,"Vincent Liu, Han Wang, Ruo Yu Tao, Khurram Javed, Adam White, Martha White",2023-07-10T20:20:20Z,Reinforcement Learning,"  Catastrophic interference is common in many network-based learning systems,
and many proposals exist for mitigating it. Before overcoming interference we
must understand it better. In this work, we provide a definition and novel
measure of interference for value-based reinforcement learning methods such as
Fitted Q-Iteration and DQN. We systematically evaluate our measure of
interference, showing that it correlates with instability in control
performance, across a variety of network architectures. Our new interference
measure allows us to ask novel scientific questions about commonly used deep
learning architectures and study learning algorithms which mitigate
interference. Lastly, we outline a class of algorithms which we call
online-aware that are designed to mitigate interference, and show they do
reduce interference according to our measure and that they improve stability
and performance in several classic control environments.
",0
Reinforcement Learning with Non-Cumulative Objective,"Wei Cui, Wei Yu",2023-07-11T01:20:09Z,Reinforcement Learning,"  In reinforcement learning, the objective is almost always defined as a
\emph{cumulative} function over the rewards along the process. However, there
are many optimal control and reinforcement learning problems in various
application fields, especially in communications and networking, where the
objectives are not naturally expressed as summations of the rewards. In this
paper, we recognize the prevalence of non-cumulative objectives in various
problems, and propose a modification to existing algorithms for optimizing such
objectives. Specifically, we dive into the fundamental building block for many
optimal control and reinforcement learning algorithms: the Bellman optimality
equation. To optimize a non-cumulative objective, we replace the original
summation operation in the Bellman update rule with a generalized operation
corresponding to the objective. Furthermore, we provide sufficient conditions
on the form of the generalized operation as well as assumptions on the Markov
decision process under which the globally optimal convergence of the
generalized Bellman updates can be guaranteed. We demonstrate the idea
experimentally with the bottleneck objective, i.e., the objectives determined
by the minimum reward along the process, on classical optimal control and
reinforcement learning tasks, as well as on two network routing problems on
maximizing the flow rates.
",0
Transformers in Reinforcement Learning: A Survey,"Pranav Agarwal, Aamer Abdul Rahman, Pierre-Luc St-Charles, Simon J. D. Prince, Samira Ebrahimi Kahou",2023-07-12T07:51:12Z,Reinforcement Learning,"  Transformers have significantly impacted domains like natural language
processing, computer vision, and robotics, where they improve performance
compared to other neural networks. This survey explores how transformers are
used in reinforcement learning (RL), where they are seen as a promising
solution for addressing challenges such as unstable training, credit
assignment, lack of interpretability, and partial observability. We begin by
providing a brief domain overview of RL, followed by a discussion on the
challenges of classical RL algorithms. Next, we delve into the properties of
the transformer and its variants and discuss the characteristics that make them
well-suited to address the challenges inherent in RL. We examine the
application of transformers to various aspects of RL, including representation
learning, transition and reward function modeling, and policy optimization. We
also discuss recent research that aims to enhance the interpretability and
efficiency of transformers in RL, using visualization techniques and efficient
training strategies. Often, the transformer architecture must be tailored to
the specific needs of a given application. We present a broad overview of how
transformers have been adapted for several applications, including robotics,
medicine, language modeling, cloud computing, and combinatorial optimization.
We conclude by discussing the limitations of using transformers in RL and
assess their potential for catalyzing future breakthroughs in this field.
",0
The complexity of non-stationary reinforcement learning,"Christos Papadimitriou, Binghui Peng",2023-07-13T16:25:04Z,Reinforcement Learning,"  The problem of continual learning in the domain of reinforcement learning,
often called non-stationary reinforcement learning, has been identified as an
important challenge to the application of reinforcement learning. We prove a
worst-case complexity result, which we believe captures this challenge:
Modifying the probabilities or the reward of a single state-action pair in a
reinforcement learning problem requires an amount of time almost as large as
the number of states in order to keep the value function up to date, unless the
strong exponential time hypothesis (SETH) is false; SETH is a widely accepted
strengthening of the P $\neq$ NP conjecture. Recall that the number of states
in current applications of reinforcement learning is typically astronomical. In
contrast, we show that just $\textit{adding}$ a new state-action pair is
considerably easier to implement.
",0
"Probabilistic Constrained Reinforcement Learning with Formal
  Interpretability","Yanran Wang, Qiuchen Qian, David Boyle",2023-07-13T22:52:22Z,Reinforcement Learning,"  Reinforcement learning can provide effective reasoning for sequential
decision-making problems with variable dynamics. Such reasoning in practical
implementation, however, poses a persistent challenge in interpreting the
reward function and the corresponding optimal policy. Consequently,
representing sequential decision-making problems as probabilistic inference can
have considerable value, as, in principle, the inference offers diverse and
powerful mathematical tools to infer the stochastic dynamics whilst suggesting
a probabilistic interpretation of policy optimization. In this study, we
propose a novel Adaptive Wasserstein Variational Optimization, namely AWaVO, to
tackle these interpretability challenges. Our approach uses formal methods to
achieve the interpretability for convergence guarantee, training transparency,
and intrinsic decision-interpretation. To demonstrate its practicality, we
showcase guaranteed interpretability with an optimal global convergence rate in
simulation and in practical quadrotor tasks. In comparison with
state-of-the-art benchmarks including TRPO-IPO, PCPO and CRPO, we empirically
verify that AWaVO offers a reasonable trade-off between high performance and
sufficient interpretability.
",0
SafeDreamer: Safe Reinforcement Learning with World Models,"Weidong Huang, Jiaming Ji, Chunhe Xia, Borong Zhang, Yaodong Yang",2023-07-14T06:00:08Z,Reinforcement Learning,"  The deployment of Reinforcement Learning (RL) in real-world applications is
constrained by its failure to satisfy safety criteria. Existing Safe
Reinforcement Learning (SafeRL) methods, which rely on cost functions to
enforce safety, often fail to achieve zero-cost performance in complex
scenarios, especially vision-only tasks. These limitations are primarily due to
model inaccuracies and inadequate sample efficiency. The integration of the
world model has proven effective in mitigating these shortcomings. In this
work, we introduce SafeDreamer, a novel algorithm incorporating
Lagrangian-based methods into world model planning processes within the
superior Dreamer framework. Our method achieves nearly zero-cost performance on
various tasks, spanning low-dimensional and vision-only input, within the
Safety-Gymnasium benchmark, showcasing its efficacy in balancing performance
and safety in RL tasks. Further details can be found in the code repository:
\url{https://github.com/PKU-Alignment/SafeDreamer}.
",0
Reinforcement Learning for Credit Index Option Hedging,"Francesco Mandelli, Marco Pinciroli, Michele Trapletti, Edoardo Vittori",2023-07-19T09:03:41Z,Reinforcement Learning,"  In this paper, we focus on finding the optimal hedging strategy of a credit
index option using reinforcement learning. We take a practical approach, where
the focus is on realism i.e. discrete time, transaction costs; even testing our
policy on real market data. We apply a state of the art algorithm, the Trust
Region Volatility Optimization (TRVO) algorithm and show that the derived
hedging strategy outperforms the practitioner's Black & Scholes delta hedge.
",0
A Definition of Continual Reinforcement Learning,"David Abel, André Barreto, Benjamin Van Roy, Doina Precup, Hado van Hasselt, Satinder Singh",2023-07-20T17:28:01Z,Reinforcement Learning,"  In a standard view of the reinforcement learning problem, an agent's goal is
to efficiently identify a policy that maximizes long-term reward. However, this
perspective is based on a restricted view of learning as finding a solution,
rather than treating learning as endless adaptation. In contrast, continual
reinforcement learning refers to the setting in which the best agents never
stop learning. Despite the importance of continual reinforcement learning, the
community lacks a simple definition of the problem that highlights its
commitments and makes its primary concepts precise and clear. To this end, this
paper is dedicated to carefully defining the continual reinforcement learning
problem. We formalize the notion of agents that ""never stop learning"" through a
new mathematical language for analyzing and cataloging agents. Using this new
language, we define a continual learning agent as one that can be understood as
carrying out an implicit search process indefinitely, and continual
reinforcement learning as the setting in which the best agents are all
continual learning agents. We provide two motivating examples, illustrating
that traditional views of multi-task reinforcement learning and continual
supervised learning are special cases of our definition. Collectively, these
definitions and perspectives formalize many intuitive concepts at the heart of
learning, and open new research pathways surrounding continual learning agents.
",0
Reinforcement Learning for Photonic Component Design,"Donald Witt, Jeff Young, Lukas Chrostowski",2023-07-14T17:27:27Z,Reinforcement Learning,"  We present a new fab-in-the-loop reinforcement learning algorithm for the
design of nano-photonic components that accounts for the imperfections present
in nanofabrication processes. As a demonstration of the potential of this
technique, we apply it to the design of photonic crystal grating couplers
fabricated on an air clad 220 nm silicon on insulator single etch platform.
This fab-in-the-loop algorithm improves the insertion loss from 8.8 to 3.24 dB.
The widest bandwidth designs produced using our fab-in-the-loop algorithm can
cover a 150 nm bandwidth with less than 10.2 dB of loss at their lowest point.
",0
Towards Generalizable Reinforcement Learning for Trade Execution,"Chuheng Zhang, Yitong Duan, Xiaoyu Chen, Jianyu Chen, Jian Li, Li Zhao",2023-05-12T02:41:11Z,Reinforcement Learning,"  Optimized trade execution is to sell (or buy) a given amount of assets in a
given time with the lowest possible trading cost. Recently, reinforcement
learning (RL) has been applied to optimized trade execution to learn smarter
policies from market data. However, we find that many existing RL methods
exhibit considerable overfitting which prevents them from real deployment. In
this paper, we provide an extensive study on the overfitting problem in
optimized trade execution. First, we model the optimized trade execution as
offline RL with dynamic context (ORDC), where the context represents market
variables that cannot be influenced by the trading policy and are collected in
an offline manner. Under this framework, we derive the generalization bound and
find that the overfitting issue is caused by large context space and limited
context samples in the offline setting. Accordingly, we propose to learn
compact representations for context to address the overfitting problem, either
by leveraging prior knowledge or in an end-to-end manner. To evaluate our
algorithms, we also implement a carefully designed simulator based on
historical limit order book (LOB) data to provide a high-fidelity benchmark for
different algorithms. Our experiments on the high-fidelity simulator
demonstrate that our algorithms can effectively alleviate overfitting and
achieve better performance.
",6
On-Robot Bayesian Reinforcement Learning for POMDPs,"Hai Nguyen, Sammie Katt, Yuchen Xiao, Christopher Amato",2023-07-22T01:16:29Z,Reinforcement Learning,"  Robot learning is often difficult due to the expense of gathering data. The
need for large amounts of data can, and should, be tackled with effective
algorithms and leveraging expert information on robot dynamics. Bayesian
reinforcement learning (BRL), thanks to its sample efficiency and ability to
exploit prior knowledge, is uniquely positioned as such a solution method.
Unfortunately, the application of BRL has been limited due to the difficulties
of representing expert knowledge as well as solving the subsequent inference
problem. This paper advances BRL for robotics by proposing a specialized
framework for physical systems. In particular, we capture this knowledge in a
factored representation, then demonstrate the posterior factorizes in a similar
shape, and ultimately formalize the model in a Bayesian framework. We then
introduce a sample-based online solution method, based on Monte-Carlo tree
search and particle filtering, specialized to solve the resulting model. This
approach can, for example, utilize typical low-level robot simulators and
handle uncertainty over unknown dynamics of the environment. We empirically
demonstrate its efficiency by performing on-robot learning in two human-robot
interaction tasks with uncertainty about human behavior, achieving near-optimal
performance after only a handful of real-world episodes. A video of learned
policies is at https://youtu.be/H9xp60ngOes.
",1
LiDAR-based drone navigation with reinforcement learning,"Pawel Miera, Hubert Szolc, Tomasz Kryjak",2023-07-26T17:23:33Z,Reinforcement Learning,"  Reinforcement learning is of increasing importance in the field of robot
control and simulation plays a~key role in this process. In the unmanned aerial
vehicles (UAVs, drones), there is also an increase in the number of published
scientific papers involving this approach. In this work, an autonomous drone
control system was prepared to fly forward (according to its coordinates
system) and pass the trees encountered in the forest based on the data from a
rotating LiDAR sensor. The Proximal Policy Optimization (PPO) algorithm, an
example of reinforcement learning (RL), was used to prepare it. A custom
simulator in the Python language was developed for this purpose. The Gazebo
environment, integrated with the Robot Operating System (ROS), was also used to
test the resulting control algorithm. Finally, the prepared solution was
implemented in the Nvidia Jetson Nano eGPU and verified in the real tests
scenarios. During them, the drone successfully completed the set task and was
able to repeatably avoid trees and fly through the forest.
",0
Reinforcement Learning by Guided Safe Exploration,"Qisong Yang, Thiago D. Simão, Nils Jansen, Simon H. Tindemans, Matthijs T. J. Spaan",2023-07-26T17:26:21Z,Reinforcement Learning,"  Safety is critical to broadening the application of reinforcement learning
(RL). Often, we train RL agents in a controlled environment, such as a
laboratory, before deploying them in the real world. However, the real-world
target task might be unknown prior to deployment. Reward-free RL trains an
agent without the reward to adapt quickly once the reward is revealed. We
consider the constrained reward-free setting, where an agent (the guide) learns
to explore safely without the reward signal. This agent is trained in a
controlled environment, which allows unsafe interactions and still provides the
safety signal. After the target task is revealed, safety violations are not
allowed anymore. Thus, the guide is leveraged to compose a safe behaviour
policy. Drawing from transfer learning, we also regularize a target policy (the
student) towards the guide while the student is unreliable and gradually
eliminate the influence of the guide as training progresses. The empirical
analysis shows that this method can achieve safe transfer learning and helps
the student solve the target task faster.
",0
TrackAgent: 6D Object Tracking via Reinforcement Learning,"Konstantin Röhrl, Dominik Bauer, Timothy Patten, Markus Vincze",2023-07-28T17:03:00Z,Reinforcement Learning,"  Tracking an object's 6D pose, while either the object itself or the observing
camera is moving, is important for many robotics and augmented reality
applications. While exploiting temporal priors eases this problem,
object-specific knowledge is required to recover when tracking is lost. Under
the tight time constraints of the tracking task, RGB(D)-based methods are often
conceptionally complex or rely on heuristic motion models. In comparison, we
propose to simplify object tracking to a reinforced point cloud (depth only)
alignment task. This allows us to train a streamlined approach from scratch
with limited amounts of sparse 3D point clouds, compared to the large datasets
of diverse RGBD sequences required in previous works. We incorporate temporal
frame-to-frame registration with object-based recovery by frame-to-model
refinement using a reinforcement learning (RL) agent that jointly solves for
both objectives. We also show that the RL agent's uncertainty and a
rendering-based mask propagation are effective reinitialization triggers.
",0
Variance Control for Distributional Reinforcement Learning,"Qi Kuang, Zhoufan Zhu, Liwen Zhang, Fan Zhou",2023-07-30T07:25:18Z,Reinforcement Learning,"  Although distributional reinforcement learning (DRL) has been widely examined
in the past few years, very few studies investigate the validity of the
obtained Q-function estimator in the distributional setting. To fully
understand how the approximation errors of the Q-function affect the whole
training process, we do some error analysis and theoretically show how to
reduce both the bias and the variance of the error terms. With this new
understanding, we construct a new estimator \emph{Quantiled Expansion Mean}
(QEM) and introduce a new DRL algorithm (QEMRL) from the statistical
perspective. We extensively evaluate our QEMRL algorithm on a variety of Atari
and Mujoco benchmark tasks and demonstrate that QEMRL achieves significant
improvement over baseline algorithms in terms of sample efficiency and
convergence performance.
",0
Reinforcement Learning for Financial Index Tracking,"Xianhua Peng, Chenyin Gong, Xue Dong He",2023-08-05T08:34:52Z,Reinforcement Learning,"  We propose the first discrete-time infinite-horizon dynamic formulation of
the financial index tracking problem under both return-based tracking error and
value-based tracking error. The formulation overcomes the limitations of
existing models by incorporating the intertemporal dynamics of market
information variables not limited to prices, allowing exact calculation of
transaction costs, accounting for the tradeoff between overall tracking error
and transaction costs, allowing effective use of data in a long time period,
etc. The formulation also allows novel decision variables of cash injection or
withdraw. We propose to solve the portfolio rebalancing equation using a Banach
fixed point iteration, which allows to accurately calculate the transaction
costs specified as nonlinear functions of trading volumes in practice. We
propose an extension of deep reinforcement learning (RL) method to solve the
dynamic formulation. Our RL method resolves the issue of data limitation
resulting from the availability of a single sample path of financial data by a
novel training scheme. A comprehensive empirical study based on a 17-year-long
testing set demonstrates that the proposed method outperforms a benchmark
method in terms of tracking accuracy and has the potential for earning extra
profit through cash withdraw strategy.
",0
AlphaStar Unplugged: Large-Scale Offline Reinforcement Learning,"Michaël Mathieu, Sherjil Ozair, Srivatsan Srinivasan, Caglar Gulcehre, Shangtong Zhang, Ray Jiang, Tom Le Paine, Richard Powell, Konrad Żołna, Julian Schrittwieser, David Choi, Petko Georgiev, Daniel Toyama, Aja Huang, Roman Ring, Igor Babuschkin, Timo Ewalds, Mahyar Bordbar, Sarah Henderson, Sergio Gómez Colmenarejo, Aäron van den Oord, Wojciech Marian Czarnecki, Nando de Freitas, Oriol Vinyals",2023-08-07T12:21:37Z,Reinforcement Learning,"  StarCraft II is one of the most challenging simulated reinforcement learning
environments; it is partially observable, stochastic, multi-agent, and
mastering StarCraft II requires strategic planning over long time horizons with
real-time low-level execution. It also has an active professional competitive
scene. StarCraft II is uniquely suited for advancing offline RL algorithms,
both because of its challenging nature and because Blizzard has released a
massive dataset of millions of StarCraft II games played by human players. This
paper leverages that and establishes a benchmark, called AlphaStar Unplugged,
introducing unprecedented challenges for offline reinforcement learning. We
define a dataset (a subset of Blizzard's release), tools standardizing an API
for machine learning methods, and an evaluation protocol. We also present
baseline agents, including behavior cloning, offline variants of actor-critic
and MuZero. We improve the state of the art of agents using only offline data,
and we achieve 90% win rate against previously published AlphaStar behavior
cloning agent.
",0
Value-Distributional Model-Based Reinforcement Learning,"Carlos E. Luis, Alessandro G. Bottero, Julia Vinogradska, Felix Berkenkamp, Jan Peters",2023-08-12T14:59:19Z,Reinforcement Learning,"  Quantifying uncertainty about a policy's long-term performance is important
to solve sequential decision-making tasks. We study the problem from a
model-based Bayesian reinforcement learning perspective, where the goal is to
learn the posterior distribution over value functions induced by parameter
(epistemic) uncertainty of the Markov decision process. Previous work restricts
the analysis to a few moments of the distribution over values or imposes a
particular distribution shape, e.g., Gaussians. Inspired by distributional
reinforcement learning, we introduce a Bellman operator whose fixed-point is
the value distribution function. Based on our theory, we propose Epistemic
Quantile-Regression (EQR), a model-based algorithm that learns a value
distribution function. We combine EQR with soft actor-critic (SAC) for policy
optimization with an arbitrary differentiable objective function of the learned
value distribution. Evaluation across several continuous-control tasks shows
performance benefits with respect to both model-based and model-free
algorithms. The code is available at
https://github.com/boschresearch/dist-mbrl.
",0
Discrete Prompt Compression with Reinforcement Learning,"Hoyoun Jung, Kyung-Joong Kim",2023-08-17T03:10:17Z,Reinforcement Learning,"  Compressed prompts aid instruction-tuned language models (LMs) in overcoming
context window limitations and reducing computational costs. Existing methods,
which primarily based on training embeddings, face various challenges
associated with interpretability, the fixed number of embedding tokens,
reusability across different LMs, and inapplicability when interacting with
black-box APIs. This study proposes prompt compression with reinforcement
learning (PCRL), which is a discrete prompt compression method that addresses
these issues. The proposed PCRL method utilizes a computationally efficient
policy network that edits prompts directly. The training approach employed in
the proposed PCRLs can be applied flexibly to various types of LMs, including
both decoder-only and encoder-decoder architecture and it can be trained
without gradient access to the LMs or labeled data. The proposed PCRL achieves
an average reduction of 24.6% in terms of the token count across various
instruction prompts while maintaining sufficient performance. In addition, we
demonstrate that the learned policy can be transferred to larger LMs, and
through a comprehensive analysis, we explore the token importance within the
prompts. Our code is accessible at
https://github.com/nenomigami/PromptCompressor.
",0
Language Reward Modulation for Pretraining Reinforcement Learning,"Ademi Adeniji, Amber Xie, Carmelo Sferrazza, Younggyo Seo, Stephen James, Pieter Abbeel",2023-08-23T17:37:51Z,Reinforcement Learning,"  Using learned reward functions (LRFs) as a means to solve sparse-reward
reinforcement learning (RL) tasks has yielded some steady progress in
task-complexity through the years. In this work, we question whether today's
LRFs are best-suited as a direct replacement for task rewards. Instead, we
propose leveraging the capabilities of LRFs as a pretraining signal for RL.
Concretely, we propose $\textbf{LA}$nguage Reward $\textbf{M}$odulated
$\textbf{P}$retraining (LAMP) which leverages the zero-shot capabilities of
Vision-Language Models (VLMs) as a $\textit{pretraining}$ utility for RL as
opposed to a downstream task reward. LAMP uses a frozen, pretrained VLM to
scalably generate noisy, albeit shaped exploration rewards by computing the
contrastive alignment between a highly diverse collection of language
instructions and the image observations of an agent in its pretraining
environment. LAMP optimizes these rewards in conjunction with standard
novelty-seeking exploration rewards with reinforcement learning to acquire a
language-conditioned, pretrained policy. Our VLM pretraining approach, which is
a departure from previous attempts to use LRFs, can warmstart sample-efficient
learning on robot manipulation tasks in RLBench.
",0
Traffic Light Control with Reinforcement Learning,Taoyu Pan,2023-08-28T04:29:49Z,Reinforcement Learning,"  Traffic light control is important for reducing congestion in urban mobility
systems. This paper proposes a real-time traffic light control method using
deep Q learning. Our approach incorporates a reward function considering queue
lengths, delays, travel time, and throughput. The model dynamically decides
phase changes based on current traffic conditions. The training of the deep Q
network involves an offline stage from pre-generated data with fixed schedules
and an online stage using real-time traffic data. A deep Q network structure
with a ""phase gate"" component is used to simplify the model's learning task
under different phases. A ""memory palace"" mechanism is used to address sample
imbalance during the training process. We validate our approach using both
synthetic and real-world traffic flow data on a road intersecting in Hangzhou,
China. Results demonstrate significant performance improvements of the proposed
method in reducing vehicle waiting time (57.1% to 100%), queue lengths (40.9%
to 100%), and total travel time (16.8% to 68.0%) compared to traditional fixed
signal plans.
",0
Reinforcement Learning for Generative AI: A Survey,"Yuanjiang Cao, Quan Z. Sheng, Julian McAuley, Lina Yao",2023-08-28T06:15:14Z,Reinforcement Learning,"  Deep Generative AI has been a long-standing essential topic in the machine
learning community, which can impact a number of application areas like text
generation and computer vision. The major paradigm to train a generative model
is maximum likelihood estimation, which pushes the learner to capture and
approximate the target data distribution by decreasing the divergence between
the model distribution and the target distribution. This formulation
successfully establishes the objective of generative tasks, while it is
incapable of satisfying all the requirements that a user might expect from a
generative model. Reinforcement learning, serving as a competitive option to
inject new training signals by creating new objectives that exploit novel
signals, has demonstrated its power and flexibility to incorporate human
inductive bias from multiple angles, such as adversarial learning,
hand-designed rules and learned reward model to build a performant model.
Thereby, reinforcement learning has become a trending research field and has
stretched the limits of generative AI in both model design and application. It
is reasonable to summarize and conclude advances in recent years with a
comprehensive review. Although there are surveys in different application areas
recently, this survey aims to shed light on a high-level review that spans a
range of application areas. We provide a rigorous taxonomy in this area and
make sufficient coverage on various models and applications. Notably, we also
surveyed the fast-developing large language model area. We conclude this survey
by showing the potential directions that might tackle the limit of current
models and expand the frontiers for generative AI.
",0
Target-independent XLA optimization using Reinforcement Learning,"Milan Ganai, Haichen Li, Theodore Enns, Yida Wang, Randy Huang",2023-08-28T07:23:03Z,Reinforcement Learning,"  An important challenge in Machine Learning compilers like XLA is multi-pass
optimization and analysis. There has been recent interest chiefly in XLA
target-dependent optimization on the graph-level, subgraph-level, and
kernel-level phases. We specifically focus on target-independent optimization
XLA HLO pass ordering: our approach aims at finding the optimal sequence of
compiler optimization passes, which is decoupled from target-dependent
optimization. However, there is little domain specific study in pass ordering
for XLA HLO. To this end, we propose introducing deep Reinforcement Learning
(RL) based search for optimal XLA HLO pass ordering. We also propose
enhancements to the deep RL algorithms to further improve optimal search
performance and open the research direction for domain-specific guidance for
RL. We create an XLA Gym experimentation framework as a tool to enable RL
algorithms to interact with the compiler for passing optimizations and thereby
train agents. Overall, in our experimentation we observe an average of $13.3\%$
improvement in operation count reduction on a benchmark of GPT-2 training
graphs and $10.4\%$ improvement on a diverse benchmark including GPT-2, BERT,
and ResNet graphs using the proposed approach over the compiler's default phase
ordering.
",0
Shielded Reinforcement Learning for Hybrid Systems,"Asger Horn Brorholt, Peter Gjøl Jensen, Kim Guldstrand Larsen, Florian Lorber, Christian Schilling",2023-08-28T09:04:52Z,Reinforcement Learning,"  Safe and optimal controller synthesis for switched-controlled hybrid systems,
which combine differential equations and discrete changes of the system's
state, is known to be intricately hard. Reinforcement learning has been
leveraged to construct near-optimal controllers, but their behavior is not
guaranteed to be safe, even when it is encouraged by reward engineering. One
way of imposing safety to a learned controller is to use a shield, which is
correct by design. However, obtaining a shield for non-linear and hybrid
environments is itself intractable. In this paper, we propose the construction
of a shield using the so-called barbaric method, where an approximate finite
representation of an underlying partition-based two-player safety game is
extracted via systematically picked samples of the true transition function.
While hard safety guarantees are out of reach, we experimentally demonstrate
strong statistical safety guarantees with a prototype implementation and UPPAAL
STRATEGO. Furthermore, we study the impact of the synthesized shield when
applied as either a pre-shield (applied before learning a controller) or a
post-shield (only applied after learning a controller). We experimentally
demonstrate superiority of the pre-shielding approach. We apply our technique
on a range of case studies, including two industrial examples, and further
study post-optimization of the post-shielding approach.
",0
Deep Inductive Logic Programming meets Reinforcement Learning,"Andreas Bueff, Vaishak Belle",2023-08-30T09:08:46Z,Reinforcement Learning,"  One approach to explaining the hierarchical levels of understanding within a
machine learning model is the symbolic method of inductive logic programming
(ILP), which is data efficient and capable of learning first-order logic rules
that can entail data behaviour. A differentiable extension to ILP, so-called
differentiable Neural Logic (dNL) networks, are able to learn Boolean functions
as their neural architecture includes symbolic reasoning. We propose an
application of dNL in the field of Relational Reinforcement Learning (RRL) to
address dynamic continuous environments. This represents an extension of
previous work in applying dNL-based ILP in RRL settings, as our proposed model
updates the architecture to enable it to solve problems in continuous RL
environments. The goal of this research is to improve upon current ILP methods
for use in RRL by incorporating non-linear continuous predicates, allowing RRL
agents to reason and make decisions in dynamic and continuous environments.
",0
Neurosymbolic Reinforcement Learning and Planning: A Survey,"K. Acharya, W. Raza, C. M. J. M. Dourado Jr, A. Velasquez, H. Song",2023-09-02T23:41:35Z,Reinforcement Learning,"  The area of Neurosymbolic Artificial Intelligence (Neurosymbolic AI) is
rapidly developing and has become a popular research topic, encompassing
sub-fields such as Neurosymbolic Deep Learning (Neurosymbolic DL) and
Neurosymbolic Reinforcement Learning (Neurosymbolic RL). Compared to
traditional learning methods, Neurosymbolic AI offers significant advantages by
simplifying complexity and providing transparency and explainability.
Reinforcement Learning(RL), a long-standing Artificial Intelligence(AI) concept
that mimics human behavior using rewards and punishment, is a fundamental
component of Neurosymbolic RL, a recent integration of the two fields that has
yielded promising results. The aim of this paper is to contribute to the
emerging field of Neurosymbolic RL by conducting a literature survey. Our
evaluation focuses on the three components that constitute Neurosymbolic RL:
neural, symbolic, and RL. We categorize works based on the role played by the
neural and symbolic parts in RL, into three taxonomies:Learning for Reasoning,
Reasoning for Learning and Learning-Reasoning. These categories are further
divided into sub-categories based on their applications. Furthermore, we
analyze the RL components of each research work, including the state space,
action space, policy module, and RL algorithm. Additionally, we identify
research opportunities and challenges in various applications within this
dynamic field.
",0
LoopTune: Optimizing Tensor Computations with Reinforcement Learning,"Dejan Grubisic, Bram Wasti, Chris Cummins, John Mellor-Crummey, Aleksandar Zlateski",2023-09-04T21:30:15Z,Reinforcement Learning,"  Advanced compiler technology is crucial for enabling machine learning
applications to run on novel hardware, but traditional compilers fail to
deliver performance, popular auto-tuners have long search times and
expert-optimized libraries introduce unsustainable costs. To address this, we
developed LoopTune, a deep reinforcement learning compiler that optimizes
tensor computations in deep learning models for the CPU. LoopTune optimizes
tensor traversal order while using the ultra-fast lightweight code generator
LoopNest to perform hardware-specific optimizations. With a novel graph-based
representation and action space, LoopTune speeds up LoopNest by 3.2x,
generating an order of magnitude faster code than TVM, 2.8x faster than
MetaSchedule, and 1.08x faster than AutoTVM, consistently performing at the
level of the hand-tuned library Numpy. Moreover, LoopTune tunes code in order
of seconds.
",0
Deep Reinforcement Learning from Hierarchical Preference Design,"Alexander Bukharin, Yixiao Li, Pengcheng He, Tuo Zhao",2023-09-06T00:44:29Z,Reinforcement Learning,"  Reward design is a fundamental, yet challenging aspect of reinforcement
learning (RL). Researchers typically utilize feedback signals from the
environment to handcraft a reward function, but this process is not always
effective due to the varying scale and intricate dependencies of the feedback
signals. This paper shows by exploiting certain structures, one can ease the
reward design process. Specifically, we propose a hierarchical reward modeling
framework -- HERON for scenarios: (I) The feedback signals naturally present
hierarchy; (II) The reward is sparse, but with less important surrogate
feedback to help policy learning. Both scenarios allow us to design a
hierarchical decision tree induced by the importance ranking of the feedback
signals to compare RL trajectories. With such preference data, we can then
train a reward model for policy learning. We apply HERON to several RL
applications, and we find that our framework can not only train high performing
agents on a variety of difficult tasks, but also provide additional benefits
such as improved sample efficiency and robustness. Our code is available at
\url{https://github.com/abukharin3/HERON}.
",0
Adaptive REST API Testing with Reinforcement Learning,"Myeongsoo Kim, Saurabh Sinha, Alessandro Orso",2023-09-08T20:27:05Z,Reinforcement Learning,"  Modern web services increasingly rely on REST APIs. Effectively testing these
APIs is challenging due to the vast search space to be explored, which involves
selecting API operations for sequence creation, choosing parameters for each
operation from a potentially large set of parameters, and sampling values from
the virtually infinite parameter input space. Current testing tools lack
efficient exploration mechanisms, treating all operations and parameters
equally (i.e., not considering their importance or complexity) and lacking
prioritization strategies. Furthermore, these tools struggle when response
schemas are absent in the specification or exhibit variants. To address these
limitations, we present an adaptive REST API testing technique that
incorporates reinforcement learning to prioritize operations and parameters
during exploration. Our approach dynamically analyzes request and response data
to inform dependent parameters and adopts a sampling-based strategy for
efficient processing of dynamic API feedback. We evaluated our technique on ten
RESTful services, comparing it against state-of-the-art REST testing tools with
respect to code coverage achieved, requests generated, operations covered, and
service failures triggered. Additionally, we performed an ablation study on
prioritization, dynamic feedback analysis, and sampling to assess their
individual effects. Our findings demonstrate that our approach outperforms
existing REST API testing tools in terms of effectiveness, efficiency, and
fault-finding ability.
",4
Verifiable Reinforcement Learning Systems via Compositionality,"Cyrus Neary, Aryaman Singh Samyal, Christos Verginis, Murat Cubuktepe, Ufuk Topcu",2023-09-09T17:11:44Z,Reinforcement Learning,"  We propose a framework for verifiable and compositional reinforcement
learning (RL) in which a collection of RL subsystems, each of which learns to
accomplish a separate subtask, are composed to achieve an overall task. The
framework consists of a high-level model, represented as a parametric Markov
decision process, which is used to plan and analyze compositions of subsystems,
and of the collection of low-level subsystems themselves. The subsystems are
implemented as deep RL agents operating under partial observability. By
defining interfaces between the subsystems, the framework enables automatic
decompositions of task specifications, e.g., reach a target set of states with
a probability of at least 0.95, into individual subtask specifications, i.e.
achieve the subsystem's exit conditions with at least some minimum probability,
given that its entry conditions are met. This in turn allows for the
independent training and testing of the subsystems. We present theoretical
results guaranteeing that if each subsystem learns a policy satisfying its
subtask specification, then their composition is guaranteed to satisfy the
overall task specification. Conversely, if the subtask specifications cannot
all be satisfied by the learned policies, we present a method, formulated as
the problem of finding an optimal set of parameters in the high-level model, to
automatically update the subtask specifications to account for the observed
shortcomings. The result is an iterative procedure for defining subtask
specifications, and for training the subsystems to meet them. Experimental
results demonstrate the presented framework's novel capabilities in
environments with both full and partial observability, discrete and continuous
state and action spaces, as well as deterministic and stochastic dynamics.
",0
Safe Reinforcement Learning with Dual Robustness,"Zeyang Li, Chuxiong Hu, Yunan Wang, Yujie Yang, Shengbo Eben Li",2023-09-13T09:34:21Z,Reinforcement Learning,"  Reinforcement learning (RL) agents are vulnerable to adversarial
disturbances, which can deteriorate task performance or compromise safety
specifications. Existing methods either address safety requirements under the
assumption of no adversary (e.g., safe RL) or only focus on robustness against
performance adversaries (e.g., robust RL). Learning one policy that is both
safe and robust remains a challenging open problem. The difficulty is how to
tackle two intertwined aspects in the worst cases: feasibility and optimality.
Optimality is only valid inside a feasible region, while identification of
maximal feasible region must rely on learning the optimal policy. To address
this issue, we propose a systematic framework to unify safe RL and robust RL,
including problem formulation, iteration scheme, convergence analysis and
practical algorithm design. This unification is built upon constrained
two-player zero-sum Markov games. A dual policy iteration scheme is proposed,
which simultaneously optimizes a task policy and a safety policy. The
convergence of this iteration scheme is proved. Furthermore, we design a deep
RL algorithm for practical implementation, called dually robust actor-critic
(DRAC). The evaluations with safety-critical benchmarks demonstrate that DRAC
achieves high performance and persistent safety under all scenarios (no
adversary, safety adversary, performance adversary), outperforming all
baselines significantly.
",2
Efficient Reinforcement Learning for Jumping Monopods,"Riccardo Bussola, Michele Focchi, Andrea Del Prete, Daniele Fontanelli, Luigi Palopoli",2023-09-13T15:46:40Z,Reinforcement Learning,"  In this work, we consider the complex control problem of making a monopod
reach a target with a jump. The monopod can jump in any direction and the
terrain underneath its foot can be uneven. This is a template of a much larger
class of problems, which are extremely challenging and computationally
expensive to solve using standard optimisation-based techniques. Reinforcement
Learning (RL) could be an interesting alternative, but the application of an
end-to-end approach in which the controller must learn everything from scratch,
is impractical. The solution advocated in this paper is to guide the learning
process within an RL framework by injecting physical knowledge. This expedient
brings to widespread benefits, such as a drastic reduction of the learning
time, and the ability to learn and compensate for possible errors in the
low-level controller executing the motion. We demonstrate the advantage of our
approach with respect to both optimization-based and end-to-end RL approaches.
",0
Contrastive Initial State Buffer for Reinforcement Learning,"Nico Messikommer, Yunlong Song, Davide Scaramuzza",2023-09-18T13:26:40Z,Reinforcement Learning,"  In Reinforcement Learning, the trade-off between exploration and exploitation
poses a complex challenge for achieving efficient learning from limited
samples. While recent works have been effective in leveraging past experiences
for policy updates, they often overlook the potential of reusing past
experiences for data collection. Independent of the underlying RL algorithm, we
introduce the concept of a Contrastive Initial State Buffer, which
strategically selects states from past experiences and uses them to initialize
the agent in the environment in order to guide it toward more informative
states. We validate our approach on two complex robotic tasks without relying
on any prior information about the environment: (i) locomotion of a quadruped
robot traversing challenging terrains and (ii) a quadcopter drone racing
through a track. The experimental results show that our initial state buffer
achieves higher task performance than the nominal baseline while also speeding
up training convergence.
",0
Hierarchical reinforcement learning with natural language subgoals,"Arun Ahuja, Kavya Kopparapu, Rob Fergus, Ishita Dasgupta",2023-09-20T18:03:04Z,Reinforcement Learning,"  Hierarchical reinforcement learning has been a compelling approach for
achieving goal directed behavior over long sequences of actions. However, it
has been challenging to implement in realistic or open-ended environments. A
main challenge has been to find the right space of sub-goals over which to
instantiate a hierarchy. We present a novel approach where we use data from
humans solving these tasks to softly supervise the goal space for a set of long
range tasks in a 3D embodied environment. In particular, we use unconstrained
natural language to parameterize this space. This has two advantages: first, it
is easy to generate this data from naive human participants; second, it is
flexible enough to represent a vast range of sub-goals in human-relevant tasks.
Our approach outperforms agents that clone expert behavior on these tasks, as
well as HRL from scratch without this supervised sub-goal space. Our work
presents a novel approach to combining human expert supervision with the
benefits and flexibility of reinforcement learning.
",0
Learning to Recover for Safe Reinforcement Learning,"Haoyu Wang, Xin Yuan, Qinqing Ren",2023-09-21T09:17:38Z,Reinforcement Learning,"  Safety controllers is widely used to achieve safe reinforcement learning.
Most methods that apply a safety controller are using handcrafted safety
constraints to construct the safety controller. However, when the environment
dynamics are sophisticated, handcrafted safety constraints become unavailable.
Therefore, it worth to research on constructing safety controllers by learning
algorithms. We propose a three-stage architecture for safe reinforcement
learning, namely TU-Recovery Architecture. A safety critic and a recovery
policy is learned before task training. They form a safety controller to ensure
safety in task training. Then a phenomenon induced by disagreement between task
policy and recovery policy, called adversarial phenomenon, which reduces
learning efficiency and model performance, is described. Auxiliary reward is
proposed to mitigate adversarial phenomenon, while help the task policy to
learn to recover from high-risk states. A series of experiments are conducted
in a robot navigation environment. Experiments demonstrate that TU-Recovery
outperforms unconstrained counterpart in both reward gaining and constraint
violations during task training, and auxiliary reward further improve
TU-Recovery in reward-to-cost ratio by significantly reduce constraint
violations.
",0
Iterative Reachability Estimation for Safe Reinforcement Learning,"Milan Ganai, Zheng Gong, Chenning Yu, Sylvia Herbert, Sicun Gao",2023-09-24T02:36:42Z,Reinforcement Learning,"  Ensuring safety is important for the practical deployment of reinforcement
learning (RL). Various challenges must be addressed, such as handling
stochasticity in the environments, providing rigorous guarantees of persistent
state-wise safety satisfaction, and avoiding overly conservative behaviors that
sacrifice performance. We propose a new framework, Reachability Estimation for
Safe Policy Optimization (RESPO), for safety-constrained RL in general
stochastic settings. In the feasible set where there exist violation-free
policies, we optimize for rewards while maintaining persistent safety. Outside
this feasible set, our optimization produces the safest behavior by
guaranteeing entrance into the feasible set whenever possible with the least
cumulative discounted violations. We introduce a class of algorithms using our
novel reachability estimation function to optimize in our proposed framework
and in similar frameworks such as those concurrently handling multiple hard and
soft constraints. We theoretically establish that our algorithms almost surely
converge to locally optimal policies of our safe optimization framework. We
evaluate the proposed methods on a diverse suite of safe RL environments from
Safety Gym, PyBullet, and MuJoCo, and show the benefits in improving both
reward performance and safety compared with state-of-the-art baselines.
",9
DISeR: Designing Imaging Systems with Reinforcement Learning,"Tzofi Klinghoffer, Kushagra Tiwary, Nikhil Behari, Bhavya Agrawalla, Ramesh Raskar",2023-09-25T03:35:51Z,Reinforcement Learning,"  Imaging systems consist of cameras to encode visual information about the
world and perception models to interpret this encoding. Cameras contain (1)
illumination sources, (2) optical elements, and (3) sensors, while perception
models use (4) algorithms. Directly searching over all combinations of these
four building blocks to design an imaging system is challenging due to the size
of the search space. Moreover, cameras and perception models are often designed
independently, leading to sub-optimal task performance. In this paper, we
formulate these four building blocks of imaging systems as a context-free
grammar (CFG), which can be automatically searched over with a learned camera
designer to jointly optimize the imaging system with task-specific perception
models. By transforming the CFG to a state-action space, we then show how the
camera designer can be implemented with reinforcement learning to intelligently
search over the combinatorial space of possible imaging system configurations.
We demonstrate our approach on two tasks, depth estimation and camera rig
design for autonomous vehicles, showing that our method yields rigs that
outperform industry-wide standards. We believe that our proposed approach is an
important step towards automating imaging system design.
",0
Hierarchical Reinforcement Learning Based on Planning Operators,"Jing Zhang, Emmanuel Dean, Karinne Ramirez-Amaro",2023-09-25T15:54:32Z,Reinforcement Learning,"  Long-horizon manipulation tasks such as stacking represent a longstanding
challenge in the field of robotic manipulation, particularly when using
reinforcement learning (RL) methods which often struggle to learn the correct
sequence of actions for achieving these complex goals. To learn this sequence,
symbolic planning methods offer a good solution based on high-level reasoning,
however, planners often fall short in addressing the low-level control
specificity needed for precise execution. This paper introduces a novel
framework that integrates symbolic planning with hierarchical RL through the
cooperation of high-level operators and low-level policies. Our contribution
integrates planning operators (e.g. preconditions and effects) as part of the
hierarchical RL algorithm based on the Scheduled Auxiliary Control (SAC-X)
method. We developed a dual-purpose high-level operator, which can be used both
in holistic planning and as independent, reusable policies. Our approach offers
a flexible solution for long-horizon tasks, e.g., stacking a cube. The
experimental results show that our proposed method obtained an average of 97.2%
success rate for learning and executing the whole stack sequence, and the
success rate for learning independent policies, e.g. reach (98.9%), lift
(99.7%), stack (85%), etc. The training time is also reduced by 68% when using
our proposed approach.
",0
Adapting Double Q-Learning for Continuous Reinforcement Learning,Arsenii Kuznetsov,2023-09-25T19:09:54Z,Reinforcement Learning,"  Majority of off-policy reinforcement learning algorithms use overestimation
bias control techniques. Most of these techniques rooted in heuristics,
primarily addressing the consequences of overestimation rather than its
fundamental origins. In this work we present a novel approach to the bias
correction, similar in spirit to Double Q-Learning. We propose using a policy
in form of a mixture with two components. Each policy component is maximized
and assessed by separate networks, which removes any basis for the
overestimation bias. Our approach shows promising near-SOTA results on a small
set of MuJoCo environments.
",0
Decoding trust: A reinforcement learning perspective,"Guozhong Zheng, Jiqiang Zhang, Jing Zhang, Weiran Cai, Li Chen",2023-09-26T01:06:29Z,Reinforcement Learning,"  Behavioral experiments on the trust game have shown that trust and
trustworthiness are universal among human beings, contradicting the prediction
by assuming \emph{Homo economicus} in orthodox Economics. This means some
mechanism must be at work that favors their emergence. Most previous
explanations however need to resort to some factors based upon imitative
learning, a simple version of social learning. Here, we turn to the paradigm of
reinforcement learning, where individuals update their strategies by evaluating
the long-term return through accumulated experience. Specifically, we
investigate the trust game with the Q-learning algorithm, where each
participant is associated with two evolving Q-tables that guide one's decision
making as trustor and trustee respectively. In the pairwise scenario, we reveal
that high levels of trust and trustworthiness emerge when individuals
appreciate both their historical experience and returns in the future.
Mechanistically, the evolution of the Q-tables shows a crossover that resembles
human's psychological changes. We also provide the phase diagram for the game
parameters, where the boundary analysis is conducted. These findings are robust
when the scenario is extended to a latticed population. Our results thus
provide a natural explanation for the emergence of trust and trustworthiness
without external factors involved. More importantly, the proposed paradigm
shows the potential in deciphering many puzzles in human behaviors.
",0
Tempo Adaptation in Non-stationary Reinforcement Learning,"Hyunin Lee, Yuhao Ding, Jongmin Lee, Ming Jin, Javad Lavaei, Somayeh Sojoudi",2023-09-26T15:01:21Z,Reinforcement Learning,"  We first raise and tackle a ``time synchronization'' issue between the agent
and the environment in non-stationary reinforcement learning (RL), a crucial
factor hindering its real-world applications. In reality, environmental changes
occur over wall-clock time ($t$) rather than episode progress ($k$), where
wall-clock time signifies the actual elapsed time within the fixed duration $t
\in [0, T]$. In existing works, at episode $k$, the agent rolls a trajectory
and trains a policy before transitioning to episode $k+1$. In the context of
the time-desynchronized environment, however, the agent at time $t_{k}$
allocates $\Delta t$ for trajectory generation and training, subsequently moves
to the next episode at $t_{k+1}=t_{k}+\Delta t$. Despite a fixed total number
of episodes ($K$), the agent accumulates different trajectories influenced by
the choice of interaction times ($t_1,t_2,...,t_K$), significantly impacting
the suboptimality gap of the policy. We propose a Proactively Synchronizing
Tempo ($\texttt{ProST}$) framework that computes a suboptimal sequence
{$t_1,t_2,...,t_K$} (= { $t_{1:K}$}) by minimizing an upper bound on its
performance measure, i.e., the dynamic regret. Our main contribution is that we
show that a suboptimal {$t_{1:K}$} trades-off between the policy training time
(agent tempo) and how fast the environment changes (environment tempo).
Theoretically, this work develops a suboptimal {$t_{1:K}$} as a function of the
degree of the environment's non-stationarity while also achieving a sublinear
dynamic regret. Our experimental evaluation on various high-dimensional
non-stationary environments shows that the $\texttt{ProST}$ framework achieves
a higher online return at suboptimal {$t_{1:K}$} than the existing methods.
",0
Estimation and Inference in Distributional Reinforcement Learning,"Liangyu Zhang, Yang Peng, Jiadong Liang, Wenhao Yang, Zhihua Zhang",2023-09-29T14:14:53Z,Reinforcement Learning,"  In this paper, we study distributional reinforcement learning from the
perspective of statistical efficiency. We investigate distributional policy
evaluation, aiming to estimate the complete return distribution (denoted
$\eta^\pi$) attained by a given policy $\pi$. We use the certainty-equivalence
method to construct our estimator $\hat\eta^\pi$, given a generative model is
available. In this circumstance we need a dataset of size $\widetilde
O\left(\frac{|\mathcal{S}||\mathcal{A}|}{\varepsilon^{2p}(1-\gamma)^{2p+2}}\right)$
to guarantee the $p$-Wasserstein metric between $\hat\eta^\pi$ and $\eta^\pi$
less than $\varepsilon$ with high probability. This implies the distributional
policy evaluation problem can be solved with sample efficiency. Also, we show
that under different mild assumptions a dataset of size $\widetilde
O\left(\frac{|\mathcal{S}||\mathcal{A}|}{\varepsilon^{2}(1-\gamma)^{4}}\right)$
suffices to ensure the Kolmogorov metric and total variation metric between
$\hat\eta^\pi$ and $\eta^\pi$ is below $\varepsilon$ with high probability.
Furthermore, we investigate the asymptotic behavior of $\hat\eta^\pi$. We
demonstrate that the ``empirical process'' $\sqrt{n}(\hat\eta^\pi-\eta^\pi)$
converges weakly to a Gaussian process in the space of bounded functionals on
Lipschitz function class $\ell^\infty(\mathcal{F}_{\text{W}})$, also in the
space of bounded functionals on indicator function class
$\ell^\infty(\mathcal{F}_{\text{KS}})$ and bounded measurable function class
$\ell^\infty(\mathcal{F}_{\text{TV}})$ when some mild conditions hold. Our
findings give rise to a unified approach to statistical inference of a wide
class of statistical functionals of $\eta^\pi$.
",0
"Prioritized Soft Q-Decomposition for Lexicographic Reinforcement
  Learning","Finn Rietz, Erik Schaffernicht, Stefan Heinrich, Johannes Andreas Stork",2023-10-03T18:36:21Z,Other,"  Reinforcement learning (RL) for complex tasks remains a challenge, primarily
due to the difficulties of engineering scalar reward functions and the inherent
inefficiency of training models from scratch. Instead, it would be better to
specify complex tasks in terms of elementary subtasks and to reuse subtask
solutions whenever possible. In this work, we address continuous space
lexicographic multi-objective RL problems, consisting of prioritized subtasks,
which are notoriously difficult to solve. We show that these can be scalarized
with a subtask transformation and then solved incrementally using value
decomposition. Exploiting this insight, we propose prioritized soft
Q-decomposition (PSQD), a novel algorithm for learning and adapting subtask
solutions under lexicographic priorities in continuous state-action spaces.
PSQD offers the ability to reuse previously learned subtask solutions in a
zero-shot composition, followed by an adaptation step. Its ability to use
retained subtask training data for offline learning eliminates the need for new
environment interaction during adaptation. We demonstrate the efficacy of our
approach by presenting successful learning, reuse, and adaptation results for
both low- and high-dimensional simulated robot control tasks, as well as
offline learning results. In contrast to baseline approaches, PSQD does not
trade off between conflicting subtasks or priority constraints and satisfies
subtask priorities during learning. PSQD provides an intuitive framework for
tackling complex RL problems, offering insights into the inner workings of the
subtask composition.
",0
Enhancing Exfiltration Path Analysis Using Reinforcement Learning,"Riddam Rishu, Akshay Kakkar, Cheng Wang, Abdul Rahman, Christopher Redino, Dhruv Nandakumar, Tyler Cody, Ryan Clark, Daniel Radke, Edward Bowen",2023-10-05T16:43:12Z,Reinforcement Learning,"  Building on previous work using reinforcement learning (RL) focused on
identification of exfiltration paths, this work expands the methodology to
include protocol and payload considerations. The former approach to
exfiltration path discovery, where reward and state are associated specifically
with the determination of optimal paths, are presented with these additional
realistic characteristics to account for nuances in adversarial behavior. The
paths generated are enhanced by including communication payload and protocol
into the Markov decision process (MDP) in order to more realistically emulate
attributes of network based exfiltration events. The proposed method will help
emulate complex adversarial considerations such as the size of a payload being
exported over time or the protocol on which it occurs, as is the case where
threat actors steal data over long periods of time using system native ports or
protocols to avoid detection. As such, practitioners will be able to improve
identification of expected adversary behavior under various payload and
protocol assumptions more comprehensively.
",0
Reinforcement Learning with Fast and Forgetful Memory,"Steven Morad, Ryan Kortvelesy, Stephan Liwicki, Amanda Prorok",2023-10-06T09:56:26Z,Reinforcement Learning,"  Nearly all real world tasks are inherently partially observable,
necessitating the use of memory in Reinforcement Learning (RL). Most model-free
approaches summarize the trajectory into a latent Markov state using memory
models borrowed from Supervised Learning (SL), even though RL tends to exhibit
different training and efficiency characteristics. Addressing this discrepancy,
we introduce Fast and Forgetful Memory, an algorithm-agnostic memory model
designed specifically for RL. Our approach constrains the model search space
via strong structural priors inspired by computational psychology. It is a
drop-in replacement for recurrent neural networks (RNNs) in recurrent RL
algorithms, achieving greater reward than RNNs across various recurrent
benchmarks and algorithms without changing any hyperparameters. Moreover, Fast
and Forgetful Memory exhibits training speeds two orders of magnitude faster
than RNNs, attributed to its logarithmic time and linear space complexity. Our
implementation is available at https://github.com/proroklab/ffm.
",0
Terrain-Aware Quadrupedal Locomotion via Reinforcement Learning,"Haojie Shi, Qingxu Zhu, Lei Han, Wanchao Chi, Tingguang Li, Max Q. -H. Meng",2023-10-07T03:20:26Z,Reinforcement Learning,"  In nature, legged animals have developed the ability to adapt to challenging
terrains through perception, allowing them to plan safe body and foot
trajectories in advance, which leads to safe and energy-efficient locomotion.
Inspired by this observation, we present a novel approach to train a Deep
Neural Network (DNN) policy that integrates proprioceptive and exteroceptive
states with a parameterized trajectory generator for quadruped robots to
traverse rough terrains. Our key idea is to use a DNN policy that can modify
the parameters of the trajectory generator, such as foot height and frequency,
to adapt to different terrains. To encourage the robot to step on safe regions
and save energy consumption, we propose foot terrain reward and lifting foot
height reward, respectively. By incorporating these rewards, our method can
learn a safer and more efficient terrain-aware locomotion policy that can move
a quadruped robot flexibly in any direction. To evaluate the effectiveness of
our approach, we conduct simulation experiments on challenging terrains,
including stairs, stepping stones, and poles. The simulation results
demonstrate that our approach can successfully direct the robot to traverse
such tough terrains in any direction. Furthermore, we validate our method on a
real legged robot, which learns to traverse stepping stones with gaps over
25.5cm.
",0
Reinforcement learning for freeform robot design,"Muhan Li, David Matthews, Sam Kriegman",2023-10-09T12:39:44Z,Reinforcement Learning,"  Inspired by the necessity of morphological adaptation in animals, a growing
body of work has attempted to expand robot training to encompass physical
aspects of a robot's design. However, reinforcement learning methods capable of
optimizing the 3D morphology of a robot have been restricted to reorienting or
resizing the limbs of a predetermined and static topological genus. Here we
show policy gradients for designing freeform robots with arbitrary external and
internal structure. This is achieved through actions that deposit or remove
bundles of atomic building blocks to form higher-level nonparametric
macrostructures such as appendages, organs and cavities. Although results are
provided for open loop control only, we discuss how this method could be
adapted for closed loop control and sim2real transfer to physical machines in
future.
",0
Hierarchical Reinforcement Learning for Temporal Pattern Prediction,"Faith Johnson, Kristin Dana",2023-10-09T13:15:57Z,Reinforcement Learning,"  In this work, we explore the use of hierarchical reinforcement learning (HRL)
for the task of temporal sequence prediction. Using a combination of deep
learning and HRL, we develop a stock agent to predict temporal price sequences
from historical stock price data and a vehicle agent to predict steering angles
from first person, dash cam images. Our results in both domains indicate that a
type of HRL, called feudal reinforcement learning, provides significant
improvements to training speed and stability and prediction accuracy over
standard RL. A key component to this success is the multi-resolution structure
that introduces both temporal and spatial abstraction into the network
hierarchy.
",0
When is Agnostic Reinforcement Learning Statistically Tractable?,"Zeyu Jia, Gene Li, Alexander Rakhlin, Ayush Sekhari, Nathan Srebro",2023-10-09T19:40:54Z,Reinforcement Learning,"  We study the problem of agnostic PAC reinforcement learning (RL): given a
policy class $\Pi$, how many rounds of interaction with an unknown MDP (with a
potentially large state and action space) are required to learn an
$\epsilon$-suboptimal policy with respect to $\Pi$? Towards that end, we
introduce a new complexity measure, called the \emph{spanning capacity}, that
depends solely on the set $\Pi$ and is independent of the MDP dynamics. With a
generative model, we show that for any policy class $\Pi$, bounded spanning
capacity characterizes PAC learnability. However, for online RL, the situation
is more subtle. We show there exists a policy class $\Pi$ with a bounded
spanning capacity that requires a superpolynomial number of samples to learn.
This reveals a surprising separation for agnostic learnability between
generative access and online access models (as well as between
deterministic/stochastic MDPs under online access). On the positive side, we
identify an additional \emph{sunflower} structure, which in conjunction with
bounded spanning capacity enables statistically efficient online RL via a new
algorithm called POPLER, which takes inspiration from classical importance
sampling methods as well as techniques for reachable-state identification and
policy evaluation in reward-free exploration.
",0
Robust Safe Reinforcement Learning under Adversarial Disturbances,"Zeyang Li, Chuxiong Hu, Shengbo Eben Li, Jia Cheng, Yunan Wang",2023-10-11T05:34:46Z,Reinforcement Learning,"  Safety is a primary concern when applying reinforcement learning to
real-world control tasks, especially in the presence of external disturbances.
However, existing safe reinforcement learning algorithms rarely account for
external disturbances, limiting their applicability and robustness in practice.
To address this challenge, this paper proposes a robust safe reinforcement
learning framework that tackles worst-case disturbances. First, this paper
presents a policy iteration scheme to solve for the robust invariant set, i.e.,
a subset of the safe set, where persistent safety is only possible for states
within. The key idea is to establish a two-player zero-sum game by leveraging
the safety value function in Hamilton-Jacobi reachability analysis, in which
the protagonist (i.e., control inputs) aims to maintain safety and the
adversary (i.e., external disturbances) tries to break down safety. This paper
proves that the proposed policy iteration algorithm converges monotonically to
the maximal robust invariant set. Second, this paper integrates the proposed
policy iteration scheme into a constrained reinforcement learning algorithm
that simultaneously synthesizes the robust invariant set and uses it for
constrained policy optimization. This algorithm tackles both optimality and
safety, i.e., learning a policy that attains high rewards while maintaining
safety under worst-case disturbances. Experiments on classic control tasks show
that the proposed method achieves zero constraint violation with learned
worst-case adversarial disturbances, while other baseline algorithms violate
the safety constraints substantially. Our proposed method also attains
comparable performance as the baselines even in the absence of the adversary.
",0
Virtual Augmented Reality for Atari Reinforcement Learning,Christian A. Schiller,2023-10-12T19:42:42Z,Reinforcement Learning,"  Reinforcement Learning (RL) has achieved significant milestones in the gaming
domain, most notably Google DeepMind's AlphaGo defeating human Go champion Ken
Jie. This victory was also made possible through the Atari Learning Environment
(ALE): The ALE has been foundational in RL research, facilitating significant
RL algorithm developments such as AlphaGo and others. In current Atari video
game RL research, RL agents' perceptions of its environment is based on raw
pixel data from the Atari video game screen with minimal image preprocessing.
Contrarily, cutting-edge ML research, external to the Atari video game RL
research domain, is focusing on enhancing image perception. A notable example
is Meta Research's ""Segment Anything Model"" (SAM), a foundation model capable
of segmenting images without prior training (zero-shot). This paper addresses a
novel methodical question: Can state-of-the-art image segmentation models such
as SAM improve the performance of RL agents playing Atari video games? The
results suggest that SAM can serve as a ""virtual augmented reality"" for the RL
agent, boosting its Atari video game playing performance under certain
conditions. Comparing RL agent performance results from raw and augmented pixel
inputs provides insight into these conditions. Although this paper was limited
by computational constraints, the findings show improved RL agent performance
for augmented pixel inputs and can inform broader research agendas in the
domain of ""virtual augmented reality for video game playing RL agents"".
",0
Deep Reinforcement Learning with Explicit Context Representation,"Francisco Munguia-Galeano, Ah-Hwee Tan, Ze Ji",2023-10-15T19:23:05Z,Reinforcement Learning,"  Reinforcement learning (RL) has shown an outstanding capability for solving
complex computational problems. However, most RL algorithms lack an explicit
method that would allow learning from contextual information. Humans use
context to identify patterns and relations among elements in the environment,
along with how to avoid making wrong actions. On the other hand, what may seem
like an obviously wrong decision from a human perspective could take hundreds
of steps for an RL agent to learn to avoid. This paper proposes a framework for
discrete environments called Iota explicit context representation (IECR). The
framework involves representing each state using contextual key frames (CKFs),
which can then be used to extract a function that represents the affordances of
the state; in addition, two loss functions are introduced with respect to the
affordances of the state. The novelty of the IECR framework lies in its
capacity to extract contextual information from the environment and learn from
the CKFs' representation. We validate the framework by developing four new
algorithms that learn using context: Iota deep Q-network (IDQN), Iota double
deep Q-network (IDDQN), Iota dueling deep Q-network (IDuDQN), and Iota dueling
double deep Q-network (IDDDQN). Furthermore, we evaluate the framework and the
new algorithms in five discrete environments. We show that all the algorithms,
which use contextual information, converge in around 40,000 training steps of
the neural networks, significantly outperforming their state-of-the-art
equivalents.
",0
Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark,"Jiaming Ji, Borong Zhang, Jiayi Zhou, Xuehai Pan, Weidong Huang, Ruiyang Sun, Yiran Geng, Yifan Zhong, Juntao Dai, Yaodong Yang",2023-10-19T08:19:28Z,Reinforcement Learning,"  Artificial intelligence (AI) systems possess significant potential to drive
societal progress. However, their deployment often faces obstacles due to
substantial safety concerns. Safe reinforcement learning (SafeRL) emerges as a
solution to optimize policies while simultaneously adhering to multiple
constraints, thereby addressing the challenge of integrating reinforcement
learning in safety-critical scenarios. In this paper, we present an environment
suite called Safety-Gymnasium, which encompasses safety-critical tasks in both
single and multi-agent scenarios, accepting vector and vision-only input.
Additionally, we offer a library of algorithms named Safe Policy Optimization
(SafePO), comprising 16 state-of-the-art SafeRL algorithms. This comprehensive
library can serve as a validation tool for the research community. By
introducing this benchmark, we aim to facilitate the evaluation and comparison
of safety performance, thus fostering the development of reinforcement learning
for safer, more reliable, and responsible real-world applications. The website
of this project can be accessed at
https://sites.google.com/view/safety-gymnasium.
",0
Diverse Priors for Deep Reinforcement Learning,"Chenfan Weng, Zhongguo Li",2023-10-23T12:33:59Z,Reinforcement Learning,"  In Reinforcement Learning (RL), agents aim at maximizing cumulative rewards
in a given environment. During the learning process, RL agents face the dilemma
of exploitation and exploration: leveraging existing knowledge to acquire
rewards or seeking potentially higher ones. Using uncertainty as a guiding
principle provides an active and effective approach to solving this dilemma and
ensemble-based methods are one of the prominent avenues for quantifying
uncertainty. Nevertheless, conventional ensemble-based uncertainty estimation
lacks an explicit prior, deviating from Bayesian principles. Besides, this
method requires diversity among members to generate less biased uncertainty
estimation results. To address the above problems, previous research has
incorporated random functions as priors. Building upon these foundational
efforts, our work introduces an innovative approach with delicately designed
prior NNs, which can incorporate maximal diversity in the initial value
functions of RL. Our method has demonstrated superior performance compared with
the random prior approaches in solving classic control problems and general
exploration tasks, significantly improving sample efficiency.
",0
Hyperparameter Optimization for Multi-Objective Reinforcement Learning,"Florian Felten, Daniel Gareev, El-Ghazali Talbi, Grégoire Danoy",2023-10-25T09:17:25Z,Reinforcement Learning,"  Reinforcement learning (RL) has emerged as a powerful approach for tackling
complex problems. The recent introduction of multi-objective reinforcement
learning (MORL) has further expanded the scope of RL by enabling agents to make
trade-offs among multiple objectives. This advancement not only has broadened
the range of problems that can be tackled but also created numerous
opportunities for exploration and advancement. Yet, the effectiveness of RL
agents heavily relies on appropriately setting their hyperparameters. In
practice, this task often proves to be challenging, leading to unsuccessful
deployments of these techniques in various instances. Hence, prior research has
explored hyperparameter optimization in RL to address this concern.
  This paper presents an initial investigation into the challenge of
hyperparameter optimization specifically for MORL. We formalize the problem,
highlight its distinctive challenges, and propose a systematic methodology to
address it. The proposed methodology is applied to a well-known environment
using a state-of-the-art MORL algorithm, and preliminary results are reported.
Our findings indicate that the proposed methodology can effectively provide
hyperparameter configurations that significantly enhance the performance of
MORL agents. Furthermore, this study identifies various future research
opportunities to further advance the field of hyperparameter optimization for
MORL.
",0
Privately Aligning Language Models with Reinforcement Learning,"Fan Wu, Huseyin A. Inan, Arturs Backurs, Varun Chandrasekaran, Janardhan Kulkarni, Robert Sim",2023-10-25T19:58:51Z,Reinforcement Learning,"  Positioned between pre-training and user deployment, aligning large language
models (LLMs) through reinforcement learning (RL) has emerged as a prevailing
strategy for training instruction following-models such as ChatGPT. In this
work, we initiate the study of privacy-preserving alignment of LLMs through
Differential Privacy (DP) in conjunction with RL. Following the influential
work of Ziegler et al. (2020), we study two dominant paradigms: (i) alignment
via RL without human in the loop (e.g., positive review generation) and (ii)
alignment via RL from human feedback (RLHF) (e.g., summarization in a
human-preferred way). We give a new DP framework to achieve alignment via RL,
and prove its correctness. Our experimental results validate the effectiveness
of our approach, offering competitive utility while ensuring strong privacy
protections.
",0
MAG-GNN: Reinforcement Learning Boosted Graph Neural Network,"Lecheng Kong, Jiarui Feng, Hao Liu, Dacheng Tao, Yixin Chen, Muhan Zhang",2023-10-29T20:32:21Z,Reinforcement Learning,"  While Graph Neural Networks (GNNs) recently became powerful tools in graph
learning tasks, considerable efforts have been spent on improving GNNs'
structural encoding ability. A particular line of work proposed subgraph GNNs
that use subgraph information to improve GNNs' expressivity and achieved great
success. However, such effectivity sacrifices the efficiency of GNNs by
enumerating all possible subgraphs. In this paper, we analyze the necessity of
complete subgraph enumeration and show that a model can achieve a comparable
level of expressivity by considering a small subset of the subgraphs. We then
formulate the identification of the optimal subset as a combinatorial
optimization problem and propose Magnetic Graph Neural Network (MAG-GNN), a
reinforcement learning (RL) boosted GNN, to solve the problem. Starting with a
candidate subgraph set, MAG-GNN employs an RL agent to iteratively update the
subgraphs to locate the most expressive set for prediction. This reduces the
exponential complexity of subgraph enumeration to the constant complexity of a
subgraph search algorithm while keeping good expressivity. We conduct extensive
experiments on many datasets, showing that MAG-GNN achieves competitive
performance to state-of-the-art methods and even outperforms many subgraph
GNNs. We also demonstrate that MAG-GNN effectively reduces the running time of
subgraph GNNs.
",0
Rethinking Decision Transformer via Hierarchical Reinforcement Learning,"Yi Ma, Chenjun Xiao, Hebin Liang, Jianye Hao",2023-11-01T03:32:13Z,Reinforcement Learning,"  Decision Transformer (DT) is an innovative algorithm leveraging recent
advances of the transformer architecture in reinforcement learning (RL).
However, a notable limitation of DT is its reliance on recalling trajectories
from datasets, losing the capability to seamlessly stitch sub-optimal
trajectories together. In this work we introduce a general sequence modeling
framework for studying sequential decision making through the lens of
Hierarchical RL. At the time of making decisions, a high-level policy first
proposes an ideal prompt for the current state, a low-level policy subsequently
generates an action conditioned on the given prompt. We show DT emerges as a
special case of this framework with certain choices of high-level and low-level
policies, and discuss the potential failure of these choices. Inspired by these
observations, we study how to jointly optimize the high-level and low-level
policies to enable the stitching ability, which further leads to the
development of new offline RL algorithms. Our empirical results clearly show
that the proposed algorithms significantly surpass DT on several control and
navigation benchmarks. We hope our contributions can inspire the integration of
transformer architectures within the field of RL.
",0
Diffusion Models for Reinforcement Learning: A Survey,"Zhengbang Zhu, Hanye Zhao, Haoran He, Yichao Zhong, Shenyu Zhang, Haoquan Guo, Tingting Chen, Weinan Zhang",2023-11-02T13:23:39Z,Reinforcement Learning,"  Diffusion models surpass previous generative models in sample quality and
training stability. Recent works have shown the advantages of diffusion models
in improving reinforcement learning (RL) solutions. This survey aims to provide
an overview of this emerging field and hopes to inspire new avenues of
research. First, we examine several challenges encountered by RL algorithms.
Then, we present a taxonomy of existing methods based on the roles of diffusion
models in RL and explore how the preceding challenges are addressed. We further
outline successful applications of diffusion models in various RL-related
tasks. Finally, we conclude the survey and offer insights into future research
directions. We are actively maintaining a GitHub repository for papers and
other related resources in utilizing diffusion models in RL:
https://github.com/apexrl/Diff4RLSurvey.
",0
Anytime-Competitive Reinforcement Learning with Policy Prior,"Jianyi Yang, Pengfei Li, Tongxin Li, Adam Wierman, Shaolei Ren",2023-11-02T19:44:59Z,Reinforcement Learning,"  This paper studies the problem of Anytime-Competitive Markov Decision Process
(A-CMDP). Existing works on Constrained Markov Decision Processes (CMDPs) aim
to optimize the expected reward while constraining the expected cost over
random dynamics, but the cost in a specific episode can still be
unsatisfactorily high. In contrast, the goal of A-CMDP is to optimize the
expected reward while guaranteeing a bounded cost in each round of any episode
against a policy prior. We propose a new algorithm, called Anytime-Competitive
Reinforcement Learning (ACRL), which provably guarantees the anytime cost
constraints. The regret analysis shows the policy asymptotically matches the
optimal reward achievable under the anytime competitive constraints.
Experiments on the application of carbon-intelligent computing verify the
reward performance and cost constraint guarantee of ACRL.
",0
Causal Question Answering with Reinforcement Learning,"Lukas Blübaum, Stefan Heindorf",2023-11-05T20:33:18Z,Reinforcement Learning,"  Causal questions inquire about causal relationships between different events
or phenomena. They are important for a variety of use cases, including virtual
assistants and search engines. However, many current approaches to causal
question answering cannot provide explanations or evidence for their answers.
Hence, in this paper, we aim to answer causal questions with a causality graph,
a large-scale dataset of causal relations between noun phrases along with the
relations' provenance data. Inspired by recent, successful applications of
reinforcement learning to knowledge graph tasks, such as link prediction and
fact-checking, we explore the application of reinforcement learning on a
causality graph for causal question answering. We introduce an
Actor-Critic-based agent which learns to search through the graph to answer
causal questions. We bootstrap the agent with a supervised learning procedure
to deal with large action spaces and sparse rewards. Our evaluation shows that
the agent successfully prunes the search space to answer binary causal
questions by visiting less than 30 nodes per question compared to over 3,000
nodes by a naive breadth-first search. Our ablation study indicates that our
supervised learning strategy provides a strong foundation upon which our
reinforcement learning agent improves. The paths returned by our agent explain
the mechanisms by which a cause produces an effect. Moreover, for each edge on
a path, our causality graph provides its original source allowing for easy
verification of paths.
",0
Kindness in Multi-Agent Reinforcement Learning,"Farinaz Alamiyan-Harandi, Mersad Hassanjani, Pouria Ramazi",2023-11-06T19:53:26Z,Reinforcement Learning,"  In human societies, people often incorporate fairness in their decisions and
treat reciprocally by being kind to those who act kindly. They evaluate the
kindness of others' actions not only by monitoring the outcomes but also by
considering the intentions. This behavioral concept can be adapted to train
cooperative agents in Multi-Agent Reinforcement Learning (MARL). We propose the
KindMARL method, where agents' intentions are measured by counterfactual
reasoning over the environmental impact of the actions that were available to
the agents. More specifically, the current environment state is compared with
the estimation of the current environment state provided that the agent had
chosen another action. The difference between each agent's reward, as the
outcome of its action, with that of its fellow, multiplied by the intention of
the fellow is then taken as the fellow's ""kindness"". If the result of each
reward-comparison confirms the agent's superiority, it perceives the fellow's
kindness and reduces its own reward. Experimental results in the Cleanup and
Harvest environments show that training based on the KindMARL method enabled
the agents to earn 89\% (resp. 37\%) and 44% (resp. 43\%) more total rewards
than training based on the Inequity Aversion and Social Influence methods. The
effectiveness of KindMARL is further supported by experiments in a traffic
light control problem.
",0
Environmental-Impact Based Multi-Agent Reinforcement Learning,"Farinaz Alamiyan-Harandi, Pouria Ramazi",2023-11-06T20:30:11Z,Reinforcement Learning,"  To promote cooperation and strengthen the individual impact on the collective
outcome in social dilemmas, we propose the Environmental-impact Multi-Agent
Reinforcement Learning (EMuReL) method where each agent estimates the
""environmental impact"" of every other agent, that is, the difference in the
current environment state compared to the hypothetical environment in the
absence of that other agent. Inspired by the Inequity Aversion model, the agent
then compares its own reward with those of its fellows multiplied by their
environmental impacts. If its reward exceeds the scaled reward of one of its
fellows, the agent takes ""social responsibility"" toward that fellow by reducing
its own reward. Therefore, the less influential an agent is in reaching the
current state, the more social responsibility is taken by other agents.
Experiments in the Cleanup (resp. Harvest) test environment demonstrate that
agents trained based on EMuReL learn to cooperate more effectively and obtain
$54\%$ ($39\%$) and $20\%$ ($44\%$) more total rewards while preserving the
same cooperation levels compared to when they are trained based on the two
state-of-the-art reward reshaping methods inequity aversion and social
influence.
",0
Towards Continual Reinforcement Learning for Quadruped Robots,"Giovanni Minelli, Vassilis Vassiliades",2023-11-12T12:54:44Z,Reinforcement Learning,"  Quadruped robots have emerged as an evolving technology that currently
leverages simulators to develop a robust controller capable of functioning in
the real-world without the need for further training. However, since it is
impossible to predict all possible real-world situations, our research explores
the possibility of enabling them to continue learning even after their
deployment. To this end, we designed two continual learning scenarios,
sequentially training the robot on different environments while simultaneously
evaluating its performance across all of them. Our approach sheds light on the
extent of both forward and backward skill transfer, as well as the degree to
which the robot might forget previously acquired skills. By addressing these
factors, we hope to enhance the adaptability and performance of quadruped
robots in real-world scenarios.
",0
Model-assisted Reinforcement Learning of a Quadrotor,Arshad Javeed,2023-11-12T18:05:56Z,Reinforcement Learning,"  In recent times, reinforcement learning has produced baffling results when it
comes to performing control tasks with highly non-linear systems. The
impressive results always outweigh the potential vulnerabilities or
uncertainties associated with the agents when deployed in the real-world. While
the performance is remarkable compared to the classical control algorithms, the
reinforcement learning-based methods suffer from two flaws, robustness and
interpretability, which are vital for contemporary real-world applications. The
paper attempts to alleviate such problems with reinforcement learning and
proposes the concept of model-assisted reinforcement learning to induce a
notion of conservativeness in the agents. The control task considered for the
experiment involves navigating a CrazyFlie quadrotor. The paper also describes
a way of reformulating the task to have the flexibility of tuning the level of
conservativeness via multi-objective reinforcement learning. The results
include a comparison of the vanilla reinforcement learning approaches and the
proposed approach. The metrics are evaluated by systematically injecting
disturbances to classify the inherent robustness and conservativeness of the
agents. More concrete arguments are made by computing and comparing the
backward reachability tubes of the RL policies by solving the
Hamilton-Jacobi-Bellman partial differential equation (HJ PDE).
",0
An introduction to reinforcement learning for neuroscience,Kristopher T. Jensen,2023-11-13T13:10:52Z,Reinforcement Learning,"  Reinforcement learning has a rich history in neuroscience, from early work on
dopamine as a reward prediction error signal for temporal difference learning
(Schultz et al., 1997) to recent work suggesting that dopamine could implement
a form of 'distributional reinforcement learning' popularized in deep learning
(Dabney et al., 2020). Throughout this literature, there has been a tight link
between theoretical advances in reinforcement learning and neuroscientific
experiments and findings. As a result, the theories describing our experimental
data have become increasingly complex and difficult to navigate. In this
review, we cover the basic theory underlying classical work in reinforcement
learning and build up to an introductory overview of methods in modern deep
reinforcement learning that have found applications in systems neuroscience. We
start with an overview of the reinforcement learning problem and classical
temporal difference algorithms, followed by a discussion of 'model-free' and
'model-based' reinforcement learning together with methods such as DYNA and
successor representations that fall in between these two extremes. Throughout
these sections, we highlight the close parallels between such machine learning
methods and related work in both experimental and theoretical neuroscience. We
then provide an introduction to deep reinforcement learning with examples of
how these methods have been used to model different learning phenomena in
systems neuroscience, such as meta-reinforcement learning (Wang et al., 2018)
and distributional reinforcement learning (Dabney et al., 2020). Code that
implements the methods discussed in this work and generates the figures is also
provided.
",0
When Mining Electric Locomotives Meet Reinforcement Learning,"Ying Li, Zhencai Zhu, Xiaoqiang Li, Chunyu Yang, Hao Lu",2023-11-14T13:29:01Z,Reinforcement Learning,"  As the most important auxiliary transportation equipment in coal mines,
mining electric locomotives are mostly operated manually at present. However,
due to the complex and ever-changing coal mine environment, electric locomotive
safety accidents occur frequently these years. A mining electric locomotive
control method that can adapt to different complex mining environments is
needed. Reinforcement Learning (RL) is concerned with how artificial agents
ought to take actions in an environment so as to maximize reward, which can
help achieve automatic control of mining electric locomotive. In this paper, we
present how to apply RL to the autonomous control of mining electric
locomotives. To achieve more precise control, we further propose an improved
epsilon-greedy (IEG) algorithm which can better balance the exploration and
exploitation. To verify the effectiveness of this method, a co-simulation
platform for autonomous control of mining electric locomotives is built which
can complete closed-loop simulation of the vehicles. The simulation results
show that this method ensures the locomotives following the front vehicle
safely and responding promptly in the event of sudden obstacles on the road
when the vehicle in complex and uncertain coal mine environments.
",0
Augmenting Unsupervised Reinforcement Learning with Self-Reference,"Andrew Zhao, Erle Zhu, Rui Lu, Matthieu Lin, Yong-Jin Liu, Gao Huang",2023-11-16T09:07:34Z,Reinforcement Learning,"  Humans possess the ability to draw on past experiences explicitly when
learning new tasks and applying them accordingly. We believe this capacity for
self-referencing is especially advantageous for reinforcement learning agents
in the unsupervised pretrain-then-finetune setting. During pretraining, an
agent's past experiences can be explicitly utilized to mitigate the
nonstationarity of intrinsic rewards. In the finetuning phase, referencing
historical trajectories prevents the unlearning of valuable exploratory
behaviors. Motivated by these benefits, we propose the Self-Reference (SR)
approach, an add-on module explicitly designed to leverage historical
information and enhance agent performance within the pretrain-finetune
paradigm. Our approach achieves state-of-the-art results in terms of
Interquartile Mean (IQM) performance and Optimality Gap reduction on the
Unsupervised Reinforcement Learning Benchmark for model-free methods, recording
an 86% IQM and a 16% Optimality Gap. Additionally, it improves current
algorithms by up to 17% IQM and reduces the Optimality Gap by 31%. Beyond
performance enhancement, the Self-Reference add-on also increases sample
efficiency, a crucial attribute for real-world applications.
",0
RLIF: Interactive Imitation Learning as Reinforcement Learning,"Jianlan Luo, Perry Dong, Yuexiang Zhai, Yi Ma, Sergey Levine",2023-11-21T21:05:21Z,Reinforcement Learning,"  Although reinforcement learning methods offer a powerful framework for
automatic skill acquisition, for practical learning-based control problems in
domains such as robotics, imitation learning often provides a more convenient
and accessible alternative. In particular, an interactive imitation learning
method such as DAgger, which queries a near-optimal expert to intervene online
to collect correction data for addressing the distributional shift challenges
that afflict na\""ive behavioral cloning, can enjoy good performance both in
theory and practice without requiring manually specified reward functions and
other components of full reinforcement learning methods. In this paper, we
explore how off-policy reinforcement learning can enable improved performance
under assumptions that are similar but potentially even more practical than
those of interactive imitation learning. Our proposed method uses reinforcement
learning with user intervention signals themselves as rewards. This relaxes the
assumption that intervening experts in interactive imitation learning should be
near-optimal and enables the algorithm to learn behaviors that improve over the
potential suboptimal human expert. We also provide a unified framework to
analyze our RL method and DAgger; for which we present the asymptotic analysis
of the suboptimal gap for both methods as well as the non-asymptotic sample
complexity bound of our method. We then evaluate our method on challenging
high-dimensional continuous control simulation benchmarks as well as real-world
robotic vision-based manipulation tasks. The results show that it strongly
outperforms DAgger-like approaches across the different tasks, especially when
the intervening experts are suboptimal. Code and videos can be found on the
project website: https://rlif-page.github.io
",0
Probabilistic Inference in Reinforcement Learning Done Right,"Jean Tarbouriech, Tor Lattimore, Brendan O'Donoghue",2023-11-22T10:23:14Z,Reinforcement Learning,"  A popular perspective in Reinforcement learning (RL) casts the problem as
probabilistic inference on a graphical model of the Markov decision process
(MDP). The core object of study is the probability of each state-action pair
being visited under the optimal policy. Previous approaches to approximate this
quantity can be arbitrarily poor, leading to algorithms that do not implement
genuine statistical inference and consequently do not perform well in
challenging problems. In this work, we undertake a rigorous Bayesian treatment
of the posterior probability of state-action optimality and clarify how it
flows through the MDP. We first reveal that this quantity can indeed be used to
generate a policy that explores efficiently, as measured by regret.
Unfortunately, computing it is intractable, so we derive a new variational
Bayesian approximation yielding a tractable convex optimization problem and
establish that the resulting policy also explores efficiently. We call our
approach VAPOR and show that it has strong connections to Thompson sampling,
K-learning, and maximum entropy exploration. We conclude with some experiments
demonstrating the performance advantage of a deep RL version of VAPOR.
",0
Approximation of Convex Envelope Using Reinforcement Learning,"Vivek S. Borkar, Adit Akarsh",2023-11-24T11:47:08Z,Reinforcement Learning,"  Oberman gave a stochastic control formulation of the problem of estimating
the convex envelope of a non-convex function. Based on this, we develop a
reinforcement learning scheme to approximate the convex envelope, using a
variant of Q-learning for controlled optimal stopping. It shows very promising
results on a standard library of test problems.
",0
Maximum Entropy Model Correction in Reinforcement Learning,"Amin Rakhsha, Mete Kemertas, Mohammad Ghavamzadeh, Amir-massoud Farahmand",2023-11-29T18:00:41Z,Reinforcement Learning,"  We propose and theoretically analyze an approach for planning with an
approximate model in reinforcement learning that can reduce the adverse impact
of model error. If the model is accurate enough, it accelerates the convergence
to the true value function too. One of its key components is the MaxEnt Model
Correction (MoCo) procedure that corrects the model's next-state distributions
based on a Maximum Entropy density estimation formulation. Based on MoCo, we
introduce the Model Correcting Value Iteration (MoCoVI) algorithm, and its
sampled-based variant MoCoDyna. We show that MoCoVI and MoCoDyna's convergence
can be much faster than the conventional model-free algorithms. Unlike
traditional model-based algorithms, MoCoVI and MoCoDyna effectively utilize an
approximate model and still converge to the correct value function.
",0
Optimizing ZX-Diagrams with Deep Reinforcement Learning,"Maximilian Nägele, Florian Marquardt",2023-11-30T14:29:18Z,Reinforcement Learning,"  ZX-diagrams are a powerful graphical language for the description of quantum
processes with applications in fundamental quantum mechanics, quantum circuit
optimization, tensor network simulation, and many more. The utility of
ZX-diagrams relies on a set of local transformation rules that can be applied
to them without changing the underlying quantum process they describe. These
rules can be exploited to optimize the structure of ZX-diagrams for a range of
applications. However, finding an optimal sequence of transformation rules is
generally an open problem. In this work, we bring together ZX-diagrams with
reinforcement learning, a machine learning technique designed to discover an
optimal sequence of actions in a decision-making problem and show that a
trained reinforcement learning agent can significantly outperform other
optimization techniques like a greedy strategy, simulated annealing, and
state-of-the-art hand-crafted algorithms. The use of graph neural networks to
encode the policy of the agent enables generalization to diagrams much bigger
than seen during the training phase.
",0
Optimal Attack and Defense for Reinforcement Learning,"Jeremy McMahan, Young Wu, Xiaojin Zhu, Qiaomin Xie",2023-11-30T21:21:47Z,Reinforcement Learning,"  To ensure the usefulness of Reinforcement Learning (RL) in real systems, it
is crucial to ensure they are robust to noise and adversarial attacks. In
adversarial RL, an external attacker has the power to manipulate the victim
agent's interaction with the environment. We study the full class of online
manipulation attacks, which include (i) state attacks, (ii) observation attacks
(which are a generalization of perceived-state attacks), (iii) action attacks,
and (iv) reward attacks. We show the attacker's problem of designing a stealthy
attack that maximizes its own expected reward, which often corresponds to
minimizing the victim's value, is captured by a Markov Decision Process (MDP)
that we call a meta-MDP since it is not the true environment but a higher level
environment induced by the attacked interaction. We show that the attacker can
derive optimal attacks by planning in polynomial time or learning with
polynomial sample complexity using standard RL techniques. We argue that the
optimal defense policy for the victim can be computed as the solution to a
stochastic Stackelberg game, which can be further simplified into a
partially-observable turn-based stochastic game (POTBSG). Neither the attacker
nor the victim would benefit from deviating from their respective optimal
policies, thus such solutions are truly robust. Although the defense problem is
NP-hard, we show that optimal Markovian defenses can be computed (learned) in
polynomial time (sample complexity) in many scenarios.
",0
Harnessing Discrete Representations For Continual Reinforcement Learning,"Edan Meyer, Adam White, Marlos C. Machado",2023-12-02T18:55:26Z,Reinforcement Learning,"  Reinforcement learning (RL) agents make decisions using nothing but
observations from the environment, and consequently, heavily rely on the
representations of those observations. Though some recent breakthroughs have
used vector-based categorical representations of observations, often referred
to as discrete representations, there is little work explicitly assessing the
significance of such a choice. In this work, we provide a thorough empirical
investigation of the advantages of representing observations as vectors of
categorical values within the context of reinforcement learning. We perform
evaluations on world-model learning, model-free RL, and ultimately continual RL
problems, where the benefits best align with the needs of the problem setting.
We find that, when compared to traditional continuous representations, world
models learned over discrete representations accurately model more of the world
with less capacity, and that agents trained with discrete representations learn
better policies with less data. In the context of continual RL, these benefits
translate into faster adapting agents. Additionally, our analysis suggests that
the observed performance improvements can be attributed to the information
contained within the latent vectors and potentially the encoding of the
discrete representation itself.
",0
BenchMARL: Benchmarking Multi-Agent Reinforcement Learning,"Matteo Bettini, Amanda Prorok, Vincent Moens",2023-12-03T18:15:58Z,Reinforcement Learning,"  The field of Multi-Agent Reinforcement Learning (MARL) is currently facing a
reproducibility crisis. While solutions for standardized reporting have been
proposed to address the issue, we still lack a benchmarking tool that enables
standardization and reproducibility, while leveraging cutting-edge
Reinforcement Learning (RL) implementations. In this paper, we introduce
BenchMARL, the first MARL training library created to enable standardized
benchmarking across different algorithms, models, and environments. BenchMARL
uses TorchRL as its backend, granting it high performance and maintained
state-of-the-art implementations while addressing the broad community of MARL
PyTorch users. Its design enables systematic configuration and reporting, thus
allowing users to create and run complex benchmarks from simple one-line
inputs. BenchMARL is open-sourced on GitHub:
https://github.com/facebookresearch/BenchMARL
",0
Pearl: A Production-ready Reinforcement Learning Agent,"Zheqing Zhu, Rodrigo de Salvo Braz, Jalaj Bhandari, Daniel Jiang, Yi Wan, Yonathan Efroni, Liyuan Wang, Ruiyang Xu, Hongbo Guo, Alex Nikulkov, Dmytro Korenkevych, Urun Dogan, Frank Cheng, Zheng Wu, Wanqiao Xu",2023-12-06T18:29:23Z,Reinforcement Learning,"  Reinforcement learning (RL) is a versatile framework for optimizing long-term
goals. Although many real-world problems can be formalized with RL, learning
and deploying a performant RL policy requires a system designed to address
several important challenges, including the exploration-exploitation dilemma,
partial observability, dynamic action spaces, and safety concerns. While the
importance of these challenges has been well recognized, existing open-source
RL libraries do not explicitly address them. This paper introduces Pearl, a
Production-Ready RL software package designed to embrace these challenges in a
modular way. In addition to presenting benchmarking results, we also highlight
examples of Pearl's ongoing industry adoption to demonstrate its advantages for
production use cases. Pearl is open sourced on GitHub at
github.com/facebookresearch/pearl and its official website is
pearlagent.github.io.
",0
Molecular Autonomous Pathfinder using Deep Reinforcement Learning,"Ken-ichi Nomura, Ankit Mishra, Tian Sang, Rajiv K. Kalia, Aiichiro Nakano, Priya Vashishta",2023-12-09T03:09:20Z,Reinforcement Learning,"  Diffusion in solids is a slow process that dictates rate-limiting processes
in key chemical reactions. Unlike crystalline solids that offer well-defined
diffusion pathways, the lack of similar structural motifs in amorphous or
glassy materials poses a great scientific challenge in estimating slow
diffusion time. To tackle this problem, we have developed an AI-guided
long-time atomistic simulation approach: Molecular Autonomous Pathfinder (MAP)
framework based on Deep Reinforcement Learning (RL), where RL agent is trained
to uncover energy efficient diffusion pathways. We employ Deep Q-Network
architecture with distributed prioritized replay buffer enabling fully online
agent training with accelerated experience sampling by an ensemble of
asynchronous agents. After training, the agents provide atomistic
configurations of diffusion pathways with their energy profile. We use a
piecewise Nudged Elastic Band to refine the energy profile of the obtained
pathway and corresponding diffusion time on the basis of transition state
theory. With MAP, we have successfully identified atomistic mechanisms along
molecular diffusion pathways in amorphous silica, with time scales comparable
to experiments.
",0
The Generalization Gap in Offline Reinforcement Learning,"Ishita Mediratta, Qingfei You, Minqi Jiang, Roberta Raileanu",2023-12-10T03:40:52Z,Reinforcement Learning,"  Despite recent progress in offline learning, these methods are still trained
and tested on the same environment. In this paper, we compare the
generalization abilities of widely used online and offline learning methods
such as online reinforcement learning (RL), offline RL, sequence modeling, and
behavioral cloning. Our experiments show that offline learning algorithms
perform worse on new environments than online learning ones. We also introduce
the first benchmark for evaluating generalization in offline learning,
collecting datasets of varying sizes and skill-levels from Procgen (2D video
games) and WebShop (e-commerce websites). The datasets contain trajectories for
a limited number of game levels or natural language instructions and at test
time, the agent has to generalize to new levels or instructions. Our
experiments reveal that existing offline learning algorithms struggle to match
the performance of online RL on both train and test environments. Behavioral
cloning is a strong baseline, outperforming state-of-the-art offline RL and
sequence modeling approaches when trained on data from multiple environments
and tested on new ones. Finally, we find that increasing the diversity of the
data, rather than its size, improves performance on new environments for all
offline learning algorithms. Our study demonstrates the limited generalization
of current offline learning algorithms highlighting the need for more research
in this area.
",0
Spreeze: High-Throughput Parallel Reinforcement Learning Framework,"Jing Hou, Guang Chen, Ruiqi Zhang, Zhijun Li, Shangding Gu, Changjun Jiang",2023-12-11T05:25:01Z,Reinforcement Learning,"  The promotion of large-scale applications of reinforcement learning (RL)
requires efficient training computation. While existing parallel RL frameworks
encompass a variety of RL algorithms and parallelization techniques, the
excessively burdensome communication frameworks hinder the attainment of the
hardware's limit for final throughput and training effects on a single desktop.
In this paper, we propose Spreeze, a lightweight parallel framework for RL that
efficiently utilizes a single desktop hardware resource to approach the
throughput limit. We asynchronously parallelize the experience sampling,
network update, performance evaluation, and visualization operations, and
employ multiple efficient data transmission techniques to transfer various
types of data between processes. The framework can automatically adjust the
parallelization hyperparameters based on the computing ability of the hardware
device in order to perform efficient large-batch updates. Based on the
characteristics of the ""Actor-Critic"" RL algorithm, our framework uses dual
GPUs to independently update the network of actors and critics in order to
further improve throughput. Simulation results show that our framework can
achieve up to 15,000Hz experience sampling and 370,000Hz network update frame
rate using only a personal desktop computer, which is an order of magnitude
higher than other mainstream parallel RL frameworks, resulting in a 73%
reduction of training time. Our work on fully utilizing the hardware resources
of a single desktop computer is fundamental to enabling efficient large-scale
distributed RL training.
",0
Reward Certification for Policy Smoothed Reinforcement Learning,"Ronghui Mu, Leandro Soriano Marcolino, Tianle Zhang, Yanghao Zhang, Xiaowei Huang, Wenjie Ruan",2023-12-11T15:07:58Z,Reinforcement Learning,"  Reinforcement Learning (RL) has achieved remarkable success in
safety-critical areas, but it can be weakened by adversarial attacks. Recent
studies have introduced ""smoothed policies"" in order to enhance its robustness.
Yet, it is still challenging to establish a provable guarantee to certify the
bound of its total reward. Prior methods relied primarily on computing bounds
using Lipschitz continuity or calculating the probability of cumulative reward
above specific thresholds. However, these techniques are only suited for
continuous perturbations on the RL agent's observations and are restricted to
perturbations bounded by the $l_2$-norm. To address these limitations, this
paper proposes a general black-box certification method capable of directly
certifying the cumulative reward of the smoothed policy under various
$l_p$-norm bounded perturbations. Furthermore, we extend our methodology to
certify perturbations on action spaces. Our approach leverages f-divergence to
measure the distinction between the original distribution and the perturbed
distribution, subsequently determining the certification bound by solving a
convex optimisation problem. We provide a comprehensive theoretical analysis
and run sufficient experiments in multiple environments. Our results show that
our method not only improves the certified lower bound of mean cumulative
reward but also demonstrates better efficiency than state-of-the-art
techniques.
",0
Evolving Reservoirs for Meta Reinforcement Learning,"Corentin Léger, Gautier Hamon, Eleni Nisioti, Xavier Hinaut, Clément Moulin-Frier",2023-12-09T16:11:48Z,Reinforcement Learning,"  Animals often demonstrate a remarkable ability to adapt to their environments
during their lifetime. They do so partly due to the evolution of morphological
and neural structures. These structures capture features of environments shared
between generations to bias and speed up lifetime learning. In this work, we
propose a computational model for studying a mechanism that can enable such a
process. We adopt a computational framework based on meta reinforcement
learning as a model of the interplay between evolution and development. At the
evolutionary scale, we evolve reservoirs, a family of recurrent neural networks
that differ from conventional networks in that one optimizes not the synaptic
weights, but hyperparameters controlling macro-level properties of the
resulting network architecture. At the developmental scale, we employ these
evolved reservoirs to facilitate the learning of a behavioral policy through
Reinforcement Learning (RL). Within an RL agent, a reservoir encodes the
environment state before providing it to an action policy. We evaluate our
approach on several 2D and 3D simulated environments. Our results show that the
evolution of reservoirs can improve the learning of diverse challenging tasks.
We study in particular three hypotheses: the use of an architecture combining
reservoirs and reinforcement learning could enable (1) solving tasks with
partial observability, (2) generating oscillatory dynamics that facilitate the
learning of locomotion tasks, and (3) facilitating the generalization of
learned behaviors to new tasks unknown during the evolution phase.
",0
An Invitation to Deep Reinforcement Learning,"Bernhard Jaeger, Andreas Geiger",2023-12-13T18:57:23Z,Reinforcement Learning,"  Training a deep neural network to maximize a target objective has become the
standard recipe for successful machine learning over the last decade. These
networks can be optimized with supervised learning, if the target objective is
differentiable. For many interesting problems, this is however not the case.
Common objectives like intersection over union (IoU), bilingual evaluation
understudy (BLEU) score or rewards cannot be optimized with supervised
learning. A common workaround is to define differentiable surrogate losses,
leading to suboptimal solutions with respect to the actual objective.
Reinforcement learning (RL) has emerged as a promising alternative for
optimizing deep neural networks to maximize non-differentiable objectives in
recent years. Examples include aligning large language models via human
feedback, code generation, object detection or control problems. This makes RL
techniques relevant to the larger machine learning audience. The subject is,
however, time intensive to approach due to the large range of methods, as well
as the often very theoretical presentation. In this introduction, we take an
alternative approach, different from classic reinforcement learning textbooks.
Rather than focusing on tabular problems, we introduce reinforcement learning
as a generalization of supervised learning, which we first apply to
non-differentiable objectives and later to temporal problems. Assuming only
basic knowledge of supervised learning, the reader will be able to understand
state-of-the-art deep RL algorithms like proximal policy optimization (PPO)
after reading this tutorial.
",0
Personalized Path Recourse for Reinforcement Learning Agents,"Dat Hong, Tong Wang",2023-12-14T08:10:57Z,Reinforcement Learning,"  This paper introduces Personalized Path Recourse, a novel method that
generates recourse paths for a reinforcement learning agent. The goal is to
edit a given path of actions to achieve desired goals (e.g., better outcomes
compared to the agent's original path) while ensuring a high similarity to the
agent's original paths and being personalized to the agent. Personalization
refers to the extent to which the new path is tailored to the agent's observed
behavior patterns from their policy function. We train a personalized recourse
agent to generate such personalized paths, which are obtained using reward
functions that consider the goal, similarity, and personalization. The proposed
method is applicable to both reinforcement learning and supervised learning
settings for correcting or improving sequences of actions or sequences of data
to achieve a pre-determined goal. The method is evaluated in various settings.
Experiments show that our model not only recourses for a better outcome but
also adapts to different agents' behavior.
",0
Multi-agent Reinforcement Learning: A Comprehensive Survey,"Dom Huh, Prasant Mohapatra",2023-12-15T23:16:54Z,Reinforcement Learning,"  Multi-agent systems (MAS) are widely prevalent and crucially important in
numerous real-world applications, where multiple agents must make decisions to
achieve their objectives in a shared environment. Despite their ubiquity, the
development of intelligent decision-making agents in MAS poses several open
challenges to their effective implementation. This survey examines these
challenges, placing an emphasis on studying seminal concepts from game theory
(GT) and machine learning (ML) and connecting them to recent advancements in
multi-agent reinforcement learning (MARL), i.e. the research of data-driven
decision-making within MAS. Therefore, the objective of this survey is to
provide a comprehensive perspective along the various dimensions of MARL,
shedding light on the unique opportunities that are presented in MARL
applications while highlighting the inherent challenges that accompany this
potential. Therefore, we hope that our work will not only contribute to the
field by analyzing the current landscape of MARL but also motivate future
directions with insights for deeper integration of concepts from related
domains of GT and ML. With this in mind, this work delves into a detailed
exploration of recent and past efforts of MARL and its related fields and
describes prior solutions that were proposed and their limitations, as well as
their applications.
",0
Active Reinforcement Learning for Robust Building Control,"Doseok Jang, Larry Yan, Lucas Spangher, Costas Spanos",2023-12-16T02:18:45Z,Reinforcement Learning,"  Reinforcement learning (RL) is a powerful tool for optimal control that has
found great success in Atari games, the game of Go, robotic control, and
building optimization. RL is also very brittle; agents often overfit to their
training environment and fail to generalize to new settings. Unsupervised
environment design (UED) has been proposed as a solution to this problem, in
which the agent trains in environments that have been specially selected to
help it learn. Previous UED algorithms focus on trying to train an RL agent
that generalizes across a large distribution of environments. This is not
necessarily desirable when we wish to prioritize performance in one environment
over others. In this work, we will be examining the setting of robust RL
building control, where we wish to train an RL agent that prioritizes
performing well in normal weather while still being robust to extreme weather
conditions. We demonstrate a novel UED algorithm, ActivePLR, that uses
uncertainty-aware neural network architectures to generate new training
environments at the limit of the RL agent's ability while being able to
prioritize performance in a desired base environment. We show that ActivePLR is
able to outperform state-of-the-art UED algorithms in minimizing energy usage
while maximizing occupant comfort in the setting of building control.
",0
Advancing RAN Slicing with Offline Reinforcement Learning,"Kun Yang, Shu-ping Yeh, Menglei Zhang, Jerry Sydir, Jing Yang, Cong Shen",2023-12-16T22:09:50Z,Reinforcement Learning,"  Dynamic radio resource management (RRM) in wireless networks presents
significant challenges, particularly in the context of Radio Access Network
(RAN) slicing. This technology, crucial for catering to varying user
requirements, often grapples with complex optimization scenarios. Existing
Reinforcement Learning (RL) approaches, while achieving good performance in RAN
slicing, typically rely on online algorithms or behavior cloning. These methods
necessitate either continuous environmental interactions or access to
high-quality datasets, hindering their practical deployment. Towards addressing
these limitations, this paper introduces offline RL to solving the RAN slicing
problem, marking a significant shift towards more feasible and adaptive RRM
methods. We demonstrate how offline RL can effectively learn near-optimal
policies from sub-optimal datasets, a notable advancement over existing
practices. Our research highlights the inherent flexibility of offline RL,
showcasing its ability to adjust policy criteria without the need for
additional environmental interactions. Furthermore, we present empirical
evidence of the efficacy of offline RL in adapting to various service-level
requirements, illustrating its potential in diverse RAN slicing scenarios.
",0
Prediction and Control in Continual Reinforcement Learning,"Nishanth Anand, Doina Precup",2023-12-18T19:23:42Z,Reinforcement Learning,"  Temporal difference (TD) learning is often used to update the estimate of the
value function which is used by RL agents to extract useful policies. In this
paper, we focus on value function estimation in continual reinforcement
learning. We propose to decompose the value function into two components which
update at different timescales: a permanent value function, which holds general
knowledge that persists over time, and a transient value function, which allows
quick adaptation to new situations. We establish theoretical results showing
that our approach is well suited for continual learning and draw connections to
the complementary learning systems (CLS) theory from neuroscience. Empirically,
this approach improves performance significantly on both prediction and control
problems.
",0
Optimizing Heat Alert Issuance with Reinforcement Learning,"Ellen M. Considine, Rachel C. Nethery, Gregory A. Wellenius, Francesca Dominici, Mauricio Tec",2023-12-21T00:50:21Z,Reinforcement Learning,"  A key strategy in societal adaptation to climate change is the use of alert
systems to reduce the adverse health impacts of extreme heat events by
prompting preventative action. In this work, we investigate reinforcement
learning (RL) as a tool to optimize the effectiveness of such systems. Our
contributions are threefold. First, we introduce a novel RL environment
enabling the evaluation of the effectiveness of heat alert policies to reduce
heat-related hospitalizations. The rewards model is trained from a
comprehensive dataset of historical weather, Medicare health records, and
socioeconomic/geographic features. We use variational Bayesian techniques to
address low-signal effects and spatial heterogeneity, which are commonly
encountered in climate & health settings. The transition model incorporates
real historical weather patterns enriched by a data augmentation mechanism
based on climate region similarity. Second, we use this environment to evaluate
standard RL algorithms in the context of heat alert issuance. Our analysis
shows that policy constraints are needed to improve the initially poor
performance of RL. Lastly, a post hoc contrastive analysis provides insight
into scenarios where our modified heat alert-RL policies yield significant
gains/losses over the current National Weather Service alert policy in the
United States.
",0
Deep Reinforcement Learning for Quantitative Trading,"Maochun Xu, Zixun Lan, Zheng Tao, Jiawei Du, Zongao Ye",2023-12-25T13:58:53Z,Reinforcement Learning,"  Artificial Intelligence (AI) and Machine Learning (ML) are transforming the
domain of Quantitative Trading (QT) through the deployment of advanced
algorithms capable of sifting through extensive financial datasets to pinpoint
lucrative investment openings. AI-driven models, particularly those employing
ML techniques such as deep learning and reinforcement learning, have shown
great prowess in predicting market trends and executing trades at a speed and
accuracy that far surpass human capabilities. Its capacity to automate critical
tasks, such as discerning market conditions and executing trading strategies,
has been pivotal. However, persistent challenges exist in current QT methods,
especially in effectively handling noisy and high-frequency financial data.
Striking a balance between exploration and exploitation poses another challenge
for AI-driven trading agents. To surmount these hurdles, our proposed solution,
QTNet, introduces an adaptive trading model that autonomously formulates QT
strategies through an intelligent trading agent. Incorporating deep
reinforcement learning (DRL) with imitative learning methodologies, we bolster
the proficiency of our model. To tackle the challenges posed by volatile
financial datasets, we conceptualize the QT mechanism within the framework of a
Partially Observable Markov Decision Process (POMDP). Moreover, by embedding
imitative learning, the model can capitalize on traditional trading tactics,
nurturing a balanced synergy between discovery and utilization. For a more
realistic simulation, our trading agent undergoes training using
minute-frequency data sourced from the live financial market. Experimental
findings underscore the model's proficiency in extracting robust market
features and its adaptability to diverse market conditions.
",0
OpenRL: A Unified Reinforcement Learning Framework,"Shiyu Huang, Wentse Chen, Yiwen Sun, Fuqing Bie, Wei-Wei Tu",2023-12-20T12:04:06Z,Reinforcement Learning,"  We present OpenRL, an advanced reinforcement learning (RL) framework designed
to accommodate a diverse array of tasks, from single-agent challenges to
complex multi-agent systems. OpenRL's robust support for self-play training
empowers agents to develop advanced strategies in competitive settings.
Notably, OpenRL integrates Natural Language Processing (NLP) with RL, enabling
researchers to address a combination of RL training and language-centric tasks
effectively. Leveraging PyTorch's robust capabilities, OpenRL exemplifies
modularity and a user-centric approach. It offers a universal interface that
simplifies the user experience for beginners while maintaining the flexibility
experts require for innovation and algorithm development. This equilibrium
enhances the framework's practicality, adaptability, and scalability,
establishing a new standard in RL research. To delve into OpenRL's features, we
invite researchers and enthusiasts to explore our GitHub repository at
https://github.com/OpenRL-Lab/openrl and access our comprehensive documentation
at https://openrl-docs.readthedocs.io.
",0
Causal State Distillation for Explainable Reinforcement Learning,"Wenhao Lu, Xufeng Zhao, Thilo Fryen, Jae Hee Lee, Mengdi Li, Sven Magg, Stefan Wermter",2023-12-30T00:01:22Z,Reinforcement Learning,"  Reinforcement learning (RL) is a powerful technique for training intelligent
agents, but understanding why these agents make specific decisions can be quite
challenging. This lack of transparency in RL models has been a long-standing
problem, making it difficult for users to grasp the reasons behind an agent's
behaviour. Various approaches have been explored to address this problem, with
one promising avenue being reward decomposition (RD). RD is appealing as it
sidesteps some of the concerns associated with other methods that attempt to
rationalize an agent's behaviour in a post-hoc manner. RD works by exposing
various facets of the rewards that contribute to the agent's objectives during
training. However, RD alone has limitations as it primarily offers insights
based on sub-rewards and does not delve into the intricate cause-and-effect
relationships that occur within an RL agent's neural model. In this paper, we
present an extension of RD that goes beyond sub-rewards to provide more
informative explanations. Our approach is centred on a causal learning
framework that leverages information-theoretic measures for explanation
objectives that encourage three crucial properties of causal factors: causal
sufficiency, sparseness, and orthogonality. These properties help us distill
the cause-and-effect relationships between the agent's states and actions or
rewards, allowing for a deeper understanding of its decision-making processes.
Our framework is designed to generate local explanations and can be applied to
a wide range of RL tasks with multiple reward channels. Through a series of
experiments, we demonstrate that our approach offers more meaningful and
insightful explanations for the agent's action selections.
",0
Policy-regularized Offline Multi-objective Reinforcement Learning,"Qian Lin, Chao Yu, Zongkai Liu, Zifan Wu",2024-01-04T12:54:10Z,Reinforcement Learning,"  In this paper, we aim to utilize only offline trajectory data to train a
policy for multi-objective RL. We extend the offline policy-regularized method,
a widely-adopted approach for single-objective offline RL problems, into the
multi-objective setting in order to achieve the above goal. However, such
methods face a new challenge in offline MORL settings, namely the
preference-inconsistent demonstration problem. We propose two solutions to this
problem: 1) filtering out preference-inconsistent demonstrations via
approximating behavior preferences, and 2) adopting regularization techniques
with high policy expressiveness. Moreover, we integrate the
preference-conditioned scalarized update method into policy-regularized offline
RL, in order to simultaneously learn a set of policies using a single policy
network, thus reducing the computational cost induced by the training of a
large number of individual policies for various preferences. Finally, we
introduce Regularization Weight Adaptation to dynamically determine appropriate
regularization weights for arbitrary target preferences during deployment.
Empirical results on various multi-objective datasets demonstrate the
capability of our approach in solving offline MORL problems.
",0
Inverse Reinforcement Learning with Sub-optimal Experts,"Riccardo Poiani, Gabriele Curti, Alberto Maria Metelli, Marcello Restelli",2024-01-08T12:39:25Z,Reinforcement Learning,"  Inverse Reinforcement Learning (IRL) techniques deal with the problem of
deducing a reward function that explains the behavior of an expert agent who is
assumed to act optimally in an underlying unknown task. In several problems of
interest, however, it is possible to observe the behavior of multiple experts
with different degree of optimality (e.g., racing drivers whose skills ranges
from amateurs to professionals). For this reason, in this work, we extend the
IRL formulation to problems where, in addition to demonstrations from the
optimal agent, we can observe the behavior of multiple sub-optimal experts.
Given this problem, we first study the theoretical properties of the class of
reward functions that are compatible with a given set of experts, i.e., the
feasible reward set. Our results show that the presence of multiple sub-optimal
experts can significantly shrink the set of compatible rewards. Furthermore, we
study the statistical complexity of estimating the feasible reward set with a
generative model. To this end, we analyze a uniform sampling algorithm that
results in being minimax optimal whenever the sub-optimal experts' performance
level is sufficiently close to the one of the optimal agent.
",0
Safe reinforcement learning in uncertain contexts,"Dominik Baumann, Thomas B. Schön",2024-01-11T12:35:36Z,Reinforcement Learning,"  When deploying machine learning algorithms in the real world, guaranteeing
safety is an essential asset. Existing safe learning approaches typically
consider continuous variables, i.e., regression tasks. However, in practice,
robotic systems are also subject to discrete, external environmental changes,
e.g., having to carry objects of certain weights or operating on frozen, wet,
or dry surfaces. Such influences can be modeled as discrete context variables.
In the existing literature, such contexts are, if considered, mostly assumed to
be known. In this work, we drop this assumption and show how we can perform
safe learning when we cannot directly measure the context variables. To achieve
this, we derive frequentist guarantees for multi-class classification, allowing
us to estimate the current context from measurements. Further, we propose an
approach for identifying contexts through experiments. We discuss under which
conditions we can retain theoretical guarantees and demonstrate the
applicability of our algorithm on a Furuta pendulum with camera measurements of
different weights that serve as contexts.
",0
Quantum Advantage Actor-Critic for Reinforcement Learning,"Michael Kölle, Mohamad Hgog, Fabian Ritz, Philipp Altmann, Maximilian Zorn, Jonas Stein, Claudia Linnhoff-Popien",2024-01-13T11:08:45Z,Reinforcement Learning,"  Quantum computing offers efficient encapsulation of high-dimensional states.
In this work, we propose a novel quantum reinforcement learning approach that
combines the Advantage Actor-Critic algorithm with variational quantum circuits
by substituting parts of the classical components. This approach addresses
reinforcement learning's scalability concerns while maintaining high
performance. We empirically test multiple quantum Advantage Actor-Critic
configurations with the well known Cart Pole environment to evaluate our
approach in control tasks with continuous state spaces. Our results indicate
that the hybrid strategy of using either a quantum actor or quantum critic with
classical post-processing yields a substantial performance increase compared to
pure classical and pure quantum variants with similar parameter counts. They
further reveal the limits of current quantum approaches due to the hardware
constraints of noisy intermediate-scale quantum computers, suggesting further
research to scale hybrid approaches for larger and more complex control tasks.
",0
PRewrite: Prompt Rewriting with Reinforcement Learning,"Weize Kong, Spurthi Amba Hombaiah, Mingyang Zhang, Qiaozhu Mei, Michael Bendersky",2024-01-16T08:04:50Z,Reinforcement Learning,"  Prompt engineering is critical for the development of LLM-based applications.
However, it is usually done manually in a ""trial and error"" fashion that can be
time consuming, ineffective, and sub-optimal. Even for the prompts which
seemingly work well, there is always a lingering question: can the prompts be
made better with further modifications?
  To address these problems, we investigate automated prompt engineering in
this paper. Specifically, we propose PRewrite, an automated method to rewrite
an under-optimized prompt to a more effective prompt. We instantiate the prompt
rewriter using a LLM. The rewriter LLM is trained using reinforcement learning
to optimize the performance on a given downstream task. We conduct experiments
on diverse benchmark datasets, which demonstrates the effectiveness of
PRewrite.
",0
Deployable Reinforcement Learning with Variable Control Rate,"Dong Wang, Giovanni Beltrame",2024-01-17T15:40:11Z,Reinforcement Learning,"  Deploying controllers trained with Reinforcement Learning (RL) on real robots
can be challenging: RL relies on agents' policies being modeled as Markov
Decision Processes (MDPs), which assume an inherently discrete passage of time.
The use of MDPs results in that nearly all RL-based control systems employ a
fixed-rate control strategy with a period (or time step) typically chosen based
on the developer's experience or specific characteristics of the application
environment. Unfortunately, the system should be controlled at the highest,
worst-case frequency to ensure stability, which can demand significant
computational and energy resources and hinder the deployability of the
controller on onboard hardware. Adhering to the principles of reactive
programming, we surmise that applying control actions only when necessary
enables the use of simpler hardware and helps reduce energy consumption. We
challenge the fixed frequency assumption by proposing a variant of RL with
variable control rate. In this approach, the policy decides the action the
agent should take as well as the duration of the time step associated with that
action. In our new setting, we expand Soft Actor-Critic (SAC) to compute the
optimal policy with a variable control rate, introducing the Soft Elastic
Actor-Critic (SEAC) algorithm. We show the efficacy of SEAC through a
proof-of-concept simulation driving an agent with Newtonian kinematics. Our
experiments show higher average returns, shorter task completion times, and
reduced computational resources when compared to fixed rate policies.
",0
Harnessing Density Ratios for Online Reinforcement Learning,"Philip Amortila, Dylan J. Foster, Nan Jiang, Ayush Sekhari, Tengyang Xie",2024-01-18T02:21:06Z,Reinforcement Learning,"  The theories of offline and online reinforcement learning, despite having
evolved in parallel, have begun to show signs of the possibility for a
unification, with algorithms and analysis techniques for one setting often
having natural counterparts in the other. However, the notion of density ratio
modeling, an emerging paradigm in offline RL, has been largely absent from
online RL, perhaps for good reason: the very existence and boundedness of
density ratios relies on access to an exploratory dataset with good coverage,
but the core challenge in online RL is to collect such a dataset without having
one to start. In this work we show -- perhaps surprisingly -- that density
ratio-based algorithms have online counterparts. Assuming only the existence of
an exploratory distribution with good coverage, a structural condition known as
coverability (Xie et al., 2023), we give a new algorithm (GLOW) that uses
density ratio realizability and value function realizability to perform
sample-efficient online exploration. GLOW addresses unbounded density ratios
via careful use of truncation, and combines this with optimism to guide
exploration. GLOW is computationally inefficient; we complement it with a more
efficient counterpart, HyGLOW, for the Hybrid RL setting (Song et al., 2022)
wherein online RL is augmented with additional offline data. HyGLOW is derived
as a special case of a more general meta-algorithm that provides a provable
black-box reduction from hybrid RL to offline RL, which may be of independent
interest.
",0
A HPC Co-Scheduler with Reinforcement Learning,"Abel Souza, Kristiaan Pelckmans, Johan Tordsson",2024-01-18T03:32:10Z,Reinforcement Learning,"  Although High Performance Computing (HPC) users understand basic resource
requirements such as the number of CPUs and memory limits, internal
infrastructural utilization data is exclusively leveraged by cluster operators,
who use it to configure batch schedulers. This task is challenging and
increasingly complex due to ever larger cluster scales and heterogeneity of
modern scientific workflows. As a result, HPC systems achieve low utilization
with long job completion times (makespans). To tackle these challenges, we
propose a co-scheduling algorithm based on an adaptive reinforcement learning
algorithm, where application profiling is combined with cluster monitoring. The
resulting cluster scheduler matches resource utilization to application
performance in a fine-grained manner (i.e., operating system level). As opposed
to nominal allocations, we apply decision trees to model applications' actual
resource usage, which are used to estimate how much resource capacity from one
allocation can be co-allocated to additional applications. Our algorithm learns
from incorrect co-scheduling decisions and adapts from changing environment
conditions, and evaluates when such changes cause resource contention that
impacts quality of service metrics such as jobs slowdowns. We integrate our
algorithm in an HPC resource manager that combines Slurm and Mesos for job
scheduling and co-allocation, respectively. Our experimental evaluation
performed in a dedicated cluster executing a mix of four real different
scientific workflows demonstrates improvements on cluster utilization of up to
51% even in high load scenarios, with 55% average queue makespan reductions
under low loads.
",0
Large-scale Reinforcement Learning for Diffusion Models,"Yinan Zhang, Eric Tzeng, Yilun Du, Dmitry Kislyuk",2024-01-20T08:10:43Z,Reinforcement Learning,"  Text-to-image diffusion models are a class of deep generative models that
have demonstrated an impressive capacity for high-quality image generation.
However, these models are susceptible to implicit biases that arise from
web-scale text-image training pairs and may inaccurately model aspects of
images we care about. This can result in suboptimal samples, model bias, and
images that do not align with human ethics and preferences. In this paper, we
present an effective scalable algorithm to improve diffusion models using
Reinforcement Learning (RL) across a diverse set of reward functions, such as
human preference, compositionality, and fairness over millions of images. We
illustrate how our approach substantially outperforms existing methods for
aligning diffusion models with human preferences. We further illustrate how
this substantially improves pretrained Stable Diffusion (SD) models, generating
samples that are preferred by humans 80.3% of the time over those from the base
SD model while simultaneously improving both the composition and diversity of
generated samples.
",0
Emergent Dominance Hierarchies in Reinforcement Learning Agents,"Ram Rachum, Yonatan Nakar, Bill Tomlinson, Nitay Alon, Reuth Mirsky",2024-01-21T16:59:45Z,Reinforcement Learning,"  Modern Reinforcement Learning (RL) algorithms are able to outperform humans
in a wide variety of tasks. Multi-agent reinforcement learning (MARL) settings
present additional challenges, and successful cooperation in mixed-motive
groups of agents depends on a delicate balancing act between individual and
group objectives. Social conventions and norms, often inspired by human
institutions, are used as tools for striking this balance.
  In this paper, we examine a fundamental, well-studied social convention that
underlies cooperation in both animal and human societies: dominance
hierarchies.
  We adapt the ethological theory of dominance hierarchies to artificial
agents, borrowing the established terminology and definitions with as few
amendments as possible. We demonstrate that populations of RL agents, operating
without explicit programming or intrinsic rewards, can invent, learn, enforce,
and transmit a dominance hierarchy to new populations. The dominance
hierarchies that emerge have a similar structure to those studied in chickens,
mice, fish, and other species.
",0
Reward-Relevance-Filtered Linear Offline Reinforcement Learning,Angela Zhou,2024-01-23T17:42:17Z,Reinforcement Learning,"  This paper studies offline reinforcement learning with linear function
approximation in a setting with decision-theoretic, but not estimation
sparsity. The structural restrictions of the data-generating process presume
that the transitions factor into a sparse component that affects the reward and
could affect additional exogenous dynamics that do not affect the reward.
Although the minimally sufficient adjustment set for estimation of full-state
transition properties depends on the whole state, the optimal policy and
therefore state-action value function depends only on the sparse component: we
call this causal/decision-theoretic sparsity. We develop a method for
reward-filtering the estimation of the state-action value function to the
sparse component by a modification of thresholded lasso in least-squares policy
evaluation. We provide theoretical guarantees for our reward-filtered linear
fitted-Q-iteration, with sample complexity depending only on the size of the
sparse component.
",0
Symbolic Equation Solving via Reinforcement Learning,"Lennart Dabelow, Masahito Ueda",2024-01-24T13:42:24Z,Reinforcement Learning,"  Machine-learning methods are gradually being adopted in a great variety of
social, economic, and scientific contexts, yet they are notorious for
struggling with exact mathematics. A typical example is computer algebra, which
includes tasks like simplifying mathematical terms, calculating formal
derivatives, or finding exact solutions of algebraic equations. Traditional
software packages for these purposes are commonly based on a huge database of
rules for how a specific operation (e.g., differentiation) transforms a certain
term (e.g., sine function) into another one (e.g., cosine function). Thus far,
these rules have usually needed to be discovered and subsequently programmed by
humans. Focusing on the paradigmatic example of solving linear equations in
symbolic form, we demonstrate how the process of finding elementary
transformation rules and step-by-step solutions can be automated using
reinforcement learning with deep neural networks.
",0
Off-Policy Primal-Dual Safe Reinforcement Learning,"Zifan Wu, Bo Tang, Qian Lin, Chao Yu, Shangqin Mao, Qianlong Xie, Xingxing Wang, Dong Wang",2024-01-26T10:33:38Z,Reinforcement Learning,"  Primal-dual safe RL methods commonly perform iterations between the primal
update of the policy and the dual update of the Lagrange Multiplier. Such a
training paradigm is highly susceptible to the error in cumulative cost
estimation since this estimation serves as the key bond connecting the primal
and dual update processes. We show that this problem causes significant
underestimation of cost when using off-policy methods, leading to the failure
to satisfy the safety constraint. To address this issue, we propose
conservative policy optimization, which learns a policy in a
constraint-satisfying area by considering the uncertainty in cost estimation.
This improves constraint satisfaction but also potentially hinders reward
maximization. We then introduce local policy convexification to help eliminate
such suboptimality by gradually reducing the estimation uncertainty. We provide
theoretical interpretations of the joint coupling effect of these two
ingredients and further verify them by extensive experiments. Results on
benchmark tasks show that our method not only achieves an asymptotic
performance comparable to state-of-the-art on-policy methods while using much
fewer samples, but also significantly reduces constraint violation during
training. Our code is available at https://github.com/ZifanWu/CAL.
",0
Zero-Shot Reinforcement Learning via Function Encoders,"Tyler Ingebrand, Amy Zhang, Ufuk Topcu",2024-01-30T17:04:47Z,Reinforcement Learning,"  Although reinforcement learning (RL) can solve many challenging sequential
decision making problems, achieving zero-shot transfer across related tasks
remains a challenge. The difficulty lies in finding a good representation for
the current task so that the agent understands how it relates to previously
seen tasks. To achieve zero-shot transfer, we introduce the function encoder, a
representation learning algorithm which represents a function as a weighted
combination of learned, non-linear basis functions. By using a function encoder
to represent the reward function or the transition function, the agent has
information on how the current task relates to previously seen tasks via a
coherent vector representation. Thus, the agent is able to achieve transfer
between related tasks at run time with no additional training. We demonstrate
state-of-the-art data efficiency, asymptotic performance, and training
stability in three RL fields by augmenting basic RL algorithms with a function
encoder task representation.
",0
Accelerating Inverse Reinforcement Learning with Expert Bootstrapping,"David Wu, Sanjiban Choudhury",2024-02-04T20:49:53Z,Reinforcement Learning,"  Existing inverse reinforcement learning methods (e.g. MaxEntIRL, $f$-IRL)
search over candidate reward functions and solve a reinforcement learning
problem in the inner loop. This creates a rather strange inversion where a
harder problem, reinforcement learning, is in the inner loop of a presumably
easier problem, imitation learning. In this work, we show that better
utilization of expert demonstrations can reduce the need for hard exploration
in the inner RL loop, hence accelerating learning. Specifically, we propose two
simple recipes: (1) placing expert transitions into the replay buffer of the
inner RL algorithm (e.g. Soft-Actor Critic) which directly informs the learner
about high reward states instead of forcing the learner to discover them
through extensive exploration, and (2) using expert actions in Q value
bootstrapping in order to improve the target Q value estimates and more
accurately describe high value expert states. Our methods show significant
gains over a MaxEntIRL baseline on the benchmark MuJoCo suite of tasks,
speeding up recovery to 70\% of deterministic expert performance by 2.13x on
HalfCheetah-v2, 2.6x on Ant-v2, 18x on Hopper-v2, and 3.36x on Walker2d-v2.
",0
No-Regret Reinforcement Learning in Smooth MDPs,"Davide Maran, Alberto Maria Metelli, Matteo Papini, Marcello Restell",2024-02-06T08:18:14Z,Reinforcement Learning,"  Obtaining no-regret guarantees for reinforcement learning (RL) in the case of
problems with continuous state and/or action spaces is still one of the major
open challenges in the field. Recently, a variety of solutions have been
proposed, but besides very specific settings, the general problem remains
unsolved. In this paper, we introduce a novel structural assumption on the
Markov decision processes (MDPs), namely $\nu-$smoothness, that generalizes
most of the settings proposed so far (e.g., linear MDPs and Lipschitz MDPs). To
face this challenging scenario, we propose two algorithms for regret
minimization in $\nu-$smooth MDPs. Both algorithms build upon the idea of
constructing an MDP representation through an orthogonal feature map based on
Legendre polynomials. The first algorithm, \textsc{Legendre-Eleanor}, archives
the no-regret property under weaker assumptions but is computationally
inefficient, whereas the second one, \textsc{Legendre-LSVI}, runs in polynomial
time, although for a smaller class of problems. After analyzing their regret
properties, we compare our results with state-of-the-art ones from RL theory,
showing that our algorithms achieve the best guarantees.
",0
Analyzing Adversarial Inputs in Deep Reinforcement Learning,"Davide Corsi, Guy Amir, Guy Katz, Alessandro Farinelli",2024-02-07T21:58:40Z,Reinforcement Learning,"  In recent years, Deep Reinforcement Learning (DRL) has become a popular
paradigm in machine learning due to its successful applications to real-world
and complex systems. However, even the state-of-the-art DRL models have been
shown to suffer from reliability concerns -- for example, their susceptibility
to adversarial inputs, i.e., small and abundant input perturbations that can
fool the models into making unpredictable and potentially dangerous decisions.
This drawback limits the deployment of DRL systems in safety-critical contexts,
where even a small error cannot be tolerated. In this work, we present a
comprehensive analysis of the characterization of adversarial inputs, through
the lens of formal verification. Specifically, we introduce a novel metric, the
Adversarial Rate, to classify models based on their susceptibility to such
perturbations, and present a set of tools and algorithms for its computation.
Our analysis empirically demonstrates how adversarial inputs can affect the
safety of a given DRL system with respect to such perturbations. Moreover, we
analyze the behavior of these configurations to suggest several useful
practices and guidelines to help mitigate the vulnerability of trained DRL
networks.
",0
Differentially Private Deep Model-Based Reinforcement Learning,"Alexandre Rio, Merwan Barlier, Igor Colin, Albert Thomas",2024-02-08T10:05:11Z,Reinforcement Learning,"  We address private deep offline reinforcement learning (RL), where the goal
is to train a policy on standard control tasks that is differentially private
(DP) with respect to individual trajectories in the dataset. To achieve this,
we introduce PriMORL, a model-based RL algorithm with formal differential
privacy guarantees. PriMORL first learns an ensemble of trajectory-level DP
models of the environment from offline data. It then optimizes a policy on the
penalized private model, without any further interaction with the system or
access to the dataset. In addition to offering strong theoretical foundations,
we demonstrate empirically that PriMORL enables the training of private RL
agents on offline continuous control tasks with deep function approximations,
whereas current methods are limited to simpler tabular and linear Markov
Decision Processes (MDPs). We furthermore outline the trade-offs involved in
achieving privacy in this setting.
",0
Discovering Temporally-Aware Reinforcement Learning Algorithms,"Matthew Thomas Jackson, Chris Lu, Louis Kirsch, Robert Tjarko Lange, Shimon Whiteson, Jakob Nicolaus Foerster",2024-02-08T17:07:42Z,Reinforcement Learning,"  Recent advancements in meta-learning have enabled the automatic discovery of
novel reinforcement learning algorithms parameterized by surrogate objective
functions. To improve upon manually designed algorithms, the parameterization
of this learned objective function must be expressive enough to represent novel
principles of learning (instead of merely recovering already established ones)
while still generalizing to a wide range of settings outside of its
meta-training distribution. However, existing methods focus on discovering
objective functions that, like many widely used objective functions in
reinforcement learning, do not take into account the total number of steps
allowed for training, or ""training horizon"". In contrast, humans use a plethora
of different learning objectives across the course of acquiring a new ability.
For instance, students may alter their studying techniques based on the
proximity to exam deadlines and their self-assessed capabilities. This paper
contends that ignoring the optimization time horizon significantly restricts
the expressive potential of discovered learning algorithms. We propose a simple
augmentation to two existing objective discovery approaches that allows the
discovered algorithm to dynamically update its objective function throughout
the agent's training procedure, resulting in expressive schedules and increased
generalization across different training horizons. In the process, we find that
commonly used meta-gradient approaches fail to discover such adaptive objective
functions while evolution strategies discover highly dynamic learning rules. We
demonstrate the effectiveness of our approach on a wide range of tasks and
analyze the resulting learned algorithms, which we find effectively balance
exploration and exploitation by modifying the structure of their learning rules
throughout the agent's lifetime.
",0
Informativeness of Reward Functions in Reinforcement Learning,"Rati Devidze, Parameswaran Kamalaruban, Adish Singla",2024-02-10T18:36:42Z,Reinforcement Learning,"  Reward functions are central in specifying the task we want a reinforcement
learning agent to perform. Given a task and desired optimal behavior, we study
the problem of designing informative reward functions so that the designed
rewards speed up the agent's convergence. In particular, we consider
expert-driven reward design settings where an expert or teacher seeks to
provide informative and interpretable rewards to a learning agent. Existing
works have considered several different reward design formulations; however,
the key challenge is formulating a reward informativeness criterion that adapts
w.r.t. the agent's current policy and can be optimized under specified
structural constraints to obtain interpretable rewards. In this paper, we
propose a novel reward informativeness criterion, a quantitative measure that
captures how the agent's current policy will improve if it receives rewards
from a specific reward function. We theoretically showcase the utility of the
proposed informativeness criterion for adaptively designing rewards for an
agent. Experimental results on two navigation tasks demonstrate the
effectiveness of our adaptive reward informativeness criterion.
",0
Reward Poisoning Attack Against Offline Reinforcement Learning,"Yinglun Xu, Rohan Gumaste, Gagandeep Singh",2024-02-15T04:08:49Z,Reinforcement Learning,"  We study the problem of reward poisoning attacks against general offline
reinforcement learning with deep neural networks for function approximation. We
consider a black-box threat model where the attacker is completely oblivious to
the learning algorithm and its budget is limited by constraining both the
amount of corruption at each data point, and the total perturbation. We propose
an attack strategy called `policy contrast attack'. The high-level idea is to
make some low-performing policies appear as high-performing while making
high-performing policies appear as low-performing. To the best of our
knowledge, we propose the first black-box reward poisoning attack in the
general offline RL setting. We provide theoretical insights on the attack
design and empirically show that our attack is efficient against current
state-of-the-art offline RL algorithms in different kinds of learning datasets.
",0
Performative Reinforcement Learning in Gradually Shifting Environments,"Ben Rank, Stelios Triantafyllou, Debmalya Mandal, Goran Radanovic",2024-02-15T10:00:13Z,Reinforcement Learning,"  When Reinforcement Learning (RL) agents are deployed in practice, they might
impact their environment and change its dynamics. We propose a new framework to
model this phenomenon, where the current environment depends on the deployed
policy as well as its previous dynamics. This is a generalization of
Performative RL (PRL) [Mandal et al., 2023]. Unlike PRL, our framework allows
to model scenarios where the environment gradually adjusts to a deployed
policy. We adapt two algorithms from the performative prediction literature to
our setting and propose a novel algorithm called Mixed Delayed Repeated
Retraining (MDRR). We provide conditions under which these algorithms converge
and compare them using three metrics: number of retrainings, approximation
guarantee, and number of samples per deployment. MDRR is the first algorithm in
this setting which combines samples from multiple deployments in its training.
This makes MDRR particularly suitable for scenarios where the environment's
response strongly depends on its previous dynamics, which are common in
practice. We experimentally compare the algorithms using a simulation-based
testbed and our results show that MDRR converges significantly faster than
previous approaches.
",0
Revisiting Recurrent Reinforcement Learning with Memory Monoids,"Steven Morad, Chris Lu, Ryan Kortvelesy, Stephan Liwicki, Jakob Foerster, Amanda Prorok",2024-02-15T11:56:53Z,Reinforcement Learning,"  Memory models such as Recurrent Neural Networks (RNNs) and Transformers
address Partially Observable Markov Decision Processes (POMDPs) by mapping
trajectories to latent Markov states. Neither model scales particularly well to
long sequences, especially compared to an emerging class of memory models
sometimes called linear recurrent models. We discover that we can model the
recurrent update of these models using a monoid, leading us to reformulate
existing models using a novel memory monoid framework. We revisit the
traditional approach to batching in recurrent RL, highlighting both theoretical
and empirical deficiencies. We leverage the properties of memory monoids to
propose a batching method that improves sample efficiency, increases the
return, and simplifies the implementation of recurrent loss functions in RL.
",0
Large Scale Constrained Clustering With Reinforcement Learning,"Benedikt Schesch, Marco Caserta",2024-02-15T18:27:18Z,Reinforcement Learning,"  Given a network, allocating resources at clusters level, rather than at each
node, enhances efficiency in resource allocation and usage. In this paper, we
study the problem of finding fully connected disjoint clusters to minimize the
intra-cluster distances and maximize the number of nodes assigned to the
clusters, while also ensuring that no two nodes within a cluster exceed a
threshold distance. While the problem can easily be formulated using a binary
linear model, traditional combinatorial optimization solvers struggle when
dealing with large-scale instances. We propose an approach to solve this
constrained clustering problem via reinforcement learning. Our method involves
training an agent to generate both feasible and (near) optimal solutions. The
agent learns problem-specific heuristics, tailored to the instances encountered
in this task. In the results section, we show that our algorithm finds near
optimal solutions, even for large scale instances.
",0
Theoretical foundations for programmatic reinforcement learning,"Guruprerana Shabadi, Nathanaël Fijalkow, Théo Matricon",2024-02-18T17:02:39Z,Reinforcement Learning,"  The field of Reinforcement Learning (RL) is concerned with algorithms for
learning optimal policies in unknown stochastic environments. Programmatic RL
studies representations of policies as programs, meaning involving higher order
constructs such as control loops. Despite attracting a lot of attention at the
intersection of the machine learning and formal methods communities, very
little is known on the theoretical front about programmatic RL: what are good
classes of programmatic policies? How large are optimal programmatic policies?
How can we learn them? The goal of this paper is to give first answers to these
questions, initiating a theoretical study of programmatic RL.
",0
Revisiting Data Augmentation in Deep Reinforcement Learning,"Jianshu Hu, Yunpeng Jiang, Paul Weng",2024-02-19T14:42:10Z,Reinforcement Learning,"  Various data augmentation techniques have been recently proposed in
image-based deep reinforcement learning (DRL). Although they empirically
demonstrate the effectiveness of data augmentation for improving sample
efficiency or generalization, which technique should be preferred is not always
clear. To tackle this question, we analyze existing methods to better
understand them and to uncover how they are connected. Notably, by expressing
the variance of the Q-targets and that of the empirical actor/critic losses of
these methods, we can analyze the effects of their different components and
compare them. We furthermore formulate an explanation about how these methods
may be affected by choosing different data augmentation transformations in
calculating the target Q-values. This analysis suggests recommendations on how
to exploit data augmentation in a more principled way. In addition, we include
a regularization term called tangent prop, previously proposed in computer
vision, but whose adaptation to DRL is novel to the best of our knowledge. We
evaluate our proposition and validate our analysis in several domains. Compared
to different relevant baselines, we demonstrate that it achieves
state-of-the-art performance in most environments and shows higher sample
efficiency and better generalization ability in some complex environments.
",0
Enhancing Reinforcement Learning Agents with Local Guides,"Paul Daoudi, Bogdan Robu, Christophe Prieur, Ludovic Dos Santos, Merwan Barlier",2024-02-21T16:52:26Z,Reinforcement Learning,"  This paper addresses the problem of integrating local guide policies into a
Reinforcement Learning agent. For this, we show how to adapt existing
algorithms to this setting before introducing a novel algorithm based on a
noisy policy-switching procedure. This approach builds on a proper Approximate
Policy Evaluation (APE) scheme to provide a perturbation that carefully leads
the local guides towards better actions. We evaluated our method on a set of
classical Reinforcement Learning problems, including safety-critical systems
where the agent cannot enter some areas at the risk of triggering catastrophic
consequences. In all the proposed environments, our agent proved to be
efficient at leveraging those policies to improve the performance of any
APE-based Reinforcement Learning algorithm, especially in its first learning
stages.
",0
Reinforcement Learning with Elastic Time Steps,"Dong Wang, Giovanni Beltrame",2024-02-22T20:49:04Z,Reinforcement Learning,"  Traditional Reinforcement Learning (RL) policies are typically implemented
with fixed control rates, often disregarding the impact of control rate
selection. This can lead to inefficiencies as the optimal control rate varies
with task requirements. We propose the Multi-Objective Soft Elastic
Actor-Critic (MOSEAC), an off-policy actor-critic algorithm that uses elastic
time steps to dynamically adjust the control frequency. This approach minimizes
computational resources by selecting the lowest viable frequency. We show that
MOSEAC converges and produces stable policies at the theoretical level, and
validate our findings in a real-time 3D racing game. MOSEAC significantly
outperformed other variable time step approaches in terms of energy efficiency
and task effectiveness. Additionally, MOSEAC demonstrated faster and more
stable training, showcasing its potential for real-world RL applications in
robotics.
",0
Q-FOX Learning: Breaking Tradition in Reinforcement Learning,"Mahmood A. Jumaah, Yossra H. Ali, Tarik A. Rashid",2024-02-26T13:39:04Z,Reinforcement Learning,"  Reinforcement learning (RL) is a subset of artificial intelligence (AI) where
agents learn the best action by interacting with the environment, making it
suitable for tasks that do not require labeled data or direct supervision.
Hyperparameters (HP) tuning refers to choosing the best parameter that leads to
optimal solutions in RL algorithms. Manual or random tuning of the HP may be a
crucial process because variations in this parameter lead to changes in the
overall learning aspects and different rewards. In this paper, a novel and
automatic HP-tuning method called Q-FOX is proposed. This uses both the FOX
optimizer, a new optimization method inspired by nature that mimics red foxes'
hunting behavior, and the commonly used, easy-to-implement RL Q-learning
algorithm to solve the problem of HP tuning. Moreover, a new objective function
is proposed which prioritizes the reward over the mean squared error (MSE) and
learning time (steps). Q-FOX has been evaluated on two OpenAI Gym environment
control tasks: Cart Pole and Frozen Lake. It exposed greater cumulative rewards
than HP tuning with other optimizers, such as PSO, GA, Bee, or randomly
selected HP. The cumulative reward for the Cart Pole task was 32.08, and for
the Frozen Lake task was 0.95. Despite the robustness of Q-FOX, it has
limitations. It cannot be used directly in real-word problems before choosing
the HP in a simulation environment because its processes work iteratively,
making it time-consuming. The results indicate that Q-FOX has played an
essential role in HP tuning for RL algorithms to effectively solve different
control tasks.
",0
Program-Based Strategy Induction for Reinforcement Learning,"Carlos G. Correa, Thomas L. Griffiths, Nathaniel D. Daw",2024-02-26T15:40:46Z,Reinforcement Learning,"  Typical models of learning assume incremental estimation of
continuously-varying decision variables like expected rewards. However, this
class of models fails to capture more idiosyncratic, discrete heuristics and
strategies that people and animals appear to exhibit. Despite recent advances
in strategy discovery using tools like recurrent networks that generalize the
classic models, the resulting strategies are often onerous to interpret, making
connections to cognition difficult to establish. We use Bayesian program
induction to discover strategies implemented by programs, letting the
simplicity of strategies trade off against their effectiveness. Focusing on
bandit tasks, we find strategies that are difficult or unexpected with
classical incremental learning, like asymmetric learning from rewarded and
unrewarded trials, adaptive horizon-dependent random exploration, and discrete
state switching.
",0
Deep Reinforcement Learning: A Convex Optimization Approach,Ather Gattami,2024-02-29T14:41:31Z,Reinforcement Learning,"  In this paper, we consider reinforcement learning of nonlinear systems with
continuous state and action spaces. We present an episodic learning algorithm,
where we for each episode use convex optimization to find a two-layer neural
network approximation of the optimal $Q$-function. The convex optimization
approach guarantees that the weights calculated at each episode are optimal,
with respect to the given sampled states and actions of the current episode.
For stable nonlinear systems, we show that the algorithm converges and that the
converging parameters of the trained neural network can be made arbitrarily
close to the optimal neural network parameters. In particular, if the
regularization parameter in the training phase is given by $\rho$, then the
parameters of the trained neural network converge to $w$, where the distance
between $w$ and the optimal parameters $w^\star$ is bounded by
$\mathcal{O}(\rho)$. That is, when the number of episodes goes to infinity,
there exists a constant $C$ such that \[
  \|w-w^\star\| \le C\rho. \]
  In particular, our algorithm converges arbitrarily close to the optimal
neural network parameters as the regularization parameter goes to zero. As a
consequence, our algorithm converges fast due to the polynomial-time
convergence of convex optimization algorithms.
",0
SplAgger: Split Aggregation for Meta-Reinforcement Learning,"Jacob Beck, Matthew Jackson, Risto Vuorio, Zheng Xiong, Shimon Whiteson",2024-03-05T14:57:04Z,Reinforcement Learning,"  A core ambition of reinforcement learning (RL) is the creation of agents
capable of rapid learning in novel tasks. Meta-RL aims to achieve this by
directly learning such agents. Black box methods do so by training
off-the-shelf sequence models end-to-end. By contrast, task inference methods
explicitly infer a posterior distribution over the unknown task, typically
using distinct objectives and sequence models designed to enable task
inference. Recent work has shown that task inference methods are not necessary
for strong performance. However, it remains unclear whether task inference
sequence models are beneficial even when task inference objectives are not. In
this paper, we present evidence that task inference sequence models are indeed
still beneficial. In particular, we investigate sequence models with
permutation invariant aggregation, which exploit the fact that, due to the
Markov property, the task posterior does not depend on the order of data. We
empirically confirm the advantage of permutation invariant sequence models
without the use of task inference objectives. However, we also find,
surprisingly, that there are multiple conditions under which permutation
variance remains useful. Therefore, we propose SplAgger, which uses both
permutation variant and invariant components to achieve the best of both
worlds, outperforming all baselines evaluated on continuous control and memory
environments. Code is provided at https://github.com/jacooba/hyper.
",0
Why Online Reinforcement Learning is Causal,"Oliver Schulte, Pascal Poupart",2024-03-07T04:49:48Z,Reinforcement Learning,"  Reinforcement learning (RL) and causal modelling naturally complement each
other. The goal of causal modelling is to predict the effects of interventions
in an environment, while the goal of reinforcement learning is to select
interventions that maximize the rewards the agent receives from the
environment. Reinforcement learning includes the two most powerful sources of
information for estimating causal relationships: temporal ordering and the
ability to act on an environment. This paper examines which reinforcement
learning settings we can expect to benefit from causal modelling, and how. In
online learning, the agent has the ability to interact directly with their
environment, and learn from exploring it. Our main argument is that in online
learning, conditional probabilities are causal, and therefore offline RL is the
setting where causal learning has the most potential to make a difference.
Essentially, the reason is that when an agent learns from their {\em own}
experience, there are no unobserved confounders that influence both the agent's
own exploratory actions and the rewards they receive. Our paper formalizes this
argument. For offline RL, where an agent may and typically does learn from the
experience of {\em others}, we describe previous and new methods for leveraging
a causal model, including support for counterfactual queries.
",0
Transferable Reinforcement Learning via Generalized Occupancy Models,"Chuning Zhu, Xinqi Wang, Tyler Han, Simon S. Du, Abhishek Gupta",2024-03-10T22:27:21Z,Reinforcement Learning,"  Intelligent agents must be generalists, capable of quickly adapting to
various tasks. In reinforcement learning (RL), model-based RL learns a dynamics
model of the world, in principle enabling transfer to arbitrary reward
functions through planning. However, autoregressive model rollouts suffer from
compounding error, making model-based RL ineffective for long-horizon problems.
Successor features offer an alternative by modeling a policy's long-term state
occupancy, reducing policy evaluation under new tasks to linear reward
regression. Yet, policy improvement with successor features can be challenging.
This work proposes a novel class of models, i.e., generalized occupancy models
(GOMs), that learn a distribution of successor features from a stationary
dataset, along with a policy that acts to realize different successor features.
These models can quickly select the optimal action for arbitrary new tasks. By
directly modeling long-term outcomes in the dataset, GOMs avoid compounding
error while enabling rapid transfer across reward functions. We present a
practical instantiation of GOMs using diffusion models and show their efficacy
as a new class of transferable models, both theoretically and empirically
across various simulated robotics problems. Videos and code at
https://weirdlabuw.github.io/gom/.
",0
In-context Exploration-Exploitation for Reinforcement Learning,"Zhenwen Dai, Federico Tomasi, Sina Ghiassian",2024-03-11T15:43:14Z,Reinforcement Learning,"  In-context learning is a promising approach for online policy learning of
offline reinforcement learning (RL) methods, which can be achieved at inference
time without gradient optimization. However, this method is hindered by
significant computational costs resulting from the gathering of large training
trajectory sets and the need to train large Transformer models. We address this
challenge by introducing an In-context Exploration-Exploitation (ICEE)
algorithm, designed to optimize the efficiency of in-context policy learning.
Unlike existing models, ICEE performs an exploration-exploitation trade-off at
inference time within a Transformer model, without the need for explicit
Bayesian inference. Consequently, ICEE can solve Bayesian optimization problems
as efficiently as Gaussian process biased methods do, but in significantly less
time. Through experiments in grid world environments, we demonstrate that ICEE
can learn to solve new RL tasks using only tens of episodes, marking a
substantial improvement over the hundreds of episodes needed by the previous
in-context learning method.
",0
MetroGNN: Metro Network Expansion with Reinforcement Learning,"Hongyuan Su, Yu Zheng, Jingtao Ding, Depeng Jin, Yong Li",2024-03-14T09:09:15Z,Reinforcement Learning,"  Selecting urban regions for metro network expansion to meet maximal
transportation demands is crucial for urban development, while computationally
challenging to solve. The expansion process relies not only on complicated
features like urban demographics and origin-destination (OD) flow but is also
constrained by the existing metro network and urban geography. In this paper,
we introduce a reinforcement learning framework to address a Markov decision
process within an urban heterogeneous multi-graph. Our approach employs an
attentive policy network that intelligently selects nodes based on information
captured by a graph neural network. Experiments on real-world urban data
demonstrate that our proposed methodology substantially improve the satisfied
transportation demands by over 30\% when compared with state-of-the-art
methods. Codes are published at https://github.com/tsinghua-fib-lab/MetroGNN.
",0
Parameter Efficient Reinforcement Learning from Human Feedback,"Hakim Sidahmed, Samrat Phatale, Alex Hutcheson, Zhuonan Lin, Zhang Chen, Zac Yu, Jarvis Jin, Simral Chaudhary, Roman Komarytsia, Christiane Ahlheim, Yonghao Zhu, Bowen Li, Saravanan Ganesh, Bill Byrne, Jessica Hoffmann, Hassan Mansoor, Wei Li, Abhinav Rastogi, Lucas Dixon",2024-03-15T21:43:46Z,Reinforcement Learning,"  While Reinforcement Learning from Human Feedback (RLHF) effectively aligns
pretrained Large Language and Vision-Language Models (LLMs, and VLMs) with
human preferences, its computational cost and complexity hamper its wider
adoption. To alleviate some of the computational burden of fine-tuning,
parameter efficient methods, like LoRA were introduced. In this work, we
empirically evaluate the setup of Parameter Efficient Reinforcement Learning
from Human Feedback (PE-RLHF) that leverages LoRA fine-tuning for Reward
Modeling, and Reinforcement Learning. We benchmark the PE-RLHF setup on six
diverse datasets spanning summarization, harmless/helpful response generation,
UI automation, and visual question answering in terms of effectiveness of the
trained models, and the training resources required. Our findings show, for the
first time, that PE-RLHF achieves comparable performance to RLHF, while
significantly reducing training time (up to 90% faster for reward models, and
30% faster for RL), and memory footprint (up to 50% reduction for reward
models, and 27% for RL). We provide comprehensive ablations across LoRA ranks,
and model sizes for both reward modeling and reinforcement learning. By
mitigating the computational burden associated with RLHF, we push for a broader
adoption of PE-RLHF as an alignment technique for LLMs and VLMs.
",0
Reinforcement Learning with Options and State Representation,"Ayoub Ghriss, Masashi Sugiyama, Alessandro Lazaric",2024-03-16T08:30:55Z,Reinforcement Learning,"  The current thesis aims to explore the reinforcement learning field and build
on existing methods to produce improved ones to tackle the problem of learning
in high-dimensional and complex environments. It addresses such goals by
decomposing learning tasks in a hierarchical fashion known as Hierarchical
Reinforcement Learning.
  We start in the first chapter by getting familiar with the Markov Decision
Process framework and presenting some of its recent techniques that the
following chapters use. We then proceed to build our Hierarchical Policy
learning as an answer to the limitations of a single primitive policy. The
hierarchy is composed of a manager agent at the top and employee agents at the
lower level.
  In the last chapter, which is the core of this thesis, we attempt to learn
lower-level elements of the hierarchy independently of the manager level in
what is known as the ""Eigenoption"". Based on the graph structure of the
environment, Eigenoptions allow us to build agents that are aware of the
geometric and dynamic properties of the environment. Their decision-making has
a special property: it is invariant to symmetric transformations of the
environment, allowing as a consequence to greatly reduce the complexity of the
learning task.
",0
Offline Multitask Representation Learning for Reinforcement Learning,"Haque Ishfaq, Thanh Nguyen-Tang, Songtao Feng, Raman Arora, Mengdi Wang, Ming Yin, Doina Precup",2024-03-18T08:50:30Z,Reinforcement Learning,"  We study offline multitask representation learning in reinforcement learning
(RL), where a learner is provided with an offline dataset from different tasks
that share a common representation and is asked to learn the shared
representation. We theoretically investigate offline multitask low-rank RL, and
propose a new algorithm called MORL for offline multitask representation
learning. Furthermore, we examine downstream RL in reward-free, offline and
online scenarios, where a new task is introduced to the agent that shares the
same representation as the upstream offline tasks. Our theoretical results
demonstrate the benefits of using the learned representation from the upstream
offline task instead of directly learning the representation of the low-rank
model.
",0
Supervised Fine-Tuning as Inverse Reinforcement Learning,Hao Sun,2024-03-18T17:52:57Z,Reinforcement Learning,"  The prevailing approach to aligning Large Language Models (LLMs) typically
relies on human or AI feedback and assumes access to specific types of
preference datasets. In our work, we question the efficacy of such datasets and
explore various scenarios where alignment with expert demonstrations proves
more realistic. We build a sequential decision-making framework to formulate
the problem of aligning LLMs using demonstration datasets. Drawing insights
from inverse reinforcement learning and imitation learning, we introduce
various approaches for divergence minimization in the LLM alignment tasks. Our
analysis highlights the mass-covering and mode-seeking behaviors of these
different approaches. Inclusively, we examine the pros and cons of the
classical supervised fine-tuning method, elaborating on scenarios where
different methods shine.
",0
Advanced Statistical Arbitrage with Reinforcement Learning,"Boming Ning, Kiseop Lee",2024-03-18T18:51:43Z,"RAG, Reinforcement Learning","  Statistical arbitrage is a prevalent trading strategy which takes advantage
of mean reverse property of spread of paired stocks. Studies on this strategy
often rely heavily on model assumption. In this study, we introduce an
innovative model-free and reinforcement learning based framework for
statistical arbitrage. For the construction of mean reversion spreads, we
establish an empirical reversion time metric and optimize asset coefficients by
minimizing this empirical mean reversion time. In the trading phase, we employ
a reinforcement learning framework to identify the optimal mean reversion
strategy. Diverging from traditional mean reversion strategies that primarily
focus on price deviations from a long-term mean, our methodology creatively
constructs the state space to encapsulate the recent trends in price movements.
Additionally, the reward function is carefully tailored to reflect the unique
characteristics of mean reversion trading.
",0
Bin Packing Optimization via Deep Reinforcement Learning,"Baoying Wang, Huixu Dong",2024-03-19T04:07:45Z,Reinforcement Learning,"  The Bin Packing Problem (BPP) has attracted enthusiastic research interest
recently, owing to widespread applications in logistics and warehousing
environments. It is truly essential to optimize the bin packing to enable more
objects to be packed into boxes. Object packing order and placement strategy
are the two crucial optimization objectives of the BPP. However, existing
optimization methods for BPP, such as the genetic algorithm (GA), emerge as the
main issues in highly computational cost and relatively low accuracy, making it
difficult to implement in realistic scenarios. To well relieve the research
gaps, we present a novel optimization methodology of two-dimensional (2D)-BPP
and three-dimensional (3D)-BPP for objects with regular shapes via deep
reinforcement learning (DRL), maximizing the space utilization and minimizing
the usage number of boxes. First, an end-to-end DRL neural network constructed
by a modified Pointer Network consisting of an encoder, a decoder and an
attention module is proposed to achieve the optimal object packing order.
Second, conforming to the top-down operation mode, the placement strategy based
on a height map is used to arrange the ordered objects in the boxes, preventing
the objects from colliding with boxes and other objects in boxes. Third, the
reward and loss functions are defined as the indicators of the compactness,
pyramid, and usage number of boxes to conduct the training of the DRL neural
network based on an on-policy actor-critic framework. Finally, a series of
experiments are implemented to compare our method with conventional packing
methods, from which we conclude that our method outperforms these packing
methods in both packing accuracy and efficiency.
",0
Policy Bifurcation in Safe Reinforcement Learning,"Wenjun Zou, Yao Lyu, Jie Li, Yujie Yang, Shengbo Eben Li, Jingliang Duan, Xianyuan Zhan, Jingjing Liu, Yaqin Zhang, Keqiang Li",2024-03-19T15:54:38Z,Reinforcement Learning,"  Safe reinforcement learning (RL) offers advanced solutions to constrained
optimal control problems. Existing studies in safe RL implicitly assume
continuity in policy functions, where policies map states to actions in a
smooth, uninterrupted manner; however, our research finds that in some
scenarios, the feasible policy should be discontinuous or multi-valued,
interpolating between discontinuous local optima can inevitably lead to
constraint violations. We are the first to identify the generating mechanism of
such a phenomenon, and employ topological analysis to rigorously prove the
existence of policy bifurcation in safe RL, which corresponds to the
contractibility of the reachable tuple. Our theorem reveals that in scenarios
where the obstacle-free state space is non-simply connected, a feasible policy
is required to be bifurcated, meaning its output action needs to change
abruptly in response to the varying state. To train such a bifurcated policy,
we propose a safe RL algorithm called multimodal policy optimization (MUPO),
which utilizes a Gaussian mixture distribution as the policy output. The
bifurcated behavior can be achieved by selecting the Gaussian component with
the highest mixing coefficient. Besides, MUPO also integrates spectral
normalization and forward KL divergence to enhance the policy's capability of
exploring different modes. Experiments with vehicle control tasks show that our
algorithm successfully learns the bifurcated policy and ensures satisfying
safety, while a continuous policy suffers from inevitable constraint
violations.
",0
Simple Ingredients for Offline Reinforcement Learning,"Edoardo Cetin, Andrea Tirinzoni, Matteo Pirotta, Alessandro Lazaric, Yann Ollivier, Ahmed Touati",2024-03-19T18:57:53Z,Reinforcement Learning,"  Offline reinforcement learning algorithms have proven effective on datasets
highly connected to the target downstream task. Yet, leveraging a novel testbed
(MOOD) in which trajectories come from heterogeneous sources, we show that
existing methods struggle with diverse data: their performance considerably
deteriorates as data collected for related but different tasks is simply added
to the offline buffer. In light of this finding, we conduct a large empirical
study where we formulate and test several hypotheses to explain this failure.
Surprisingly, we find that scale, more than algorithmic considerations, is the
key factor influencing performance. We show that simple methods like AWAC and
IQL with increased network size overcome the paradoxical failure modes from the
inclusion of additional data in MOOD, and notably outperform prior
state-of-the-art algorithms on the canonical D4RL benchmark.
",0
Fast Value Tracking for Deep Reinforcement Learning,"Frank Shih, Faming Liang",2024-03-19T22:18:19Z,Reinforcement Learning,"  Reinforcement learning (RL) tackles sequential decision-making problems by
creating agents that interacts with their environment. However, existing
algorithms often view these problem as static, focusing on point estimates for
model parameters to maximize expected rewards, neglecting the stochastic
dynamics of agent-environment interactions and the critical role of uncertainty
quantification. Our research leverages the Kalman filtering paradigm to
introduce a novel and scalable sampling algorithm called Langevinized Kalman
Temporal-Difference (LKTD) for deep reinforcement learning. This algorithm,
grounded in Stochastic Gradient Markov Chain Monte Carlo (SGMCMC), efficiently
draws samples from the posterior distribution of deep neural network
parameters. Under mild conditions, we prove that the posterior samples
generated by the LKTD algorithm converge to a stationary distribution. This
convergence not only enables us to quantify uncertainties associated with the
value function and model parameters but also allows us to monitor these
uncertainties during policy updates throughout the training phase. The LKTD
algorithm paves the way for more robust and adaptable reinforcement learning
approaches.
",0
Reinforcement Learning Design for Quickest Change Detection,"Austin Cooper, Sean Meyn",2024-03-21T03:41:10Z,Reinforcement Learning,"  The field of quickest change detection (QCD) concerns design and analysis of
algorithms to estimate in real time the time at which an important event takes
place, and identify properties of the post-change behavior. It is shown in this
paper that approaches based on reinforcement learning (RL) can be adapted based
on any ""surrogate information state"" that is adapted to the observations. Hence
we are left to choose both the surrogate information state process and the
algorithm. For the former, it is argued that there are many choices available,
based on a rich theory of asymptotic statistics for QCD. Two approaches to RL
design are considered: (i) Stochastic gradient descent based on an actor-critic
formulation. Theory is largely complete for this approach: the algorithm is
unbiased, and will converge to a local minimum. However, it is shown that
variance of stochastic gradients can be very large, necessitating the need for
commensurately long run times; (ii) Q-learning algorithms based on a version of
the projected Bellman equation. It is shown that the algorithm is stable, in
the sense of bounded sample paths, and that a solution to the projected Bellman
equation exists under mild conditions. Numerical experiments illustrate these
findings, and provide a roadmap for algorithm design in more general settings.
",0
Reactor Optimization Benchmark by Reinforcement Learning,"Deborah Schwarcz, Nadav Schneider, Gal Oren, Uri Steinitz",2024-03-21T10:26:47Z,Reinforcement Learning,"  Neutronic calculations for reactors are a daunting task when using Monte
Carlo (MC) methods. As high-performance computing has advanced, the simulation
of a reactor is nowadays more readily done, but design and optimization with
multiple parameters is still a computational challenge. MC transport
simulations, coupled with machine learning techniques, offer promising avenues
for enhancing the efficiency and effectiveness of nuclear reactor optimization.
This paper introduces a novel benchmark problem within the OpenNeoMC framework
designed specifically for reinforcement learning. The benchmark involves
optimizing a unit cell of a research reactor with two varying parameters (fuel
density and water spacing) to maximize neutron flux while maintaining reactor
criticality. The test case features distinct local optima, representing
different physical regimes, thus posing a challenge for learning algorithms.
Through extensive simulations utilizing evolutionary and neuroevolutionary
algorithms, we demonstrate the effectiveness of reinforcement learning in
navigating complex optimization landscapes with strict constraints.
Furthermore, we propose acceleration techniques within the OpenNeoMC framework,
including model updating and cross-section usage by RAM utilization, to
expedite simulation times. Our findings emphasize the importance of machine
learning integration in reactor optimization and contribute to advancing
methodologies for addressing intricate optimization challenges in nuclear
engineering. The sources of this work are available at our GitHub repository:
https://github.com/Scientific-Computing-Lab-NRCN/RLOpenNeoMC
",0
Testing for Fault Diversity in Reinforcement Learning,"Quentin Mazouni, Helge Spieker, Arnaud Gotlieb, Mathieu Acher",2024-03-22T09:46:30Z,Reinforcement Learning,"  Reinforcement Learning is the premier technique to approach sequential
decision problems, including complex tasks such as driving cars and landing
spacecraft. Among the software validation and verification practices, testing
for functional fault detection is a convenient way to build trustworthiness in
the learned decision model. While recent works seek to maximise the number of
detected faults, none consider fault characterisation during the search for
more diversity. We argue that policy testing should not find as many failures
as possible (e.g., inputs that trigger similar car crashes) but rather aim at
revealing as informative and diverse faults as possible in the model. In this
paper, we explore the use of quality diversity optimisation to solve the
problem of fault diversity in policy testing. Quality diversity (QD)
optimisation is a type of evolutionary algorithm to solve hard combinatorial
optimisation problems where high-quality diverse solutions are sought. We
define and address the underlying challenges of adapting QD optimisation to the
test of action policies. Furthermore, we compare classical QD optimisers to
state-of-the-art frameworks dedicated to policy testing, both in terms of
search efficiency and fault diversity. We show that QD optimisation, while
being conceptually simple and generally applicable, finds effectively more
diverse faults in the decision model, and conclude that QD-based policy testing
is a promising approach.
",0
Automated Feature Selection for Inverse Reinforcement Learning,"Daulet Baimukashev, Gokhan Alcan, Ville Kyrki",2024-03-22T10:05:21Z,Reinforcement Learning,"  Inverse reinforcement learning (IRL) is an imitation learning approach to
learning reward functions from expert demonstrations. Its use avoids the
difficult and tedious procedure of manual reward specification while retaining
the generalization power of reinforcement learning. In IRL, the reward is
usually represented as a linear combination of features. In continuous state
spaces, the state variables alone are not sufficiently rich to be used as
features, but which features are good is not known in general. To address this
issue, we propose a method that employs polynomial basis functions to form a
candidate set of features, which are shown to allow the matching of statistical
moments of state distributions. Feature selection is then performed for the
candidates by leveraging the correlation between trajectory probabilities and
feature expectations. We demonstrate the approach's effectiveness by recovering
reward functions that capture expert policies across non-linear control tasks
of increasing complexity. Code, data, and videos are available at
https://sites.google.com/view/feature4irl.
",0
Imitating Cost-Constrained Behaviors in Reinforcement Learning,"Qian Shao, Pradeep Varakantham, Shih-Fen Cheng",2024-03-26T07:41:54Z,Reinforcement Learning,"  Complex planning and scheduling problems have long been solved using various
optimization or heuristic approaches. In recent years, imitation learning that
aims to learn from expert demonstrations has been proposed as a viable
alternative to solving these problems. Generally speaking, imitation learning
is designed to learn either the reward (or preference) model or directly the
behavioral policy by observing the behavior of an expert. Existing work in
imitation learning and inverse reinforcement learning has focused on imitation
primarily in unconstrained settings (e.g., no limit on fuel consumed by the
vehicle). However, in many real-world domains, the behavior of an expert is
governed not only by reward (or preference) but also by constraints. For
instance, decisions on self-driving delivery vehicles are dependent not only on
the route preferences/rewards (depending on past demand data) but also on the
fuel in the vehicle and the time available. In such problems, imitation
learning is challenging as decisions are not only dictated by the reward model
but are also dependent on a cost-constrained model. In this paper, we provide
multiple methods that match expert distributions in the presence of trajectory
cost constraints through (a) Lagrangian-based method; (b) Meta-gradients to
find a good trade-off between expected return and minimizing constraint
violation; and (c) Cost-violation-based alternating gradient. We empirically
show that leading imitation learning approaches imitate cost-constrained
behaviors poorly and our meta-gradient-based approach achieves the best
performance.
",0
Uncertainty-aware Distributional Offline Reinforcement Learning,"Xiaocong Chen, Siyu Wang, Tong Yu, Lina Yao",2024-03-26T12:28:04Z,Reinforcement Learning,"  Offline reinforcement learning (RL) presents distinct challenges as it relies
solely on observational data. A central concern in this context is ensuring the
safety of the learned policy by quantifying uncertainties associated with
various actions and environmental stochasticity. Traditional approaches
primarily emphasize mitigating epistemic uncertainty by learning risk-averse
policies, often overlooking environmental stochasticity. In this study, we
propose an uncertainty-aware distributional offline RL method to simultaneously
address both epistemic uncertainty and environmental stochasticity. We propose
a model-free offline RL algorithm capable of learning risk-averse policies and
characterizing the entire distribution of discounted cumulative rewards, as
opposed to merely maximizing the expected value of accumulated discounted
returns. Our method is rigorously evaluated through comprehensive experiments
in both risk-sensitive and risk-neutral benchmarks, demonstrating its superior
performance.
",0
Image Deraining via Self-supervised Reinforcement Learning,"He-Hao Liao, Yan-Tsung Peng, Wen-Tao Chu, Ping-Chun Hsieh, Chung-Chi Tsai",2024-03-27T05:52:39Z,Reinforcement Learning,"  The quality of images captured outdoors is often affected by the weather. One
factor that interferes with sight is rain, which can obstruct the view of
observers and computer vision applications that rely on those images. The work
aims to recover rain images by removing rain streaks via Self-supervised
Reinforcement Learning (RL) for image deraining (SRL-Derain). We locate rain
streak pixels from the input rain image via dictionary learning and use
pixel-wise RL agents to take multiple inpainting actions to remove rain
progressively. To our knowledge, this work is the first attempt where
self-supervised RL is applied to image deraining. Experimental results on
several benchmark image-deraining datasets show that the proposed SRL-Derain
performs favorably against state-of-the-art few-shot and self-supervised
deraining and denoising methods.
",0
Reinforcement Learning with Generalizable Gaussian Splatting,"Jiaxu Wang, Qiang Zhang, Jingkai Sun, Jiahang Cao, Gang Han, Wen Zhao, Weining Zhang, Yecheng Shao, Yijie Guo, Renjing Xu",2024-03-18T16:50:23Z,Reinforcement Learning,"  An excellent representation is crucial for reinforcement learning (RL)
performance, especially in vision-based reinforcement learning tasks. The
quality of the environment representation directly influences the achievement
of the learning task. Previous vision-based RL typically uses explicit or
implicit ways to represent environments, such as images, points, voxels, and
neural radiance fields. However, these representations contain several
drawbacks. They cannot either describe complex local geometries or generalize
well to unseen scenes, or require precise foreground masks. Moreover, these
implicit neural representations are akin to a ``black box"", significantly
hindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit
scene representation and differentiable rendering nature, is considered a
revolutionary change for reconstruction and representation methods. In this
paper, we propose a novel Generalizable Gaussian Splatting framework to be the
representation of RL tasks, called GSRL. Through validation in the RoboMimic
environment, our method achieves better results than other baselines in
multiple tasks, improving the performance by 10%, 44%, and 15% compared with
baselines on the hardest task. This work is the first attempt to leverage
generalizable 3DGS as a representation for RL.
",0
Deep Reinforcement Learning for Modelling Protein Complexes,"Ziqi Gao, Tao Feng, Jiaxuan You, Chenyi Zi, Yan Zhou, Chen Zhang, Jia Li",2024-03-11T12:33:33Z,Reinforcement Learning,"  AlphaFold can be used for both single-chain and multi-chain protein structure
prediction, while the latter becomes extremely challenging as the number of
chains increases. In this work, by taking each chain as a node and assembly
actions as edges, we show that an acyclic undirected connected graph can be
used to predict the structure of multi-chain protein complexes (a.k.a., protein
complex modelling, PCM). However, there are still two challenges: 1) The huge
combinatorial optimization space of $N^{N-2}$ ($N$ is the number of chains) for
the PCM problem can easily lead to high computational cost. 2) The scales of
protein complexes exhibit distribution shift due to variance in chain numbers,
which calls for the generalization in modelling complexes of various scales. To
address these challenges, we propose GAPN, a Generative Adversarial Policy
Network powered by domain-specific rewards and adversarial loss through policy
gradient for automatic PCM prediction. Specifically, GAPN learns to efficiently
search through the immense assembly space and optimize the direct docking
reward through policy gradient. Importantly, we design an adversarial reward
function to enhance the receptive field of our model. In this way, GAPN will
simultaneously focus on a specific batch of complexes and the global assembly
rules learned from complexes with varied chain numbers. Empirically, we have
achieved both significant accuracy (measured by RMSD and TM-Score) and
efficiency improvements compared to leading PCM softwares.
",0
"Towards Responsible AI: A Design Space Exploration of Human-Centered
  Artificial Intelligence User Interfaces to Investigate Fairness","Yuri Nakao, Lorenzo Strappelli, Simone Stumpf, Aisha Naseer, Daniele Regoli, Giulia Del Gamba",2022-06-01T13:08:37Z,Responsible AI,"  With Artificial intelligence (AI) to aid or automate decision-making
advancing rapidly, a particular concern is its fairness. In order to create
reliable, safe and trustworthy systems through human-centred artificial
intelligence (HCAI) design, recent efforts have produced user interfaces (UIs)
for AI experts to investigate the fairness of AI models. In this work, we
provide a design space exploration that supports not only data scientists but
also domain experts to investigate AI fairness. Using loan applications as an
example, we held a series of workshops with loan officers and data scientists
to elicit their requirements. We instantiated these requirements into FairHIL,
a UI to support human-in-the-loop fairness investigations, and describe how
this UI could be generalized to other use cases. We evaluated FairHIL through a
think-aloud user study. Our work contributes better designs to investigate an
AI model's fairness-and move closer towards responsible AI.
",0
"Connecting the Dots in Trustworthy Artificial Intelligence: From AI
  Principles, Ethics, and Key Requirements to Responsible AI Systems and
  Regulation","Natalia Díaz-Rodríguez, Javier Del Ser, Mark Coeckelbergh, Marcos López de Prado, Enrique Herrera-Viedma, Francisco Herrera",2023-05-02T09:49:53Z,Responsible AI,"  Trustworthy Artificial Intelligence (AI) is based on seven technical
requirements sustained over three main pillars that should be met throughout
the system's entire life cycle: it should be (1) lawful, (2) ethical, and (3)
robust, both from a technical and a social perspective. However, attaining
truly trustworthy AI concerns a wider vision that comprises the trustworthiness
of all processes and actors that are part of the system's life cycle, and
considers previous aspects from different lenses. A more holistic vision
contemplates four essential axes: the global principles for ethical use and
development of AI-based systems, a philosophical take on AI ethics, a
risk-based approach to AI regulation, and the mentioned pillars and
requirements. The seven requirements (human agency and oversight; robustness
and safety; privacy and data governance; transparency; diversity,
non-discrimination and fairness; societal and environmental wellbeing; and
accountability) are analyzed from a triple perspective: What each requirement
for trustworthy AI is, Why it is needed, and How each requirement can be
implemented in practice. On the other hand, a practical approach to implement
trustworthy AI systems allows defining the concept of responsibility of
AI-based systems facing the law, through a given auditing process. Therefore, a
responsible AI system is the resulting notion we introduce in this work, and a
concept of utmost necessity that can be realized through auditing processes,
subject to the challenges posed by the use of regulatory sandboxes. Our
multidisciplinary vision of trustworthy AI culminates in a debate on the
diverging views published lately about the future of AI. Our reflections in
this matter conclude that regulation is a key for reaching a consensus among
these views, and that trustworthy and responsible AI systems will be crucial
for the present and future of our society.
",0
"RAI Guidelines: Method for Generating Responsible AI Guidelines Grounded
  in Regulations and Usable by (Non-)Technical Roles","Marios Constantinides, Edyta Bogucka, Daniele Quercia, Susanna Kallio, Mohammad Tahaei",2023-07-27T19:17:43Z,Responsible AI,"  Many guidelines for responsible AI have been suggested to help AI
practitioners in the development of ethical and responsible AI systems.
However, these guidelines are often neither grounded in regulation nor usable
by different roles, from developers to decision makers. To bridge this gap, we
developed a four-step method to generate a list of responsible AI guidelines;
these steps are: (1) manual coding of 17 papers on responsible AI; (2)
compiling an initial catalog of responsible AI guidelines; (3) refining the
catalog through interviews and expert panels; and (4) finalizing the catalog.
To evaluate the resulting 22 guidelines, we incorporated them into an
interactive tool and assessed them in a user study with 14 AI researchers,
engineers, designers, and managers from a large technology company. Through
interviews with these practitioners, we found that the guidelines were grounded
in current regulations and usable across roles, encouraging self-reflection on
ethical considerations at early stages of development. This significantly
contributes to the concept of `Responsible AI by Design' -- a design-first
approach that embeds responsible AI values throughout the development lifecycle
and across various business roles.
",0
"Decentralised Governance-Driven Architecture for Designing Foundation
  Model based Systems: Exploring the Role of Blockchain in Responsible AI","Yue Liu, Qinghua Lu, Liming Zhu, Hye-Young Paik",2023-08-11T06:41:47Z,Responsible AI,"  Foundation models including large language models (LLMs) are increasingly
attracting interest worldwide for their distinguished capabilities and
potential to perform a wide variety of tasks. Nevertheless, people are
concerned about whether foundation model based AI systems are properly governed
to ensure the trustworthiness and to prevent misuse that could harm humans,
society and the environment. In this paper, we identify eight governance
challenges of foundation model based AI systems regarding the three fundamental
dimensions of governance: decision rights, incentives, and accountability.
Furthermore, we explore the potential of blockchain as an architectural
solution to address the challenges by providing a distributed ledger to
facilitate decentralised governance. We present an architecture that
demonstrates how blockchain can be leveraged to realise governance in
foundation model based AI systems.
",0
"REIN-2: Giving Birth to Prepared Reinforcement Learning Agents Using
  Reinforcement Learning Agents","Aristotelis Lazaridis, Ioannis Vlahavas",2021-10-11T10:13:49Z,Reinforcement Learning,"  Deep Reinforcement Learning (Deep RL) has been in the spotlight for the past
few years, due to its remarkable abilities to solve problems which were
considered to be practically unsolvable using traditional Machine Learning
methods. However, even state-of-the-art Deep RL algorithms have various
weaknesses that prevent them from being used extensively within industry
applications, with one such major weakness being their sample-inefficiency. In
an effort to patch these issues, we integrated a meta-learning technique in
order to shift the objective of learning to solve a task into the objective of
learning how to learn to solve a task (or a set of tasks), which we empirically
show that improves overall stability and performance of Deep RL algorithms. Our
model, named REIN-2, is a meta-learning scheme formulated within the RL
framework, the goal of which is to develop a meta-RL agent (meta-learner) that
learns how to produce other RL agents (inner-learners) that are capable of
solving given environments. For this task, we convert the typical interaction
of an RL agent with the environment into a new, single environment for the
meta-learner to interact with. Compared to traditional state-of-the-art Deep RL
algorithms, experimental results show remarkable performance of our model in
popular OpenAI Gym environments in terms of scoring and sample efficiency,
including the Mountain Car hard-exploration environment.
",0
"Combining Reinforcement Learning and Inverse Reinforcement Learning for
  Asset Allocation Recommendations","Igor Halperin, Jiayu Liu, Xiao Zhang",2022-01-06T00:08:17Z,Reinforcement Learning,"  We suggest a simple practical method to combine the human and artificial
intelligence to both learn best investment practices of fund managers, and
provide recommendations to improve them. Our approach is based on a combination
of Inverse Reinforcement Learning (IRL) and RL. First, the IRL component learns
the intent of fund managers as suggested by their trading history, and recovers
their implied reward function. At the second step, this reward function is used
by a direct RL algorithm to optimize asset allocation decisions. We show that
our method is able to improve over the performance of individual fund managers.
",0
"Quantile Constrained Reinforcement Learning: A Reinforcement Learning
  Framework Constraining Outage Probability","Whiyoung Jung, Myungsik Cho, Jongeui Park, Youngchul Sung",2022-11-28T03:46:56Z,Reinforcement Learning,"  Constrained reinforcement learning (RL) is an area of RL whose objective is
to find an optimal policy that maximizes expected cumulative return while
satisfying a given constraint. Most of the previous constrained RL works
consider expected cumulative sum cost as the constraint. However, optimization
with this constraint cannot guarantee a target probability of outage event that
the cumulative sum cost exceeds a given threshold. This paper proposes a
framework, named Quantile Constrained RL (QCRL), to constrain the quantile of
the distribution of the cumulative sum cost that is a necessary and sufficient
condition to satisfy the outage constraint. This is the first work that tackles
the issue of applying the policy gradient theorem to the quantile and provides
theoretical results for approximating the gradient of the quantile. Based on
the derived theoretical results and the technique of the Lagrange multiplier,
we construct a constrained RL algorithm named Quantile Constrained Policy
Optimization (QCPO). We use distributional RL with the Large Deviation
Principle (LDP) to estimate quantiles and tail probability of the cumulative
sum cost for the implementation of QCPO. The implemented algorithm satisfies
the outage probability constraint after the training period.
",0
"Is Inverse Reinforcement Learning Harder than Standard Reinforcement
  Learning? A Theoretical Perspective","Lei Zhao, Mengdi Wang, Yu Bai",2023-11-29T00:09:01Z,Reinforcement Learning,"  Inverse Reinforcement Learning (IRL) -- the problem of learning reward
functions from demonstrations of an \emph{expert policy} -- plays a critical
role in developing intelligent systems. While widely used in applications,
theoretical understandings of IRL present unique challenges and remain less
developed compared with standard RL. For example, it remains open how to do IRL
efficiently in standard \emph{offline} settings with pre-collected data, where
states are obtained from a \emph{behavior policy} (which could be the expert
policy itself), and actions are sampled from the expert policy.
  This paper provides the first line of results for efficient IRL in vanilla
offline and online settings using polynomial samples and runtime. Our
algorithms and analyses seamlessly adapt the pessimism principle commonly used
in offline RL, and achieve IRL guarantees in stronger metrics than considered
in existing work. We provide lower bounds showing that our sample complexities
are nearly optimal. As an application, we also show that the learned rewards
can \emph{transfer} to another target MDP with suitable guarantees when the
target MDP satisfies certain similarity assumptions with the original (source)
MDP.
",0
"Utility-Based Reinforcement Learning: Unifying Single-objective and
  Multi-objective Reinforcement Learning","Peter Vamplew, Cameron Foale, Conor F. Hayes, Patrick Mannion, Enda Howley, Richard Dazeley, Scott Johnson, Johan Källström, Gabriel Ramos, Roxana Rădulescu, Willem Röpke, Diederik M. Roijers",2024-02-05T01:42:28Z,Reinforcement Learning,"  Research in multi-objective reinforcement learning (MORL) has introduced the
utility-based paradigm, which makes use of both environmental rewards and a
function that defines the utility derived by the user from those rewards. In
this paper we extend this paradigm to the context of single-objective
reinforcement learning (RL), and outline multiple potential benefits including
the ability to perform multi-policy learning across tasks relating to uncertain
objectives, risk-aware RL, discounting, and safe RL. We also examine the
algorithmic implications of adopting a utility-based approach.
",0
"Unbox the Black-box for the Medical Explainable AI via Multi-modal and
  Multi-centre Data Fusion: A Mini-Review, Two Showcases and Beyond","Guang Yang, Qinghao Ye, Jun Xia",2021-02-03T10:56:58Z,Explainable AI,"  Explainable Artificial Intelligence (XAI) is an emerging research topic of
machine learning aimed at unboxing how AI systems' black-box choices are made.
This research field inspects the measures and models involved in
decision-making and seeks solutions to explain them explicitly. Many of the
machine learning algorithms can not manifest how and why a decision has been
cast. This is particularly true of the most popular deep neural network
approaches currently in use. Consequently, our confidence in AI systems can be
hindered by the lack of explainability in these black-box models. The XAI
becomes more and more crucial for deep learning powered applications,
especially for medical and healthcare studies, although in general these deep
neural networks can return an arresting dividend in performance. The
insufficient explainability and transparency in most existing AI systems can be
one of the major reasons that successful implementation and integration of AI
tools into routine clinical practice are uncommon. In this study, we first
surveyed the current progress of XAI and in particular its advances in
healthcare applications. We then introduced our solutions for XAI leveraging
multi-modal and multi-centre data fusion, and subsequently validated in two
showcases following real clinical scenarios. Comprehensive quantitative and
qualitative analyses can prove the efficacy of our proposed XAI solutions, from
which we can envisage successful applications in a broader range of clinical
questions.
",0
"Explainable AI for Engineering Design: A Unified Approach of Systems
  Engineering and Component- Based Deep Learning Demonstrated by Energy-
  Efficient Building Design","Philipp Geyer, Manav Mahan Singh, Xia Chen",2021-08-30T07:20:58Z,Explainable AI,"  Data-driven models created by machine learning, gain in importance in all
fields of design and engineering. They, have high potential to assist
decision-makers in creating novel, artefacts with better performance and
sustainability. However,, limited generalization and the black-box nature of
these models, lead to limited explainability and reusability. To overcome this,
situation, we propose a component-based approach to create, partial component
models by machine learning (ML). This, component-based approach aligns deep
learning with systems, engineering (SE). The key contribution of the
component-based, method is that activations at interfaces between the
components, are interpretable engineering quantities. In this way, the,
hierarchical component system forms a deep neural network, (DNN) that a priori
integrates information for engineering, explainability. The, approach adapts
the model structure to engineering methods of, systems engineering and to
domain knowledge. We examine the, performance of the approach by the field of
energy-efficient, building design: First, we observed better generalization of
the, component-based method by analyzing prediction accuracy, outside the
training data. Especially for representative designs, different in structure,
we observe a much higher accuracy, (R2 = 0.94) compared to conventional
monolithic methods, (R2 = 0.71). Second, we illustrate explainability by
exemplary, demonstrating how sensitivity information from SE and rules, from
low-depth decision trees serve engineering. Third, we, evaluate explainability
by qualitative and quantitative methods, demonstrating the matching of
preliminary knowledge and data-driven, derived strategies and show correctness
of activations at, component interfaces compared to white-box simulation
results, (envelope components: R2 = 0.92..0.99; zones: R2 = 0.78..0.93).
",0
"An Investigation of the Impact of COVID-19 Non-Pharmaceutical
  Interventions and Economic Support Policies on Foreign Exchange Markets with
  Explainable AI Techniques","Siyuan Liu, Mehmet Orcun Yalcin, Hsuan Fu, Xiuyi Fan",2021-11-02T07:02:28Z,Explainable AI,"  Since the onset of the the COVID-19 pandemic, many countries across the world
have implemented various non-pharmaceutical interventions (NPIs) to contain the
spread of virus, as well as economic support policies (ESPs) to save their
economies. The pandemic and the associated NPIs have triggered unprecedented
waves of economic shocks to the financial markets, including the foreign
exchange (FX) markets. Although there are some studies exploring the impact of
the NPIs and ESPs on FX markets, the relative impact of individual NPIs or ESPs
has not been studied in a combined framework. In this work, we investigate the
relative impact of NPIs and ESPs with Explainable AI (XAI) techniques.
Experiments over exchange rate data of G10 currencies during the period from
January 1, 2020 to January 13, 2021 suggest strong impacts on exchange rate
markets by all measures of the strict lockdown, such as stay at home
requirements, workplace closing, international travel control, and restrictions
on internal movement. Yet, the impact of individual NPI and ESP can vary across
different currencies. To the best of our knowledge, this is the first work that
uses XAI techniques to study the relative impact of NPIs and ESPs on the FX
market. The derived insights can guide governments and policymakers to make
informed decisions when facing with the ongoing pandemic and a similar
situation in the near future.
",0
"Do We Need Another Explainable AI Method? Toward Unifying Post-hoc XAI
  Evaluation Methods into an Interactive and Multi-dimensional Benchmark","Mohamed Karim Belaid, Eyke Hüllermeier, Maximilian Rabus, Ralf Krestel",2022-06-08T06:13:39Z,Explainable AI,"  In recent years, Explainable AI (xAI) attracted a lot of attention as various
countries turned explanations into a legal right. xAI allows for improving
models beyond the accuracy metric by, e.g., debugging the learned pattern and
demystifying the AI's behavior. The widespread use of xAI brought new
challenges. On the one hand, the number of published xAI algorithms underwent a
boom, and it became difficult for practitioners to select the right tool. On
the other hand, some experiments did highlight how easy data scientists could
misuse xAI algorithms and misinterpret their results. To tackle the issue of
comparing and correctly using feature importance xAI algorithms, we propose
Compare-xAI, a benchmark that unifies all exclusive functional testing methods
applied to xAI algorithms. We propose a selection protocol to shortlist
non-redundant functional tests from the literature, i.e., each targeting a
specific end-user requirement in explaining a model. The benchmark encapsulates
the complexity of evaluating xAI methods into a hierarchical scoring of three
levels, namely, targeting three end-user groups: researchers, practitioners,
and laymen in xAI. The most detailed level provides one score per test. The
second level regroups tests into five categories (fidelity, fragility,
stability, simplicity, and stress tests). The last level is the aggregated
comprehensibility score, which encapsulates the ease of correctly interpreting
the algorithm's output in one easy to compare value. Compare-xAI's interactive
user interface helps mitigate errors in interpreting xAI results by quickly
listing the recommended xAI solutions for each ML task and their current
limitations. The benchmark is made available at
https://karim-53.github.io/cxai/
",0
"Trust and ethical considerations in a multi-modal, explainable AI-driven
  chatbot tutoring system: The case of collaboratively solving Rubik's Cube","Kausik Lakkaraju, Vedant Khandelwal, Biplav Srivastava, Forest Agostinelli, Hengtao Tang, Prathamjeet Singh, Dezhi Wu, Matt Irvin, Ashish Kundu",2024-01-30T16:33:21Z,Explainable AI,"  Artificial intelligence (AI) has the potential to transform education with
its power of uncovering insights from massive data about student learning
patterns. However, ethical and trustworthy concerns of AI have been raised but
are unsolved. Prominent ethical issues in high school AI education include data
privacy, information leakage, abusive language, and fairness. This paper
describes technological components that were built to address ethical and
trustworthy concerns in a multi-modal collaborative platform (called ALLURE
chatbot) for high school students to collaborate with AI to solve the Rubik's
cube. In data privacy, we want to ensure that the informed consent of children,
parents, and teachers, is at the center of any data that is managed. Since
children are involved, language, whether textual, audio, or visual, is
acceptable both from users and AI and the system can steer interaction away
from dangerous situations. In information management, we also want to ensure
that the system, while learning to improve over time, does not leak information
about users from one group to another.
",0
Ontology in quantum mechanics,Gerard t Hooft,2021-07-29T17:28:03Z,Ontology,"  It is suspected that the quantum evolution equations describing the
micro-world as we know it are of a special kind that allows transformations to
a special set of basis states in Hilbert space, such that, in this basis, the
evolution is given by elements of the permutation group. This would restore an
ontological interpretation. It is shown how, at low energies per particle
degree of freedom, almost any quantum system allows for such a transformation.
This contradicts Bell's theorem, and we emphasise why some of the assumptions
made by Bell to prove his theorem cannot hold for the models studied here. We
speculate how an approach of this kind may become helpful in isolating the most
likely version of the Standard Model, combined with General Relativity. A link
is suggested with black hole physics.
",0
Ontology-Enhanced Slot Filling,"Yuhao Ding, Yik-Cheung Tam",2021-08-25T14:54:47Z,Ontology,"  Slot filling is a fundamental task in dialog state tracking in task-oriented
dialog systems. In multi-domain task-oriented dialog system, user utterances
and system responses may mention multiple named entities and attributes values.
A system needs to select those that are confirmed by the user and fill them
into destined slots. One difficulty is that since a dialogue session contains
multiple system-user turns, feeding in all the tokens into a deep model such as
BERT can be challenging due to limited capacity of input word tokens and GPU
memory. In this paper, we investigate an ontology-enhanced approach by matching
the named entities occurred in all dialogue turns using ontology. The matched
entities in the previous dialogue turns will be accumulated and encoded as
additional inputs to a BERT-based dialogue state tracker. In addition, our
improvement includes ontology constraint checking and the correction of slot
name tokenization. Experimental results showed that our ontology-enhanced
dialogue state tracker improves the joint goal accuracy (slot F1) from 52.63%
(91.64%) to 53.91% (92%) on MultiWOZ 2.1 corpus.
",0
Merging Ontologies Algebraically,"Xiuzhan Guo, Arthur Berrill, Ajinkya Kulkarni, Kostya Belezko, Min Luo",2022-08-18T08:57:58Z,Other,"  Ontology operations, e.g., aligning and merging, were studied and implemented
extensively in different settings, such as, categorical operations, relation
algebras, typed graph grammars, with different concerns. However, aligning and
merging operations in the settings share some generic properties, e.g.,
idempotence, commutativity, associativity, and representativity, labeled by
(I), (C), (A), and (R), respectively, which are defined on an ontology merging
system $(\mathfrak{O},\sim,\merge)$, where $\mathfrak{O}$ is a set of the
ontologies concerned, $\sim$ is a binary relation on $\mathfrak{O}$ modeling
ontology aligning and $\merge$ is a partial binary operation on $\mathfrak{O}$
modeling ontology merging. Given an ontology repository, a finite set
$\mathbb{O}\subseteq \mathfrak{O}$, its merging closure $\widehat{\mathbb{O}}$
is the smallest set of ontologies, which contains the repository and is closed
with respect to merging. If (I), (C), (A), and (R) are satisfied, then both
$\mathfrak{O}$ and $\widehat{\mathbb{O}}$ are partially ordered naturally by
merging, $\widehat{\mathbb{O}}$ is finite and can be computed efficiently,
including sorting, selecting, and querying some specific elements, e.g.,
maximal ontologies and minimal ontologies. We also show that the ontology
merging system, given by ontology $V$-alignment pairs and pushouts, satisfies
the properties: (I), (C), (A), and (R) so that the merging system is partially
ordered and the merging closure of a given repository with respect to pushouts
can be computed efficiently.
",0
Ontomathedu Ontology Enrichment Method,"O. A. Nevzorova, K. S. Nikolaev",2022-12-01T08:57:18Z,Ontology,"  Nowadays, distance learning technologies have become very popular. The recent
pandemic has had a particularly strong impact on the development of distance
education technologies. Kazan Federal University has a distance learning system
based on LMS Moodle. This article describes the structure of the OntoMathEdu
ecosystem aimed at improving the process of teaching school mathematics
courses, and also provides a method for improving the OntoMathEdu ontology
structure based on identifying new connections between contextually related
concepts.
",0
Topic Ontologies for Arguments,"Yamen Ajjour, Johannes Kiesel, Benno Stein, Martin Potthast",2023-01-23T23:43:24Z,Other,"  Many computational argumentation tasks, like stance classification, are
topic-dependent: the effectiveness of approaches to these tasks significantly
depends on whether the approaches were trained on arguments from the same
topics as those they are tested on. So, which are these topics that researchers
train approaches on? This paper contributes the first comprehensive survey of
topic coverage, assessing 45 argument corpora. For the assessment, we take the
first step towards building an argument topic ontology, consulting three
diverse authoritative sources: the World Economic Forum, the Wikipedia list of
controversial topics, and Debatepedia. Comparing the topic sets between the
authoritative sources and corpora, our analysis shows that the corpora
topics-which are mostly those frequently discussed in public online fora - are
covered well by the sources. However, other topics from the sources are less
extensively covered by the corpora of today, revealing interesting future
directions for corpus construction.
",0
Disentangling Domain Ontologies,"Mayukh Bagchi, Subhashis Das",2023-03-21T08:36:14Z,Other,"  In this paper, we introduce and illustrate the novel phenomenon of Conceptual
Entanglement which emerges due to the representational manifoldness immanent
while incrementally modelling domain ontologies step-by-step across the
following five levels: perception, labelling, semantic alignment, hierarchical
modelling and intensional definition. In turn, we propose Conceptual
Disentanglement, a multi-level conceptual modelling strategy which enforces and
explicates, via guiding principles, semantic bijections with respect to each
level of conceptual entanglement (across all the above five levels) paving the
way for engineering conceptually disentangled domain ontologies. We also
briefly argue why state-of-the-art ontology development methodologies and
approaches are insufficient with respect to our characterization.
",0
The Music Note Ontology,"Andrea Poltronieri, Aldo Gangemi",2023-03-30T10:51:10Z,Ontology,"  In this paper we propose the Music Note Ontology, an ontology for modelling
music notes and their realisation. The ontology addresses the relation between
a note represented in a symbolic representation system, and its realisation,
i.e. a musical performance. This work therefore aims to solve the modelling and
representation issues that arise when analysing the relationships between
abstract symbolic features and the corresponding physical features of an audio
signal. The ontology is composed of three different Ontology Design Patterns
(ODP), which model the structure of the score (Score Part Pattern), the note in
the symbolic notation (Music Note Pattern) and its realisation (Musical Object
Pattern).
",0
"Semantics, Ontology and Explanation","Giancarlo Guizzardi, Nicola Guarino",2023-04-21T16:54:34Z,Ontology,"  The terms 'semantics' and 'ontology' are increasingly appearing together with
'explanation', not only in the scientific literature, but also in
organizational communication. However, all of these terms are also being
significantly overloaded. In this paper, we discuss their strong relation under
particular interpretations. Specifically, we discuss a notion of explanation
termed ontological unpacking, which aims at explaining symbolic domain
descriptions (conceptual models, knowledge graphs, logical specifications) by
revealing their ontological commitment in terms of their assumed truthmakers,
i.e., the entities in one's ontology that make the propositions in those
descriptions true. To illustrate this idea, we employ an ontological theory of
relations to explain (by revealing the hidden semantics of) a very simple
symbolic model encoded in the standard modeling language UML. We also discuss
the essential role played by ontology-driven conceptual models (resulting from
this form of explanation processes) in properly supporting semantic
interoperability tasks. Finally, we discuss the relation between ontological
unpacking and other forms of explanation in philosophy and science, as well as
in the area of Artificial Intelligence.
",0
Matching Weak Informative Ontologies,Peng Wang,2023-12-01T03:56:29Z,Other,"  Most existing ontology matching methods utilize the literal information to
discover alignments. However, some literal information in ontologies may be
opaque and some ontologies may not have sufficient literal information. In this
paper, these ontologies are named as weak informative ontologies (WIOs) and it
is challenging for existing methods to matching WIOs. On one hand, string-based
and linguistic-based matching methods cannot work well for WIOs. On the other
hand, some matching methods use external resources to improve their
performance, but collecting and processing external resources is still
time-consuming. To address this issue, this paper proposes a practical method
for matching WIOs by employing the ontology structure information to discover
alignments. First, the semantic subgraphs are extracted from the ontology graph
to capture the precise meanings of ontology elements. Then, a new similarity
propagation model is designed for matching WIOs. Meanwhile, in order to avoid
meaningless propagation, the similarity propagation is constrained by semantic
subgraphs and other conditions. Consequently, the similarity propagation model
ensures a balance between efficiency and quality during matching. Finally, the
similarity propagation model uses a few credible alignments as seeds to find
more alignments, and some useful strategies are adopted to improve the
performance. This matching method for WIOs has been implemented in the ontology
matching system Lily. Experimental results on public OAEI benchmark datasets
demonstrate that Lily significantly outperforms most of the state-of-the-art
works in both WIO matching tasks and general ontology matching tasks. In
particular, Lily increases the recall by a large margin, while it still obtains
high precision of matching results.
",0
Ontology Enhanced Claim Detection,"Zehra Melce Hüsünbeyi, Tatjana Scheffler",2024-02-19T16:50:58Z,Ontology,"  We propose an ontology enhanced model for sentence based claim detection. We
fused ontology embeddings from a knowledge base with BERT sentence embeddings
to perform claim detection for the ClaimBuster and the NewsClaims datasets. Our
ontology enhanced approach showed the best results with these small-sized
unbalanced datasets, compared to other statistical and neural machine learning
models. The experiments demonstrate that adding domain specific features
(either trained word embeddings or knowledge graph metadata) can improve
traditional ML methods. In addition, adding domain knowledge in the form of
ontology embeddings helps avoid the bias encountered in neural network based
models, for example the pure BERT model bias towards larger classes in our
small corpus.
",0
Commonsense Ontology Micropatterns,"Andrew Eells, Brandon Dave, Pascal Hitzler, Cogan Shimizu",2024-02-28T21:23:54Z,Ontology,"  The previously introduced Modular Ontology Modeling methodology (MOMo)
attempts to mimic the human analogical process by using modular patterns to
assemble more complex concepts. To support this, MOMo organizes organizes
ontology design patterns into design libraries, which are programmatically
queryable, to support accelerated ontology development, for both human and
automated processes. However, a major bottleneck to large-scale deployment of
MOMo is the (to-date) limited availability of ready-to-use ontology design
patterns. At the same time, Large Language Models have quickly become a source
of common knowledge and, in some cases, replacing search engines for questions.
In this paper, we thus present a collection of 104 ontology design patterns
representing often occurring nouns, curated from the common-sense knowledge
available in LLMs, organized into a fully-annotated modular ontology design
library ready for use with MOMo.
",0
Zero-shot Slot Filling with DPR and RAG,"Michael Glass, Gaetano Rossiello, Alfio Gliozzo",2021-04-17T18:24:51Z,RAG,"  The ability to automatically extract Knowledge Graphs (KG) from a given
collection of documents is a long-standing problem in Artificial Intelligence.
One way to assess this capability is through the task of slot filling. Given an
entity query in form of [Entity, Slot, ?], a system is asked to `fill' the slot
by generating or extracting the missing value from a relevant passage or
passages. This capability is crucial to create systems for automatic knowledge
base population, which is becoming in ever-increasing demand, especially in
enterprise applications. Recently, there has been a promising direction in
evaluating language models in the same way we would evaluate knowledge bases,
and the task of slot filling is the most suitable to this intent. The recent
advancements in the field try to solve this task in an end-to-end fashion using
retrieval-based language models. Models like Retrieval Augmented Generation
(RAG) show surprisingly good performance without involving complex information
extraction pipelines. However, the results achieved by these models on the two
slot filling tasks in the KILT benchmark are still not at the level required by
real-world information extraction systems. In this paper, we describe several
strategies we adopted to improve the retriever and the generator of RAG in
order to make it a better slot filler. Our KGI0 system (available at
https://github.com/IBM/retrieve-write-slot-filling) reached the top-1 position
on the KILT leaderboard on both T-REx and zsRE dataset with a large margin.
",0
GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval,"Daman Arora, Anush Kini, Sayak Ray Chowdhury, Nagarajan Natarajan, Gaurav Sinha, Amit Sharma",2023-10-31T03:52:08Z,RAG,"  Given a query and a document corpus, the information retrieval (IR) task is
to output a ranked list of relevant documents. Combining large language models
(LLMs) with embedding-based retrieval models, recent work shows promising
results on the zero-shot retrieval problem, i.e., no access to labeled data
from the target domain. Two such popular paradigms are generation-augmented
retrieval or GAR (generate additional context for the query and then retrieve),
and retrieval-augmented generation or RAG (retrieve relevant documents as
context and then generate answers). The success of these paradigms hinges on
(i) high-recall retrieval models, which are difficult to obtain in the
zero-shot setting, and (ii) high-precision (re-)ranking models which typically
need a good initialization. In this work, we propose a novel GAR-meets-RAG
recurrence formulation that overcomes the challenges of existing paradigms. Our
method iteratively improves retrieval (via GAR) and rewrite (via RAG) stages in
the zero-shot setting. A key design principle is that the rewrite-retrieval
stages improve the recall of the system and a final re-ranking stage improves
the precision. We conduct extensive experiments on zero-shot passage retrieval
benchmarks, BEIR and TREC-DL. Our method establishes a new state-of-the-art in
the BEIR benchmark, outperforming previous best results in Recall@100 and
nDCG@10 metrics on 6 out of 8 datasets, with up to 17% relative gains over the
previous best.
",0
ChatQA: Surpassing GPT-4 on Conversational QA and RAG,"Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi, Bryan Catanzaro",2024-01-18T18:59:11Z,RAG,"  In this work, we introduce ChatQA, a suite of models that outperform GPT-4 on
retrieval-augmented generation (RAG) and conversational question answering
(QA). To enhance generation, we propose a two-stage instruction tuning method
that significantly boosts the performance of RAG. For effective retrieval, we
introduce a dense retriever optimized for conversational QA, which yields
results comparable to the alternative state-of-the-art query rewriting models,
while substantially reducing deployment costs. We also present the ChatRAG
Bench, which encompasses ten datasets covering comprehensive evaluations on
RAG, table-related QA, arithmetic calculations, and scenarios involving
unanswerable questions. Our ChatQA-1.0-70B (score: 54.14), built on Llama2, a
weaker foundation model than GPT-4, can slightly outperform GPT-4-0613 (score:
53.90) and GPT-4-Turbo-2024-04-09 (score: 54.03) on the ChatRAG Bench, without
relying on any synthetic data from OpenAI GPT models. Notably, the
Llama3-ChatQA-1.5-70B model surpasses the accuracy of GPT-4-Turbo-2024-04-09,
achieving a 4.4% improvement. To advance research in this field, we
open-sourced the model weights, instruction tuning data, ChatRAG Bench, and
retriever for the community: https://chatqa-project.github.io/.
",0
The Power of Noise: Redefining Retrieval for RAG Systems,"Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, Fabrizio Silvestri",2024-01-26T14:14:59Z,RAG,"  Retrieval-Augmented Generation (RAG) has recently emerged as a method to
extend beyond the pre-trained knowledge of Large Language Models by augmenting
the original prompt with relevant passages or documents retrieved by an
Information Retrieval (IR) system. RAG has become increasingly important for
Generative AI solutions, especially in enterprise settings or in any domain in
which knowledge is constantly refreshed and cannot be memorized in the LLM. We
argue here that the retrieval component of RAG systems, be it dense or sparse,
deserves increased attention from the research community, and accordingly, we
conduct the first comprehensive and systematic examination of the retrieval
strategy of RAG systems. We focus, in particular, on the type of passages IR
systems within a RAG solution should retrieve. Our analysis considers multiple
factors, such as the relevance of the passages included in the prompt context,
their position, and their number. One counter-intuitive finding of this work is
that the retriever's highest-scoring documents that are not directly relevant
to the query (e.g., do not contain the answer) negatively impact the
effectiveness of the LLM. Even more surprising, we discovered that adding
random documents in the prompt improves the LLM accuracy by up to 35%. These
results highlight the need to investigate the appropriate strategies when
integrating retrieval with LLMs, thereby laying the groundwork for future
research in this area.
",0
HiQA: A Hierarchical Contextual Augmentation RAG for Multi-Documents QA,"Xinyue Chen, Pengyu Gao, Jiangjiang Song, Xiaoyang Tan",2024-02-01T02:24:15Z,RAG,"  Retrieval-augmented generation (RAG) has rapidly advanced the language model
field, particularly in question-answering (QA) systems. By integrating external
documents during the response generation phase, RAG significantly enhances the
accuracy and reliability of language models. This method elevates the quality
of responses and reduces the frequency of hallucinations, where the model
generates incorrect or misleading information. However, these methods exhibit
limited retrieval accuracy when faced with numerous indistinguishable
documents, presenting notable challenges in their practical application. In
response to these emerging challenges, we present HiQA, an advanced
multi-document question-answering (MDQA) framework that integrates cascading
metadata into content and a multi-route retrieval mechanism. We also release a
benchmark called MasQA to evaluate and research in MDQA. Finally, HiQA
demonstrates the state-of-the-art performance in multi-document environments.
",0
RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots,"Philip Feldman, James R. Foulds, Shimei Pan",2024-03-02T12:19:04Z,RAG,"  Large language models (LLMs) like ChatGPT demonstrate the remarkable progress
of artificial intelligence. However, their tendency to hallucinate -- generate
plausible but false information -- poses a significant challenge. This issue is
critical, as seen in recent court cases where ChatGPT's use led to citations of
non-existent legal rulings. This paper explores how Retrieval-Augmented
Generation (RAG) can counter hallucinations by integrating external knowledge
with prompts. We empirically evaluate RAG against standard LLMs using prompts
designed to induce hallucinations. Our results show that RAG increases accuracy
in some cases, but can still be misled when prompts directly contradict the
model's pre-trained understanding. These findings highlight the complex nature
of hallucinations and the need for more robust solutions to ensure LLM
reliability in real-world applications. We offer practical recommendations for
RAG deployment and discuss implications for the development of more trustworthy
LLMs.
",0
RAFT: Adapting Language Model to Domain Specific RAG,"Tianjun Zhang, Shishir G. Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, Joseph E. Gonzalez",2024-03-15T09:26:02Z,RAG,"  Pretraining Large Language Models (LLMs) on large corpora of textual data is
now a standard paradigm. When using these LLMs for many downstream
applications, it is common to additionally bake in new knowledge (e.g.,
time-critical news, or private domain knowledge) into the pretrained model
either through RAG-based-prompting, or fine-tuning. However, the optimal
methodology for the model to gain such new knowledge remains an open question.
In this paper, we present Retrieval Augmented FineTuning (RAFT), a training
recipe that improves the model's ability to answer questions in a ""open-book""
in-domain settings. In RAFT, given a question, and a set of retrieved
documents, we train the model to ignore those documents that don't help in
answering the question, which we call, distractor documents. RAFT accomplishes
this by citing verbatim the right sequence from the relevant document that
would help answer the question. This coupled with RAFT's chain-of-thought-style
response helps improve the model's ability to reason. In domain-specific RAG,
RAFT consistently improves the model's performance across PubMed, HotpotQA, and
Gorilla datasets, presenting a post-training recipe to improve pre-trained LLMs
to in-domain RAG. RAFT's code and demo are open-sourced at
github.com/ShishirPatil/gorilla.
",0
FIT-RAG: Black-Box RAG with Factual Information and Token Reduction,"Yuren Mao, Xuemei Dong, Wenyi Xu, Yunjun Gao, Bin Wei, Ying Zhang",2024-03-21T13:05:18Z,RAG,"  Due to the extraordinarily large number of parameters, fine-tuning Large
Language Models (LLMs) to update long-tail or out-of-date knowledge is
impractical in lots of applications. To avoid fine-tuning, we can alternatively
treat a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment
it with a Retrieval-Augmented Generation (RAG) system, namely black-box RAG.
Recently, black-box RAG has achieved success in knowledge-intensive tasks and
has gained much attention. Existing black-box RAG methods typically fine-tune
the retriever to cater to LLMs' preferences and concatenate all the retrieved
documents as the input, which suffers from two issues: (1) Ignorance of Factual
Information. The LLM preferred documents may not contain the factual
information for the given question, which can mislead the retriever and hurt
the effectiveness of black-box RAG; (2) Waste of Tokens. Simply concatenating
all the retrieved documents brings large amounts of unnecessary tokens for
LLMs, which degenerates the efficiency of black-box RAG. To address these
issues, this paper proposes a novel black-box RAG framework which utilizes the
factual information in the retrieval and reduces the number of tokens for
augmentation, dubbed FIT-RAG. FIT-RAG utilizes the factual information by
constructing a bi-label document scorer. Besides, it reduces the tokens by
introducing a self-knowledge recognizer and a sub-document-level token reducer.
FIT-RAG achieves both superior effectiveness and efficiency, which is validated
by extensive experiments across three open-domain question-answering datasets:
TriviaQA, NQ and PopQA. FIT-RAG can improve the answering accuracy of
Llama2-13B-Chat by 14.3\% on TriviaQA, 19.9\% on NQ and 27.5\% on PopQA,
respectively. Furthermore, it can save approximately half of the tokens on
average across the three datasets.
",0
